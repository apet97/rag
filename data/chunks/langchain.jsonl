{"id": 0, "chunk_id": "c16acf8a553ac24f5d77e7d3a1c6e493", "parent_id": 0, "url": "", "namespace": "langchain", "title": "Callbacks - Docs by LangChain", "headers": ["Callbacks - Docs by LangChain"], "section_index": 0, "chunk_index": 0, "text": "# Callbacks - Docs by LangChain\n\n> Source: https://docs.langchain.com/oss/javascript/integrations/callbacks/index\n\nSkip to main content\nLangGraph Platform is now part of\nLangSmith\n. Check out the\nChangelog\nfor more information.\nDocs by LangChain\nhome page\nLangChain + LangGraph\nSearch...\n\u2318\nK\nSearch...\nNavigation\nGeneral integrations\nCallbacks\nLangChain\nLangGraph\nIntegrations\nLearn\nReference\nContributing\nTypeScript\nOverview\nAll providers\nPopular Providers\nOpenAI\nAnthropic\nGoogle\nAWS\nMicrosoft\nGeneral integrations\nChat models\nTools and Toolkits\nLLMs\nKey-value stores\nDocument transformers\nModel caches\nCallbacks\nRAG integrations\nRetrievers\nText splitters\nEmbedding models\nVector stores\nDocument loaders\nKey-value stores\nDatadog Tracer\nView guide\nUpstash Rate Limit\nView guide\nEdit the source of this page on GitHub\nWas this page helpful?\nYes\nNo\nModel caches\nPrevious\nRetrievers\nNext\n\u2318\nI", "tokens": 119, "node_type": "child"}
{"id": 1, "chunk_id": "c16acf8a553ac24f5d77e7d3a1c6e493", "parent_id": 1, "url": "", "namespace": "langchain", "title": "Document transformers - Docs by LangChain", "headers": ["Document transformers - Docs by LangChain"], "section_index": 0, "chunk_index": 0, "text": "# Document transformers - Docs by LangChain\n\n> Source: https://docs.langchain.com/oss/javascript/integrations/document_transformers/index\n\nSkip to main content\nLangGraph Platform is now part of\nLangSmith\n. Check out the\nChangelog\nfor more information.\nDocs by LangChain\nhome page\nLangChain + LangGraph\nSearch...\n\u2318\nK\nSearch...\nNavigation\nGeneral integrations\nDocument transformers\nLangChain\nLangGraph\nIntegrations\nLearn\nReference\nContributing\nTypeScript\nOverview\nAll providers\nPopular Providers\nOpenAI\nAnthropic\nGoogle\nAWS\nMicrosoft\nGeneral integrations\nChat models\nTools and Toolkits\nLLMs\nKey-value stores\nDocument transformers\nModel caches\nCallbacks\nRAG integrations\nRetrievers\nText splitters\nEmbedding models\nVector stores\nDocument loaders\nKey-value stores\nhtml-to-text\nView guide\nmozilla/readability\nView guide\nOpenAI functions metadata tagger\nView guide\nEdit the source of this page on GitHub\nWas this page helpful?\nYes\nNo\nKey-value stores\nPrevious\nModel caches\nNext\n\u2318\nI", "tokens": 125, "node_type": "child"}
{"id": 2, "chunk_id": "c16acf8a553ac24f5d77e7d3a1c6e493", "parent_id": 2, "url": "", "namespace": "langchain", "title": "index", "headers": ["index"], "section_index": 0, "chunk_index": 0, "text": "# index\n\n> Source: https://docs.langchain.com/index\n\nDocumentation\nLangChain is the platform for agent engineering. AI teams at Replit, Clay, Rippling, Cloudflare, Workday, and more trust LangChain\u2019s products to engineer reliable agents.Our open source frameworks help you build agents:- LangChain helps you quickly get started building agents, with any model provider of your choice.\n- LangGraph allows you to control every step of your custom agent with low-level orchestration, memory, and human-in-the-loop support. You can manage long-running tasks with durable execution.\n- Observability to see exactly how your agent thinks and acts with detailed tracing and aggregate trend metrics.\n- Evaluation to test and score agent behavior on production data and offline datasets for continuous improvement.\n- Deployment to ship your agent in one click, using scalable infrastructure built for long-running tasks.\nGet started\nOpen source agent frameworks\n- Python\n- TypeScript\nLangSmith\nObservability\nSee exactly how your agent thinks and acts with detailed tracing and aggregate trend metrics.\nEvaluation\nTest and score agent behavior on production data or offline datasets to continuously improve performance.\nPrompt Engineering\nIterate on prompts with version control, prompt optimization, and collaboration features.\nDeployment\nShip your agent in one click, using scalable infrastructure built for long-running tasks.", "tokens": 200, "node_type": "child"}
{"id": 3, "chunk_id": "c16acf8a553ac24f5d77e7d3a1c6e493", "parent_id": 3, "url": "", "namespace": "langchain", "title": "Integrations - Docs by LangChain", "headers": ["Integrations - Docs by LangChain"], "section_index": 0, "chunk_index": 0, "text": "# Integrations - Docs by LangChain\n\n> Source: https://docs.langchain.com/langsmith/integrations\n\nSkip to main content\nLangGraph Platform is now part of\nLangSmith\n. Check out the\nChangelog\nfor more information.\nDocs by LangChain\nhome page\nLangSmith\nSearch...\n\u2318\nK\nSearch...\nNavigation\nIntegrations\nIntegrations\nGet started\nObservability\nEvaluation\nPrompt engineering\nDeployment\nHosting\nOverview\nQuickstart\nConcepts\nTrace a RAG application\nTracing setup\nIntegrations\nOverview\nLangChain\nLangGraph\nAnthropic (Python only)\nOpenAI\nAutoGen\nClaude Agent SDK\nClaude Code\nCrewAI\nGoogle ADK\nInstructor (Python only)\nOpenAI Agents SDK\nOpenTelemetry\nSemantic Kernel\nVercel AI SDK\nManual instrumentation\nConfiguration & troubleshooting\nProject & environment settings\nAdvanced tracing techniques\nData & privacy\nTroubleshooting guides\nViewing & managing traces\nFilter traces\nQuery traces (SDK)\nCompare traces\nShare or unshare a trace publicly\nView server logs for a trace\nBulk export trace data\nAutomations\nSet up automation rules\nConfigure webhook notifications for rules\nFeedback & evaluation\nLog user feedback using the SDK\nSet up online evaluators\nMonitoring & alerting\nMonitor projects with dashboards\nAlerts\nConfigure webhook notifications for alerts\nInsights (Beta)\nData type reference\nRun (span) data format\nFeedback data format\nTrace query syntax\nLangSmith\nprovides support for\nLangChain\nand\nLangGraph\nas well as integrations with a growing set of popular\nLLM providers\nand\nagent frameworks\n. For setup and usage, refer to the guide pages in the navigation bar.\n\u200b\nOpen source frameworks\nLangChain\nLangGraph\n\u200b\nLLM providers\nOpenAI\nAnthropic\nGoogle Gemini\nAmazon Bedrock\nDeepSeek\nMistral\n\u200b\nAgent frameworks\nAutoGen\nClaude Agent SDK\nCrewAI\nGoogle ADK\nOpenAI Agents\nOpenTelemetry\nSemantic Kernel\nVercel AI SDK\n\u200b\nOther\nInstructor\nClaude Code\nEdit the source of this page on GitHub\nWas this page helpful?\nYes\nNo\nTrace a RAG application tutorial\nPrevious\nTrace with LangChain (Python and JS/TS)\nNext\n\u2318\nI", "tokens": 287, "node_type": "child"}
{"id": 4, "chunk_id": "c16acf8a553ac24f5d77e7d3a1c6e493", "parent_id": 4, "url": "", "namespace": "langchain", "title": "langsmith-add-auth-server", "headers": ["langsmith-add-auth-server"], "section_index": 0, "chunk_index": 0, "text": "# langsmith-add-auth-server\n\n> Source: https://docs.langchain.com/langsmith/add-auth-server\n\nAuth\nobject and resource-level access control, but upgrade authentication to use Supabase as your identity provider. While Supabase is used in this tutorial, the concepts apply to any OAuth2 provider. You\u2019ll learn how to:\n- Replace test tokens with real JWT tokens\n- Integrate with OAuth2 providers for secure user authentication\n- Handle user sessions and metadata while maintaining our existing authorization logic\nBackground\nOAuth2 involves three main roles:- Authorization server: The identity provider (e.g., Supabase, Auth0, Google) that handles user authentication and issues tokens\n- Application backend: Your LangGraph application. This validates tokens and serves protected resources (conversation data)\n- Client application: The web or mobile app where users interact with your service\nPrerequisites\nBefore you start this tutorial, ensure you have:- The bot from the second tutorial running without errors.\n- A Supabase project to use its authentication server.\n1. Install dependencies\nInstall the required dependencies. Start in yourcustom-auth\ndirectory and ensure you have the langgraph-cli\ninstalled:\n2. Set up the authentication provider\nNext, fetch the URL of your auth server and the private key for authentication. Since you\u2019re using Supabase for this, you can do this in the Supabase dashboard:- In the left sidebar, click on t\ufe0f\u2699 Project Settings\u201d and then click \u201cAPI\u201d\n- Copy your project URL and add it to your\n.env\nfile\n- Copy your service role secret key and add it to your\n.env\nfile:\n- Copy your \u201canon public\u201d key and note it down. This will be used later when you set up our client code.\n3. Implement token validation\nIn the previous tutorials, you used theAuth\nobject to validate hard-coded tokens and add resource ownership.\nNow you\u2019ll upgrade your authentication to validate real JWT tokens from Supabase. The main changes will all be in the @auth.authenticate\ndecorated function:\n- Instead of checking against a hard-coded list of tokens, you\u2019ll make an HTTP request to Supabase to validate the token.\n- You\u2019ll extract real user information (ID, email) from the validated token.\n- The existing resource authorization logic remains unchanged.\nsrc/security/auth.py\nto implement this:\nsrc/security/auth.py\n4. Test authentication flow\nLet\u2019s test out the new authentication flow. You can run the following code in a file or notebook. You will need to provide:- A valid email address\n- A Supabase project URL (from above)\n- A Supabase anon public key (also from above)\n/login\nrequests until after you have confirmed your users\u2019 email.\nNow test that users can only see their own data. Make sure the server is running (run langgraph dev\n) before proceeding. The following snippet requires the \u201canon public\u201d key that you copied from the Supabase dashboard while setting up the auth provider previously.\n- Users must log in to access the bot\n- Each user can only see their own threads\nNext steps\nYou\u2019ve successfully built a production-ready authentication system for your LangGraph application! Let\u2019s review what you\u2019ve accomplished:- Set up an authentication provider (Supabase in this case)\n- Added real user accounts with email/password authentication\n- Integrated JWT token validation into your LangGraph server\n- Implemented proper authorization to ensure users can only access their own data\n- Created a foundation that\u2019s ready to handle your next authentication challenge \ud83d\ude80\n- Building a web UI with your preferred framework (see the Custom Auth template for an example)\n- Learn more about the other aspects of authentication and authorization in the conceptual guide on authentication.\n- Customize your handlers and setup further after reading the reference docs.", "tokens": 583, "node_type": "child"}
{"id": 5, "chunk_id": "c16acf8a553ac24f5d77e7d3a1c6e493", "parent_id": 5, "url": "", "namespace": "langchain", "title": "langsmith-add-human-in-the-loop", "headers": ["langsmith-add-human-in-the-loop"], "section_index": 0, "chunk_index": 0, "text": "# langsmith-add-human-in-the-loop\n\n> Source: https://docs.langchain.com/langsmith/add-human-in-the-loop\n\nDynamic interrupts\n- Python\n- JavaScript\n- cURL\n- The graph is invoked with some initial state.\n- When the graph hits the interrupt, it returns an interrupt object with the payload and metadata.\n3. The graph is resumed with a\nCommand(resume=...)\n, injecting the human\u2019s input and continuing execution.\nExtended example: using `interrupt`\nExtended example: using `interrupt`\nThis is an example graph you can run in the LangGraph API server.\nSee LangSmith quickstart for more details.\ninterrupt(...)\npauses execution athuman_node\n, surfacing the given payload to a human.- Any JSON serializable value can be passed to the\ninterrupt\nfunction. Here, a dict containing the text to revise. - Once resumed, the return value of\ninterrupt(...)\nis the human-provided input, which is used to update the state.\n- Python\n- JavaScript\n- cURL\n- The graph is invoked with some initial state.\n- When the graph hits the interrupt, it returns an interrupt object with the payload and metadata.\n3. The graph is resumed with a\nCommand(resume=...)\n, injecting the human\u2019s input and continuing execution.\nStatic interrupts\nStatic interrupts (also known as static breakpoints) are triggered either before or after a node executes.Static interrupts are not recommended for human-in-the-loop workflows. They are best used for debugging and testing.\ninterrupt_before\nand interrupt_after\nat compile time:\n- The breakpoints are set during\ncompile\ntime. interrupt_before\nspecifies the nodes where execution should pause before the node is executed.interrupt_after\nspecifies the nodes where execution should pause after the node is executed.\n- Python\n- JavaScript\n- cURL\nclient.runs.wait\nis called with theinterrupt_before\nandinterrupt_after\nparameters. This is a run-time configuration and can be changed for every invocation.interrupt_before\nspecifies the nodes where execution should pause before the node is executed.interrupt_after\nspecifies the nodes where execution should pause after the node is executed.\n- Python\n- JavaScript\n- cURL\n- The graph is run until the first breakpoint is hit.\n- The graph is resumed by passing in\nNone\nfor the input. This will run the graph until the next breakpoint is hit.\nLearn more\n- Human-in-the-loop conceptual guide: learn more about LangGraph human-in-the-loop features.\n- Common patterns: learn how to implement patterns like approving/rejecting actions, requesting user input, tool call review, and validating human input.", "tokens": 374, "node_type": "child"}
{"id": 6, "chunk_id": "c16acf8a553ac24f5d77e7d3a1c6e493", "parent_id": 6, "url": "", "namespace": "langchain", "title": "langsmith-add-metadata-tags", "headers": ["langsmith-add-metadata-tags"], "section_index": 0, "chunk_index": 0, "text": "# langsmith-add-metadata-tags\n\n> Source: https://docs.langchain.com/langsmith/add-metadata-tags\n\nLangSmith supports sending arbitrary metadata and tags along with traces.Tags are strings that can be used to categorize or label a trace. Metadata is a dictionary of key-value pairs that can be used to store additional information about a trace.Both are useful for associating additional information with a trace, such as the environment in which it was executed, the user who initiated it, or an internal correlation ID. For more information on tags and metadata, see the Concepts page. For information on how to query traces and runs by metadata and tags, see the Filter traces in the application page.\nCopy\nimport openaiimport langsmith as lsfrom langsmith.wrappers import wrap_openaiclient = openai.Client()messages = [ {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"}, {\"role\": \"user\", \"content\": \"Hello!\"}] # You can set metadata & tags **statically** when decorating a function # Use the @traceable decorator with tags and metadata # Ensure that the LANGSMITH_TRACING environment variables are set for @traceable to work @ls.traceable( run_type=\"llm\", name=\"OpenAI Call Decorator\", tags=[\"my-tag\"], metadata={\"my-key\": \"my-value\"} ) def call_openai( messages: list[dict], model: str = \"gpt-4o-mini\" ) -> str: # You can also dynamically set metadata on the parent run: rt = ls.get_current_run_tree() rt.metadata[\"some-conditional-key\"] = \"some-val\" rt.tags.extend([\"another-tag\"]) return client.chat.completions.create( model=model, messages=messages, ).choices[0].message.content call_openai( messages, # To add at **invocation time**, when calling the function. # via the langsmith_extra parameter langsmith_extra={\"tags\": [\"my-other-tag\"], \"metadata\": {\"my-other-key\": \"my-value\"}} ) # Alternatively, you can use the context manager with ls.trace( name=\"OpenAI Call Trace\", run_type=\"llm\", inputs={\"messages\": messages}, tags=[\"my-tag\"], metadata={\"my-key\": \"my-value\"}, ) as rt: chat_completion = client.chat.completions.create( model=\"gpt-4o-mini\", messages=messages, ) rt.metadata[\"some-conditional-key\"] = \"some-val\" rt.end(outputs={\"output\": chat_completion})# You can use the same techniques with the wrapped clientpatched_client = wrap_openai( client, tracing_extra={\"metadata\": {\"my-key\": \"my-value\"}, \"tags\": [\"a-tag\"]})chat_completion = patched_client.chat.completions.create( model=\"gpt-4o-mini\", messages=messages, langsmith_extra={ \"tags\": [\"my-other-tag\"], \"metadata\": {\"my-other-key\": \"my-value\"}, },)", "tokens": 292, "node_type": "child"}
{"id": 7, "chunk_id": "c16acf8a553ac24f5d77e7d3a1c6e493", "parent_id": 7, "url": "", "namespace": "langchain", "title": "langsmith-administration-overview", "headers": ["langsmith-administration-overview"], "section_index": 0, "chunk_index": 0, "text": "# langsmith-administration-overview > Source: https://docs.langchain.com/langsmith/administration-overview Resource Hierarchy Organizations An organization is a logical grouping of users within LangSmith with its own billing configuration. Typically, there is one organization per company. An organization can have multiple workspaces. For more details, see the setup guide. When you log in for the first time, a personal organization will be created for you automatically. If you\u2019d like to collaborate with others, you can create a separate organization and invite your team members to join. There are a few important differences between your personal organization and shared organizations:| Feature | Personal | Shared | |---|---|---| | Maximum workspaces | 1 | Variable, depending on plan (see pricing page | | Collaboration | Cannot invite users | Can invite users | | Billing: paid plans | Developer plan only | All other plans available | Workspaces Workspaces were formerly called Tenants. Some code and APIs may still reference the old name for a period of time during the transition. | Resource/Setting | Scope | |---|---| | Trace Projects | Workspace | | Annotation Queues | Workspace | | Deployments | Workspace | | Datasets & Experiments | Workspace | | Prompts | Workspace | | Resource Tags | Workspace | | API Keys | Workspace | | Settings including Secrets, Feedback config, Models, Rules, and Shared URLs | Workspace | | User management: Invite User to Workspace | Workspace | | RBAC: Assigning Workspace Roles | Workspace | | Data Retention, Usage Limits | Workspace* | | Plans and Billing, Credits, Invoices | Organization | | User management: Invite User to Organization | Organization** | | Adding Workspaces | Organization | | Assigning Organization Roles | Organization | | RBAC: Creating/Editing/Deleting Custom Roles | Organization | Resource tags Resource tags allow you to organize resources within a workspaces. Each tag is a key-value pair that can be assigned to a resource. Tags can be used to filter workspace-scoped resources in the UI and API: Projects, Datasets, Annotation Queues, Deployments, and Experiments. Each new workspace comes with two default tag keys:Application and Environment ; as the names suggest, these tags can be used to categorize resources based on the application and environment they belong to. More tags can be added as needed. LangSmith resource tags are very similar to tags in cloud services like AWS. User Management and RBAC Users A user is a person who has access to LangSmith. Users can be members of one or more organizations and workspaces within those organizations. Organization members are managed in organization settings: And workspace members are managed in workspace settings:API keys We ended support for legacy API keys prefixed with ls__ on October 22, 2024 in favor of personal access tokens (PATs) and service keys. We require using PATs and service keys for all new integrations. API keys prefixed with ls__ will no longer work as of October 22, 2024.Expiration Dates When you create an API key, you have the option to set an expiration date. Adding an expiration date keys enhances security and minimize the risk of unauthorized access. For example, you may set expiration dates on keys for temporary tasks that require elevated access. By default, keys never expire. Once expired, an API key is no longer valid and cannot be reactivated or have its expiration modified.Personal Access Tokens (PATs) Personal Access Tokens (PATs) are used to authenticate requests to the LangSmith API. They are created by users and scoped to a user. The PAT will have the same permissions as the user that created it. We recommend not using these to authenticate requests from your application, but rather using them for personal scripts or tools that interact with the LangSmith API. If the user associated with the PAT is removed from the organization, the PAT will no longer work. PATs are prefixed withlsv2_pt_ Service keys Service keys are similar to PATs, but are used to authenticate requests to the LangSmith API on behalf of a service account. Only admins can create service keys. We recommend using these for applications / services that need to interact with the LangSmith API, such as LangGraph agents or other integrations. Service keys may be scoped to a single workspace, multiple workspaces, or the entire organization, and can be used to authenticate requests to the LangSmith API for whichever workspace(s) it has access to. Service keys are prefixed withlsv2_sk_ Use the X-Tenant-Id header to specify the target workspace.- When using PATs: If this header is omitted, requests will run against the default workspace associated with the key. - When using organization-scoped service keys: You must include the X-Tenant-Id header when accessing workspace-scoped resources. Without it, the request will fail with a403 Forbidden error. Organization roles Organization roles are distinct from the Enterprise feature (RBAC) below and are used in the context of multiple workspaces. Your organization role determines your workspace membership characteristics and your organization-level permissions. See the organization setup guide for more information. The organization role selected also impacts workspace membership as described here:Organization Admin grants full access to manage all organization configuration, users, billing, and workspaces. AnOrganization Admin hasAdmin access to all workspaces in an organizationOrganization User may read organization information but cannot execute any write actions at the organization level. AnOrganization User can be added to a subset of workspaces and assigned workspace roles as usual (if RBAC is enabled), which specify permissions at the workspace level. The Organization User role is only available in organizations on plans with multiple workspaces. In organizations limited to a single workspace, all users are Organization Admins . Custom organization-scoped roles are not available yet.| Organization User | Organization Admin | | |---|---|---| | View organization configuration | \u2705 | \u2705 | | View organization roles | \u2705 | \u2705 | | View organization members | \u2705 | \u2705 | | View data retention settings | \u2705 | \u2705 | | View usage limits | \u2705 | \u2705 | | Admin access to all workspaces | \u2705 | | | Manage", "tokens": 1000, "node_type": "child"}
{"id": 8, "chunk_id": "9b0fd3b850698317bef57fa6e71a38ba", "parent_id": 7, "url": "", "namespace": "langchain", "title": "langsmith-administration-overview", "headers": ["langsmith-administration-overview"], "section_index": 0, "chunk_index": 1, "text": "hasAdmin access to all workspaces in an organizationOrganization User may read organization information but cannot execute any write actions at the organization level. AnOrganization User can be added to a subset of workspaces and assigned workspace roles as usual (if RBAC is enabled), which specify permissions at the workspace level. The Organization User role is only available in organizations on plans with multiple workspaces. In organizations limited to a single workspace, all users are Organization Admins . Custom organization-scoped roles are not available yet.| Organization User | Organization Admin | | |---|---|---| | View organization configuration | \u2705 | \u2705 | | View organization roles | \u2705 | \u2705 | | View organization members | \u2705 | \u2705 | | View data retention settings | \u2705 | \u2705 | | View usage limits | \u2705 | \u2705 | | Admin access to all workspaces | \u2705 | | | Manage billing settings | \u2705 | | | Create workspaces | \u2705 | | | Create, edit, and delete organization roles | \u2705 | | | Invite new users to organization | \u2705 | | | Delete user invites | \u2705 | | | Remove users from an organization | \u2705 | | | Update data retention settings* | \u2705 | | | Update usage limits* | \u2705 | Workspace roles (RBAC) RBAC (Role-Based Access Control) is a feature that is only available to Enterprise customers. If you are interested in this feature, contact our sales team. Other plans default to using the Admin role for all users. Admin - has full access to all resources within the workspaceViewer - has read-only access to all resources within the workspaceEditor - has full permissions except for workspace management (adding/removing users, changing roles, configuring service keys) Roles tab: For more details on assigning and creating roles, see the access control setup guide. Best Practices Environment Separation Use resource tags to organize resources by environment using the default tag keyEnvironment and different values for the environment (e.g. dev , staging , prod ). This tagging structure will allow you to organize your tracing projects today and easily enforce permissions when we release attribute based access control (ABAC). ABAC on the resource tag will provide a fine-grained way to restrict access to production tracing projects, for example. We do not recommend that you use Workspaces for environment separation as you cannot share resources across Workspaces. If you would like to promote a prompt from staging to prod , we recommend you use commit tags instead. See docs for more information. Usage and Billing Data Retention In May 2024, LangSmith introduced a maximum data retention period on traces of 400 days. In June 2024, LangSmith introduced a new data retention based pricing model where customers can configure a shorter data retention period on traces in exchange for savings up to 10x. On this page, we\u2019ll go through how data retention works and is priced in LangSmith.Why retention matters - Privacy: Many data privacy regulations, such as GDPR in Europe or CCPA in California, require organizations to delete personal data once it\u2019s no longer necessary for the purposes for which it was collected. Setting retention periods aids in compliance with such regulations. - Cost: LangSmith charges less for traces that have low data retention. See our tutorial on how to optimize spend for details. How it works LangSmith now has two tiers of traces based on Data Retention with the following characteristics:| Base | Extended | | |---|---|---| | Price | $.50 / 1k traces | $5 / 1k traces | | Retention Period | 14 days | 400 days | Auto upgrades can have an impact on your bill. Please read this section carefully to fully understand your estimated LangSmith tracing costs. base tier traces, their data retention will be automatically upgraded to extended tier. This will increase both the retention period, and the cost of the trace. The complete list of scenarios in which a trace will upgrade when: - Feedback is added to any run on the trace - An Annotation Queue receives any run from the trace - A Run Rule matches any run within a trace - We think that traces that match any of these conditions are fundamentally more interesting than other traces, and therefore it is good for users to be able to keep them around longer. - We philosophically want to charge customers an order of magnitude lower for traces that may not be interacted with meaningfully. We think auto-upgrades align our pricing model with the value that LangSmith brings, where only traces with meaningful interaction are charged at a higher rate. - Annotation Queues, Run Rules, and Feedback: Traces that use these features will be auto-upgraded. - Monitoring: The monitoring tab will continue to work even after a base tier trace\u2019s data retention period ends. It is powered by trace metadata that exists for >30 days, meaning that your monitoring graphs will continue to stay accurate even on base tier traces. - Datasets: Datasets have an indefinite data retention period. Restated differently, if you add a trace\u2019s inputs and outputs to a dataset, they will never be deleted. We suggest that if you are using LangSmith for data collection, you take advantage of the datasets feature. Billing model Billable metrics On your LangSmith invoice, you will see two metrics that we charge for:- LangSmith Traces (Base Charge) - LangSmith Traces (Extended Data Retention Upgrades). base tier and extended tier traces directly on the invoice? While we understand this would be more straightforward, it doesn\u2019t fit trace upgrades properly. Consider a base tier trace that was recorded on June 30, and upgraded to extended tier on July 3. The base tier trace occurred in the June billing period, but the upgrade occurred in the July billing period. Therefore, we need to be able to measure these two events independently to properly bill our customers. If your trace was recorded as an extended retention trace, then the base and extended", "tokens": 1000, "node_type": "child"}
{"id": 9, "chunk_id": "5345104ec252a41359fa4a3d2dfbec23", "parent_id": 7, "url": "", "namespace": "langchain", "title": "langsmith-administration-overview", "headers": ["langsmith-administration-overview"], "section_index": 0, "chunk_index": 2, "text": "and outputs to a dataset, they will never be deleted. We suggest that if you are using LangSmith for data collection, you take advantage of the datasets feature. Billing model Billable metrics On your LangSmith invoice, you will see two metrics that we charge for:- LangSmith Traces (Base Charge) - LangSmith Traces (Extended Data Retention Upgrades). base tier and extended tier traces directly on the invoice? While we understand this would be more straightforward, it doesn\u2019t fit trace upgrades properly. Consider a base tier trace that was recorded on June 30, and upgraded to extended tier on July 3. The base tier trace occurred in the June billing period, but the upgrade occurred in the July billing period. Therefore, we need to be able to measure these two events independently to properly bill our customers. If your trace was recorded as an extended retention trace, then the base and extended metrics will both be recorded with the same timestamp. Cost breakdown The Base Charge for a trace is .05\u00a2 per trace. We priced the upgrade such that an extended retention trace costs 10x the price of a base tier trace (.50\u00a2 per trace) including both metrics. Thus, each upgrade costs .45\u00a2. Rate Limits LangSmith has rate limits which are designed to ensure the stability of the service for all users. To ensure access and stability, LangSmith will respond with HTTP Status Code 429 indicating that rate or usage limits have been exceeded under the following circumstances:Scenarios Temporary throughput limit over a 1 minute period at our application load balancer This 429 is the the result of exceeding a fixed number of API calls over a 1 minute window on a per API key/access token basis. The start of the window will vary slightly \u2014 it is not guaranteed to start at the start of a clock minute \u2014 and may change depending on application deployment events. After the max events are received we will respond with a 429 until 60 seconds from the start of the evaluation window has been reached and then the process repeats. This 429 is thrown by our application load balancer and is a mechanism in place for all LangSmith users independent of plan tier to ensure continuity of service for all users.| Method | Endpoint | Limit | Window | |---|---|---|---| | DELETE | Sessions | 30 | 1 minute | | POST OR PATCH | Runs | 5000 | 1 minute | | POST | Feedback | 5000 | 1 minute | | * | * | 2000 | 1 minute | The LangSmith SDK takes steps to minimize the likelihood of reaching these limits on run-related endpoints by batching up to 100 runs from a single session ID into a single API call. Plan-level hourly trace event limit This 429 is the result of reaching your maximum hourly events ingested and is evaluated in a fixed window starting at the beginning of each clock hour in UTC and resets at the top of each new hour. An event in this context is the creation or update of a run. So if run is created, then subsequently updated in the same hourly window, that will count as 2 events against this limit. This is thrown by our application and varies by plan tier, with organizations on our Startup/Plus and Enterprise plan tiers having higher hourly limits than our Free and Developer Plan Tiers which are designed for personal use.| Plan | Limit | Window | |---|---|---| | Developer (no payment on file) | 50,000 events | 1 hour | | Developer (with payment on file) | 250,000 events | 1 hour | | Startup/Plus | 500,000 events | 1 hour | | Enterprise | Custom | Custom | Plan-level hourly trace data ingest limit This 429 is the result of reaching the maximum amount of data ingested across your trace inputs, outputs, and metadata and is evaluated in a fixed window starting at the beginning of each clock hour in UTC and resets at the top of each new hour. Typically, inputs, outputs, and metadata are send on both run creation and update events. So if a run is created and is 2.0MB in size at creation, and 3.0MB in size when updated in the same hourly window, that will count as 5.0MB of storage against this limit. This is thrown by our application and varies by plan tier, with organizations on our Startup/Plus and Enterprise plan tiers having higher hourly limits than our Free and Developer Plan Tiers which are designed for personal use.| Plan | Limit | Window | |---|---|---| | Developer (no payment on file) | 500MB | 1 hour | | Developer (with payment on file) | 2.5GB | 1 hour | | Startup/Plus | 5.0GB | 1 hour | | Enterprise | Custom | Custom | Plan-level monthly unique traces limit This 429 is the result of reaching your maximum monthly traces ingested and is evaluated in a fixed window starting at the beginning of each calendar month in UTC and resets at the beginning of each new month. This is thrown by our application and applies only to the Developer Plan Tier when there is no payment method on file.| Plan | Limit | Window | |---|---|---| | Developer (no payment on file) | 5,000 traces | 1 month | Self-configured monthly usage limits This 429 is the result of reaching your usage limit as configured by your organization admin and is evaluated in a fixed window starting at the beginning of each calendar month in UTC and resets at the beginning of each new month. This is thrown by our application and varies by organization based on their configured settings.Handling 429s responses in your application Since some 429 responses are temporary and may succeed on a successive call, if you are directly calling the LangSmith API in your application we recommend implementing retry logic with exponential backoff and jitter. For convenience, LangChain applications built with the LangSmith", "tokens": 1000, "node_type": "child"}
{"id": 10, "chunk_id": "0f7e5a6eec52bd2b622d96bc793bb7d8", "parent_id": 7, "url": "", "namespace": "langchain", "title": "langsmith-administration-overview", "headers": ["langsmith-administration-overview"], "section_index": 0, "chunk_index": 3, "text": "This is thrown by our application and applies only to the Developer Plan Tier when there is no payment method on file.| Plan | Limit | Window | |---|---|---| | Developer (no payment on file) | 5,000 traces | 1 month | Self-configured monthly usage limits This 429 is the result of reaching your usage limit as configured by your organization admin and is evaluated in a fixed window starting at the beginning of each calendar month in UTC and resets at the beginning of each new month. This is thrown by our application and varies by organization based on their configured settings.Handling 429s responses in your application Since some 429 responses are temporary and may succeed on a successive call, if you are directly calling the LangSmith API in your application we recommend implementing retry logic with exponential backoff and jitter. For convenience, LangChain applications built with the LangSmith SDK has this capability built-in.It is important to note that if you are saturating the endpoints for extended periods of time, retries may not be effective as your application will eventually run large enough backlogs to exhaust all retries.If that is the case, we would like to discuss your needs more specifically. Please reach out to LangSmith Support with details about your applications throughput needs and sample code and we can work with you to better understand whether the best approach is fixing a bug, changes to your application code, or a different LangSmith plan. Usage Limits LangSmith lets you configure usage limits on tracing. Note that these are usage limits, not spend limits, which mean they let you limit the quantity of occurrences of some event rather than the total amount you will spend. LangSmith lets you set two different monthly limits, mirroring our Billable Metrics discussed in the aforementioned data retention guide:- All traces limit - Extended data retention traces limit Properties of usage limiting Usage limiting is approximate, meaning that we do not guarantee the exactness of the limit. In rare cases, there may be a small period of time where additional traces are processed above the limit threshold before usage limiting begins to apply.Side effects of extended data retention traces limit The extended data retention traces limit has side effects. If the limit is already reached, any feature that could cause an auto-upgrade of tracing tiers becomes inaccessible. This is because an auto-upgrade of a trace would cause another extended retention trace to be created, which in turn should not be allowed by the limit. Therefore, you can no longer:- match run rules - add feedback to traces - add runs to annotation queues Updating usage limits Usage limits can be updated from theSettings page under Usage and Billing . Limit values are cached, so it may take a minute or two before the new limits apply. Related content - Tutorial on how to optimize spend Additional Resources - Release Versions: Learn about LangSmith\u2019s version support policy, including Active, Critical, End of Life, and Deprecated support levels.", "tokens": 501, "node_type": "child"}
{"id": 11, "chunk_id": "c16acf8a553ac24f5d77e7d3a1c6e493", "parent_id": 8, "url": "", "namespace": "langchain", "title": "langsmith-agent-auth", "headers": ["langsmith-agent-auth"], "section_index": 0, "chunk_index": 0, "text": "# langsmith-agent-auth\n\n> Source: https://docs.langchain.com/langsmith/agent-auth\n\nInstallation\nInstall the Agent Auth client library from PyPI:Quickstart\n1. Initialize the client\n2. Set up OAuth providers\nBefore agents can authenticate, you need to configure an OAuth provider using the following process:- Select a unique identifier for your OAuth provider to use in LangChain\u2019s platform (e.g., \u201cgithub-local-dev\u201d, \u201cgoogle-workspace-prod\u201d).\n- Go to your OAuth provider\u2019s developer console and create a new OAuth application.\n-\nSet LangChain\u2019s API as an available callback URL using this structure:\nFor example, if your provider_id is \u201cgithub-local-dev\u201d, use:\n-\nUse\nclient.create_oauth_provider()\nwith the credentials from your OAuth app:\n3. Authenticate from an agent\nThe clientauthenticate()\nAPI is used to get OAuth tokens from pre-configured providers. On the first call, it takes the caller through an OAuth 2.0 auth flow.\nIn LangGraph context\nBy default, tokens are scoped to the calling agent using the Assistant ID parameter.Outside LangGraph context\nProvide theauth_url\nto the user for out-of-band OAuth flows.", "tokens": 157, "node_type": "child"}
{"id": 12, "chunk_id": "c16acf8a553ac24f5d77e7d3a1c6e493", "parent_id": 9, "url": "", "namespace": "langchain", "title": "langsmith-alerts-webhook", "headers": ["langsmith-alerts-webhook"], "section_index": 0, "chunk_index": 0, "text": "# langsmith-alerts-webhook\n\n> Source: https://docs.langchain.com/langsmith/alerts-webhook\n\nSkip to main content\nOverview\nThis guide details the process for setting up webhook notifications for LangSmith alerts . Before proceeding, make sure you have followed the steps leading up to the notification step of creating the alert by following this guide . Webhooks enable integration with custom services and third-party platforms by sending HTTP POST requests when alert conditions are triggered. Use webhooks to forward alert data to ticketing systems, chat applications, or custom monitoring solutions.\nPrerequisites\nAn endpoint that can receive HTTP POST requests\nAppropriate authentication credentials for your receiving service (if required)\nIntegration Configuration\nStep 1: Prepare Your Receiving Endpoint\nBefore configuring the webhook in LangSmith, ensure your receiving endpoint:\nAccepts HTTP POST requests\nCan process JSON payloads\nIs accessible from external services\nHas appropriate authentication mechanisms (if required)\nAdditionally, if on a custom deployment of LangSmith, make sure there are no firewall settings blocking egress traffic from LangSmith services.\nIn the notification section of your alert complete the webhook configuration with the following parameters:\nRequired Fields\nURL : The complete URL of your receiving endpoint\nExample: https://api.example.com/incident-webhook\nOptional Fields\nHeaders : JSON Key-value pairs sent with the webhook request\nCommon headers include:\nAuthorization\n: For authentication tokens\nContent-Type\n: Usually set to application/json\n(default)\nX-Source\n: To identify the source as LangSmith\nIf no headers, then simply use {}\nRequest Body Template : Customize the JSON payload sent to your endpoint\nDefault: LangSmith sends the payload defined and the following additonal key-value pairs appended to the payload:\nproject_name\n: Name of the triggered alert\nalert_rule_id\n: A UUID to identify the LangSmith alert. This can be used as a de-duplication key in the webhook service.\nalert_rule_name\n: The name of the alert rule.\nalert_rule_type\n: The type of alert (as of 04/01/2025 all alerts are of type threshold\n).\nalert_rule_attribute\n: The attribute associated with the alert rule - error_count\n, feedback_score\nor latency\n.\ntriggered_metric_value\n: The value of the metric at the time the threshold was triggered.\ntriggered_threshold\n: The threshold that triggered the alert.\ntimestamp\n: The timestamp that triggered the alert.\nStep 3: Test the Webhook\nClick Send Test Alert to send the webhook notification to ensure the notification works as intended.\nTroubleshooting\nIf webhook notifications aren\u2019t being delivered:\nVerify the webhook URL is correct and accessible\nEnsure any authentication headers are properly formatted\nCheck that your receiving endpoint accepts POST requests\nExamine your endpoint\u2019s logs for received but rejected requests\nVerify your custom payload template is valid JSON format\nSecurity Considerations\nUse HTTPS for your webhook endpoints\nImplement authentication for your webhook endpoint\nConsider adding a shared secret in your headers to verify webhook sources\nValidate incoming webhook requests before processing them\nSending alerts to Slack using a webhook\nHere is an example for configuring LangSmith alerts to send notifications to Slack channels using the chat.postMessage\nAPI.\nPrerequisites\nAccess to a Slack workspace\nA LangSmith project to set up alerts\nPermissions to create Slack applications\nStep 1: Create a Slack App\nVisit the Slack API Applications page\nClick Create New App\nSelect From scratch\nProvide an App Name (e.g., \u201cLangSmith Alerts\u201d)\nSelect the workspace where you want to install the app\nClick Create App\nIn the left sidebar of your Slack app configuration, click OAuth & Permissions\nScroll down to Bot Token Scopes under Scopes and click Add an OAuth Scope\nAdd the following scopes:\nchat:write\n(Send messages as the app)\nchat:write.public\n(Send messages to channels the app isn\u2019t in)\nchannels:read\n(View basic channel information)\nStep 3: Install the App to Your Workspace\nScroll up to the top of the OAuth & Permissions page\nClick Install to Workspace\nReview the permissions and click Allow\nCopy the Bot User OAuth Token that appears (begins with xoxb-\n)\nIn LangSmith, navigate to your project\nSelect Alerts \u2192 Create Alert\nDefine your alert metrics and conditions\nIn the notification section, select Webhook\nConfigure the webhook with the following settings:\nWebhook URL\nHeaders\nNote: Replace xoxb-your-token-here\nwith your actual Bot User OAuth Token\nRequest Body Template\nNOTE: Fill in the channel_id\n, alert_name\n, project_name\nand project_url\nwhen creating the alert. You can find your project_url\nin the browser\u2019s URL bar. Copy the portion up to but not including any query parameters.\nClick Save to activate the webhook configuration\nStep 5: Test the Integration\nIn the LangSmith alert configuration, click Test Alert\nCheck your specified Slack channel for the test notification\nVerify that the message contains the expected alert information\n(Optional) Step 6: Link to the Alert Preview in the Request Body\nAfter creating an alert, you can optionally link to its preview in the webhook\u2019s request body.\nTo configure this:\nSave your alert\nFind your saved alert in the alerts table and click it\nCopy the dsiplayed URL\nClick \u201cEdit Alert\u201d\nReplace the existing project URL with the copied alert preview URL\nAdditional Resources", "tokens": 815, "node_type": "child"}
{"id": 13, "chunk_id": "c16acf8a553ac24f5d77e7d3a1c6e493", "parent_id": 10, "url": "", "namespace": "langchain", "title": "langsmith-alerts", "headers": ["langsmith-alerts"], "section_index": 0, "chunk_index": 0, "text": "# langsmith-alerts\n\n> Source: https://docs.langchain.com/langsmith/alerts\n\nSelf-hosted Version RequirementAccess to alerts requires Helm chart version 0.10.3 or later.\nOverview\nEffective observability in LLM applications requires proactive detection of failures, performance degradations, and regressions. LangSmith\u2019s alerts feature helps identify critical issues such as:- API rate limit violations from model providers\n- Latency increases for your application\n- Application changes that affect feedback scores reflecting end-user experience\nConfiguring an alert\nStep 1: Navigate To Create Alert\nFirst navigate to the Tracing project that you would like to configure alerts for. Click the Alerts icon on the top right hand corner of the page to view existing alerts for that project and set up a new alert.Step 2: Select Metric Type\nLangSmith offers threshold-based alerting on three core metrics:\n| Metric Type | Description | Use Case |\n|---|---|---|\n| Errored Runs | Track runs with an error status | Monitors for failures in an application. |\n| Feedback Score | Measures the average feedback score | Track feedback from end users or online evaluation results to alert on regressions. |\n| Latency | Measures average run execution time | Tracks the latency of your application to alert on spikes and performance bottlenecks. |\nllm\nruns tagged with support_agent\nthat encounter a RateLimitExceeded\nerror.\nStep 2: Define Alert Conditions\nAlert conditions consist of several components:- Aggregation Method: Average, Percentage, or Count\n- Comparison Operator:\n>=\n,<=\n, or exceeds threshold - Threshold Value: Numerical value triggering the alert\n- Aggregation Window: Time period for metric calculation (currently choose between 5 or 15 minutes)\n- Feedback Key (Feedback Score alerts only): Specific feedback metric to monitor\nExample: The configuration shown above would generate an alert when more than 5% of runs within the past 5 minutes result in errors.\nYou can preview alert behavior over a historical time window to understand how many datapoints\u2014and which ones\u2014would have triggered an alert at a chosen threshold (indicated in red). For example, setting an average latency threshold of 60 seconds for a project lets you visualize potential alerts, as shown in the image below.\nStep 3: Configure Notification Channel\nLangSmith supports the following notification channels: Select the appropriate channel to ensure notifications reach the responsible team members.Best Practices\n- Adjust sensitivity based on application criticality\n- Start with broader thresholds and refine based on observed patterns\n- Ensure alert routing reaches appropriate on-call personnel", "tokens": 396, "node_type": "child"}
{"id": 14, "chunk_id": "c16acf8a553ac24f5d77e7d3a1c6e493", "parent_id": 11, "url": "", "namespace": "langchain", "title": "langsmith-analyze-an-experiment", "headers": ["langsmith-analyze-an-experiment"], "section_index": 0, "chunk_index": 0, "text": "# langsmith-analyze-an-experiment\n\n> Source: https://docs.langchain.com/langsmith/analyze-an-experiment\n\n- Analyze a single experiment: View and interpret experiment results, customize columns, filter data, and compare runs.\n- Download experiment results as a CSV: Export your experiment data for external analysis and sharing.\n- Rename an experiment: Update experiment names in both the Playground and Experiments view.\nAnalyze a single experiment\nAfter running an experiment, you can use LangSmith\u2019s experiment view to analyze the results and draw insights about your experiment\u2019s performance.Open the experiment view\nTo open the experiment view, select the relevant dataset from the Dataset & Experiments page and then select the experiment you want to view.View experiment results\nCustomize columns\nBy default, the experiment view shows the input, output, and reference output for each example in the dataset, feedback scores from evaluations and experiment metrics like cost, token counts, latency and status. You can customize the columns using the Display button to make it easier to interpret experiment results:- Break out fields from inputs, outputs, and reference outputs into their own columns. This is especially helpful if you have long inputs/outputs/reference outputs and want to surface important fields.\n- Hide and reorder columns to create focused views for analysis.\n- Control decimal precision on feedback scores. By default, LangSmith surfaces numerical feedback scores with a decimal precision of 2, but you can customize this setting to be up to 6 decimals.\n- Set the Heat Map threshold to high, middle, and low for numeric feedback scores in your experiment, which affects the threshold at which score chips render as red or green:\nYou can set default configurations for an entire dataset or temporarily save settings just for yourself.\nSort and filter\nTo sort or filter feedback scores, you can use the actions in the column headers.Table views\nDepending on the view most useful for your analysis, you can change the formatting of the table by toggling between a compact view, a full, view, and a diff view.- The Compact view shows each run as a one-line row, for ease of comparing scores at a glance.\n- The Full view shows the full output for each run for digging into the details of individual runs.\n- The Diff view shows the text difference between the reference output and the output for each run.\nView the traces\nHover over any of the output cells, and click on the trace icon to view the trace for that run. This will open up a trace in the side panel. To view the entire tracing project, click on the View Project button in the top right of the header.View evaluator runs\nFor evaluator scores, you can view the source run by hovering over the evaluator score cell and clicking on the arrow icon. This will open up a trace in the side panel. If you\u2019re running a LLM-as-a-judge evaluator, you can view the prompt used for the evaluator in this run. If your experiment has repetitions, you can click on the aggregate average score to find links to all of the individual runs.Group results by metadata\nYou can add metadata to examples to categorize and organize them. For example, if you\u2019re evaluating factual accuracy on a question answering dataset, the metadata might include which subject area each question belongs to. Metadata can be added either via the UI or via the SDK. To analyze results by metadata, use the Group by dropdown in the top right corner of the experiment view and select your desired metadata key. This displays average feedback scores, latency, total tokens, and cost for each metadata group.You will only be able to group by example metadata on experiments created after February 20th, 2025. Any experiments before that date can still be grouped by metadata, but only if the metadata is on the experiment traces themselves.\nRepetitions\nIf you\u2019ve run your experiment with repetitions, there will be arrows in the output results column so you can view outputs in the table. To view each run from the repetition, hover over the output cell and click the expanded view. When you run an experiment with repetitions, LangSmith displays the average for each feedback score in the table. Click on the feedback score to view the feedback scores from individual runs, or to view the standard deviation across repetitions.Compare to another experiment\nIn the top right of the experiment view, you can select another experiment to compare to. This will open up a comparison view, where you can see how the two experiments compare. To learn more about the comparison view, see how to compare experiment results.Download experiment results as a CSV\nLangSmith lets you download experiment results as a CSV file, which allows you to analyze and share your results. To download as a CSV, click the download icon at the top of the experiment view. The icon is directly to the left of the Compact toggle.Rename an experiment\nExperiment names must be unique per workspace.\n-\nThe Playground. When running experiments in the Playground, a default name with the format\npg::prompt-name::model::uuid\n(eg.pg::gpt-4o-mini::897ee630\n) is automatically assigned. You can rename an experiment immediately after running it by editing its name in the Playground table header. - The Experiments view. When viewing results in the experiments view, you can rename an experiment by using the pencil icon beside the experiment name.", "tokens": 885, "node_type": "child"}
{"id": 15, "chunk_id": "c16acf8a553ac24f5d77e7d3a1c6e493", "parent_id": 12, "url": "", "namespace": "langchain", "title": "langsmith-annotate-code", "headers": ["langsmith-annotate-code"], "section_index": 0, "chunk_index": 0, "text": "# langsmith-annotate-code\n\n> Source: https://docs.langchain.com/langsmith/annotate-code\n\nIf you\u2019ve decided you no longer want to trace your runs, you can remove the\nLANGSMITH_TRACING\nenvironment variable. Note that this does not affect the RunTree\nobjects or API users, as these are meant to be low-level and not affected by the tracing toggle.Use @traceable\n/ traceable\nLangSmith makes it easy to log traces with minimal changes to your existing code with the @traceable\ndecorator in Python and traceable\nfunction in TypeScript.\nThe\nLANGSMITH_TRACING\nenvironment variable must be set to 'true'\nin order for traces to be logged to LangSmith, even when using @traceable\nor traceable\n. This allows you to toggle tracing on and off without changing your code.Additionally, you will need to set the LANGSMITH_API_KEY\nenvironment variable to your API key (see Setup for more information).By default, the traces will be logged to a project named default\n. To log traces to a different project, see this section.@traceable\ndecorator is a simple way to log traces from the LangSmith Python SDK. Simply decorate any function with @traceable\n.\nNote that when wrapping a sync function with traceable\n, (e.g. formatPrompt\nin the example below), you should use the await\nkeyword when calling it to\nensure the trace is logged correctly.\nUse the trace\ncontext manager (Python only)\nIn Python, you can use the trace\ncontext manager to log traces to LangSmith. This is useful in situations where:\n- You want to log traces for a specific block of code.\n- You want control over the inputs, outputs, and other attributes of the trace.\n- It is not feasible to use a decorator or wrapper.\n- Any or all of the above.\ntraceable\ndecorator and wrap_openai\nwrapper, so you can use them together in the same application.\nUse the RunTree\nAPI\nAnother, more explicit way to log traces to LangSmith is via the RunTree\nAPI. This API allows you more control over your tracing - you can manually create runs and children runs to assemble your trace. You still need to set your LANGSMITH_API_KEY\n, but LANGSMITH_TRACING\nis not necessary for this method.\nThis method is not recommended, as it\u2019s easier to make mistakes in propagating trace context.\nExample usage\nYou can extend the utilities above to conveniently trace any code. Below are some example extensions: Trace any public method in a class:Ensure all traces are submitted before exiting\nLangSmith\u2019s tracing is done in a background thread to avoid obstructing your production application. This means that your process may end before all traces are successfully posted to LangSmith. Here are some options for ensuring all traces are submitted before exiting your application.Using the LangSmith SDK\nIf you are using the LangSmith SDK standalone, you can use theflush\nmethod before exit:", "tokens": 456, "node_type": "child"}
{"id": 16, "chunk_id": "c16acf8a553ac24f5d77e7d3a1c6e493", "parent_id": 13, "url": "", "namespace": "langchain", "title": "langsmith-annotate-traces-inline", "headers": ["langsmith-annotate-traces-inline"], "section_index": 0, "chunk_index": 0, "text": "# langsmith-annotate-traces-inline\n\n> Source: https://docs.langchain.com/langsmith/annotate-traces-inline\n\nYou can attach user feedback to ANY intermediate run (span) of the trace, not just the root span.This is useful for critiquing specific parts of the LLM application, such as the retrieval step or generation step of the RAG pipeline.\nAnnotate\nin the upper right corner of trace view for any particular run that is part of the trace.\nThis will open up a pane that allows you to choose from feedback tags associated with your workspace and add a score for particular tags. You can also add a standalone comment. Follow this guide to set up feedback tags for your workspace.\nYou can also set up new feedback criteria from within the pane itself.\nYou can use the labeled keyboard shortcuts to streamline the annotation process.", "tokens": 132, "node_type": "child"}
{"id": 17, "chunk_id": "c16acf8a553ac24f5d77e7d3a1c6e493", "parent_id": 14, "url": "", "namespace": "langchain", "title": "langsmith-annotation-queues", "headers": ["langsmith-annotation-queues"], "section_index": 0, "chunk_index": 0, "text": "# langsmith-annotation-queues\n\n> Source: https://docs.langchain.com/langsmith/annotation-queues\n\nCreate an annotation queue\nTo create an annotation queue, navigate to the Annotation queues section through the homepage or left-hand navigation bar. Then click + New annotation queue in the top right corner.Basic Details\nFill in the form with the name and description of the queue. You can also assign a default dataset to queue, which will streamline the process of sending the inputs and outputs of certain runs to datasets in your LangSmith workspace.Annotation Rubric\nBegin by drafting some high-level instructions for your annotators, which will be shown in the sidebar on every run. Next, click \u201d+ Desired Feedback\u201d to add feedback keys to your annotation queue. Annotators will be presented with these feedback keys on each run. Add a description for each, as well as a short description of each category if the feedback is categorical. Reviewers will see this:Collaborator Settings\nThere are a few settings related to multiple annotators:-\nNumber of reviewers per run: This determines the number of reviewers that must mark a run as \u201cDone\u201d for it to be removed from the queue. If you check \u201cAll workspace members review each run,\u201d then a run will remain in the queue until all workspace members have marked it \u201cDone\u201d.\n- Reviewers cannot view the feedback left by other reviewers.\n- Comments on runs are visible to all reviewers.\n- Enable reservations on runs: We recommend enabling reservations. This will prevent multiple annotators from reviewing the same run at the same time.\n- How do reservations work?\n- What happens if time runs out?\nClicking \u201cRequeue at end\u201d will only move the current run to the end of the current user\u2019s queue; it won\u2019t affect the queue order of any other user. It will also release the reservation that the current user has on that run.\nAssign runs to an annotation queue\nTo assign runs to an annotation queue, either:- Click on Add to Annotation Queue in top right corner of any trace view. You can add ANY intermediate run (span) of the trace to an annotation queue, not just the root span.\n- Select multiple runs in the runs table then click Add to Annotation Queue at the bottom of the page.\n- Set up an automation rule that automatically assigns runs which pass a certain filter and sampling condition to an annotation queue.\n- Select one or multiple experiments from the dataset page and click Annotate. From the resulting popup, you may either create a new queue or add the runs to an existing one:\nIt is often a very good idea to assign runs that have a certain user feedback score (eg thumbs up, thumbs down) from the application to an annotation queue. This way, you can identify and address issues that are causing user dissatisfaction. To learn more about how to capture user feedback from your LLM application, follow this guide.", "tokens": 483, "node_type": "child"}
{"id": 18, "chunk_id": "c16acf8a553ac24f5d77e7d3a1c6e493", "parent_id": 15, "url": "", "namespace": "langchain", "title": "langsmith-api-ref-control-plane", "headers": ["langsmith-api-ref-control-plane"], "section_index": 0, "chunk_index": 0, "text": "# langsmith-api-ref-control-plane\n\n> Source: https://docs.langchain.com/langsmith/api-ref-control-plane\n\nHost\nThe control plane hosts for Cloud data regions:| US | EU |\n|---|---|\nhttps://api.host.langchain.com | https://eu.api.host.langchain.com |\n/api-host\n. For example, http(s)://<host>/api-host/v2/deployments\n. See here for more details.\nAuthentication\nTo authenticate with the control plane API, set theX-Api-Key\nheader to a valid LangSmith API key.\nExample curl\ncommand:\nVersioning\nEach endpoint path is prefixed with a version (e.g.v1\n, v2\n).\nQuick Start\n- Call\nPOST /v2/deployments\nto create a new Deployment. The response body contains the Deployment ID (id\n) and the ID of the latest (and first) revision (latest_revision_id\n). - Call\nGET /v2/deployments/{deployment_id}\nto retrieve the Deployment. Setdeployment_id\nin the URL to the value of Deployment ID (id\n). - Poll for revision\nstatus\nuntilstatus\nisDEPLOYED\nby callingGET /v2/deployments/{deployment_id}/revisions/{latest_revision_id}\n. - Call\nPATCH /v2/deployments/{deployment_id}\nto update the deployment.", "tokens": 137, "node_type": "child"}
{"id": 19, "chunk_id": "c16acf8a553ac24f5d77e7d3a1c6e493", "parent_id": 17, "url": "", "namespace": "langchain", "title": "langsmith-application-structure", "headers": ["langsmith-application-structure"], "section_index": 0, "chunk_index": 0, "text": "# langsmith-application-structure\n\n> Source: https://docs.langchain.com/langsmith/application-structure\n\nlanggraph.json\n), a file that specifies dependencies, and an optional .env\nfile that specifies environment variables.\nThis page explains how a LangSmith application is organized and how to provide the configuration details required for deployment.\nKey Concepts\nTo deploy using LangSmith, provide the following information:- A configuration file (\nlanggraph.json\n) that specifies the dependencies, graphs, and environment variables to use for the application. - The graphs that implement the logic of the application.\n- A file that specifies dependencies required to run the application.\n- Environment variables that are required for the application to run.\nFramework agnosticLangSmith Deployment supports deploying a LangGraph graph. However, the implementation of a node of a graph can contain arbitrary Python code. This means any framework can be implemented within a node and deployed on LangSmith Deployment. This lets you keep your core application logic outside LangGraph while still using LangSmith for deployment, scaling, and observability.\nFile Structure\nThe following are examples of directory structures for Python and JavaScript applications:- Python (requirements.txt)\n- Python (pyproject.toml)\n- JS (package.json)\nThe directory structure of an application can vary depending on the programming language and the package manager used.\nConfiguration File\nThelanggraph.json\nfile is a JSON file that specifies the dependencies, graphs, environment variables, and other settings required to deploy an application.\nFor details on all supported keys in the JSON file, refer to the LangGraph configuration file reference.\nExamples\n- Python\n- JavaScript\n- The dependencies involve a custom local package and the\nlangchain_openai\npackage. - A single graph will be loaded from the file\n./your_package/your_file.py\nwith the variablevariable\n. - The environment variables are loaded from the\n.env\nfile.\nDependencies\nAn application may depend on other Python packages or JavaScript libraries (depending on the programming language in which the application is written). You will generally need to specify the following information for dependencies to be set up correctly:- A file in the directory that specifies the dependencies (e.g.,\nrequirements.txt\n,pyproject.toml\n, orpackage.json\n). - A\ndependencies\nkey in the configuration file that specifies the dependencies required to run the application. - Any additional binaries or system libraries can be specified using\ndockerfile_lines\nkey in the LangGraph configuration file.\nGraphs\nUse thegraphs\nkey in the configuration file to specify which graphs will be available in the deployed application.\nYou can specify one or more graphs in the configuration file. Each graph is identified by a unique name and a path to either (1) a compiled graph or (2) a function that defines a graph.\nEnvironment Variables\nIf you\u2019re working with a deployed LangGraph application locally, you can configure environment variables in theenv\nkey of the configuration file.\nFor a production deployment, you will typically want to configure the environment variables in the deployment environment.", "tokens": 463, "node_type": "child"}
{"id": 20, "chunk_id": "c16acf8a553ac24f5d77e7d3a1c6e493", "parent_id": 18, "url": "", "namespace": "langchain", "title": "langsmith-assistants", "headers": ["langsmith-assistants"], "section_index": 0, "chunk_index": 0, "text": "# langsmith-assistants\n\n> Source: https://docs.langchain.com/langsmith/assistants\n\nAssistants allow you to manage configurations (like prompts, LLM selection, tools) separately from your graph\u2019s core logic, enabling rapid changes that don\u2019t alter the graph architecture. It is a way to create multiple specialized versions of the same graph architecture, each optimized for different use cases through configuration variations rather than structural changes.For example, imagine a general-purpose writing agent built on a common graph architecture. While the structure remains the same, different writing styles\u2014such as blog posts and tweets\u2014require tailored configurations to optimize performance. To support these variations, you can create multiple assistants (e.g., one for blogs and another for tweets) that share the underlying graph but differ in model selection and system prompt.The LangGraph API provides several endpoints for creating and managing assistants and their versions. See the API reference for more details.\nAssistants are a LangSmith concept. They are not available in the open source LangGraph library.\nAssistants build on the LangGraph open source concept of configuration.While configuration is available in the open source LangGraph library, assistants are only present in LangSmith. This is due to the fact that assistants are tightly coupled to your deployed graph. Upon deployment, LangGraph Server will automatically create a default assistant for each graph using the graph\u2019s default configuration settings.In practice, an assistant is just an instance of a graph with a specific configuration. Therefore, multiple assistants can reference the same graph but can contain different configurations (e.g. prompts, models, tools). The LangGraph Server API provides several endpoints for creating and managing assistants. See the API reference and this how-to for more details on how to create assistants.\nAssistants support versioning to track changes over time.\nOnce you\u2019ve created an assistant, subsequent edits to that assistant will create new versions. See this how-to for more details on how to manage assistant versions.\nA run is an invocation of an assistant. Each run may have its own input, configuration, and metadata, which may affect execution and output of the underlying graph. A run can optionally be executed on a thread.LangSmith API provides several endpoints for creating and managing runs. See the API reference for more details.", "tokens": 358, "node_type": "child"}
{"id": 21, "chunk_id": "c16acf8a553ac24f5d77e7d3a1c6e493", "parent_id": 19, "url": "", "namespace": "langchain", "title": "langsmith-attach-user-feedback", "headers": ["langsmith-attach-user-feedback"], "section_index": 0, "chunk_index": 0, "text": "# langsmith-attach-user-feedback\n\n> Source: https://docs.langchain.com/langsmith/attach-user-feedback\n\nChild runs\nYou can attach user feedback to ANY child run of a trace, not just the trace (root run) itself.\nThis is useful for critiquing specific steps of the LLM application, such as the retrieval step or generation step of a RAG pipeline.\nNon-blocking creation (Python only)\nThe Python client will automatically background feedback creation if you pass\ntrace_id=\nto create_feedback().\nThis is essential for low-latency environments, where you want to make sure your application isn\u2019t blocked on feedback creation.create_feedback() / createFeedback()\n. See this guide for how to get the run ID of an in-progress run.\nTo learn more about how to filter traces based on various attributes, including user feedback, see this guide.", "tokens": 121, "node_type": "child"}
{"id": 22, "chunk_id": "c16acf8a553ac24f5d77e7d3a1c6e493", "parent_id": 20, "url": "", "namespace": "langchain", "title": "langsmith-audit-evaluator-scores", "headers": ["langsmith-audit-evaluator-scores"], "section_index": 0, "chunk_index": 0, "text": "# langsmith-audit-evaluator-scores\n\n> Source: https://docs.langchain.com/langsmith/audit-evaluator-scores\n\nLLM-as-a-judge evaluators don\u2019t always get it right. Because of this, it is often useful for a human to manually audit the scores left by an evaluator and correct them where necessary. LangSmith allows you to make corrections on evaluator scores in the UI or SDK.\nIn the comparison view, you may click on any feedback tag to bring up the feedback details. From there, click the \u201cedit\u201d icon on the right to bring up the corrections view. You may then type in your desired score in the text box under \u201cMake correction\u201d. If you would like, you may also attach an explanation to your correction. This is useful if you are using a few-shot evaluator and will be automatically inserted into your few-shot examples in place of the few_shot_explanation prompt variable.\nIn the runs table, find the \u201cFeedback\u201d column and click on the feedback tag to bring up the feedback details. Again, click the \u201cedit\u201d icon on the right to bring up the corrections view.\nCorrections can be made via the SDK\u2019s update_feedback function, with the correction dict. You must specify a score key which corresponds to a number for it to be rendered in the UI.", "tokens": 202, "node_type": "child"}
{"id": 23, "chunk_id": "c16acf8a553ac24f5d77e7d3a1c6e493", "parent_id": 21, "url": "", "namespace": "langchain", "title": "langsmith-auth", "headers": ["langsmith-auth"], "section_index": 0, "chunk_index": 0, "text": "# langsmith-auth > Source: https://docs.langchain.com/langsmith/auth Core Concepts Authentication vs Authorization While often used interchangeably, these terms represent distinct security concepts:- Authentication (\u201cAuthN\u201d) verifies who you are. This runs as middleware for every request. - Authorization (\u201cAuthZ\u201d) determines what you can do. This validates the user\u2019s privileges and roles on a per-resource basis. @auth.authenticate handler, and authorization is handled by your @auth.on handlers. Default Security Models LangSmith provides different security defaults:LangSmith - Uses LangSmith API keys by default - Requires valid API key in x-api-key header - Can be customized with your auth handler Custom auth Custom auth is supported for all plans in LangSmith. Self-Hosted - No default authentication - Complete flexibility to implement your security model - You control all aspects of authentication and authorization System Architecture A typical authentication setup involves three main components:- Authentication Provider (Identity Provider/IdP) - A dedicated service that manages user identities and credentials - Handles user registration, login, password resets, etc. - Issues tokens (JWT, session tokens, etc.) after successful authentication - Examples: Auth0, Supabase Auth, Okta, or your own auth server - LangGraph Backend (Resource Server) - Your LangGraph application that contains business logic and protected resources - Validates tokens with the auth provider - Enforces access control based on user identity and permissions - Doesn\u2019t store user credentials directly - Client Application (Frontend) - Web app, mobile app, or API client - Collects time-sensitive user credentials and sends to auth provider - Receives tokens from auth provider - Includes these tokens in requests to LangGraph backend @auth.authenticate handler in LangGraph handles steps 4-6, while your @auth.on handlers implement step 7. Authentication Authentication in LangGraph runs as middleware on every request. Your@auth.authenticate handler receives request information and should: - Validate the credentials - Return user info containing the user\u2019s identity and user information if valid - Raise an HTTP exception or AssertionError if invalid - To your authorization handlers via ctx.user - In your application via config[\"configuration\"][\"langgraph_auth_user\"] Supported Parameters Supported Parameters The @auth.authenticate handler can accept any of the following parameters by name:- request (Request): The raw ASGI request object - body (dict): The parsed request body - path (str): The request path, e.g., \"/threads/abcd-1234-abcd-1234/runs/abcd-1234-abcd-1234/stream\" - method (str): The HTTP method, e.g., \"GET\" - path_params (dict[str, str]): URL path parameters, e.g., {\"thread_id\": \"abcd-1234-abcd-1234\", \"run_id\": \"abcd-1234-abcd-1234\"} - query_params (dict[str, str]): URL query parameters, e.g., {\"stream\": \"true\"} - headers (dict[bytes, bytes]): Request headers - authorization (str | None): The Authorization header value (e.g., \"Bearer <token>\" ) Agent authentication Custom authentication permits delegated access. The values you return in@auth.authenticate are added to the run context, giving agents user-scoped credentials lets them access resources on the user\u2019s behalf. After authentication, the platform creates a special configuration object that is passed to your graph and all nodes via the configurable context. This object contains information about the current user, including any custom fields you return from your @auth.authenticate handler. To enable an agent to act on behalf of the user, use custom authentication middleware. This will allow the agent to interact with external systems like MCP servers, external databases, and even other agents on behalf of the user. For more information, see the Use custom auth guide. Agent authentication with MCP For information on how to authenticate an agent to an MCP server, see the MCP conceptual guide.Authorization After authentication, LangGraph calls your@auth.on handlers to control access to specific resources (e.g., threads, assistants, crons). These handlers can: - Add metadata to be saved during resource creation by mutating the value[\"metadata\"] dictionary directly. See the supported actions table for the list of types the value can take for each action. - Filter resources by metadata during search/list or read operations by returning a filter dictionary. - Raise an HTTP exception if access is denied. @auth.on handler for all resources and actions. If you want to have different control depending on the resource and action, you can use resource-specific handlers. See the Supported Resources section for a full list of the resources that support access control. Resource-Specific Handlers You can register handlers for specific resources and actions by chaining the resource and action names together with the@auth.on decorator. When a request is made, the most specific handler that matches that resource and action is called. Below is an example of how to register handlers for specific resources and actions. For the following setup: - Authenticated users are able to create threads, read threads, and create runs on threads - Only users with the \u201cassistants:create\u201d permission are allowed to create new assistants - All other endpoints (e.g., e.g., delete assistant, crons, store) are disabled for all users. thread would match the on_thread_create handler but NOT the reject_unhandled_requests handler. A request to update a thread, however would be handled by the global handler, since we don\u2019t have a more specific handler for that resource and action. Filter Operations Authorization handlers can returnNone , a boolean, or a filter dictionary. None andTrue mean \u201cauthorize access to all underling resources\u201dFalse means \u201cdeny access to all underling resources (raises a 403 exception)\u201d- A metadata filter dictionary will restrict access to resources - The default value is a shorthand for exact match, or \u201c$eq\u201d, below. For example, {\"owner\": user_id} will include only resources with metadata containing{\"owner\": user_id} $eq : Exact match (e.g.,{\"owner\": {\"$eq\": user_id}} ) - this is equivalent to the shorthand above,{\"owner\": user_id} $contains : List membership (e.g.,{\"allowed_users\": {\"$contains\": user_id}} ) or list containment (e.g.,{\"allowed_users\": {\"$contains\": [user_id_1, user_id_2]}} ). The value here must be an element of the list or a subset of the elements of the list, respectively. The metadata in the stored resource must be a list/container type. AND filter. For example, {\"owner\": org_id, \"allowed_users\": {\"$contains\": user_id}} will only match resources with metadata whose \u201cowner\u201d is org_id and whose \u201callowed_users\u201d list contains user_id . See the reference here for more information. Common Access Patterns Here are some typical authorization patterns:Single-Owner Resources This common pattern lets you scope all threads, assistants, crons, and runs to a single user. It\u2019s useful for common single-user", "tokens": 1000, "node_type": "child"}
{"id": 24, "chunk_id": "9b0fd3b850698317bef57fa6e71a38ba", "parent_id": 21, "url": "", "namespace": "langchain", "title": "langsmith-auth", "headers": ["langsmith-auth"], "section_index": 0, "chunk_index": 1, "text": "value is a shorthand for exact match, or \u201c$eq\u201d, below. For example, {\"owner\": user_id} will include only resources with metadata containing{\"owner\": user_id} $eq : Exact match (e.g.,{\"owner\": {\"$eq\": user_id}} ) - this is equivalent to the shorthand above,{\"owner\": user_id} $contains : List membership (e.g.,{\"allowed_users\": {\"$contains\": user_id}} ) or list containment (e.g.,{\"allowed_users\": {\"$contains\": [user_id_1, user_id_2]}} ). The value here must be an element of the list or a subset of the elements of the list, respectively. The metadata in the stored resource must be a list/container type. AND filter. For example, {\"owner\": org_id, \"allowed_users\": {\"$contains\": user_id}} will only match resources with metadata whose \u201cowner\u201d is org_id and whose \u201callowed_users\u201d list contains user_id . See the reference here for more information. Common Access Patterns Here are some typical authorization patterns:Single-Owner Resources This common pattern lets you scope all threads, assistants, crons, and runs to a single user. It\u2019s useful for common single-user use cases like regular chatbot-style apps.Permission-based Access This pattern lets you control access based on permissions. It\u2019s useful if you want certain roles to have broader or more restricted access to resources.Supported Resources LangGraph provides three levels of authorization handlers, from most general to most specific:- Global Handler ( @auth.on ): Matches all resources and actions - Resource Handler (e.g., @auth.on.threads ,@auth.on.assistants ,@auth.on.crons ): Matches all actions for a specific resource - Action Handler (e.g., @auth.on.threads.create ,@auth.on.threads.read ): Matches a specific action on a specific resource @auth.on.threads.create takes precedence over @auth.on.threads for thread creation. If a more specific handler is registered, the more general handler will not be called for that resource and action. \u201cType Safety\u201d Each handler has type hints available for its More specific handlers provide better type hints since they handle fewer action types. value parameter at Auth.types.on.<resource>.<action>.value . For example:Supported actions and types Here are all the supported action handlers:| Resource | Handler | Description | Value Type | |---|---|---|---| | Threads | @auth.on.threads.create | Thread creation | ThreadsCreate | @auth.on.threads.read | Thread retrieval | ThreadsRead | | @auth.on.threads.update | Thread updates | ThreadsUpdate | | @auth.on.threads.delete | Thread deletion | ThreadsDelete | | @auth.on.threads.search | Listing threads | ThreadsSearch | | @auth.on.threads.create_run | Creating or updating a run | RunsCreate | | | Assistants | @auth.on.assistants.create | Assistant creation | AssistantsCreate | @auth.on.assistants.read | Assistant retrieval | AssistantsRead | | @auth.on.assistants.update | Assistant updates | AssistantsUpdate | | @auth.on.assistants.delete | Assistant deletion | AssistantsDelete | | @auth.on.assistants.search | Listing assistants | AssistantsSearch | | | Crons | @auth.on.crons.create | Cron job creation | CronsCreate | @auth.on.crons.read | Cron job retrieval | CronsRead | | @auth.on.crons.update | Cron job updates | CronsUpdate | | @auth.on.crons.delete | Cron job deletion | CronsDelete | | @auth.on.crons.search | Listing cron jobs | CronsSearch | \u201cAbout Runs\u201dRuns are scoped to their parent thread for access control. This means permissions are typically inherited from the thread, reflecting the conversational nature of the data model. All run operations (reading, listing) except creation are controlled by the thread\u2019s handlers. There is a specific create_run handler for creating new runs because it had more arguments that you can view in the handler.Next Steps For implementation details:- Check out the introductory tutorial on setting up authentication - See the how-to guide on implementing a custom auth handlers", "tokens": 542, "node_type": "child"}
{"id": 25, "chunk_id": "c16acf8a553ac24f5d77e7d3a1c6e493", "parent_id": 22, "url": "", "namespace": "langchain", "title": "langsmith-authentication-methods", "headers": ["langsmith-authentication-methods"], "section_index": 0, "chunk_index": 0, "text": "# langsmith-authentication-methods\n\n> Source: https://docs.langchain.com/langsmith/authentication-methods\n\nCloud\nEmail/Password\nUsers can use an email address and password to sign up and login to LangSmith.Social Providers\nUsers can alternatively use their credentials from GitHub or Google.SAML SSO\nEnterprise customers can configure SAML SSO and SCIMSelf-Hosted\nSelf-hosted customers have more control over how their users can login to LangSmith. For more in-depth coverage of configuration options, see the self-hosting docs and Helm chart.SSO with OAuth 2.0 and OIDC\nProduction installations should configure SSO in order to use an external identity provider. This enables users to login through an identity platform like Auth0/Okta. LangSmith supports almost any OIDC-compliant provider. Learn more about configuring SSO in the SSO configuration guideEmail/Password a.k.a. basic auth\nThis auth method requires very little configuration as it does not require an external identity provider. It is most appropriate to use for self-hosted trials. Learn more in the basic auth configuration guideNone\nThis authentication mode will be removed after the launch of Basic Auth.", "tokens": 162, "node_type": "child"}
{"id": 26, "chunk_id": "c16acf8a553ac24f5d77e7d3a1c6e493", "parent_id": 23, "url": "", "namespace": "langchain", "title": "langsmith-autogen-integration", "headers": ["langsmith-autogen-integration"], "section_index": 0, "chunk_index": 0, "text": "# langsmith-autogen-integration\n\n> Source: https://docs.langchain.com/langsmith/autogen-integration\n\nHow to integrate LangGraph with AutoGen, CrewAI, and other frameworks\nThis guide shows how to integrate AutoGen agents with LangGraph to leverage features like persistence, streaming, and memory, and then deploy the integrated solution to LangSmith for scalable production use. In this guide we show how to build a LangGraph chatbot that integrates with AutoGen, but you can follow the same approach with other frameworks.Integrating AutoGen with LangGraph provides several benefits:\nCreate an AutoGen agent that can execute code. This example is adapted from AutoGen\u2019s official tutorials:\nCopy\nimport autogenimport osconfig_list = [{\"model\": \"gpt-4o\", \"api_key\": os.environ[\"OPENAI_API_KEY\"]}]llm_config = { \"timeout\": 600, \"cache_seed\": 42, \"config_list\": config_list, \"temperature\": 0,}autogen_agent = autogen.AssistantAgent( name=\"assistant\", llm_config=llm_config,)user_proxy = autogen.UserProxyAgent( name=\"user_proxy\", human_input_mode=\"NEVER\", max_consecutive_auto_reply=10, is_termination_msg=lambda x: x.get(\"content\", \"\").rstrip().endswith(\"TERMINATE\"), code_execution_config={ \"work_dir\": \"web\", \"use_docker\": False, }, # Please set use_docker=True if docker is available to run the generated code. Using docker is safer than running the generated code directly. llm_config=llm_config, system_message=\"Reply TERMINATE if the task has been solved at full satisfaction. Otherwise, reply CONTINUE, or the reason why the task is not solved yet.\",)\nWe will now create a LangGraph chatbot graph that calls AutoGen agent.\nCopy\nfrom langchain_core.messages import convert_to_openai_messagesfrom langgraph.graph import StateGraph, MessagesState, STARTfrom langgraph.checkpoint.memory import MemorySaverdef call_autogen_agent(state: MessagesState): # Convert LangGraph messages to OpenAI format for AutoGen messages = convert_to_openai_messages(state[\"messages\"]) # Get the last user message last_message = messages[-1] # Pass previous message history as context (excluding the last message) carryover = messages[:-1] if len(messages) > 1 else [] # Initiate chat with AutoGen response = user_proxy.initiate_chat( autogen_agent, message=last_message, carryover=carryover ) # Extract the final response from the agent final_content = response.chat_history[-1][\"content\"] # Return the response in LangGraph format return {\"messages\": {\"role\": \"assistant\", \"content\": final_content}}# Create the graph with memory for persistencecheckpointer = MemorySaver()# Build the graphbuilder = StateGraph(MessagesState)builder.add_node(\"autogen\", call_autogen_agent)builder.add_edge(START, \"autogen\")# Compile with checkpointer for persistencegraph = builder.compile(checkpointer=checkpointer)\nCopy\nfrom IPython.display import display, Imagedisplay(Image(graph.get_graph().draw_mermaid_png()))\nBefore deploying to LangSmith, you can test the graph locally:\nCopy\n# pass the thread ID to persist agent outputs for future interactionsconfig = {\"configurable\": {\"thread_id\": \"1\"}}for chunk in graph.stream( { \"messages\": [ { \"role\": \"user\", \"content\": \"Find numbers between 10 and 30 in fibonacci sequence\", } ] }, config,): print(chunk)\nOutput:\nCopy\nuser_proxy (to assistant):Find numbers between 10 and 30 in fibonacci sequence--------------------------------------------------------------------------------assistant (to user_proxy):To find numbers between 10 and 30 in the Fibonacci sequence, we can generate the Fibonacci sequence and check which numbers fall within this range. Here's a plan:1. Generate Fibonacci numbers starting from 0.2. Continue generating until the numbers exceed 30.3. Collect and print the numbers that are between 10 and 30....\nSince we\u2019re leveraging LangGraph\u2019s persistence features we can now continue the conversation using the same thread ID \u2014 LangGraph will automatically pass previous history to the AutoGen agent:\nCopy\nfor chunk in graph.stream( { \"messages\": [ { \"role\": \"user\", \"content\": \"Multiply the last number by 3\", } ] }, config,): print(chunk)\nOutput:\nCopy\nuser_proxy (to assistant):Multiply the last number by 3Context:Find numbers between 10 and 30 in fibonacci sequenceThe Fibonacci numbers between 10 and 30 are 13 and 21.These numbers are part of the Fibonacci sequence, which is generated by adding the two preceding numbers to get the next number, starting from 0 and 1.The sequence goes: 0, 1, 1, 2, 3, 5, 8, 13, 21, 34, ...As you can see, 13 and 21 are the only numbers in this sequence that fall between 10 and 30.TERMINATE--------------------------------------------------------------------------------assistant (to user_proxy):The last number in the Fibonacci sequence between 10 and 30 is 21. Multiplying 21 by 3 gives:21 * 3 = 63TERMINATE--------------------------------------------------------------------------------{'call_autogen_agent': {'messages': {'role': 'assistant', 'content': 'The last number in the Fibonacci sequence between 10 and 30 is 21. Multiplying 21 by 3 gives:\\n\\n21 * 3 = 63\\n\\nTERMINATE'}}}", "tokens": 614, "node_type": "child"}
{"id": 27, "chunk_id": "c16acf8a553ac24f5d77e7d3a1c6e493", "parent_id": 24, "url": "", "namespace": "langchain", "title": "langsmith-background-run", "headers": ["langsmith-background-run"], "section_index": 0, "chunk_index": 0, "text": "# langsmith-background-run > Source: https://docs.langchain.com/langsmith/background-run Setup First let\u2019s set up our client and thread:- Python - Javascript - CURL Copy from langgraph_sdk import get_client client = get_client(url=<DEPLOYMENT_URL>) # Using the graph deployed with the name \"agent\" assistant_id = \"agent\" # create thread thread = await client.threads.create() print(thread) Copy { 'thread_id': '5cb1e8a1-34b3-4a61-a34e-71a9799bd00d', 'created_at': '2024-08-30T20:35:52.062934+00:00', 'updated_at': '2024-08-30T20:35:52.062934+00:00', 'metadata': {}, 'status': 'idle', 'config': {}, 'values': None } Check runs on thread If we list the current runs on this thread, we will see that it\u2019s empty:- Python - Javascript - CURL Copy runs = await client.runs.list(thread[\"thread_id\"]) print(runs) Copy [] Start runs on thread Now let\u2019s kick off a run:- Python - Javascript - CURL Copy input = {\"messages\": [{\"role\": \"user\", \"content\": \"what's the weather in sf\"}]} run = await client.runs.create(thread[\"thread_id\"], assistant_id, input=input) status=pending : - Python - Javascript - CURL Copy print(await client.runs.get(thread[\"thread_id\"], run[\"run_id\"])) Copy { \"run_id\": \"1ef6a5f8-bd86-6763-bbd6-bff042db7b1b\", \"thread_id\": \"7885f0cf-94ad-4040-91d7-73f7ba007c8a\", \"assistant_id\": \"fe096781-5601-53d2-b2f6-0d3403f7e9ca\", \"created_at\": \"2024-09-04T01:46:47.244887+00:00\", \"updated_at\": \"2024-09-04T01:46:47.244887+00:00\", \"metadata\": {}, \"status\": \"pending\", \"kwargs\": { \"input\": { \"messages\": [ { \"role\": \"user\", \"content\": \"what's the weather in sf\" } ] }, \"config\": { \"metadata\": { \"created_by\": \"system\" }, \"configurable\": { \"run_id\": \"1ef6a5f8-bd86-6763-bbd6-bff042db7b1b\", \"user_id\": \"\", \"graph_id\": \"agent\", \"thread_id\": \"7885f0cf-94ad-4040-91d7-73f7ba007c8a\", \"assistant_id\": \"fe096781-5601-53d2-b2f6-0d3403f7e9ca\", \"checkpoint_id\": null } }, \"webhook\": null, \"temporary\": false, \"stream_mode\": [ \"values\" ], \"feedback_keys\": null, \"interrupt_after\": null, \"interrupt_before\": null }, \"multitask_strategy\": \"reject\" } - Python - Javascript - CURL Copy await client.runs.join(thread[\"thread_id\"], run[\"run_id\"]) print(await client.runs.get(thread[\"thread_id\"], run[\"run_id\"])) Copy { \"run_id\": \"1ef6a5f8-bd86-6763-bbd6-bff042db7b1b\", \"thread_id\": \"7885f0cf-94ad-4040-91d7-73f7ba007c8a\", \"assistant_id\": \"fe096781-5601-53d2-b2f6-0d3403f7e9ca\", \"created_at\": \"2024-09-04T01:46:47.244887+00:00\", \"updated_at\": \"2024-09-04T01:46:47.244887+00:00\", \"metadata\": {}, \"status\": \"success\", \"kwargs\": { \"input\": { \"messages\": [ { \"role\": \"user\", \"content\": \"what's the weather in sf\" } ] }, \"config\": { \"metadata\": { \"created_by\": \"system\" }, \"configurable\": { \"run_id\": \"1ef6a5f8-bd86-6763-bbd6-bff042db7b1b\", \"user_id\": \"\", \"graph_id\": \"agent\", \"thread_id\": \"7885f0cf-94ad-4040-91d7-73f7ba007c8a\", \"assistant_id\": \"fe096781-5601-53d2-b2f6-0d3403f7e9ca\", \"checkpoint_id\": null } }, \"webhook\": null, \"temporary\": false, \"stream_mode\": [ \"values\" ], \"feedback_keys\": null, \"interrupt_after\": null, \"interrupt_before\": null }, \"multitask_strategy\": \"reject\" } - Python - Javascript - CURL Copy final_result = await client.threads.get_state(thread[\"thread_id\"]) print(final_result) Copy { \"values\": { \"messages\": [ { \"content\": \"what's the weather in sf\", \"additional_kwargs\": {}, \"response_metadata\": {}, \"type\": \"human\", \"name\": null, \"id\": \"beba31bf-320d-4125-9c37-cadf526ac47a\", \"example\": false }, { \"content\": [ { \"id\": \"toolu_01AaNPSPzqia21v7aAKwbKYm\", \"input\": {}, \"name\": \"tavily_search_results_json\", \"type\": \"tool_use\", \"index\": 0, \"partial_json\": \"{\\\"query\\\": \\\"weather in san francisco\\\"}\" } ], \"additional_kwargs\": {}, \"response_metadata\": { \"stop_reason\": \"tool_use\", \"stop_sequence\": null }, \"type\": \"ai\", \"name\": null, \"id\": \"run-f220faf8-1d27-4f73-ad91-6bb3f47e8639\", \"example\": false, \"tool_calls\": [ { \"name\": \"tavily_search_results_json\", \"args\": { \"query\": \"weather in san francisco\" }, \"id\": \"toolu_01AaNPSPzqia21v7aAKwbKYm\", \"type\": \"tool_call\" } ], \"invalid_tool_calls\": [], \"usage_metadata\": { \"input_tokens\": 273, \"output_tokens\": 61, \"total_tokens\": 334 } }, { \"content\": \"[{\\\"url\\\": \\\"https://www.weatherapi.com/\\\", \\\"content\\\": \\\"{'location': {'name': 'San Francisco', 'region': 'California', 'country': 'United States of America', 'lat': 37.78, 'lon': -122.42, 'tz_id': 'America/Los_Angeles', 'localtime_epoch': 1725052131, 'localtime': '2024-08-30 14:08'}, 'current': {'last_updated_epoch': 1725051600, 'last_updated': '2024-08-30 14:00', 'temp_c': 21.1, 'temp_f': 70.0, 'is_day': 1, 'condition': {'text': 'Partly cloudy', 'icon': '//cdn.weatherapi.com/weather/64x64/day/116.png', 'code': 1003}, 'wind_mph': 11.9, 'wind_kph': 19.1, 'wind_degree': 290, 'wind_dir': 'WNW', 'pressure_mb': 1018.0, 'pressure_in': 30.07, 'precip_mm': 0.0, 'precip_in': 0.0, 'humidity': 59, 'cloud': 25, 'feelslike_c': 21.1, 'feelslike_f': 70.0, 'windchill_c': 18.6, 'windchill_f': 65.5, 'heatindex_c': 18.6, 'heatindex_f': 65.5, 'dewpoint_c': 12.2, 'dewpoint_f': 54.0, 'vis_km': 16.0, 'vis_miles': 9.0, 'uv': 5.0, 'gust_mph': 15.0, 'gust_kph': 24.2}}\\\"}]\", \"additional_kwargs\": {}, \"response_metadata\": {}, \"type\": \"tool\", \"name\": \"tavily_search_results_json\", \"id\": \"686b2487-f332-4e58-9508-89b3a814cd81\", \"tool_call_id\": \"toolu_01AaNPSPzqia21v7aAKwbKYm\", \"artifact\": { \"query\": \"weather in san francisco\", \"follow_up_questions\": null, \"answer\": null, \"images\": [], \"results\": [ { \"title\": \"Weather in San Francisco\", \"url\": \"https://www.weatherapi.com/\", \"content\": \"{'location': {'name': 'San Francisco', 'region': 'California', 'country': 'United States of America', 'lat': 37.78, 'lon': -122.42, 'tz_id': 'America/Los_Angeles', 'localtime_epoch': 1725052131, 'localtime': '2024-08-30 14:08'}, 'current': {'last_updated_epoch': 1725051600, 'last_updated': '2024-08-30 14:00', 'temp_c': 21.1, 'temp_f': 70.0, 'is_day': 1, 'condition': {'text': 'Partly cloudy', 'icon': '//cdn.weatherapi.com/weather/64x64/day/116.png', 'code': 1003}, 'wind_mph': 11.9, 'wind_kph': 19.1, 'wind_degree': 290, 'wind_dir': 'WNW', 'pressure_mb': 1018.0, 'pressure_in': 30.07, 'precip_mm': 0.0, 'precip_in': 0.0, 'humidity': 59, 'cloud': 25, 'feelslike_c': 21.1, 'feelslike_f': 70.0, 'windchill_c': 18.6, 'windchill_f': 65.5, 'heatindex_c': 18.6, 'heatindex_f': 65.5, 'dewpoint_c': 12.2, 'dewpoint_f': 54.0, 'vis_km': 16.0, 'vis_miles': 9.0, 'uv': 5.0, 'gust_mph': 15.0, 'gust_kph': 24.2}}\", \"score\": 0.976148, \"raw_content\": null } ], \"response_time\": 3.07 }, \"status\": \"success\" }, { \"content\": [ { \"text\": \"\\n\\nThe search results provide the current weather conditions in San Francisco. According to the data, as of 2:00 PM on August 30, 2024, the temperature in San Francisco is 70\\u00b0F (21.1\\u00b0C) with partly cloudy skies. The wind is blowing from the west-northwest at around 12 mph (19 km/h). The humidity is 59% and visibility is 9 miles (16 km). Overall, it looks like a nice late summer day in San Francisco with comfortable temperatures and partly sunny conditions.\", \"type\": \"text\", \"index\": 0 } ], \"additional_kwargs\": {}, \"response_metadata\": { \"stop_reason\": \"end_turn\", \"stop_sequence\": null }, \"type\": \"ai\", \"name\": null, \"id\": \"run-8fecc61d-3d9f-4e16-8e8a-92f702be498a\", \"example\": false, \"tool_calls\": [], \"invalid_tool_calls\": [], \"usage_metadata\": { \"input_tokens\": 837, \"output_tokens\": 124, \"total_tokens\": 961 } } ] }, \"next\": [], \"tasks\": [], \"metadata\": { \"step\": 3, \"run_id\": \"1ef67140-eb23-684b-8253-91d4c90bb05e\", \"source\": \"loop\", \"writes\": { \"agent\": { \"messages\": [ { \"id\": \"run-8fecc61d-3d9f-4e16-8e8a-92f702be498a\", \"name\": null, \"type\": \"ai\", \"content\": [ { \"text\": \"\\n\\nThe search results provide the current weather conditions in San Francisco. According to the data, as of 2:00 PM on August 30, 2024, the temperature in San Francisco is 70\\u00b0F (21.1\\u00b0C) with partly cloudy skies. The wind is blowing from the west-northwest at around 12 mph (19 km/h). The humidity is 59% and visibility is 9 miles (16 km). Overall, it looks like a nice late summer day in San Francisco with comfortable temperatures and partly sunny conditions.\", \"type\": \"text\", \"index\": 0 } ], \"example\": false, \"tool_calls\": [], \"usage_metadata\": { \"input_tokens\": 837, \"total_tokens\": 961, \"output_tokens\": 124 }, \"additional_kwargs\": {}, \"response_metadata\": { \"stop_reason\": \"end_turn\", \"stop_sequence\": null }, \"invalid_tool_calls\": [] } ] } }, \"user_id\": \"\", \"graph_id\": \"agent\", \"thread_id\": \"5cb1e8a1-34b3-4a61-a34e-71a9799bd00d\", \"created_by\": \"system\", \"assistant_id\": \"fe096781-5601-53d2-b2f6-0d3403f7e9ca\" }, \"created_at\": \"2024-08-30T21:09:00.079909+00:00\", \"checkpoint_id\": \"1ef67141-3ca2-6fae-8003-fe96832e57d6\", \"parent_checkpoint_id\": \"1ef67141-2129-6b37-8002-61fc3bf69cb5\" } - Python - Javascript - CURL Copy print(final_result['values']['messages'][-1]['content'][0]['text']) Copy The search results provide the current weather conditions in San Francisco. According to the data, as of 2:00 PM on August 30, 2024, the temperature in San Francisco is 70\u00b0F (21.1\u00b0C) with partly cloudy skies. The wind is blowing from the west-northwest at around 12 mph (19 km/h). The humidity is 59% and visibility is 9 miles (16 km). Overall, it looks like a nice late summer day in", "tokens": 1000, "node_type": "child"}
{"id": 28, "chunk_id": "9b0fd3b850698317bef57fa6e71a38ba", "parent_id": 24, "url": "", "namespace": "langchain", "title": "langsmith-background-run", "headers": ["langsmith-background-run"], "section_index": 0, "chunk_index": 1, "text": "km). Overall, it looks like a nice late summer day in San Francisco with comfortable temperatures and partly sunny conditions.\", \"type\": \"text\", \"index\": 0 } ], \"example\": false, \"tool_calls\": [], \"usage_metadata\": { \"input_tokens\": 837, \"total_tokens\": 961, \"output_tokens\": 124 }, \"additional_kwargs\": {}, \"response_metadata\": { \"stop_reason\": \"end_turn\", \"stop_sequence\": null }, \"invalid_tool_calls\": [] } ] } }, \"user_id\": \"\", \"graph_id\": \"agent\", \"thread_id\": \"5cb1e8a1-34b3-4a61-a34e-71a9799bd00d\", \"created_by\": \"system\", \"assistant_id\": \"fe096781-5601-53d2-b2f6-0d3403f7e9ca\" }, \"created_at\": \"2024-08-30T21:09:00.079909+00:00\", \"checkpoint_id\": \"1ef67141-3ca2-6fae-8003-fe96832e57d6\", \"parent_checkpoint_id\": \"1ef67141-2129-6b37-8002-61fc3bf69cb5\" } - Python - Javascript - CURL Copy print(final_result['values']['messages'][-1]['content'][0]['text']) Copy The search results provide the current weather conditions in San Francisco. According to the data, as of 2:00 PM on August 30, 2024, the temperature in San Francisco is 70\u00b0F (21.1\u00b0C) with partly cloudy skies. The wind is blowing from the west-northwest at around 12 mph (19 km/h). The humidity is 59% and visibility is 9 miles (16 km). Overall, it looks like a nice late summer day in San Francisco with comfortable temperatures and partly sunny conditions.", "tokens": 159, "node_type": "child"}
{"id": 29, "chunk_id": "c16acf8a553ac24f5d77e7d3a1c6e493", "parent_id": 25, "url": "", "namespace": "langchain", "title": "langsmith-billing", "headers": ["langsmith-billing"], "section_index": 0, "chunk_index": 0, "text": "# langsmith-billing > Source: https://docs.langchain.com/langsmith/billing - Set up billing for your account: Complete the billing setup process for Developer and Plus plans, including special instructions for legacy accounts. - Update your information: Modify invoice email addresses, business information, and tax IDs for your organization. - Optimize your tracing spend: Learn how to reduce costs through data retention management and usage limits. Set up billing for your account Before using this guide, note the following: - If you are interested in the Enterprise plan, please contact sales. This guide is only for our self-serve billing plans. - If you created your LangSmith organization before pricing was introduced on April 2nd, 2024, please skip to the final section. Developer Plan: set up billing on your personal organization Personal organizations are limited to 5000 traces per month until a credit card is added. You can add a credit card on the Plans and Billing page as follows:- Click Set up Billing. - Add your credit card information. After this step, you will no longer be rate limited to 5000 traces, and you will be charged for any excess traces at rates specified on the pricing page. Plus Plan: set up billing on a shared organization If you have not yet created an organization, you need to follow this guide before setting up billing. The following steps assume you are already in a new organization.You can\u2019t use a new organization until you enter credit card information. After you complete the following steps, you will gain complete access to LangSmith. - Click Subscribe on the Plus page. If you are a startup building with AI, instead click Apply Now on the Startup Plan. You may be eligible for discounted prices and a free, monthly trace allotment. - Review your existing members. Before subscribing, LangSmith lets you remove any added users that you do not want to be included in the bill. - Enter your credit card information. Then, enter business information, invoice email, and tax ID. If this organization belongs to a business, check the This is a business checkbox and enter the information accordingly. Set up billing for accounts created before pricing introduction If you joined LangSmith before pricing was introduced on April 2, 2024, you have the option to upgrade your existing account to set up billing. If you did not set up billing by July 8, 2024, then your account is now rate limited to a maximum of 5,000 traces per month.- Navigate to the Settings page. - Click Set up Billing. - Enter your credit card information. If you are on a Personal organization, this will add you to the Developer plan. If you are on a shared organization, this will add you to the Plus plan. For more information, refer to the guides for the Developer or Plus plans respectively, starting at step 2. - Claim free credits as a thank you for being an early LangSmith user. Update your information To update business information for your LangSmith organization, head to the Usage and Billing page under Settings and click on the Plans and Billing tab.Business information, tax ID, and invoice email can only be updated for the Plus and Startup plans. Free and Developer plans cannot update this information. Invoice email To update the email address for invoices, follow these steps:- Navigate to the Plans and Billing tab. - Locate the section beneath the payment method, where the current invoice email is displayed. - Enter the new email address for invoices in the provided field. - The new email address will be automatically saved. Business information and tax ID In certain jurisdictions, LangSmith is required to collect sales tax. If you are a business, providing your tax ID may qualify you for a sales tax exemption. - Navigate to the Plans and Billing tab. - Below the invoice email section, you will find a checkbox labeled Business. - Check the Business checkbox if your organization belongs to a business. - A business information section will appear, allowing you to enter or update the following details: - Business Name - Address - Tax ID for applicable jurisdictions - A Tax ID field will appear for applicable jurisdictions after you select a country. - After entering the necessary information, click the Save button to save your changes. Optimize your tracing spend Some of the features mentioned in this guide are not currently available on Enterprise plan due to its custom nature of billing. If you are on the Enterprise plan and have questions about cost optimization, reach out to your sales rep or support@langchain.dev. - Reducing existing costs with data retention policies. - Preventing future overspend with usage limits. Dev , Staging , and Prod ): Understand your current usage The first step of any optimization process is to understand current usage. LangSmith provides two ways to do this: Usage graph and Invoices.Usage graph The usage graph lets you examine how much of each usage-based pricing metric you have consumed. It does not directly show spend (which you will review later in the draft invoice). You can navigate to the usage graph under Settings -> Usage and Billing -> Usage Graph. This graph shows that there are two usage metrics that LangSmith charges for:- LangSmith Traces (Base Charge): tracks all traces that you send to LangSmith. - LangSmith Traces (Extended Data Retention Upgrades): tracks all traces that also have our Extended 400 Day Data Retention. Invoices You understand what usage looks like in terms of traces, but you now need to translate that into spend. To do so, navigate to the Invoices tab. The first invoice that will appear on screen is a draft of your current month\u2019s invoice, which shows your running spend thus far this month.LangSmith\u2019s Usage Graph and Invoice use the term tenant_id to refer to a workspace ID. They are interchangeable.- You use extended data retention tracing, which means by default your traces are retained for 400 days. - You use base data retention tracing and use", "tokens": 1000, "node_type": "child"}
{"id": 30, "chunk_id": "9b0fd3b850698317bef57fa6e71a38ba", "parent_id": 25, "url": "", "namespace": "langchain", "title": "langsmith-billing", "headers": ["langsmith-billing"], "section_index": 0, "chunk_index": 1, "text": "graph under Settings -> Usage and Billing -> Usage Graph. This graph shows that there are two usage metrics that LangSmith charges for:- LangSmith Traces (Base Charge): tracks all traces that you send to LangSmith. - LangSmith Traces (Extended Data Retention Upgrades): tracks all traces that also have our Extended 400 Day Data Retention. Invoices You understand what usage looks like in terms of traces, but you now need to translate that into spend. To do so, navigate to the Invoices tab. The first invoice that will appear on screen is a draft of your current month\u2019s invoice, which shows your running spend thus far this month.LangSmith\u2019s Usage Graph and Invoice use the term tenant_id to refer to a workspace ID. They are interchangeable.- You use extended data retention tracing, which means by default your traces are retained for 400 days. - You use base data retention tracing and use a feature that automatically extends the data retention of a trace. (Refer to the Auto-Upgrade conceptual docs.) Optimization 1: manage data retention LangSmith charges differently based on a trace\u2019s data retention, where short-lived traces are an order of magnitude less expensive than ones that last for a long time. In this optimization, you\u2019ll learn how to get optimal settings for data retention without sacrificing historical observability, and see the effect it has on the bill.Change org level retention defaults for new projects Navigate to the Usage configuration tab, and look at the organization level retention settings. Modifying this setting affects all new projects that are created going forward in all workspaces in the organizaton.For backwards compatibility, older organizations may have this defaulted to Extended. Organizations created after June 3rd, 2024 have this defaulted to Base. Change project level retention defaults Data retention settings are adjustable per project on the tracing project page. Navigate to Projects > Your project name > Select Retention and modify the default retention of the project to Base. This will only affect retention (and pricing) for traces going forward.Apply extended data retention to a percentage of traces You may not want all traces to expire after 14 days. You can automatically extend the retention of traces that match some criteria by creating an automation rule. You might want to apply extended data retention to specific types of traces, such as:- 10% of all traces: For general analysis or analyzing trends long term. - Errored traces: To investigate and debug issues thoroughly. - Traces with specific metadata: For long-term examination of particular features or user flows. - Navigate to Projects > Your project name > Select + New > Select New Automation. - Name your rule and optionally apply filters or a sample rate. For more information on configuring filters, refer to filtering techniques. For example, this is the expected configuration to keep 10% of all traces for extended data retention: If you want to keep a subset of traces for longer than 400 days for data collection purposes, you can create another run rule that sends some runs to a dataset of your choosing. A dataset allows you to store the trace inputs and outputs (e.g., as a key-value dataset), and will persist indefinitely, even after the trace gets deleted. See results after 7 days While the total amount of traces per day stayed the same, the extended data retention traces was cut heavily. In the invoice, the spend reduced to roughly $900 in the last 7 days, as opposed to $2,000 in the previous 4. That\u2019s a cost reduction of nearly 75% per day.Optimization 2: limit usage In the previous section, you managed data retention settings to optimize existing spend. In this section, you will use usage limits to prevent future overspend. LangSmith has two usage limits: total traces and extended retention traces. These correspond to the two metrics tracked on the usage graph. You can use these in tandem to have granular control over spend. To set limits, navigate back to Settings -> Usage and Billing -> Usage configuration. There is a table at the bottom of the page that lets you set usage limits per workspace. For each workspace, the two limits appear, along with a cost estimate: Start by setting limits on production usage, since that is where the majority of spend comes from.Set a good total traces limit Picking the right total traces limit depends on the expected load of traces that you will send to LangSmith. It is important to consider potential growth before setting a limit. For example:- Current Load: The gen AI application is called between 1.2-1.5 times per second, and each API request has a trace associated with it, meaning it logs around 100,000-130,000 traces per day. - Expected Growth in Load: The expectation is that this will double in size in the near future. When set without the extended data retention traces limit, the maximum cost estimator assumes that all traces are using extended data retention. Cut maximum spend with an extended data retention limit From Optimization 1, you learned that the easiest way to cut cost was through managing data retention. The same is true for limits. If you only want to keep roughly 10% of traces to be around more than 14 days, you can set a limit on the maximum high retention traces you can keep. This would result in.10 * 7,800,000 = 780,000 . The maximum cost is cut from ~40k per month to ~7.5k per month, because you no longer allow as many expensive data retention upgrades. This ensures that new users on the platform will not accidentally cause cost to balloon. Set dev/staging limits and view total spent limit across workspaces Following a similar logic for thedev and staging environments, you can set limits at 10% of the production limit on usage for each workspace. While this works with this usage pattern, setting good dev and staging limits may vary depending on your use case with LangSmith. For example, if you run evals as part of CI/CD in dev or", "tokens": 1000, "node_type": "child"}
{"id": 31, "chunk_id": "5345104ec252a41359fa4a3d2dfbec23", "parent_id": 25, "url": "", "namespace": "langchain", "title": "langsmith-billing", "headers": ["langsmith-billing"], "section_index": 0, "chunk_index": 2, "text": "for limits. If you only want to keep roughly 10% of traces to be around more than 14 days, you can set a limit on the maximum high retention traces you can keep. This would result in.10 * 7,800,000 = 780,000 . The maximum cost is cut from ~40k per month to ~7.5k per month, because you no longer allow as many expensive data retention upgrades. This ensures that new users on the platform will not accidentally cause cost to balloon. Set dev/staging limits and view total spent limit across workspaces Following a similar logic for thedev and staging environments, you can set limits at 10% of the production limit on usage for each workspace. While this works with this usage pattern, setting good dev and staging limits may vary depending on your use case with LangSmith. For example, if you run evals as part of CI/CD in dev or staging, you may want to be more flexible with your usage limits to avoid test failures. With the limits set, LangSmith shows a maximum spend estimate across all workspaces: You can use the cost estimate to plan for your invoice total.", "tokens": 191, "node_type": "child"}
{"id": 32, "chunk_id": "c16acf8a553ac24f5d77e7d3a1c6e493", "parent_id": 26, "url": "", "namespace": "langchain", "title": "langsmith-bind-evaluator-to-dataset", "headers": ["langsmith-bind-evaluator-to-dataset"], "section_index": 0, "chunk_index": 0, "text": "# langsmith-bind-evaluator-to-dataset\n\n> Source: https://docs.langchain.com/langsmith/bind-evaluator-to-dataset\n\n- Programmatically, by specifying evaluators in your code (see this guide for details)\n- By binding evaluators to a dataset in the UI. This will automatically run the evaluators on any new experiments created, in addition to any evaluators you\u2019ve set up via the SDK. This is useful when you\u2019re iterating on your application (target function), and have a standard set of evaluators you want to run for all experiments.\nConfiguring an evaluator on a dataset\n- Click on the Datasets and Experiments tab in the sidebar.\n- Select the dataset you want to configure the evaluator for.\n- Click on the + Evaluator button to add an evaluator to the dataset. This will open a pane you can use to configure the evaluator.\nWhen you configure an evaluator for a dataset, it will only affect the experiment runs that are created after the evaluator is configured. It will not affect the evaluation of experiment runs that were created before the evaluator was configured.\nLLM-as-a-judge evaluators\nThe process for binding evaluators to a dataset is very similar to the process for configuring a LLM-as-a-judge evaluator in the Playground. View instructions for configuring an LLM-as-a-judge evaluator in the Playground.Custom code evaluators\nThe process for binding a code evaluators to a dataset is very similar to the process for configuring a code evaluator in online evaluation. View instruction for configuring code evaluators. The only difference between configuring a code evaluator in online evaluation and binding a code evaluator to a dataset is that the custom code evaluator can reference outputs that are part of the dataset\u2019sExample\n.\nFor custom code evaluators bound to a dataset, the evaluator function takes in two arguments:\n- A\nRun\n(reference). This represents the new run in your experiment. For example, if you ran an experiment via SDK, this would contain the input/output from your chain or model you are testing. - An\nExample\n(reference). This represents the reference example in your dataset that the chain or model you are testing uses. Theinputs\nto the Run and Example should be the same. If your Example has a referenceoutputs\n, then you can use this to compare to the run\u2019s output for scoring.\nNext steps\n- Analyze your experiment results in the experiments tab\n- Compare your experiment results in the comparison view", "tokens": 390, "node_type": "child"}
{"id": 33, "chunk_id": "c16acf8a553ac24f5d77e7d3a1c6e493", "parent_id": 27, "url": "", "namespace": "langchain", "title": "langsmith-cli", "headers": ["langsmith-cli"], "section_index": 0, "chunk_index": 0, "text": "# langsmith-cli > Source: https://docs.langchain.com/langsmith/cli LangGraph CLI is a command-line tool for building and running the LangGraph API server locally. The resulting server exposes all API endpoints for runs, threads, assistants, etc., and includes supporting services such as a managed database for checkpointing and storage. Installation - Ensure Docker is installed (e.g., docker --version ). - Install the CLI: - Verify the install Quick commands | Command | What it does | |---| langgraph dev | Starts a lightweight local dev server (no Docker required), ideal for rapid testing. | langgraph build | Builds a Docker image of your LangGraph API server for deployment. | langgraph dockerfile | Emits a Dockerfile derived from your config for custom builds. | langgraph up | Starts the LangGraph API server locally in Docker. Requires Docker running; LangSmith API key for local dev; license for production. | For JS, use npx @langchain/langgraph-cli <command> (or langgraphjs if installed globally). Configuration file The LangGraph CLI requires a JSON configuration file that follows this schema. It contains the following properties: The LangGraph CLI defaults to using the configuration file langgraph.json in the current directory. | Key | Description | |---| dependencies | Required. Array of dependencies for LangSmith API server. Dependencies can be one of the following: - A single period ( \".\" ), which will look for local Python packages. - The directory path where pyproject.toml , setup.py or requirements.txt is located. For example, if requirements.txt is located in the root of the project directory, specify \"./\" . If it\u2019s located in a subdirectory called local_package , specify \"./local_package\" . Do not specify the string \"requirements.txt\" itself. - A Python package name. | graphs | Required. Mapping from graph ID to path where the compiled graph or a function that makes a graph is defined. Example: ./your_package/your_file.py:variable , where variable is an instance of langgraph.graph.state.CompiledStateGraph ./your_package/your_file.py:make_graph , where make_graph is a function that takes a config dictionary (langchain_core.runnables.RunnableConfig ) and returns an instance of langgraph.graph.state.StateGraph or langgraph.graph.state.CompiledStateGraph . See how to rebuild a graph at runtime for more details. | auth | (Added in v0.0.11) Auth configuration containing the path to your authentication handler. Example: ./your_package/auth.py:auth , where auth is an instance of langgraph_sdk.Auth . See authentication guide for details. | base_image | Optional. Base image to use for the LangGraph API server. Defaults to langchain/langgraph-api or langchain/langgraphjs-api . Use this to pin your builds to a particular version of the langgraph API, such as \"langchain/langgraph-server:0.2\" . See https://hub.docker.com/r/langchain/langgraph-server/tags for more details. (added in langgraph-cli==0.2.8 ) | image_distro | Optional. Linux distribution for the base image. Must be one of \"debian\" , \"wolfi\" , \"bookworm\" , or \"bullseye\" . If omitted, defaults to \"debian\" . Available in langgraph-cli>=0.2.11 . | env | Path to .env file or a mapping from environment variable to its value. | store | Configuration for adding semantic search and/or time-to-live (TTL) to the BaseStore. Contains the following fields: index (optional): Configuration for semantic search indexing with fields embed , dims , and optional fields .ttl (optional): Configuration for item expiration. An object with optional fields: refresh_on_read (boolean, defaults to true ), default_ttl (float, lifespan in minutes; applied to newly created items only; existing items are unchanged; defaults to no expiration), and sweep_interval_minutes (integer, how often to check for expired items, defaults to no sweeping). | ui | Optional. Named definitions of UI components emitted by the agent, each pointing to a JS/TS file. (added in langgraph-cli==0.1.84 ) | python_version | 3.11 , 3.12 , or 3.13 . Defaults to 3.11 . | node_version | Specify node_version: 20 to use LangGraph.js. | pip_config_file | Path to pip config file. | pip_installer | (Added in v0.3) Optional. Python package installer selector. It can be set to \"auto\" , \"pip\" , or \"uv\" . From version 0.3 onward the default strategy is to run uv pip , which typically delivers faster builds while remaining a drop-in replacement. In the uncommon situation where uv cannot handle your dependency graph or the structure of your pyproject.toml , specify \"pip\" here to revert to the earlier behaviour. | keep_pkg_tools | (Added in v0.3.4) Optional. Control whether to retain Python packaging tools (pip , setuptools , wheel ) in the final image. Accepted values: true : Keep all three tools (skip uninstall).false / omitted : Uninstall all three tools (default behaviour).list[str] : Names of tools to retain. Each value must be one of \u201cpip\u201d, \u201csetuptools\u201d, \u201cwheel\u201d. . By default, all three tools are uninstalled. | dockerfile_lines | Array of additional lines to add to Dockerfile following the import from parent image. | checkpointer | Configuration for the checkpointer. Contains a ttl field which is an object with the following keys: strategy : How to handle expired checkpoints (e.g., \"delete\" ).sweep_interval_minutes : How often to check for expired checkpoints (integer).default_ttl : Default time-to-live for checkpoints in minutes (integer); applied to newly created checkpoints/threads only (existing data is unchanged). Defines how long checkpoints are kept before the specified strategy is applied. | http | HTTP server configuration with the following fields: app : Path to custom Starlette/FastAPI app (e.g., \"./src/agent/webapp.py:app\" ). See custom routes guide.cors : CORS configuration with fields for allow_origins , allow_methods , allow_headers , etc.configurable_headers : Define which request headers to exclude or include as a run\u2019s configurable values.disable_assistants : Disable /assistants routesdisable_mcp : Disable /mcp routesdisable_meta : Disable /ok , /info , /metrics , and /docs routesdisable_runs : Disable /runs routesdisable_store : Disable /store routesdisable_threads : Disable /threads routesdisable_ui : Disable /ui routesdisable_webhooks : Disable webhooks calls on run completion in all routesmount_prefix : Prefix for mounted routes (e.g., \u201c/my-deployment/api\u201d) | api_version | (Added in v0.3.7) Which semantic version of the LangGraph API server to use (e.g., \"0.3\" ). Defaults to latest. Check the server changelog for details on each release. | Examples Basic Configuration Using Wolfi Base Images You can specify the Linux distribution for your base image using the image_distro field. Valid options are debian , wolfi , bookworm , or bullseye . Wolfi", "tokens": 1000, "node_type": "child"}
{"id": 34, "chunk_id": "9b0fd3b850698317bef57fa6e71a38ba", "parent_id": 27, "url": "", "namespace": "langchain", "title": "langsmith-cli", "headers": ["langsmith-cli"], "section_index": 0, "chunk_index": 1, "text": "). See custom routes guide.cors : CORS configuration with fields for allow_origins , allow_methods , allow_headers , etc.configurable_headers : Define which request headers to exclude or include as a run\u2019s configurable values.disable_assistants : Disable /assistants routesdisable_mcp : Disable /mcp routesdisable_meta : Disable /ok , /info , /metrics , and /docs routesdisable_runs : Disable /runs routesdisable_store : Disable /store routesdisable_threads : Disable /threads routesdisable_ui : Disable /ui routesdisable_webhooks : Disable webhooks calls on run completion in all routesmount_prefix : Prefix for mounted routes (e.g., \u201c/my-deployment/api\u201d) | api_version | (Added in v0.3.7) Which semantic version of the LangGraph API server to use (e.g., \"0.3\" ). Defaults to latest. Check the server changelog for details on each release. | Examples Basic Configuration Using Wolfi Base Images You can specify the Linux distribution for your base image using the image_distro field. Valid options are debian , wolfi , bookworm , or bullseye . Wolfi is the recommended option as it provides smaller and more secure images. This is available in langgraph-cli>=0.2.11 .Adding semantic search to the store All deployments come with a DB-backed BaseStore. Adding an \u201cindex\u201d configuration to your langgraph.json will enable semantic search within the BaseStore of your deployment.The index.fields configuration determines which parts of your documents to embed: - If omitted or set to [\"$\"] , the entire document will be embedded - To embed specific fields, use JSON path notation: [\"metadata.title\", \"content.text\"] - Documents missing specified fields will still be stored but won\u2019t have embeddings for those fields - You can still override which fields to embed on a specific item at put time using the index parameter Common model dimensions openai:text-embedding-3-large : 3072 openai:text-embedding-3-small : 1536 openai:text-embedding-ada-002 : 1536 cohere:embed-english-v3.0 : 1024 cohere:embed-english-light-v3.0 : 384 cohere:embed-multilingual-v3.0 : 1024 cohere:embed-multilingual-light-v3.0 : 384 Semantic search with a custom embedding function If you want to use semantic search with a custom embedding function, you can pass a path to a custom embedding function:The embed field in store configuration can reference a custom function that takes a list of strings and returns a list of embeddings. Example implementation:Adding custom authentication See the authentication conceptual guide for details, and the setting up custom authentication guide for a practical walk through of the process.Configuring Store Item Time-to-Live You can configure default data expiration for items/memories in the BaseStore using the store.ttl key. This determines how long items are retained after they are last accessed (with reads potentially refreshing the timer based on refresh_on_read ). Note that these defaults can be overwritten on a per-call basis by modifying the corresponding arguments in get , search , etc.The ttl configuration is an object containing optional fields: refresh_on_read : If true (the default), accessing an item via get or search resets its expiration timer. Set to false to only refresh TTL on writes (put ). default_ttl : The default lifespan of an item in minutes. Applies only to newly created items; existing items are not modified. If not set, items do not expire by default. sweep_interval_minutes : How frequently (in minutes) the system should run a background process to delete expired items. If not set, sweeping does not occur automatically. Here is an example enabling a 7-day TTL (10080 minutes), refreshing on reads, and sweeping every hour:Configuring Checkpoint Time-to-Live You can configure the time-to-live (TTL) for checkpoints using the checkpointer key. This determines how long checkpoint data is retained before being automatically handled according to the specified strategy (e.g., deletion). The ttl configuration is an object containing: strategy : The action to take on expired checkpoints (currently \"delete\" is the only accepted option). sweep_interval_minutes : How frequently (in minutes) the system checks for expired checkpoints. default_ttl : The default lifespan of a checkpoint in minutes. Applies only to checkpoints/threads created after deployment; existing data is not modified. Here\u2019s an example setting a default TTL of 30 days (43200 minutes):In this example, checkpoints older than 30 days will be deleted, and the check runs every 10 minutes.Pinning API Version (Added in v0.3.7)You can pin the API version of the LangGraph server by using the api_version key. This is useful if you want to ensure that your server uses a specific version of the API. By default, builds in Cloud deployments use the latest stable version of the server. This can be pinned by setting the api_version key to a specific version. Commands Usage The base command for the LangGraph CLI is langgraph . dev Run LangGraph API server in development mode with hot reloading and debugging capabilities. This lightweight server requires no Docker installation and is suitable for development and testing. State is persisted to a local directory.Currently, the CLI only supports Python >= 3.11. InstallationThis command requires the \u201cinmem\u201d extra to be installed:UsageOptions| Option | Default | Description | |---| -c, --config FILE | langgraph.json | Path to configuration file declaring dependencies, graphs and environment variables | --host TEXT | 127.0.0.1 | Host to bind the server to | --port INTEGER | 2024 | Port to bind the server to | --no-reload | | Disable auto-reload | --n-jobs-per-worker INTEGER | | Number of jobs per worker. Default is 10 | --debug-port INTEGER | | Port for debugger to listen on | --wait-for-client | False | Wait for a debugger client to connect to the debug port before starting the server | --no-browser | | Skip automatically opening the browser when the server starts | --studio-url TEXT | | URL of the Studio instance to connect to. Defaults to https://smith.langchain.com | --allow-blocking | False | Do not raise errors for synchronous I/O blocking operations in your code (added in 0.2.6 ) | --tunnel | False | Expose the local server via a public tunnel (Cloudflare) for remote frontend access. This avoids issues with browsers like Safari or networks blocking localhost connections | --help | | Display command documentation | build Build LangSmith API server Docker image.UsageOptions| Option | Default | Description | |---| --platform TEXT | | Target platform(s) to build the Docker image for. Example: langgraph build --platform", "tokens": 1000, "node_type": "child"}
{"id": 35, "chunk_id": "5345104ec252a41359fa4a3d2dfbec23", "parent_id": 27, "url": "", "namespace": "langchain", "title": "langsmith-cli", "headers": ["langsmith-cli"], "section_index": 0, "chunk_index": 2, "text": "10 | --debug-port INTEGER | | Port for debugger to listen on | --wait-for-client | False | Wait for a debugger client to connect to the debug port before starting the server | --no-browser | | Skip automatically opening the browser when the server starts | --studio-url TEXT | | URL of the Studio instance to connect to. Defaults to https://smith.langchain.com | --allow-blocking | False | Do not raise errors for synchronous I/O blocking operations in your code (added in 0.2.6 ) | --tunnel | False | Expose the local server via a public tunnel (Cloudflare) for remote frontend access. This avoids issues with browsers like Safari or networks blocking localhost connections | --help | | Display command documentation | build Build LangSmith API server Docker image.UsageOptions| Option | Default | Description | |---| --platform TEXT | | Target platform(s) to build the Docker image for. Example: langgraph build --platform linux/amd64,linux/arm64 | -t, --tag TEXT | | Required. Tag for the Docker image. Example: langgraph build -t my-image | --pull / --no-pull | --pull | Build with latest remote Docker image. Use --no-pull for running the LangSmith API server with locally built images. | -c, --config FILE | langgraph.json | Path to configuration file declaring dependencies, graphs and environment variables. | --build-command TEXT * | | Build command to run. Runs from the directory where your langgraph.json file lives. Example: langgraph build --build-command \"yarn run turbo build\" | --install-command TEXT * | | Install command to run. Runs from the directory where you call langgraph build from. Example: langgraph build --install-command \"yarn install\" | --help | | Display command documentation. | *Only supported for JS deployments, will have no impact on Python deployments. Start LangGraph API server. For local testing, requires a LangSmith API key with access to LangSmith. Requires a license key for production use.UsageOptions| Option | Default | Description | |---| --wait | | Wait for services to start before returning. Implies \u2014detach | --base-image TEXT | langchain/langgraph-api | Base image to use for the LangGraph API server. Pin to specific versions using version tags. | --image TEXT | | Docker image to use for the langgraph-api service. If specified, skips building and uses this image directly. | --postgres-uri TEXT | Local database | Postgres URI to use for the database. | --watch | | Restart on file changes | --debugger-base-url TEXT | http://127.0.0.1:[PORT] | URL used by the debugger to access LangGraph API. | --debugger-port INTEGER | | Pull the debugger image locally and serve the UI on specified port | --verbose | | Show more output from the server logs. | -c, --config FILE | langgraph.json | Path to configuration file declaring dependencies, graphs and environment variables. | -d, --docker-compose FILE | | Path to docker-compose.yml file with additional services to launch. | -p, --port INTEGER | 8123 | Port to expose. Example: langgraph up --port 8000 | --pull / --no-pull | pull | Pull latest images. Use --no-pull for running the server with locally-built images. Example: langgraph up --no-pull | --recreate / --no-recreate | no-recreate | Recreate containers even if their configuration and image haven\u2019t changed | --help | | Display command documentation. | dockerfile Generate a Dockerfile for building a LangSmith API server Docker image.UsageOptions| Option | Default | Description | |---| -c, --config FILE | langgraph.json | Path to the configuration file declaring dependencies, graphs and environment variables. | --help | | Show this message and exit. | Example:This generates a Dockerfile that looks similar to:The langgraph dockerfile command translates all the configuration in your langgraph.json file into Dockerfile commands. When using this command, you will have to re-run it whenever you update your langgraph.json file. Otherwise, your changes will not be reflected when you build or run the dockerfile.", "tokens": 627, "node_type": "child"}
{"id": 36, "chunk_id": "c16acf8a553ac24f5d77e7d3a1c6e493", "parent_id": 28, "url": "", "namespace": "langchain", "title": "langsmith-cloud-architecture-and-scalability", "headers": ["langsmith-cloud-architecture-and-scalability"], "section_index": 0, "chunk_index": 0, "text": "# langsmith-cloud-architecture-and-scalability\n\n> Source: https://docs.langchain.com/langsmith/cloud-architecture-and-scalability\n\nThis section is only relevant for the cloud-managed LangSmith services available at https://smith.langchain.com and https://eu.smith.langchain.com.For information on the self-hosted LangSmith solution, please refer to the self-hosted documentation.\nArchitecture\nThe US-based LangSmith service is deployed in theus-central1\n(Iowa) region of GCP.\nNOTE: The EU-based LangSmith service is now available (as of mid-July 2024) and is deployed in the europe-west4\n(Netherlands) region of GCP. If you are interested in an enterprise plan in this region, contact our sales team.\nRegional storage\nThe resources and services in this table are stored in the location corresponding to the URL where sign-up occurred (either the US or EU). Cloud-managed LangSmith uses Supabase for authentication/authorization and ClickHouse Cloud for data warehouse.| US | EU | |\n|---|---|---|\n| URL | https://smith.langchain.com | https://eu.smith.langchain.com |\n| API URL | https://api.smith.langchain.com | https://eu.api.smith.langchain.com |\n| GCP | us-central1 (Iowa) | europe-west4 (Netherlands) |\n| Supabase | AWS us-east-1 (N. Virginia) | AWS eu-central-1 (Germany) |\n| ClickHouse Cloud | us-central1 (Iowa) | europe-west4 (Netherlands) |\n| LangSmith deployment | us-central1 (Iowa) | europe-west4 (Netherlands) |\nRegion-independent storage\nData listed here is stored exclusively in the US:- Payment and billing information with Stripe and Metronome\nGCP services\nLangSmith is composed of the following services, all deployed on Google Kubernetes Engine (GKE):- LangSmith Frontend: serves the LangSmith UI.\n- LangSmith Backend: serves the LangSmith API.\n- LangSmith Platform Backend: handles authentication and other high-volume tasks. (Internal service)\n- LangSmith Playground: handles forwarding requests to various LLM providers for the Playground feature.\n- LangSmith Queue: handles processing of asynchronous tasks. (Internal service)\n- Google Cloud Storage (GCS) for runs inputs and outputs.\n- Google Cloud SQL PostgreSQL for transactional workloads.\n- Google Cloud Memorystore for Redis for queuing and caching.\n- Clickhouse Cloud on GCP for trace ingestion and analytics. Our services connect to Clickhouse Cloud, which is hosted in the same GCP region, via a private endpoint.\n- Google Cloud Load Balancer for routing traffic to the LangSmith services.\n- Google Cloud CDN for caching static assets.\n- Google Cloud Armor for security and rate limits. For more information on rate limits we enforce, please refer to this guide.\nAllowlisting IP addresses\nEgress from LangChain SaaS\nAll traffic leaving LangSmith services will be routed through a NAT gateway. All traffic will appear to originate from the following IP addresses:| US | EU |\n|---|---|\n| 34.59.65.97 | 34.13.192.67 |\n| 34.67.51.221 | 34.147.105.64 |\n| 34.46.212.37 | 34.90.22.166 |\n| 34.132.150.88 | 34.147.36.213 |\n| 35.188.222.201 | 34.32.137.113 |\n| 34.58.194.127 | 34.91.238.184 |\n| 34.59.97.173 | 35.204.101.241 |\n| 104.198.162.55 | 35.204.48.32 |\nIngress into LangChain SaaS\nThe langchain endpoints map to the following static IP addresses:| US | EU |\n|---|---|\n| 34.8.121.39 | 34.95.92.214 |\n| 34.107.251.234 | 34.13.73.122 |\napi.smith.langchain.com\n, smith.langchain.com\n, beacon.langchain.com\n, eu.api.smith.langchain.com\n, eu.smith.langchain.com\n, eu.beacon.langchain.com\n).", "tokens": 480, "node_type": "child"}
{"id": 37, "chunk_id": "c16acf8a553ac24f5d77e7d3a1c6e493", "parent_id": 30, "url": "", "namespace": "langchain", "title": "langsmith-code-evaluator", "headers": ["langsmith-code-evaluator"], "section_index": 0, "chunk_index": 0, "text": "# langsmith-code-evaluator\n\n> Source: https://docs.langchain.com/langsmith/code-evaluator\n\nCode evaluators are just functions that take a dataset example and the resulting application output, and return one or more metrics. These functions can be passed directly into evaluate() / aevaluate().\ncode evaluator functions must have specific argument names. They can take any subset of the following arguments:\nrun: Run: The full Run object generated by the application on the given example.\nexample: Example: The full dataset Example, including the example inputs, outputs (if available), and metdata (if available).\ninputs: dict: A dictionary of the inputs corresponding to a single example in a dataset.\noutputs: dict: A dictionary of the outputs generated by the application on the given inputs.\nreference_outputs/referenceOutputs: dict: A dictionary of the reference outputs associated with the example, if available.\nFor most use cases you\u2019ll only need inputs, outputs, and reference_outputs. run and example are useful only if you need some extra trace or example metadata outside of the actual inputs and outputs of the application.When using JS/TS these should all be passed in as part of a single object argument.\nCode evaluators are expected to return one of the following types:Python and JS/TS\ndict: dicts of the form {\"score\" | \"value\": ..., \"key\": ...} allow you to customize the metric type (\u201cscore\u201d for numerical and \u201cvalue\u201d for categorical) and metric name. This if useful if, for example, you want to log an integer as a categorical metric.\nPython only\nint | float | bool: this is interepreted as an continuous metric that can be averaged, sorted, etc. The function name is used as the name of the metric.\nstr: this is intepreted as a categorical metric. The function name is used as the name of the metric.\nlist[dict]: return multiple metrics using a single function.\nfrom langsmith import evaluate, wrappersfrom langsmith.schemas import Run, Examplefrom openai import AsyncOpenAI# Assumes you've installed pydantic.from pydantic import BaseModel# We can still pass in Run and Example objects if we'd likedef correct_old_signature(run: Run, example: Example) -> dict: \"\"\"Check if the answer exactly matches the expected answer.\"\"\" return {\"key\": \"correct\", \"score\": run.outputs[\"answer\"] == example.outputs[\"answer\"]}# Just evaluate actual outputsdef concision(outputs: dict) -> int: \"\"\"Score how concise the answer is. 1 is the most concise, 5 is the least concise.\"\"\" return min(len(outputs[\"answer\"]) // 1000, 4) + 1# Use an LLM-as-a-judgeoai_client = wrappers.wrap_openai(AsyncOpenAI())async def valid_reasoning(inputs: dict, outputs: dict) -> bool: \"\"\"Use an LLM to judge if the reasoning and the answer are consistent.\"\"\" instructions = \"\"\"Given the following question, answer, and reasoning, determine if the reasoning for theanswer is logically valid and consistent with question and the answer.\"\"\" class Response(BaseModel): reasoning_is_valid: bool msg = f\"Question: {inputs['question']}\\nAnswer: {outputs['answer']}\\nReasoning: {outputs['reasoning']}\" response = await oai_client.beta.chat.completions.parse( model=\"gpt-4o-mini\", messages=[{\"role\": \"system\", \"content\": instructions,}, {\"role\": \"user\", \"content\": msg}], response_format=Response ) return response.choices[0].message.parsed.reasoning_is_validdef dummy_app(inputs: dict) -> dict: return {\"answer\": \"hmm i'm not sure\", \"reasoning\": \"i didn't understand the question\"}results = evaluate( dummy_app, data=\"dataset_name\", evaluators=[correct_old_signature, concision, valid_reasoning])", "tokens": 478, "node_type": "child"}
{"id": 38, "chunk_id": "c16acf8a553ac24f5d77e7d3a1c6e493", "parent_id": 31, "url": "", "namespace": "langchain", "title": "langsmith-collector-proxy", "headers": ["langsmith-collector-proxy"], "section_index": 0, "chunk_index": 0, "text": "# langsmith-collector-proxy\n\n> Source: https://docs.langchain.com/langsmith/collector-proxy\n\nThis is a beta feature. The API may change in future releases.\nWhen to Use the Collector-Proxy\nThe Collector-Proxy is particularly valuable when:- You\u2019re running multiple instances of your application in parallel and need to efficiently aggregate traces\n- You want more efficient tracing than direct OTEL API calls to LangSmith (the collector optimizes batching and compression)\n- You\u2019re using a language that doesn\u2019t have a native LangSmith SDK\nKey Features\n- Efficient Data Transfer Batches multiple spans into fewer, larger uploads.\n- Compression Uses zstd to minimize payload size.\n- OTLP Support Accepts OTLP JSON and Protobuf over HTTP POST.\n- Semantic Translation Maps GenAI/OpenInference conventions to the LangSmith Run model.\n- Flexible Batching Flush by span count or time interval.\nConfiguration\nConfigure via environment variables:| Variable | Description | Default |\n|---|---|---|\nHTTP_PORT | Port to run the proxy server | 4318 |\nLANGSMITH_ENDPOINT | LangSmith backend URL | https://api.smith.langchain.com |\nLANGSMITH_API_KEY | API key for LangSmith | Required (env var or header) |\nLANGSMITH_PROJECT | Default tracing project | Default project if not specified |\nBATCH_SIZE | Spans per upload batch | 100 |\nFLUSH_INTERVAL_MS | Flush interval in milliseconds | 1000 |\nMAX_BUFFER_BYTES | Max uncompressed buffer size | 10485760 (10 MB) |\nMAX_BODY_BYTES | Max incoming request body size | 209715200 (200 MB) |\nMAX_RETRIES | Retry attempts for failed uploads | 3 |\nRETRY_BACKOFF_MS | Initial backoff in milliseconds | 100 |\nProject Configuration\nThe Collector-Proxy supports LangSmith project configuration with the following priority:- If a project is specified in the request headers (\nLangsmith-Project\n), that project will be used - If no project is specified in headers, it will use the project set in the\nLANGSMITH_PROJECT\nenvironment variable - If neither is set, it will trace to the\ndefault\nproject.\nAuthentication\nThe API key can be provided either:- As an environment variable (\nLANGSMITH_API_KEY\n) - In the request headers (\nX-API-Key\n)\nDeployment (Docker)\nYou can deploy the Collector-Proxy with Docker:-\nBuild the image\n-\nRun the container\nUsage\nPoint any OTLP-compatible client or the OpenTelemetry Collector exporter at:Health & Scaling\n- Liveness:\nGET /live\n\u2192 200 - Readiness:\nGET /ready\n\u2192 200\nHorizontal Scaling\nTo ensure full traces are batched correctly, route spans with the same trace ID to the same instance (e.g., via consistent hashing).Fork & Extend\nFork the Collector-Proxy repo on GitHub and implement your own converter:- Create a custom\nGenAiConverter\nor modify the existing one ininternal/translator/otel_converter.go\n- Register the custom converter in\ninternal/translator/translator.go", "tokens": 421, "node_type": "child"}
{"id": 39, "chunk_id": "c16acf8a553ac24f5d77e7d3a1c6e493", "parent_id": 32, "url": "", "namespace": "langchain", "title": "langsmith-compare-experiment-results", "headers": ["langsmith-compare-experiment-results"], "section_index": 0, "chunk_index": 0, "text": "# langsmith-compare-experiment-results\n\n> Source: https://docs.langchain.com/langsmith/compare-experiment-results\n\nWhen you are iterating on your LLM application (such as changing the model or the prompt), you will want to compare the results of different experiments.LangSmith supports a comparison view that lets you hone in on key differences, regressions, and improvements between different experiments.\nYou can toggle between different views by clicking Full or Compact at the top of the Comparing Experiments page.Toggling Full will show the full text of the input, output, and reference output for each run. If the reference output is too long to display in the table, you can click on Expand detailed view to view the full content.You can also select and hide individual feedback keys or individual metrics in the Display settings dropdown to isolate the information you need in the comparison view.\nIn the comparison view, runs that regressed on your specified feedback key against your baseline experiment will be highlighted in red, while runs that improved will be highlighted in green. At the top of each column, you can find how many runs in that experiment did better and how many did worse than your baseline experiment.Click on the regressions or improvements buttons on the top of each column to filter to the runs that regressed or improved in that specific experiment.\nIn the Baseline dropdown at the top of the comparison view, select a Baseline experiment against which to compare. By default, the newest experiment is selected as the baseline.\nSelect a Feedback key (evaluation metric) you want to focus compare against. One will be assigned by default, but you can adjust as needed.\nConfigure whether a higher score is better for the selected feedback key. This preference will be stored.\nIf the example you\u2019re evaluating is from an ingested run, you can hover over the output cell and click on the trace icon to open the trace view for that run. This will open up a trace in the side panel.\nFrom any cell, you can click on the expand icon in the hover state to open up a detailed view of all experiment results on that particular example input, along with feedback keys and scores.\nYou can configure the x-axis labels for the charts based on experiment metadata.Select a metadata key in the x-axis dropdown to change the chart labels.", "tokens": 386, "node_type": "child"}
{"id": 40, "chunk_id": "c16acf8a553ac24f5d77e7d3a1c6e493", "parent_id": 34, "url": "", "namespace": "langchain", "title": "langsmith-components", "headers": ["langsmith-components"], "section_index": 0, "chunk_index": 0, "text": "# langsmith-components\n\n> Source: https://docs.langchain.com/langsmith/components\n\nWhen running the self-hosted LangSmith with deployment, your installation includes several key components. Together these tools and services provide a complete solution for building, deploying, and managing graphs (including agentic applications) in your own infrastructure:\nLangGraph Server: Defines an opinionated API and runtime for deploying graphs and agents. Handles execution, state management, and persistence so you can focus on building logic rather than server infrastructure.\nLangGraph CLI: A command-line interface to build, package, and interact with graphs locally and prepare them for deployment.\nStudio: A specialized IDE for visualization, interaction, and debugging. Connects to a local LangGraph Server for developing and testing your graph.\nPython/JS SDK: The Python/JS SDK provides a programmatic way to interact with deployed graphs and agents from your applications.\nRemoteGraph: Allows you to interact with a deployed graph as though it were running locally.\nControl Plane: The UI and APIs for creating, updating, and managing LangGraph Server deployments.\nData plane: The runtime layer that executes your graphs, including LangGraph Servers, their backing services (PostgreSQL, Redis, etc.), and the listener that reconciles state from the control plane.", "tokens": 185, "node_type": "child"}
{"id": 41, "chunk_id": "c16acf8a553ac24f5d77e7d3a1c6e493", "parent_id": 35, "url": "", "namespace": "langchain", "title": "langsmith-composite-evaluators", "headers": ["langsmith-composite-evaluators"], "section_index": 0, "chunk_index": 0, "text": "# langsmith-composite-evaluators\n\n> Source: https://docs.langchain.com/langsmith/composite-evaluators\n\nCreate a composite evaluator using the UI\nYou can create composite evaluators on a tracing project (for online evaluations) or a dataset (for offline evaluations). With composite evaluators in the UI, you can compute a weighted average or weighted sum of multiple evaluator scores, with configurable weights.1. Navigate to the tracing project or dataset\nTo start configuring a composite evaluator, navigate to the Tracing Projects or Dataset & Experiments tab and select a project or dataset.- From within a tracing project: + New > Evaluator > Composite score\n- From within a dataset: + Evaluator > Composite score\n2. Configure the composite evaluator\n- Name your evaluator.\n- Select an aggregation method, either Average or Sum.\n- Average: \u2211(weight*score) / \u2211(weight).\n- Sum: \u2211(weight*score).\n- Add the feedback keys you want to include in the composite score.\n- Add the weights for the feedback keys. By default, the weights are equal for each feedback key. Adjust the weights to increase or decrease the importance of specific feedback keys in the final score.\n- Click Create to save the evaluator.\nIf you need to adjust the weights for the composite scores, they can be updated after the evaluator is created. The resulting scores will be updated for all runs that have the evaluator configured.\n3. View composite evaluator results\nComposite scores are attached to a run as feedback, similarly to feedback from a single evaluator. How you can view them depends on where the evaluation was run: On a tracing project:- Composite scores appear as feedback on runs.\n- Filter for runs with a composite score, or where the composite score meets a certain threshold.\n- Create a chart to visualize trends in the composite score over time.\n- View the composite scores in the experiments tab. You can also filter and sort experiments based on the average composite score of their runs.\n- Click into an experiment to view the composite score for each run.\nIf any of the constituent evaluators are not configured on the run, the composite score will not be calculated for that run.\nCreate composite feedback with the SDK\nThis guide describes setting up an evaluation that uses multiple evaluators and combines their scores with a custom aggregation function. Requires langsmith>=0.4.29\n1. Configure evaluators on a dataset\nStart by configuring your evaluators. In this example, the application generates a tweet from a blog introduction and uses three evaluators \u2014 summary, tone, and formatting \u2014 to assess the output. If you already have your own dataset with evaluators configured, you can skip this step.Configure evaluators on a dataset.\nConfigure evaluators on a dataset.\n2. Create composite feedback\nCreate composite feedback that aggregates the individual evaluator scores using your custom function. This example uses a weighted average of the individual evaluator scores.Create a composite feedback.\nCreate a composite feedback.", "tokens": 476, "node_type": "child"}
{"id": 42, "chunk_id": "c16acf8a553ac24f5d77e7d3a1c6e493", "parent_id": 37, "url": "", "namespace": "langchain", "title": "langsmith-configuration-cloud", "headers": ["langsmith-configuration-cloud"], "section_index": 0, "chunk_index": 0, "text": "# langsmith-configuration-cloud\n\n> Source: https://docs.langchain.com/langsmith/configuration-cloud\n\ncall_model\nnode and context schema.\nObserve that this node tries to read and use the model_name\nas defined by the context\nobject\u2019s model_name\nfield.\n- Python\n- Javascript\nCreate an assistant\nLangGraph SDK\nTo create an assistant, use the LangGraph SDKcreate\nmethod. See the Python and JS SDK reference docs for more information.\nThis example uses the same context schema as above, and creates an assistant with model_name\nset to openai\n.\n- Python\n- Javascript\n- CURL\nLangSmith UI\nYou can also create assistants from the LangSmith UI. Inside your deployment, select the \u201cAssistants\u201d tab. This will load a table of all of the assistants in your deployment, across all graphs. To create a new assistant, select the \u201d+ New assistant\u201d button. This will open a form where you can specify the graph this assistant is for, as well as provide a name, description, and the desired configuration for the assistant based on the configuration schema for that graph. To confirm, click \u201cCreate assistant\u201d. This will take you to Studio where you can test the assistant. If you go back to the \u201cAssistants\u201d tab in the deployment, you will see the newly created assistant in the table.Use an assistant\nLangGraph SDK\nWe have now created an assistant called \u201cOpen AI Assistant\u201d that hasmodel_name\ndefined as openai\n. We can now use this assistant with this configuration:\n- Python\n- Javascript\n- CURL\nLangSmith UI\nInside your deployment, select the \u201cAssistants\u201d tab. For the assistant you would like to use, click the Studio button. This will open Studio with the selected assistant. When you submit an input (either in Graph or Chat mode), the selected assistant and its configuration will be used.Create a new version for your assistant\nLangGraph SDK\nTo edit the assistant, use theupdate\nmethod. This will create a new version of the assistant with the provided edits. See the Python and JS SDK reference docs for more information.\nNote\nYou must pass in the ENTIRE context (and metadata if you are using it). The update endpoint creates new versions completely from scratch and does not rely on previous versions.\n- Python\n- Javascript\n- CURL\nLangSmith UI\nYou can also edit assistants from the LangSmith UI. Inside your deployment, select the \u201cAssistants\u201d tab. This will load a table of all of the assistants in your deployment, across all graphs. To edit an existing assistant, select the \u201cEdit\u201d button for the specified assistant. This will open a form where you can edit the assistant\u2019s name, description, and configuration. Additionally, if using Studio, you can edit the assistants and create new versions via the \u201cManage Assistants\u201d button.Use a previous assistant version\nLangGraph SDK\nYou can also change the active version of your assistant. To do so, use thesetLatest\nmethod.\nIn the example above, to rollback to the first version of the assistant:\n- Python\n- Javascript\n- CURL\nLangSmith UI\nIf using Studio, to set the active version of your assistant, click the \u201cManage Assistants\u201d button and locate the assistant you would like to use. Select the assistant and the version, and then click the \u201cActive\u201d toggle. This will update the assistant to make the selected version active.Deleting Assistants\nDeleting as assistant will delete ALL of its versions. There is currently no way to delete a single version, but by pointing your assistant to the correct version you can skip any versions that you don\u2019t wish to use.", "tokens": 574, "node_type": "child"}
{"id": 43, "chunk_id": "c16acf8a553ac24f5d77e7d3a1c6e493", "parent_id": 38, "url": "", "namespace": "langchain", "title": "langsmith-configure-ttl", "headers": ["langsmith-configure-ttl"], "section_index": 0, "chunk_index": 0, "text": "# langsmith-configure-ttl\n\n> Source: https://docs.langchain.com/langsmith/configure-ttl\n\nLangSmith persists both checkpoints (thread state) and cross-thread memories (store items). Configure Time-to-Live (TTL) policies in\nlanggraph.json\nto automatically manage the lifecycle of this data, preventing indefinite accumulation.\nConfiguring Checkpoint TTL\nCheckpoints capture the state of conversation threads. Setting a TTL ensures old checkpoints and threads are automatically deleted. Add acheckpointer.ttl\nconfiguration to your langgraph.json\nfile:\nstrategy\n: Specifies the action taken on expiration. Currently, only\"delete\"\nis supported, which deletes all checkpoints in the thread upon expiration.sweep_interval_minutes\n: Defines how often, in minutes, the system checks for expired checkpoints.default_ttl\n: Sets the default lifespan of threads (and corresponding checkpoints) in minutes (e.g., 43200 minutes = 30 days). Applies only to checkpoints created after this configuration is deployed; existing checkpoints/threads are not changed. To clear older data, delete it explicitly.\nConfiguring Store Item TTL\nStore items allow cross-thread data persistence. Configuring TTL for store items helps manage memory by removing stale data. Add astore.ttl\nconfiguration to your langgraph.json\nfile:\nrefresh_on_read\n: (Optional, defaulttrue\n) Iftrue\n, accessing an item viaget\norsearch\nresets its expiration timer. Iffalse\n, TTL only refreshes onput\n.sweep_interval_minutes\n: (Optional) Defines how often, in minutes, the system checks for expired items. If omitted, no sweeping occurs.default_ttl\n: (Optional) Sets the default lifespan of store items in minutes (e.g., 10080 minutes = 7 days). Applies only to items created after this configuration is deployed; existing items are not changed. If you need to clear older items, delete them manually. If omitted, items do not expire by default.\nCombining TTL Configurations\nYou can configure TTLs for both checkpoints and store items in the samelanggraph.json\nfile to set different policies for each data type. Here is an example:\nRuntime Overrides\nThe defaultstore.ttl\nsettings from langgraph.json\ncan be overridden at runtime by providing specific TTL values in SDK method calls like get\n, put\n, and search\n.\nDeployment Process\nAfter configuring TTLs inlanggraph.json\n, deploy or restart your LangGraph application for the changes to take effect. Use langgraph dev\nfor local development or langgraph up\nfor Docker deployment.\nSee the langgraph.json CLI reference for more details on the other configurable options.", "tokens": 356, "node_type": "child"}
{"id": 44, "chunk_id": "c16acf8a553ac24f5d77e7d3a1c6e493", "parent_id": 39, "url": "", "namespace": "langchain", "title": "langsmith-control-plane", "headers": ["langsmith-control-plane"], "section_index": 0, "chunk_index": 0, "text": "# langsmith-control-plane\n\n> Source: https://docs.langchain.com/langsmith/control-plane\n\nControl plane UI\nFrom the control plane UI, you can:- View a list of outstanding deployments.\n- View details of an individual deployment.\n- Create a new deployment.\n- Update a deployment.\n- Update environment variables for a deployment.\n- View build and server logs of a deployment.\n- View deployment metrics such as CPU and memory usage.\n- Delete a deployment.\nControl plane API\nThis section describes the data model of the control plane API. The API is used to create, update, and delete deployments. See the control plane API reference for more details.Integrations\nAn integration is an abstraction for agit\nrepository provider (e.g. GitHub). It contains all of the required metadata needed to connect with and deploy from a git\nrepository.\nDeployments\nA deployment is an instance of a LangGraph Server. A single deployment can have many revisions.Revisions\nA revision is an iteration of a deployment. When a new deployment is created, an initial revision is automatically created. To deploy code changes or update secrets for a deployment, a new revision must be created.Listeners\nA listener is an instance of a \u201clistener\u201d application. A listener contains metadata about the application (e.g. version) and metadata about the compute infrastructure where it can deploy to (e.g. Kubernetes namespaces). The listener data model only applies for Hybrid and Self-Hosted deployments.Control Plane Features\nThis section describes various features of the control plane.Deployment Types\nFor simplicity, the control plane offers two deployment types with different resource allocations:Development\nand Production\n.\n| Deployment Type | CPU/Memory | Scaling | Database |\n|---|---|---|---|\n| Development | 1 CPU, 1 GB RAM | Up to 1 replica | 10 GB disk, no backups |\n| Production | 2 CPU, 2 GB RAM | Up to 10 replicas | Autoscaling disk, automatic backups, highly available (multi-zone configuration) |\nImmutable Deployment Type\nOnce a deployment is created, the deployment type cannot be changed.\nProduction\nProduction\ntype deployments are suitable for \u201cproduction\u201d workloads. For example, select Production\nfor customer-facing applications in the critical path.\nResources for Production\ntype deployments can be manually increased on a case-by-case basis depending on use case and capacity constraints. Contact support@langchain.dev to request an increase in resources.\nDevelopment\nDevelopment\ntype deployments are suitable development and testing. For example, select Development\nfor internal testing environments. Development\ntype deployments are not suitable for \u201cproduction\u201d workloads.\nPreemptible Compute Infrastructure\nDevelopment\ntype deployments (API server, queue server, and database) are provisioned on preemptible compute infrastructure. This means the compute infrastructure may be terminated at any time without notice. This may result in intermittent\u2026- Redis connection timeouts/errors\n- Postgres connection timeouts/errors\n- Failed or retrying background runs\nDevelopment\ntype deployment. By design, LangGraph Server is fault-tolerant. The implementation will automatically attempt to recover from Redis/Postgres connection errors and retry failed background runs.Production\ntype deployments are provisioned on durable compute infrastructure, not preemptible compute infrastructure.Development\ntype deployments can be manually increased on a case-by-case basis depending on use case and capacity constraints. For most use cases, TTLs should be configured to manage disk usage. Contact support@langchain.dev to request an increase in resources.\nDatabase provisioning\nThe control plane and data plane \u201clistener\u201d application coordinate to automatically create a Postgres database for each deployment. The database serves as the persistence layer for the deployment. When implementing a LangGraph application, a checkpointer does not need to be configured by the developer. Instead, a checkpointer is automatically configured for the graph. Any checkpointer configured for a graph will be replaced by the one that is automatically configured. There is no direct access to the database. All access to the database occurs through the LangGraph Server. The database is never deleted until the deployment itself is deleted.Asynchronous deployment\nInfrastructure for deployments and revisions are provisioned and deployed asynchronously. They are not deployed immediately after submission. Currently, deployment can take up to several minutes.- When a new deployment is created, a new database is created for the deployment. Database creation is a one-time step. This step contributes to a longer deployment time for the initial revision of the deployment.\n- When a subsequent revision is created for a deployment, there is no database creation step. The deployment time for a subsequent revision is significantly faster compared to the deployment time of the initial revision.\n- The deployment process for each revision contains a build step, which can take up to a few minutes.\nMonitoring\nAfter a deployment is ready, the control plane monitors the deployment and records various metrics, such as:- CPU and memory usage of the deployment.\n- Number of container restarts.\n- Number of replicas (this will increase with autoscaling).\n- PostgreSQL CPU, memory usage, and disk usage.\n- LangGraph Server queue pending/active run count.\n- LangGraph Server API success response count, error response count, and latency.\nLangSmith integration\nA LangSmith tracing project is automatically created for each deployment. The tracing project has the same name as the deployment. When creating a deployment, theLANGCHAIN_TRACING\nand LANGSMITH_API_KEY\n/LANGCHAIN_API_KEY\nenvironment variables do not need to be specified; they are set automatically by the control plane.\nWhen a deployment is deleted, the traces and the tracing project are not deleted.", "tokens": 858, "node_type": "child"}
{"id": 45, "chunk_id": "c16acf8a553ac24f5d77e7d3a1c6e493", "parent_id": 40, "url": "", "namespace": "langchain", "title": "langsmith-create-a-prompt", "headers": ["langsmith-create-a-prompt"], "section_index": 0, "chunk_index": 0, "text": "# langsmith-create-a-prompt\n\n> Source: https://docs.langchain.com/langsmith/create-a-prompt\n\nCompose your prompt\nOn the left is an editable view of the prompt. The prompt is made up of messages, each of which has a \u201crole\u201d - includingsystem\n, human\n, and ai\n.\nTemplate format\nThe default template format isf-string\n, but you can change the prompt template format to mustache\nby clicking on the settings icon next to the model -> prompt format -> template format. Learn more about template formats here.\nAdd a template variable\nThe power of prompts comes from the ability to use variables in your prompt. You can use variables to add dynamic content to your prompt. Add a template variable in one of two ways:-\nAdd\n{{variable_name}}\nto your prompt (with one curly brace on each side forf-string\nand two formustache\n). - Highlight text you want to templatize and click the tooltip button that shows up. Enter a name for your variable, and convert.\nStructured output\nAdding an output schema to your prompt will get output in a structured format. Learn more about structured output here.Tools\nYou can also add a tool by clicking the+ Tool\nbutton at the bottom of the prompt editor. See here for more information on how to use tools.\nRun the prompt\nClick \u201cStart\u201d to run the prompt.Save your prompt\nTo save your prompt, click the \u201cSave\u201d button, name your prompt, and decide if you want it to be \u201cprivate\u201d or \u201cpublic\u201d. Private prompts are only visible to your workspace, while public prompts are discoverable to anyone. The model and configuration you select in the Playground settings will be saved with the prompt. When you reopen the prompt, the model and configuration will automatically load from the saved version.The first time you create a public prompt, you\u2019ll be asked to set a LangChain Hub handle. All your public prompts will be linked to this handle. In a shared workspace, this handle will be set for the whole workspace.", "tokens": 326, "node_type": "child"}
{"id": 46, "chunk_id": "c16acf8a553ac24f5d77e7d3a1c6e493", "parent_id": 41, "url": "", "namespace": "langchain", "title": "langsmith-create-account-api-key", "headers": ["langsmith-create-account-api-key"], "section_index": 0, "chunk_index": 0, "text": "# langsmith-create-account-api-key\n\n> Source: https://docs.langchain.com/langsmith/create-account-api-key\n\nAPI keys\nLangSmith supports two types of API keys: Service Keys and Personal Access Tokens. Both types of tokens can be used to authenticate requests to the LangSmith API, but they have different use cases. Read more about the differences between Service Keys and Personal Access Tokens under admin conceptsCreate an API key\nTo log traces and run evaluations with LangSmith, you will need to create an API key to authenticate your requests. API keys can be scoped to a set of workspaces, or the entire organization. To create either type of API key head to the Settings page, then scroll to the API Keys section. For service keys, choose between an organization-scoped and workspace-scoped key. If the key is workspace-scoped, the workspaces must then be specified. Enterprise users are also able to assign specific roles to the key, which adjusts its permissions. Set the key\u2019s expiration; the key will become unusable after the number of days chosen, or never, if that is selected. Then click Create API Key.The API key will be shown only once, so make sure to copy it and store it in a safe place.\nConfigure the SDK\nYou may set the following environment variables in addition toLANGSMITH_API_KEY\n.\nThis is only required if using the EU instance.\nLANGSMITH_ENDPOINT=https://eu.api.smith.langchain.com\nThis is only required for keys scoped to more than one workspace.\nLANGSMITH_WORKSPACE_ID=<Workspace ID>", "tokens": 232, "node_type": "child"}
{"id": 47, "chunk_id": "c16acf8a553ac24f5d77e7d3a1c6e493", "parent_id": 42, "url": "", "namespace": "langchain", "title": "langsmith-create-few-shot-evaluators", "headers": ["langsmith-create-few-shot-evaluators"], "section_index": 0, "chunk_index": 0, "text": "# langsmith-create-few-shot-evaluators\n\n> Source: https://docs.langchain.com/langsmith/create-few-shot-evaluators\n\nHow few-shot examples work\n- Few-shot examples are added to your evaluator prompt using the\n{{Few-shot examples}}\nvariable - Creating an evaluator with few-shot examples, will automatically create a dataset for you, which will be auto-populated with few-shot examples once you start making corrections\n- At runtime, these examples will inserted into the evaluator to serve as a guide for its outputs - this will help the evaluator to better align with human preferences\nConfigure your evaluator\nFew-shot examples are not currently supported in LLM-as-a-judge evaluators that use the prompt hub and are only compatible with prompts that use mustache formatting.\n1. Configure variable mapping\nEach few-shot example is formatted according to the variable mapping specified in the configuration. The variable mapping for few-shot examples, should contain the same variables as your main prompt, plus afew_shot_explanation\nand a score\nvariable which should have the same name as your feedback key.\nFor example, if your main prompt has variables question\nand response\n, and your evaluator outputs a correctness\nscore, then your few-shot prompt should have the vartiables question\n, response\n, few_shot_explanation\n, and correctness\n.\n2. Specify the number of few-shot examples to use\nYou may also specify the number of few-shot examples to use. The default is 5. If your examples are very long, you may want to set this number lower to save tokens - whereas if your examples tend to be short, you can set a higher number in order to give your evaluator more examples to learn from. If you have more examples in your dataset than this number, we will randomly choose them for you.Make corrections\nAs you start logging traces or running experiments, you will likely disagree with some of the scores that your evaluator has given. When you make corrections to these scores, you will begin seeing examples populated inside your corrections dataset. As you make corrections, make sure to attach explanations - these will get populated into your evaluator prompt in place of thefew_shot_explanation\nvariable.\nThe inputs to the few-shot examples will be the relevant fields from the inputs, outputs, and reference (if this an offline evaluator) of your chain/dataset. The outputs will be the corrected evaluator score and the explanations that you created when you left the corrections. Feel free to edit these to your liking. Here is an example of a few-shot example in a corrections dataset:\nNote that the corrections may take a minute or two to be populated into your few-shot dataset. Once they are there, future runs of your evaluator will include them in the prompt!\nView your corrections dataset\nIn order to view your corrections dataset:- Online evaluators: Select your run rule and click Edit Rule\n- Offline evaluators: Select your evaluator and click Edit Evaluator", "tokens": 466, "node_type": "child"}
{"id": 48, "chunk_id": "c16acf8a553ac24f5d77e7d3a1c6e493", "parent_id": 43, "url": "", "namespace": "langchain", "title": "langsmith-cron-jobs", "headers": ["langsmith-cron-jobs"], "section_index": 0, "chunk_index": 0, "text": "# langsmith-cron-jobs\n\n> Source: https://docs.langchain.com/langsmith/cron-jobs\n\nThere are many situations in which it is useful to run an assistant on a schedule.For example, say that you\u2019re building an assistant that runs daily and sends an email summary\nof the day\u2019s news. You could use a cron job to run the assistant every day at 8:00 PM.LangSmith Deployment supports cron jobs, which run on a user-defined schedule. The user specifies a schedule, an assistant, and some input. After that, on the specified schedule, the server will:\nCreate a new thread with the specified assistant\nSend the specified input to that thread\nNote that this sends the same input to the thread every time.The LangGraph Server API provides several endpoints for creating and managing cron jobs. See the API reference for more details.Sometimes you don\u2019t want to run your graph based on user interaction, but rather you would like to schedule your graph to run on a schedule - for example if you wish for your graph to compose and send out a weekly email of to-dos for your team. LangSmith Deployment allows you to do this without having to write your own script by using the Crons client. To schedule a graph job, you need to pass a cron expression to inform the client when you want to run the graph. Cron jobs are run in the background and do not interfere with normal invocations of the graph.\nFirst, let\u2019s set up our SDK client, assistant, and thread:\nPython\nJavascript\nCURL\nCopy\nfrom langgraph_sdk import get_clientclient = get_client(url=<DEPLOYMENT_URL>)# Using the graph deployed with the name \"agent\"assistant_id = \"agent\"# create threadthread = await client.threads.create()print(thread)\nTo create a cron job associated with a specific thread, you can write:\nPython\nJavascript\nCURL\nCopy\n# This schedules a job to run at 15:27 (3:27PM) every daycron_job = await client.crons.create_for_thread( thread[\"thread_id\"], assistant_id, schedule=\"27 15 * * *\", input={\"messages\": [{\"role\": \"user\", \"content\": \"What time is it?\"}]},)\nNote that it is very important to delete Cron jobs that are no longer useful. Otherwise you could rack up unwanted API charges to the LLM! You can delete a Cron job using the following code:\nYou can also create stateless cron jobs by using the following code:\nPython\nJavascript\nCURL\nCopy\n# This schedules a job to run at 15:27 (3:27PM) every daycron_job_stateless = await client.crons.create( assistant_id, schedule=\"27 15 * * *\", input={\"messages\": [{\"role\": \"user\", \"content\": \"What time is it?\"}]},)\nAgain, remember to delete your job once you are done with it!", "tokens": 411, "node_type": "child"}
{"id": 49, "chunk_id": "c16acf8a553ac24f5d77e7d3a1c6e493", "parent_id": 44, "url": "", "namespace": "langchain", "title": "langsmith-custom-auth", "headers": ["langsmith-custom-auth"], "section_index": 0, "chunk_index": 0, "text": "# langsmith-custom-auth\n\n> Source: https://docs.langchain.com/langsmith/custom-auth\n\nAdd custom authentication to your deployment\nTo leverage custom authentication and access user-level metadata in your deployments, set up custom authentication to automatically populate theconfig[\"configurable\"][\"langgraph_auth_user\"]\nobject through a custom authentication handler. You can then access this object in your graph with the langgraph_auth_user\nkey to allow an agent to perform authenticated actions on behalf of the user.\n-\nImplement authentication:\nWithout a custom\n@auth.authenticate\nhandler, LangGraph sees only the API-key owner (usually the developer), so requests aren\u2019t scoped to individual end-users. To propagate custom tokens, you must implement your own handler.\n- This handler receives the request (headers, etc.), validates the user, and returns a dictionary with at least an identity field.\n- You can add any custom fields you want (e.g., OAuth tokens, roles, org IDs, etc.).\n-\nIn your\nlanggraph.json\n, add the path to your auth file: -\nOnce you\u2019ve set up authentication in your server, requests must include the required authorization information based on your chosen scheme. Assuming you are using JWT token authentication, you could access your deployments using any of the following methods:\nFor more details on RemoteGraph, refer to the Use RemoteGraph guide.\n- Python Client\n- Python RemoteGraph\n- JavaScript Client\n- JavaScript RemoteGraph\n- CURL\nEnable agent authentication\nAfter authentication, the platform creates a special configuration object (config\n) that is passed to LangSmith deployment. This object contains information about the current user, including any custom fields you return from your @auth.authenticate\nhandler.\nTo allow an agent to perform authenticated actions on behalf of the user, access this object in your graph with the langgraph_auth_user\nkey:\nFetch user credentials from a secure secret store. Storing secrets in graph state is not recommended.\nAuthorizing a user for Studio\nBy default, if you add custom authorization on your resources, this will also apply to interactions made from Studio. If you want, you can handle logged-in Studio users differently by checking is_studio_user().is_studio_user\nwas added in version 0.1.73 of the langgraph-sdk. If you\u2019re on an older version, you can still check whether isinstance(ctx.user, StudioUser)\n.", "tokens": 345, "node_type": "child"}
{"id": 50, "chunk_id": "c16acf8a553ac24f5d77e7d3a1c6e493", "parent_id": 46, "url": "", "namespace": "langchain", "title": "langsmith-custom-endpoint", "headers": ["langsmith-custom-endpoint"], "section_index": 0, "chunk_index": 0, "text": "# langsmith-custom-endpoint\n\n> Source: https://docs.langchain.com/langsmith/custom-endpoint\n\nThe LangSmith playground allows you to use your own custom models. You can deploy a model server that exposes your model\u2019s API via , an open source library for serving LangChain applications. Behind the scenes, the playground will interact with your model server to generate responses.\nFor your convenience, we have provided a sample model server that you can use as a reference. You can find the sample model server here We highly recommend using the sample model server as a starting point.Depending on your model is an instruct-style or chat-style model, you will need to implement either custom_model.py or custom_chat_model.py respectively.\nIt is often useful to configure your model with different parameters. These might include temperature, model_name, max_tokens, etc.To make your model configurable in the LangSmith playground, you need to add configurable fields to your model server. These fields can be used to change model parameters from the playground.You can add configurable fields by implementing the with_configurable_fields function in the config.py file. You can\nCopy\ndef with_configurable_fields(self) -> Runnable: \"\"\"Expose fields you want to be configurable in the playground. We will automatically expose these to the playground. If you don't want to expose any fields, you can remove this method.\"\"\" return self.configurable_fields(n=ConfigurableField( id=\"n\", name=\"Num Characters\", description=\"Number of characters to return from the input prompt.\", ))\nOnce you have deployed a model server, you can use it in the LangSmith Playground. Enter the playground and select either the ChatCustomModel or the CustomModel provider for chat-style model or instruct-style models.Enter the URL. The playground will automatically detect the available endpoints and configurable fields. You can then invoke the model with the desired parameters.If everything is set up correctly, you should see the model\u2019s response in the playground as well as the configurable fields specified in the with_configurable_fields.See how to store your model configuration for later use here.", "tokens": 311, "node_type": "child"}
{"id": 51, "chunk_id": "c16acf8a553ac24f5d77e7d3a1c6e493", "parent_id": 47, "url": "", "namespace": "langchain", "title": "langsmith-custom-lifespan", "headers": ["langsmith-custom-lifespan"], "section_index": 0, "chunk_index": 0, "text": "# langsmith-custom-lifespan\n\n> Source: https://docs.langchain.com/langsmith/custom-lifespan\n\nWhen deploying agents to LangSmith, you often need to initialize resources like database connections when your server starts up, and ensure they\u2019re properly closed when it shuts down. Lifespan events let you hook into your server\u2019s startup and shutdown sequence to handle these critical setup and teardown tasks.\nThis works the same way as adding custom routes. You just need to provide your own Starlette\napp (including FastAPI\n, FastHTML\nand other compatible apps).\nBelow is an example using FastAPI.\n\u201cPython only\u201d\nWe currently only support custom lifespan events in Python deployments with langgraph-api>=0.0.26\n.\nCreate app\nStarting from an existing LangSmith application, add the following lifespan code to your webapp.py\nfile. If you are starting from scratch, you can create a new app from a template using the CLI.\nOnce you have a LangGraph project, add the following app code:\nAdd the following to your langgraph.json\nconfiguration file. Make sure the path points to the webapp.py\nfile you created above.\nStart server\nTest the server out locally:\nYou should see your startup message printed when the server starts, and your cleanup message when you stop it with Ctrl+C\n.\nDeploying\nYou can deploy your app as-is to cloud or to your self-hosted platform.\nNext steps\nNow that you\u2019ve added lifespan events to your deployment, you can use similar techniques to add custom routes or custom middleware to further customize your server\u2019s behavior.", "tokens": 238, "node_type": "child"}
{"id": 52, "chunk_id": "c16acf8a553ac24f5d77e7d3a1c6e493", "parent_id": 48, "url": "", "namespace": "langchain", "title": "langsmith-custom-middleware", "headers": ["langsmith-custom-middleware"], "section_index": 0, "chunk_index": 0, "text": "# langsmith-custom-middleware\n\n> Source: https://docs.langchain.com/langsmith/custom-middleware\n\nStarlette\napp (including FastAPI\n, FastHTML\nand other compatible apps).\nAdding middleware lets you intercept and modify requests and responses globally across your deployment, whether they\u2019re hitting your custom endpoints or the built-in LangSmith APIs.\nBelow is an example using FastAPI.\n\u201cPython only\u201d\nWe currently only support custom middleware in Python deployments with\nlanggraph-api>=0.0.26\n.Create app\nStarting from an existing LangSmith application, add the following middleware code to yourwebapp.py\nfile. If you are starting from scratch, you can create a new app from a template using the CLI.\nConfigure langgraph.json\nAdd the following to your langgraph.json\nconfiguration file. Make sure the path points to the webapp.py\nfile you created above.\nCustomize middleware ordering\nBy default, custom middleware runs before authentication logic. To run custom middleware after authentication, setmiddleware_order\nto auth_first\nin your http\nconfiguration. (This customization is supported starting with API server v0.4.35 and later.)\nStart server\nTest the server out locally:X-Custom-Header\nin its response.", "tokens": 160, "node_type": "child"}
{"id": 53, "chunk_id": "c16acf8a553ac24f5d77e7d3a1c6e493", "parent_id": 49, "url": "", "namespace": "langchain", "title": "langsmith-custom-openai-compliant-model", "headers": ["langsmith-custom-openai-compliant-model"], "section_index": 0, "chunk_index": 0, "text": "# langsmith-custom-openai-compliant-model\n\n> Source: https://docs.langchain.com/langsmith/custom-openai-compliant-model\n\nConnect to an OpenAI compliant model provider/proxy\nThe LangSmith playground allows you to use any model that is compliant with the OpenAI API. You can utilize your model by setting the Proxy Provider for in the playground.\nYou can use these providers to deploy your model and get an API endpoint that is compliant with the OpenAI API.Take a look at the full specification for more information.\nOnce you have deployed a model server, you can use it in the LangSmith Playground.To access the Prompt Settings menu:\nUnder the Prompts heading select the gear icon next to the model name.\nIn the Model Configuration tab, select the model to edit in the dropdown.\nFor the Provider dropdown, select OpenAI Compatible Endpoint.\nAdd your OpenAI Compatible Endpoint to the Base URL input.\nIf everything is set up correctly, you should see the model\u2019s response in the playground. You can also use this functionality to invoke downstream pipelines as well.For information on how to store your model configuration , refer to Configure prompt settings.", "tokens": 177, "node_type": "child"}
{"id": 54, "chunk_id": "c16acf8a553ac24f5d77e7d3a1c6e493", "parent_id": 50, "url": "", "namespace": "langchain", "title": "langsmith-custom-routes", "headers": ["langsmith-custom-routes"], "section_index": 0, "chunk_index": 0, "text": "# langsmith-custom-routes\n\n> Source: https://docs.langchain.com/langsmith/custom-routes\n\nStarlette\napp (including FastAPI\n, FastHTML\nand other compatible apps). You make LangSmith aware of this by providing a path to the app in your langgraph.json\nconfiguration file.\nDefining a custom app object lets you add any routes you\u2019d like, so you can do anything from adding a /login\nendpoint to writing an entire full-stack web-app, all deployed in a single LangGraph Server.\nBelow is an example using FastAPI.\nCreate app\nStarting from an existing LangSmith application, add the following custom route code to yourwebapp.py\nfile. If you are starting from scratch, you can create a new app from a template using the CLI.\nConfigure langgraph.json\nAdd the following to your langgraph.json\nconfiguration file. Make sure the path points to the FastAPI application instance app\nin the webapp.py\nfile you created above.\nStart server\nTest the server out locally:localhost:2024/hello\nin your browser (2024\nis the default development port), you should see the /hello\nendpoint returning {\"Hello\": \"World\"}\n.\nShadowing default endpoints\nThe routes you create in the app are given priority over the system defaults, meaning you can shadow and redefine the behavior of any default endpoint.", "tokens": 192, "node_type": "child"}
{"id": 55, "chunk_id": "c16acf8a553ac24f5d77e7d3a1c6e493", "parent_id": 51, "url": "", "namespace": "langchain", "title": "langsmith-dashboards", "headers": ["langsmith-dashboards"], "section_index": 0, "chunk_index": 0, "text": "# langsmith-dashboards > Source: https://docs.langchain.com/langsmith/dashboards - Prebuilt dashboards: Automatically generated for every tracing project. - Custom dashboards: Fully configurable collections of charts tailored to your needs. Prebuilt dashboards Prebuilt dashboards are created automatically for each project and cover essential metrics, such as trace count, error rates, token usage, and more. By default, the prebuilt dashboard for your tracing project can be accessed using the Dashboard button on the top right of the tracing project page.You cannot modify a prebuilt dashboard. In the future, we plan to allow you to clone a default dashboard in order to have a starting point to customize it. Dashboard sections Prebuilt dashboards are broken down into the following sections:| Section | What it shows | |---|---| | Traces | Trace count, latency and error rates. A trace is a collection of runs related to a single operation. For example, if a user request triggers an agent, all runs for that agent invocation would be part of the same trace. | | LLM Calls | LLM call count and latency. Includes all runs where run type is \u201cllm\u201d. | | Cost & Tokens | Total and per-trace token counts and costs, broken down by token type. Costs are measured using LangSmith\u2019s cost tracking. | | Tools | Run counts, error rates, and latency stats for tool runs broken down by tool name. Includes runs where run type is \u201ctool\u201d. Limits to top 5 most frequently occurring tools. | | Run Types | Run counts, error rates, and latency stats for runs that are immediate children of the root run. This helps in understanding the high-level execution path of agents. Limits to top 5 most frequently occurring run names. Refer to the image following this table. | | Feedback Scores | Aggregate stats for the top 5 most frequently occurring types of feedback. Charts show average score for numerical feedback and category counts for categorical feedback. | Group by Group by run tag or metadata can be used to split data over attributes that are important to your application. The global group by setting appears on the top right hand side of the dashboard. Note that the Tool and Run Type charts already have a group by applied, so the global group by won\u2019t take effect; the global group by will apply to all other charts.When adding metadata to runs, we recommend having the same metadata on the trace, as well as the specific run (e.g. LLM call). Metadata and tags are not propagated from parent to child runs, or vice versa. So, if you want to see e.g. both your trace charts and your LLM call charts grouped on some metadata key then both your traces (root runs) and your LLM runs need to have that metadata attached. Custom Dashboards Create tailored collections of charts for tracking metrics that matter most for your application.Creating a new dashboard - Navigate to the Monitor tab in the left sidebar. - Click on the + New Dashboard button. - Give your dashboard a name and a description. - Click on Create. Adding charts to your dashboard - Within a dashboard, click on the + New Chart button to open up the chart creation pane. - Give your chart a name and a description. - Configure the chart. Chart configuration Select tracing projects and filter runs - Select one or more tracing projects to track metrics for. - Use the Chart filters section to refine the matching runs. This filter applies to all data series in the chart. For more information on filtering traces, view our guide on filtering traces in application. Pick a metric - Choose a metric from the dropdown menu to set the y-axis of your chart. With a project and a metric selected, you\u2019ll see a preview of your chart and the matching runs. - For certain metrics (such as latency, token usage, cost), we support comparing multiple metrics with the same unit. For example, you may want one chart where you can see prompt tokens and completion tokens. Each metric appears as a separate line. Split the data There are two ways to create multiple series in a chart (i.e. create multiple lines in a chart):- Group by: Group runs by run tag or metadata, run name, or run type. Group by automatically splits the data into multiple series based on the field selected. Note that group by is limited to the top 5 elements by frequency. - Data series: Manually define multiple series with individual filters. This is useful for comparing granular data within a single metric. Pick a chart type - Choose between a line chart and a bar chart for visualizing Save and manage charts - Click Save to save your chart to the dashboard. - Edit or delete a chart by clicking the triple dot button in the top right of the chart. - Clone a chart by clicking the triple line button in the top right of the chart and selecting + Clone. This will open a new chart creation pane with the same configurations as the original. Linking to a dashboard from a tracing project You can link to any dashboard directly from a tracing project. By default, the prebuilt dashboard for your tracing project is selected. If you have a custom dashboard that you would like to link instead:- In your tracing project, click the three dots next to the Dashboard button. - Choose a dashboard to set as the new default. Example: user-journey monitoring Use monitoring charts for mapping the decisions made by an agent at a particular node. Consider an email assistant agent. At a particular node it makes a decision about an email to:- send an email back - notify the user - no response needed - Metric Selection: Select the metric Run count . - Chart Filters: Add a tree filter to include all of the traces with name triage_input . This means we only include traces that hit thetriage_input node. Also add a", "tokens": 1000, "node_type": "child"}
{"id": 56, "chunk_id": "9b0fd3b850698317bef57fa6e71a38ba", "parent_id": 51, "url": "", "namespace": "langchain", "title": "langsmith-dashboards", "headers": ["langsmith-dashboards"], "section_index": 0, "chunk_index": 1, "text": "a dashboard from a tracing project You can link to any dashboard directly from a tracing project. By default, the prebuilt dashboard for your tracing project is selected. If you have a custom dashboard that you would like to link instead:- In your tracing project, click the three dots next to the Dashboard button. - Choose a dashboard to set as the new default. Example: user-journey monitoring Use monitoring charts for mapping the decisions made by an agent at a particular node. Consider an email assistant agent. At a particular node it makes a decision about an email to:- send an email back - notify the user - no response needed - Metric Selection: Select the metric Run count . - Chart Filters: Add a tree filter to include all of the traces with name triage_input . This means we only include traces that hit thetriage_input node. Also add a chart filter forIs Root istrue , so our count is not inflated by the number of nodes in the trace. - Data Series: Create a data series for each decision made at the triage_input node. The output of the decision is stored in thetriage.response field of the output object, and the value of the decision is eitherno ,email , ornotify . Each of these decisions generates a separate data series in the chart. triage_input node over time.", "tokens": 227, "node_type": "child"}
{"id": 57, "chunk_id": "c16acf8a553ac24f5d77e7d3a1c6e493", "parent_id": 52, "url": "", "namespace": "langchain", "title": "langsmith-data-export", "headers": ["langsmith-data-export"], "section_index": 0, "chunk_index": 0, "text": "# langsmith-data-export > Source: https://docs.langchain.com/langsmith/data-export LangSmith\u2019s bulk data export functionality allows you to export your traces into an external destination. This can be useful if you want to analyze the data offline in a tool such as BigQuery, Snowflake, RedShift, Jupyter Notebooks, etc. An export can be launched to target a specific LangSmith project and date range. Once a batch export is launched, our system will handle the orchestration and resilience of the export process. Please note that exporting your data may take some time depending on the size of your data. We also have a limit on how many of your exports can run at the same time. Bulk exports also have a runtime timeout of 24 hours. Destinations Currently we support exporting to an S3 bucket or S3 API compatible bucket that you provide. The data will be exported in Parquet columnar format. This format will allow you to easily import the data into other systems. The data export will contain equivalent data fields as the Run data format.Exporting Data Destinations - Providing a S3 bucket To export LangSmith data, you will need to provide an S3 bucket where the data will be exported to. The following information is needed for the export:- Bucket Name: The name of the S3 bucket where the data will be exported to. - Prefix: The root prefix within the bucket where the data will be exported to. - S3 Region: The region of the bucket - this is needed for AWS S3 buckets. - Endpoint URL: The endpoint URL for the S3 bucket - this is needed for S3 API compatible buckets. - Access Key: The access key for the S3 bucket. - Secret Key: The secret key for the S3 bucket. Preparing the Destination For self-hosted and EU region deploymentsUpdate the LangSmith URL appropriately for self-hosted installations or organizations in the EU region in the requests below. For the EU region, use eu.api.smith.langchain.com .Permissions requiredBoth the backend and queue services require write access to the destination bucket:- The backend service attempts to write a test file to the destination bucket when the export destination is created. It will delete the test file if it has permission to do so (delete access is optional). - The queue service is responsible for bulk export execution and uploading the files to the bucket. id to reference this destination in subsequent bulk export operations. If you receive an error while creating a destination, see debug destination errors for details on how to debug this. Credentials configuration Requires LangSmith Helm version >= 0.10.34 (application version >= 0.10.91 )access_key_id and secret_access_key : - To use temporary credentials that include an AWS session token, additionally provide the credentials.session_token key when creating the bulk export destination. - (Self-hosted only): To use environment-based credentials such as with AWS IAM Roles for Service Accounts (IRSA), omit the credentials key from the request when creating the bulk export destination. In this case, the standard Boto3 credentials locations will be checked in the order defined by the library. AWS S3 bucket For AWS S3, you can leave off theendpoint_url and supply the region that matches the region of your bucket. Google GCS XML S3 compatible bucket When using Google\u2019s GCS bucket, you need to use the XML S3 compatible API, and supply theendpoint_url which is typically https://storage.googleapis.com . Here is an example of the API request when using the GCS XML API which is compatible with S3: Create an export job To export data, you will need to create an export job. This job will specify the destination, the project, the date range, and filter expression of the data to export. The filter expression is used to narrow down the set of runs exported and is optional. Not setting the filter field will export all runs. Refer to our filter query language and examples to determine the correct filter expression for your export. You can use the following cURL command to create the job:The session_id is also known as the Tracing Project ID, which can be copied from the individual project view by clicking into the project in the Tracing Projects list.id to reference this export in subsequent bulk export operations. Scheduled exports Requires LangSmith Helm version >= 0.10.42 (application version >= 0.10.109 )interval_hours and remove end_time : interval_hours must be between 1 hour and 168 hours (1 week) inclusive.- For spawned exports, the first time range exported is start_time=(scheduled_export_start_time), end_time=(start_time + interval_hours) . Thenstart_time=(previous_export_end_time), end_time=(this_export_start_time + interval_hours) , and so on. end_time must be omitted for scheduled exports.end_time is still required for non-scheduled exports.- Scheduled exports can be stopped by cancelling the export. - Exports that have been spawned by a scheduled export have the source_bulk_export_id attribute filled. - If desired, these spawned bulk exports must be canceled separately from the source scheduled bulk export - canceling the source bulk export does not cancel the spawned bulk exports. - Exports that have been spawned by a scheduled export have the - Spawned exports run at end_time + 10 minutes to account for any runs that are submitted withend_time in the recent past. start_time=2025-07-16T00:00:00Z and interval_hours=6 : | Export | Start Time | End Time | Runs At | |---|---|---|---| | 1 | 2025-07-16T00:00:00Z | 2025-07-16T06:00:00Z | 2025-07-16T06:10:00Z | | 2 | 2025-07-16T06:00:00Z | 2025-07-16T12:00:00Z | 2025-07-16T12:10:00Z | | 3 | 2025-07-16T12:00:00Z | 2025-07-16T18:00:00Z | 2025-07-16T18:10:00Z | Monitoring the Export Job Monitor Export Status To monitor the status of an export job, use the following cURL command:{export_id} with the ID of the export you want to monitor. This command retrieves the current status of the specified export job. List Runs for an Export An export is typically broken up into multiple runs which correspond to a specific date partition to export. To list all runs associated with a specific export, use the following cURL command:List All Exports To retrieve a list of all export jobs, use the following cURL command:Stop an Export To stop an existing export, use the following cURL command:{export_id} with", "tokens": 1000, "node_type": "child"}
{"id": 58, "chunk_id": "9b0fd3b850698317bef57fa6e71a38ba", "parent_id": 52, "url": "", "namespace": "langchain", "title": "langsmith-data-export", "headers": ["langsmith-data-export"], "section_index": 0, "chunk_index": 1, "text": "start_time=2025-07-16T00:00:00Z and interval_hours=6 : | Export | Start Time | End Time | Runs At | |---|---|---|---| | 1 | 2025-07-16T00:00:00Z | 2025-07-16T06:00:00Z | 2025-07-16T06:10:00Z | | 2 | 2025-07-16T06:00:00Z | 2025-07-16T12:00:00Z | 2025-07-16T12:10:00Z | | 3 | 2025-07-16T12:00:00Z | 2025-07-16T18:00:00Z | 2025-07-16T18:10:00Z | Monitoring the Export Job Monitor Export Status To monitor the status of an export job, use the following cURL command:{export_id} with the ID of the export you want to monitor. This command retrieves the current status of the specified export job. List Runs for an Export An export is typically broken up into multiple runs which correspond to a specific date partition to export. To list all runs associated with a specific export, use the following cURL command:List All Exports To retrieve a list of all export jobs, use the following cURL command:Stop an Export To stop an existing export, use the following cURL command:{export_id} with the ID of the export you wish to cancel. Note that a job cannot be restarted once it has been cancelled, you will need to create a new export job instead. Partitioning Scheme Data will be exported into your bucket into the follow Hive partitioned format:Importing Data into other systems Importing data from S3 and Parquet format is commonly supported by the majority of analytical systems. See below for documentation links:BigQuery To import your data into BigQuery, see Loading Data from Parquet and also Hive Partitioned loads.Snowflake You can load data into Snowflake from S3 by following the Load from Cloud Document.RedShift You can COPY data from S3 / Parquet into RedShift by following the AWS COPY Instructions.Clickhouse You can directly query data in S3 / Parquet format in Clickhouse. As an example, if using GCS, you can query the data as follows:DuckDB You can query the data from S3 in-memory with SQL using DuckDB. See S3 import Documentation.Error Handling Debugging Destination Errors The destinations API endpoint will validate that the destination and credentials are valid and that write access is is present for the bucket. If you receive an error, and would like to debug this error, you can use the AWS CLI to test the connectivity to the bucket. You should be able to write a file with the CLI using the same data that you supplied to the destinations API above. AWS S3:--endpoint-url option. For GCS, the endpoint_url is typically https://storage.googleapis.com : Monitoring Runs You can monitor your runs using the List Runs API. If this is a known error, this will be added to theerrors field of the run. Common Errors Here are some common errors:| Error | Description | |---|---| | Access denied | The blob store credentials or bucket are not valid. This error occurs when the provided access key and secret key combination doesn\u2019t have the necessary permissions to access the specified bucket or perform the required operations. | | Bucket is not valid | The specified blob store bucket is not valid. This error is thrown when the bucket doesn\u2019t exist or there is not enough access to perform writes on the bucket. | | Key ID you provided does not exist | The blob store credentials provided are not valid. This error occurs when the access key ID used for authentication is not a valid key. | | Invalid endpoint | The endpoint_url provided is invalid. This error is raised when the specified endpoint is an invalid endpoint. Only S3 compatible endpoints are supported, for example https://storage.googleapis.com for GCS, https://play.min.io for minio, etc. If using AWS, you should omit the endpoint_url. |", "tokens": 591, "node_type": "child"}
{"id": 59, "chunk_id": "c16acf8a553ac24f5d77e7d3a1c6e493", "parent_id": 53, "url": "", "namespace": "langchain", "title": "langsmith-data-plane", "headers": ["langsmith-data-plane"], "section_index": 0, "chunk_index": 0, "text": "# langsmith-data-plane\n\n> Source: https://docs.langchain.com/langsmith/data-plane\n\nServer infrastructure\nIn addition to the LangGraph Server itself, the following infrastructure components for each server are also included in the broad definition of \u201cdata plane\u201d:- PostgreSQL: persistence layer for user, run, and memory data.\n- Redis: communication and ephemeral metadata for workers.\n- Secrets store: secure management of environment secrets.\n- Autoscalers: scale server containers based on load.\n\u201dListener\u201d application\nThe data plane \u201clistener\u201d application periodically calls control plane APIs to:- Determine if new deployments should be created.\n- Determine if existing deployments should be updated (i.e. new revisions).\n- Determine if existing deployments should be deleted.\nPostgreSQL\nPostgreSQL is the persistence layer for all user, run, and long-term memory data in a LangGraph Server. This stores both checkpoints (see more info here), server resources (threads, runs, assistants and crons), as well as items saved in the long-term memory store (see more info here).Redis\nRedis is used in each LangGraph Server as a way for server and queue workers to communicate, and to store ephemeral metadata. No user or run data is stored in Redis.Communication\nAll runs in a LangGraph Server are executed by a pool of background workers that are part of each deployment. In order to enable some features for those runs (such as cancellation and output streaming) we need a channel for two-way communication between the server and the worker handling a particular run. We use Redis to organize that communication.- A Redis list is used as a mechanism to wake up a worker as soon as a new run is created. Only a sentinel value is stored in this list, no actual run information. The run information is then retrieved from PostgreSQL by the worker.\n- A combination of a Redis string and Redis PubSub channel is used for the server to communicate a run cancellation request to the appropriate worker.\n- A Redis PubSub channel is used by the worker to broadcast streaming output from an agent while the run is being handled. Any open\n/stream\nrequest in the server will subscribe to that channel and forward any events to the response as they arrive. No events are stored in Redis at any time.\nEphemeral metadata\nRuns in a LangGraph Server may be retried for specific failures (currently only for transient PostgreSQL errors encountered during the run). In order to limit the number of retries (currently limited to 3 attempts per run) we record the attempt number in a Redis string when it is picked up. This contains no run-specific info other than its ID, and expires after a short delay.Data plane features\nThis section describes various features of the data plane.Data region\nDeployments can be created in 2 data regions: US and EU The data region for a deployment is implied by the data region of the LangSmith organization where the deployment is created. Deployments and the underlying database for the deployments cannot be migrated between data regions.Autoscaling\nProduction\ntype deployments automatically scale up to 10 containers. Scaling is based on 3 metrics:\n- CPU utilization\n- Memory utilization\n- Number of pending (in progress) runs\nStatic IP addresses\nAll traffic from deployments created after January 6th 2025 will come through a NAT gateway. This NAT gateway will have several static IP addresses depending on the data region. Refer to the table below for the list of static IP addresses:| US | EU |\n|---|---|\n| 35.197.29.146 | 34.13.192.67 |\n| 34.145.102.123 | 34.147.105.64 |\n| 34.169.45.153 | 34.90.22.166 |\n| 34.82.222.17 | 34.147.36.213 |\n| 35.227.171.135 | 34.32.137.113 |\n| 34.169.88.30 | 34.91.238.184 |\n| 34.19.93.202 | 35.204.101.241 |\n| 34.19.34.50 | 35.204.48.32 |\n| 34.59.244.194 | |\n| 34.9.99.224 | |\n| 34.68.27.146 | |\n| 34.41.178.137 | |\n| 34.123.151.210 | |\n| 34.135.61.140 | |\n| 34.121.166.52 | |\n| 34.31.121.70 |\nCustom PostgreSQL\nA custom PostgreSQL instance can be used instead of the one automatically created by the control plane. Specify thePOSTGRES_URI_CUSTOM\nenvironment variable to use a custom PostgreSQL instance.\nMultiple deployments can share the same PostgreSQL instance. For example, for Deployment A\n, POSTGRES_URI_CUSTOM\ncan be set to postgres://<user>:<password>@/<database_name_1>?host=<hostname_1>\nand for Deployment B\n, POSTGRES_URI_CUSTOM\ncan be set to postgres://<user>:<password>@/<database_name_2>?host=<hostname_1>\n. <database_name_1>\nand database_name_2\nare different databases within the same instance, but <hostname_1>\nis shared. The same database cannot be used for separate deployments.\nCustom Redis\nA custom Redis instance can be used instead of the one automatically created by the control plane. Specify the REDIS_URI_CUSTOM environment variable to use a custom Redis instance. Multiple deployments can share the same Redis instance. For example, forDeployment A\n, REDIS_URI_CUSTOM\ncan be set to redis://<hostname_1>:<port>/1\nand for Deployment B\n, REDIS_URI_CUSTOM\ncan be set to redis://<hostname_1>:<port>/2\n. 1\nand 2\nare different database numbers within the same instance, but <hostname_1>\nis shared. The same database number cannot be used for separate deployments.\nLangSmith tracing\nLangGraph Server is automatically configured to send traces to LangSmith. See the table below for details with respect to each deployment option.| Cloud | Hybrid | Self-Hosted |\n|---|---|---|\n| Required Trace to LangSmith SaaS. | Optional Disable tracing or trace to LangSmith SaaS. | Optional Disable tracing, trace to LangSmith SaaS, or trace to Self-Hosted LangSmith. |\nTelemetry\nLangGraph Server is automatically configured to report telemetry metadata for billing purposes. See the table below for details with respect to each deployment option.| Cloud | Hybrid | Self-Hosted |\n|---|---|---|\n| Telemetry sent to LangSmith SaaS. | Telemetry sent to LangSmith SaaS. | Self-reported usage (audit) for air-gapped license key. Telemetry sent to LangSmith SaaS for LangSmith License Key. |\nLicensing\nLangGraph Server is automatically configured to perform license key validation. See the table below for details with respect to each deployment option.| Cloud | Hybrid | Self-Hosted |\n|---|---|---|\n| LangSmith API Key validated against LangSmith SaaS. | LangSmith API Key validated against LangSmith SaaS. | Air-gapped license key or Platform License Key validated against LangSmith SaaS. |", "tokens": 989, "node_type": "child"}
{"id": 60, "chunk_id": "c16acf8a553ac24f5d77e7d3a1c6e493", "parent_id": 54, "url": "", "namespace": "langchain", "title": "langsmith-data-purging-compliance", "headers": ["langsmith-data-purging-compliance"], "section_index": 0, "chunk_index": 0, "text": "# langsmith-data-purging-compliance\n\n> Source: https://docs.langchain.com/langsmith/data-purging-compliance\n\nData retention\nLangSmith provides automatic data retention capabilities to help with compliance and storage management. Data retention policies can be configured at the organization and project levels. For detailed information about data retention configuration and management, please refer to the Data Retention concepts documentation.Trace deletes\nYou can use the API to complete trace deletes. The API supports two methods for deleting traces:- By trace IDs and session ID: Delete specific traces by providing a list of trace IDs and their corresponding session ID (up to 1000 traces per request)\n- By metadata: Delete traces across a workspace that match any of the specified metadata key-value pairs\nAll trace deletions will delete related entities like feedbacks, aggregations, and stats across all data storages.\nDeletion timeline\nTrace deletions are processed during non-peak usage times and are not instant, usually within a few hours. There is no confirmation of deletion - you\u2019ll need to query the data again to verify it has been removed.Delete specific traces\nTo delete specific traces by their trace IDs from a single session:Delete by metadata\nWhen deleting by metadata:- Accepts a\nmetadata\nobject of key/value pairs. KV pair matching uses an or condition. A trace will match if it has any of the key-value pairs specified in metadata (not all) - You don\u2019t need to specify a session id when deleting by metadata. Deletes will apply across the workspace.\nuser_id: \"user123\"\nor environment: \"staging\"\nin their metadata.\nRemember that you can only schedule up to 1000 traces per session per request. For larger deletions, you\u2019ll need to make multiple requests.\nExample deletes\nYou can delete dataset examples self-serve via our API, which supports both soft and hard deletion methods depending on your data retention needs.Hard deletes will permanently remove inputs, outputs, and metadata from ALL versions of the specified examples across the entire dataset history.\nDeleting examples is a two-step process\nFor bulk operations, example deletion follows a two-step process:1. Search for examples by metadata\nFind all examples with matching metadata across all datasets in a workspace. GET /examplesas_of\nmust be explicitly specified as a timestamp. Only examples created before theas_of\ndate will be returned\nuser_id: \"user123\"\nor environment: \"staging\"\nin their metadata across all datasets in your workspace.\n2. Hard delete examples\nOnce you have the example IDs, send a delete request. This will zero-out the inputs, outputs, and metadata from all versions of the dataset for that example. DELETE /examples- Specify example IDs and add\n\"hard_delete\": true\nto the query params of the request\nDeletion types\nSoft delete (default)\n- Creates tombstoned entries with NULL inputs/outputs in the dataset\n- Preserves historical data and maintains dataset versioning\n- Only affects the current version of the dataset\nHard delete\n- Permanently removes inputs, outputs, and metadata from ALL dataset versions\n- Complete data removal when compliance requires zero-out across all versions\n- Add\n\"hard_delete\": true\nto the query parameters", "tokens": 487, "node_type": "child"}
{"id": 61, "chunk_id": "c16acf8a553ac24f5d77e7d3a1c6e493", "parent_id": 55, "url": "", "namespace": "langchain", "title": "langsmith-data-storage-and-privacy", "headers": ["langsmith-data-storage-and-privacy"], "section_index": 0, "chunk_index": 0, "text": "# langsmith-data-storage-and-privacy\n\n> Source: https://docs.langchain.com/langsmith/data-storage-and-privacy\n\nlanggraph dev\n) and the local Docker server (langgraph up\n). It also describes what data is tracked when interacting with the hosted Studio frontend.\nCLI\nLangGraph CLI is the command-line interface for building and running LangGraph applications; see the CLI guide to learn more. By default, calls to most CLI commands log a single analytics event upon invocation. This helps us better prioritize improvements to the CLI experience. Each telemetry event contains the calling process\u2019s OS, OS version, Python version, the CLI version, the command name (dev\n, up\n, run\n, etc.), and booleans representing whether a flag was passed to the command. You can see the full analytics logic here.\nYou can disable all CLI telemetry by setting LANGGRAPH_CLI_NO_ANALYTICS=1\n.\nLangGraph Server\nThe LangGraph Server provides a durable execution runtime that relies on persisting checkpoints of your application state, long-term memories, thread metadata, assistants, and similar resources to the local file system or a database. Unless you have deliberately customized the storage location, this information is either written to local disk (forlanggraph dev\n) or a PostgreSQL database (for langgraph up\nand in all deployments).\nLangSmith Tracing\nWhen running the LangGraph server (either in-memory or in Docker), LangSmith tracing may be enabled to facilitate faster debugging and offer observability of graph state and LLM prompts in production. You can always disable tracing by settingLANGSMITH_TRACING=false\nin your server\u2019s runtime environment.\nIn-memory development server\nlanggraph dev\nruns an in-memory development server as a single Python process, designed for quick development and testing. It saves all checkpointing and memory data to disk within a .langgraph_api\ndirectory in the current working directory. Apart from the telemetry data described in the CLI section, no data leaves the machine unless you have enabled tracing or your graph code explicitly contacts an external service.\nStandalone Server\nlanggraph up\nbuilds your local package into a Docker image and runs the server as the data plane consisting of three containers: the API server, a PostgreSQL container, and a Redis container. All persistent data (checkpoints, assistants, etc.) are stored in the PostgreSQL database. Redis is used as a pubsub connection for real-time streaming of events. You can encrypt all checkpoints before saving to the database by setting a valid LANGGRAPH_AES_KEY\nenvironment variable. You can also specify TTLs for checkpoints and cross-thread memories in langgraph.json\nto control how long data is stored. All persisted threads, memories, and other data can be deleted via the relevant API endpoints.\nAdditional API calls are made to confirm that the server has a valid license and to track the number of executed runs and tasks. Periodically, the API server validates the provided license key (or API key).\nIf you\u2019ve disabled tracing, no user data is persisted externally unless your graph code explicitly contacts an external service.\nStudio\nStudio is a graphical interface for interacting with your LangGraph server. It does not persist any private data (the data you send to your server is not sent to LangSmith). Though the Studio interface is served at smith.langchain.com, it is run in your browser and connects directly to your local LangGraph server so that no data needs to be sent to LangSmith. If you are logged in, LangSmith does collect some usage analytics to help improve the debugging user experience. This includes:- Page visits and navigation patterns\n- User actions (button clicks)\n- Browser type and version\n- Screen resolution and viewport size\nQuick reference\nIn summary, you can opt-out of server-side telemetry by turning off CLI analytics and disabling tracing.| Variable | Purpose | Default |\n|---|---|---|\nLANGGRAPH_CLI_NO_ANALYTICS=1 | Disable CLI analytics | Analytics enabled |\nLANGSMITH_API_KEY | Enable LangSmith tracing | Tracing disabled |\nLANGSMITH_TRACING=false | Disable LangSmith tracing | Depends on environment |", "tokens": 626, "node_type": "child"}
{"id": 62, "chunk_id": "c16acf8a553ac24f5d77e7d3a1c6e493", "parent_id": 57, "url": "", "namespace": "langchain", "title": "langsmith-dataset-transformations", "headers": ["langsmith-dataset-transformations"], "section_index": 0, "chunk_index": 0, "text": "# langsmith-dataset-transformations\n\n> Source: https://docs.langchain.com/langsmith/dataset-transformations\n\nTransformation types\n| Transformation Type | Target Types | Functionality |\n|---|---|---|\n| remove_system_messages | Array[Message] | Filters a list of messages to remove any system messages. |\n| convert_to_openai_message | Message Array[Message] | Converts any incoming data from LangChain\u2019s internal serialization format to OpenAI\u2019s standard message format using langchain\u2019s convert_to_openai_messages. If the target field is marked as required, and no matching message is found upon entry, it will attempt to extract a message (or list of messages) from several well-known LangSmith tracing formats (e.g., any traced LangChain BaseChatModel run or traced run from the LangSmith OpenAI wrapper), and remove the original key containing the message. |\n| convert_to_openai_tool | Array[Tool] Only available on top level fields in the inputs dictionary. | Converts any incoming data into OpenAI standard tool formats here using langchain\u2019s convert_to_openai_tool Will extract tool definitions from a run\u2019s invocation parameters if present / no tools are found at the specified key. This is useful because LangChain chat models trace tool definitions to the extra.invocation_params field of the run rather than inputs. |\n| remove_extra_fields | Object | Removes any field not defined in the schema for this target object. |\nChat Model prebuilt schema\nThe main use case for transformations is to simplify collecting production traces into datasets in a format that can be standardized across model providers for usage in evaluations / few shot prompting / etc downstream. To simplify setup of transformations for our end users, LangSmith offers a pre-defined schema that will do the following:- Extract messages from your collected runs and transform them into the openai standard format, which makes them compatible all LangChain ChatModels and most model providers\u2019 SDK for downstream evaluation and experimentation\n- Extract any tools used by your LLM and add them to your example\u2019s input to be used for reproducability in downstream evaluation\nUsers who want to iterate on their system prompts often also add the Remove System Messages transformation on their input messages when using our Chat Model schema, which will prevent you from saving the system prompt to your dataset.", "tokens": 350, "node_type": "child"}
{"id": 63, "chunk_id": "c16acf8a553ac24f5d77e7d3a1c6e493", "parent_id": 58, "url": "", "namespace": "langchain", "title": "langsmith-define-target-function", "headers": ["langsmith-define-target-function"], "section_index": 0, "chunk_index": 0, "text": "# langsmith-define-target-function\n\n> Source: https://docs.langchain.com/langsmith/define-target-function\n\n- A dataset of test inputs and expected outputs.\n- A target function which is what you\u2019re evaluating.\n- Evaluators that score your target function\u2019s outputs.\nTarget function signature\nIn order to evaluate an application in code, we need a way to run the application. When usingevaluate()\n(Python/TypeScript)we\u2019ll do this by passing in a target function argument. This is a function that takes in a dataset Example\u2019s inputs and returns the application output as a dict. Within this function we can call our application however we\u2019d like. We can also format the output however we\u2019d like. The key is that any evaluator functions we define should work with the output format we return in our target function.\nevaluate()\nwill automatically trace your target function. This means that if you run any traceable code within your target function, this will also be traced as child runs of the target trace.Example: Single LLM call\nExample: Non-LLM component\nExample: Application or agent\nIf you have a LangGraph/LangChain agent that accepts the inputs defined in your dataset and that returns the output format you want to use in your evaluators, you can pass that object in as the target directly:", "tokens": 201, "node_type": "child"}
{"id": 64, "chunk_id": "c16acf8a553ac24f5d77e7d3a1c6e493", "parent_id": 59, "url": "", "namespace": "langchain", "title": "langsmith-deploy-hybrid", "headers": ["langsmith-deploy-hybrid"], "section_index": 0, "chunk_index": 0, "text": "# langsmith-deploy-hybrid\n\n> Source: https://docs.langchain.com/langsmith/deploy-hybrid\n\nPrerequisites\n- Use the LangGraph CLI to test your application locally.\n- Use the LangGraph CLI to build a Docker image (i.e.\nlanggraph build\n) and push it to a registry your Kubernetes cluster or Amazon ECS cluster has access to.\nKubernetes\nPrerequisites\nKEDA\nis installed on your cluster.- A valid\nIngress\ncontroller is installed on your cluster. For more information about configuring ingress for your deployment, refer to Create an ingress for installations. - You have slack space in your cluster for multiple deployments.\nCluster-Autoscaler\nis recommended to automatically provision new nodes. - You will need to enable egress to two control plane URLs. The listener polls these endpoints for deployments:\nSetup\n- Provide your LangSmith organization ID to us. Your LangSmith organization will be configured to deploy the data plane in your cloud.\n- Create a listener from the LangSmith UI. The\nListener\ndata model is configured for the actual \u201clistener\u201d application.- In the left-hand navigation, select\nDeployments\n>Listeners\n. - In the top-right of the page, select\n+ Create Listener\n. - Enter a unique\nCompute ID\nfor the listener. TheCompute ID\nis a user-defined identifier that should be unique across all listeners in the current LangSmith workspace. TheCompute ID\nis displayed to end users when they are creating a new deployment. Ensure that theCompute ID\nprovides context to the end user about where their LangGraph Server deployments will be deployed to. For example, aCompute ID\ncan be set tok8s-cluster-name-dev-01\n. In this example, the name of the Kubernetes cluster isk8s-cluster-name\n,dev\ndenotes that the cluster is reserved for \u201cdevelopment\u201d workloads, and01\nis a numerical suffix to reduce naming collisions. - Enter one or more Kubernetes namespaces. Later, the \u201clistener\u201d application will be configured to deploy to each of these namespaces.\n- In the top-right on the page, select\nSubmit\n. - After the listener is created, copy the listener ID. You will use it later when installing the actual \u201clistener\u201d application in the Kubernetes cluster (step 5).\nImportant Creating a listener from the LangSmith UI does not install the \u201clistener\u201d application in the Kubernetes cluster. - In the left-hand navigation, select\n- A Helm chart is provided to install the necesssary components in your Kubernetes cluster.\nlanggraph-listener\n: This is a service that listens to LangChain\u2019s control plane for changes to your deployments and creates/updates downstream CRDs. This is the \u201clistener\u201d application.LangGraphPlatform CRD\n: A CRD for LangSmith Deployment. This contains the spec for managing an instance of a LangSmith Deployment.langgraph-platform-operator\n: This operator handles changes to your LangSmith CRDs.\n- Configure your\nlanggraph-dataplane-values.yaml\nfile.config.langsmithApiKey\n: Thelanggraph-listener\ndeployment authenticates with LangChain\u2019s LangGraph control plane API with thelangsmithApiKey\n.config.langsmithWorkspaceId\n: Thelanggraph-listener\ndeployment is coupled to LangGraph Server deployments in the LangSmith workspace. In other words, thelanggraph-listener\ndeployment can only manage LangGraph Server deployments in the specified LangSmith workspace ID.config.langgraphListenerId\n: In addition to being coupled with a LangSmith workspace, thelanggraph-listener\ndeployment is also coupled to a listener. When a new LangGraph Server deployment is created, it is automatically coupled to alanggraphListenerId\n. SpecifyinglanggraphListenerId\nensures that thelanggraph-listener\ndeployment can only manage LangGraph Server deployments that are coupled tolanggraphListenerId\n.config.watchNamespaces\n: A comma-separated list of Kubernetes namespaces that thelanggraph-listener\ndeployment will deploy to. This list should match the list of namespaces specified in step 2d.config.enableLGPDeploymentHealthCheck\n: To disable the LangGraph Server health check, set this tofalse\n.ingress.hostname\n: As part of the deployment workflow, thelanggraph-listener\ndeployment attempts to call the LangGraph Server health check endpoint (GET /ok\n) to verify that the application has started up correctly. A typical setup involves creating a shared DNS record or domain for LangGraph Server deployments. This is not managed by LangSmith. Once created, setingress.hostname\nto the domain, which will be used to complete the health check.operator.enabled\n: There can only be 1 instance of thelanggraph-platform-operator\ndeployed in a Kubernetes namespace. Set this tofalse\nif there is already an instance oflanggraph-platform-operator\ndeployed in the current Kubernetes namespace.operator.createCRDs\n: Set this value tofalse\nif the Kubernetes cluster already has theLangGraphPlatform CRD\ninstalled. During installation, an error will occur if the CRD is already installed. This situation may occur if multiple listeners are deployed on the same Kubernetes cluster.\n- Deploy\nlanggraph-dataplane\nHelm chart. - If successful, you will see three services start up in your namespace.\n- Create a deployment from the control plane UI.\n- Select the desired listener from the list of\nCompute IDs\nin the dropdown menu. - Select the Kubernetes namespace to deploy to.\n- Fill out all other required fields and select\nSubmit\nin the top-right of the panel. - The deployment will be deployed on the Kubernetes cluster where the listener is deployed and in the Kubernetes namespace specified in step 7b.\n- Select the desired listener from the list of", "tokens": 798, "node_type": "child"}
{"id": 65, "chunk_id": "c16acf8a553ac24f5d77e7d3a1c6e493", "parent_id": 60, "url": "", "namespace": "langchain", "title": "langsmith-deploy-self-hosted-full-platform", "headers": ["langsmith-deploy-self-hosted-full-platform"], "section_index": 0, "chunk_index": 0, "text": "# langsmith-deploy-self-hosted-full-platform\n\n> Source: https://docs.langchain.com/langsmith/deploy-self-hosted-full-platform\n\nThis setup page is for adding deployment capabilities to an existing LangSmith instance.Review the self-hosted options to understand:\n- LangSmith (observability): What you should install first.\n- LangSmith with deployment: What this guide enables.\n- Standalone Server: Lightweight alternative without the UI.\nOverview\nThis guide builds on top of the Kubernetes installation guide. You must complete that guide first before continuing. This page covers the additional setup steps required to enable deployment functionality:- Installing the LangGraph operator\n- Configuring your ingress\n- Connecting to the control plane\nPrerequisites\n- You are using Kubernetes.\n- You have an instance of self-hosted LangSmith running.\n- Use the LangGraph CLI to test your application locally.\n- Use the LangGraph CLI to build a Docker image (i.e.\nlanggraph build\n) and push it to a registry your Kubernetes cluster has access to. KEDA\nis installed on your cluster.\n- Ingress Configuration\n- You must set up an ingress for your LangSmith instance. All agents will be deployed as Kubernetes services behind this ingress. Use this guide to set up an ingress for your instance.\n- You have slack space in your cluster for multiple deployments.\nCluster-Autoscaler\nis recommended to automatically provision new nodes. - A valid Dynamic PV provisioner or PVs available on your cluster. You can verify this by running:\n- Egress to\nhttps://beacon.langchain.com\nfrom your network. This is required for license verification and usage reporting if not running in air-gapped mode. See the Egress documentation for more details.\nSetup\n- As part of configuring your self-hosted LangSmith instance, you enable the\nlanggraphPlatform\noption. This will provision a few key resources.listener\n: This is a service that listens to the control plane for changes to your deployments and creates/updates downstream CRDs.LangGraphPlatform CRD\n: A CRD for LangSmith Deployment. This contains the spec for managing an instance of a LangSmith deployment.operator\n: This operator handles changes to your LangSmith CRDs.host-backend\n: This is the control plane.\n- Two additional images will be used by the chart. Use the images that are specified in the latest release.\n- In your config file for langsmith (usually\nlangsmith_config.yaml\n), enable thelanggraphPlatform\noption. Note that you must also have a valid ingress setup:\n- In your\nvalues.yaml\nfile, configure thehostBackendImage\nandoperatorImage\noptions (if you need to mirror images) - You can also configure base templates for your agents by overriding the base templates here.\n- You create a deployment from the control plane UI.", "tokens": 411, "node_type": "child"}
{"id": 66, "chunk_id": "c16acf8a553ac24f5d77e7d3a1c6e493", "parent_id": 61, "url": "", "namespace": "langchain", "title": "langsmith-deploy-standalone-server", "headers": ["langsmith-deploy-standalone-server"], "section_index": 0, "chunk_index": 0, "text": "# langsmith-deploy-standalone-server\n\n> Source: https://docs.langchain.com/langsmith/deploy-standalone-server\n\nThis is the setup page for deploying LangGraph Servers directly without the LangSmith platform.Review the self-hosted options to understand:\n- Standalone Server: What this guide covers (no UI, just servers).\n- LangSmith: For the full LangSmith platform with UI.\n- LangSmith with deployment: For UI-based deployment management.\nPrerequisites\n- Use the LangGraph CLI to test your application locally.\n-\nUse the LangGraph CLI to build a Docker image (i.e.\nlanggraph build\n). - The following environment variables are needed for a data plane deployment.\n-\nREDIS_URI\n: Connection details to a Redis instance. Redis will be used as a pub-sub broker to enable streaming real time output from background runs. The value ofREDIS_URI\nmust be a valid Redis connection URI.Shared Redis Instance Multiple self-hosted deployments can share the same Redis instance. For example, forDeployment A\n,REDIS_URI\ncan be set toredis://<hostname_1>:<port>/1\nand forDeployment B\n,REDIS_URI\ncan be set toredis://<hostname_1>:<port>/2\n.1\nand2\nare different database numbers within the same instance, but<hostname_1>\nis shared. The same database number cannot be used for separate deployments. -\nDATABASE_URI\n: Postgres connection details. Postgres will be used to store assistants, threads, runs, persist thread state and long term memory, and to manage the state of the background task queue with \u2018exactly once\u2019 semantics. The value ofDATABASE_URI\nmust be a valid Postgres connection URI.Shared Postgres Instance Multiple self-hosted deployments can share the same Postgres instance. For example, forDeployment A\n,DATABASE_URI\ncan be set topostgres://<user>:<password>@/<database_name_1>?host=<hostname_1>\nand forDeployment B\n,DATABASE_URI\ncan be set topostgres://<user>:<password>@/<database_name_2>?host=<hostname_1>\n.<database_name_1>\nanddatabase_name_2\nare different databases within the same instance, but<hostname_1>\nis shared. The same database cannot be used for separate deployments. -\nLANGSMITH_API_KEY\n: LangSmith API key. -\nLANGGRAPH_CLOUD_LICENSE_KEY\n: LangSmith license key. This will be used to authenticate ONCE at server start up. -\nLANGSMITH_ENDPOINT\n: To send traces to a self-hosted LangSmith instance, setLANGSMITH_ENDPOINT\nto the hostname of the self-hosted LangSmith instance. -\nEgress to\nhttps://beacon.langchain.com\nfrom your network. This is required for license verification and usage reporting if not running in air-gapped mode. See the Egress documentation for more details.\nKubernetes\nUse this Helm chart to deploy a LangGraph Server to a Kubernetes cluster.Docker\nRun the followingdocker\ncommand:\n- You need to replace\nmy-image\nwith the name of the image you built in the prerequisite steps (fromlanggraph build\n)\nREDIS_URI\n, DATABASE_URI\n, and LANGSMITH_API_KEY\n.- If your application requires additional environment variables, you can pass them in a similar way.\nDocker Compose\nDocker Compose YAML file:docker compose up\nwith this Docker Compose file in the same folder.\nThis will launch a LangGraph Server on port 8123\n(if you want to change this, you can change this by changing the ports in the langgraph-api\nvolume). You can test if the application is healthy by running:", "tokens": 458, "node_type": "child"}
{"id": 67, "chunk_id": "c16acf8a553ac24f5d77e7d3a1c6e493", "parent_id": 62, "url": "", "namespace": "langchain", "title": "langsmith-deploy-to-cloud", "headers": ["langsmith-deploy-to-cloud"], "section_index": 0, "chunk_index": 0, "text": "# langsmith-deploy-to-cloud > Source: https://docs.langchain.com/langsmith/deploy-to-cloud Prerequisites - LangSmith applications are deployed from GitHub repositories. Configure and upload a LangSmith application to a GitHub repository in order to deploy it to LangSmith. - Verify that the LangGraph API runs locally. If the API does not run successfully (i.e. langgraph dev ), deploying to LangSmith will fail as well. Create New Deployment Starting from the LangSmith UI:- In the left-hand navigation panel, select Deployments, which contains a list of existing deployments. - In the top-right corner, select + New Deployment to create a new deployment. - In the Create New Deployment panel, fill out the required fields. Deployment details - Select Import from GitHub and follow the GitHub OAuth workflow to install and authorize LangChain\u2019shosted-langserve GitHub app to access the selected repositories. After installation is complete, return to theCreate New Deployment panel and select the GitHub repository to deploy from the dropdown menu. Note: The GitHub user installing LangChain\u2019shosted-langserve GitHub app must be an owner of the organization or account. - Specify a name for the deployment. - Specify the desired Git Branch . A deployment is linked to a branch. When a new revision is created, code for the linked branch will be deployed. The branch can be updated later in the Deployment Settings. - Specify the full path to the LangGraph API config file including the file name. For example, if the file langgraph.json is in the root of the repository, simply specifylanggraph.json . - Check/uncheck checkbox to Automatically update deployment on push to branch . If checked, the deployment will automatically be updated when changes are pushed to the specifiedGit Branch . This setting can be enabled/disabled later in the Deployment Settings. - Select the desired Deployment Type . Development deployments are meant for non-production use cases and are provisioned with minimal resources.Production deployments can serve up to 500 requests/second and are provisioned with highly available storage with automatic backups.- Determine if the deployment should be Shareable through Studio . - If unchecked, the deployment will only be accessible with a valid LangSmith API key for the workspace. - If checked, the deployment will be accessible through Studio to any LangSmith user. A direct URL to Studio for the deployment will be provided to share with other LangSmith users. - Specify Environment Variables and secrets. See the Environment Variables reference to configure additional variables for the deployment. - Sensitive values such as API keys (e.g. OPENAI_API_KEY ) should be specified as secrets. - Additional non-secret environment variables can be specified as well. - A new LangSmith Tracing Project is automatically created with the same name as the deployment. - In the top-right corner, select Submit . After a few seconds, theDeployment view appears and the new deployment will be queued for provisioning. Create New Revision When creating a new deployment, a new revision is created by default. Subsequent revisions can be created to deploy new code changes. Starting from the LangSmith UI\u2026- In the left-hand navigation panel, select Deployments, which contains a list of existing deployments. - Select an existing deployment to create a new revision for. - In the Deployment view, in the top-right corner, select+ New Revision . - In the New Revision modal, fill out the required fields. - Specify the full path to the LangGraph API config file including the file name. For example, if the file langgraph.json is in the root of the repository, simply specifylanggraph.json . - Determine if the deployment should be Shareable through Studio . - If unchecked, the deployment will only be accessible with a valid LangSmith API key for the workspace. - If checked, the deployment will be accessible through Studio to any LangSmith user. A direct URL to Studio for the deployment will be provided to share with other LangSmith users. - Specify Environment Variables and secrets. Existing secrets and environment variables are prepopulated. See the Environment Variables reference to configure additional variables for the revision. - Add new secrets or environment variables. - Remove existing secrets or environment variables. - Update the value of existing secrets or environment variables. - Select Submit . After a few seconds, theNew Revision modal will close and the new revision will be queued for deployment. View Build and Server Logs Build and server logs are available for each revision. Starting from the Deployments view:- Select the desired revision from the Revisions table. A panel slides open from the right-hand side and theBuild tab is selected by default, which displays build logs for the revision. - In the panel, select the Server tab to view server logs for the revision. Server logs are only available after a revision has been deployed. - Within the Server tab, adjust the date/time range picker as needed. By default, the date/time range picker is set to theLast 7 days . View Deployment Metrics Starting from the LangSmith UI\u2026- In the left-hand navigation panel, select Deployments, which contains a list of existing deployments. - Select an existing deployment to monitor. - Select the Monitoring tab to view the deployment metrics. See a list of all available metrics. - Within the Monitoring tab, use the date/time range picker as needed. By default, the date/time range picker is set to theLast 15 minutes . Interrupt Revision Interrupting a revision will stop deployment of the revision.Undefined Behavior Interrupted revisions have undefined behavior. This is only useful if you need to deploy a new revision and you already have a revision \u201cstuck\u201d in progress. In the future, this feature may be removed. - Select the menu icon (three dots) on the right-hand side of the row for the desired revision from the Revisions table. - Select Interrupt from the menu. - A modal will appear. Review the confirmation message. Select Interrupt revision . Delete Deployment Starting from the LangSmith UI\u2026- In the left-hand navigation panel, select Deployments, which contains a list of existing deployments. - Select the menu icon (three dots) on the right-hand side of the row for the desired", "tokens": 1000, "node_type": "child"}
{"id": 68, "chunk_id": "9b0fd3b850698317bef57fa6e71a38ba", "parent_id": 62, "url": "", "namespace": "langchain", "title": "langsmith-deploy-to-cloud", "headers": ["langsmith-deploy-to-cloud"], "section_index": 0, "chunk_index": 1, "text": "- Within the Monitoring tab, use the date/time range picker as needed. By default, the date/time range picker is set to theLast 15 minutes . Interrupt Revision Interrupting a revision will stop deployment of the revision.Undefined Behavior Interrupted revisions have undefined behavior. This is only useful if you need to deploy a new revision and you already have a revision \u201cstuck\u201d in progress. In the future, this feature may be removed. - Select the menu icon (three dots) on the right-hand side of the row for the desired revision from the Revisions table. - Select Interrupt from the menu. - A modal will appear. Review the confirmation message. Select Interrupt revision . Delete Deployment Starting from the LangSmith UI\u2026- In the left-hand navigation panel, select Deployments, which contains a list of existing deployments. - Select the menu icon (three dots) on the right-hand side of the row for the desired deployment and select Delete . - A Confirmation modal will appear. SelectDelete . Deployment Settings Starting from the Deployments view:- In the top-right corner, select the gear icon ( Deployment Settings ). - Update the Git Branch to the desired branch. - Check/uncheck checkbox to Automatically update deployment on push to branch . - Branch creation/deletion and tag creation/deletion events will not trigger an update. Only pushes to an existing branch will trigger an update. - Pushes in quick succession to a branch will queue subsequent updates. Once a build completes, the most recent commit will begin building and the other queued builds will be skipped. Add or Remove GitHub Repositories After installing and authorizing LangChain\u2019shosted-langserve GitHub app, repository access for the app can be modified to add new repositories or remove existing repositories. If a new repository is created, it may need to be added explicitly. - From the GitHub profile, navigate to Settings >Applications >hosted-langserve > clickConfigure . - Under Repository access , selectAll repositories orOnly select repositories . IfOnly select repositories is selected, new repositories must be explicitly added. - Click Save . - When creating a new deployment, the list of GitHub repositories in the dropdown menu will be updated to reflect the repository access changes. Allowlisting IP Addresses All traffic from LangSmith deployments created after January 6th 2025 will come through a NAT gateway. This NAT gateway will have several static ip addresses depending on the region you are deploying in. Refer to the table below for the list of IP addresses to allowlist:| US | EU | |---|---| | 35.197.29.146 | 34.90.213.236 | | 34.145.102.123 | 34.13.244.114 | | 34.169.45.153 | 34.32.180.189 | | 34.82.222.17 | 34.34.69.108 | | 35.227.171.135 | 34.32.145.240 | | 34.169.88.30 | 34.90.157.44 | | 34.19.93.202 | 34.141.242.180 | | 34.19.34.50 | 34.32.141.108 | | 34.59.244.194 | | | 34.9.99.224 | | | 34.68.27.146 | | | 34.41.178.137 | | | 34.123.151.210 | | | 34.135.61.140 | | | 34.121.166.52 | | | 34.31.121.70 |", "tokens": 485, "node_type": "child"}
{"id": 69, "chunk_id": "c16acf8a553ac24f5d77e7d3a1c6e493", "parent_id": 63, "url": "", "namespace": "langchain", "title": "langsmith-deployment-quickstart", "headers": ["langsmith-deployment-quickstart"], "section_index": 0, "chunk_index": 0, "text": "# langsmith-deployment-quickstart\n\n> Source: https://docs.langchain.com/langsmith/deployment-quickstart\n\nTo deploy an application to LangSmith, your application code must reside in a GitHub repository. Both public and private repositories are supported. For this quickstart, use the new-langgraph-project template for your application:\nClick the + New Deployment button. A pane will open where you can fill in the required fields.\nIf you are a first time user or adding a private repository that has not been previously connected, click the Import from GitHub button and follow the instructions to connect your GitHub account.\nSelect your New LangGraph Project repository.\nClick Submit to deploy.\nThis may take about 15 minutes to complete. You can check the status in the Deployment details view.\nfrom langgraph_sdk import get_clientclient = get_client(url=\"your-deployment-url\", api_key=\"your-langsmith-api-key\")async for chunk in client.runs.stream( None, # Threadless run \"agent\", # Name of assistant. Defined in langgraph.json. input={ \"messages\": [{ \"role\": \"human\", \"content\": \"What is LangGraph?\", }], }, stream_mode=\"updates\",): print(f\"Receiving new event of type: {chunk.event}...\") print(chunk.data) print(\"\\n\\n\")", "tokens": 159, "node_type": "child"}
{"id": 70, "chunk_id": "c16acf8a553ac24f5d77e7d3a1c6e493", "parent_id": 64, "url": "", "namespace": "langchain", "title": "langsmith-deployments", "headers": ["langsmith-deployments"], "section_index": 0, "chunk_index": 0, "text": "# langsmith-deployments\n\n> Source: https://docs.langchain.com/langsmith/deployments\n\n- Test locally: Run your application on a local server.\n- Choose hosting: Select Cloud, Hybrid, or Self-hosted.\n- Deploy your app: Push code or build images to your chosen environment.\n- Monitor & manage: Track traces, alerts, and dashboards.\nPrerequisites: Before deploying applications, you need a LangSmith instance to deploy to. Choose a hosting option first:\n- Cloud: Fully managed\n- Hybrid: Enterprise option for data residency requirements\n- Self-hosted: Full control and data isolation\nWhat you\u2019ll learn\n- Configure your app for deployment (dependencies, project setup, and monorepo support).\n- Build, deploy, and update LangGraph Servers.\n- Secure your deployments with authentication and access control.\n- Customize your server runtime (lifespan hooks, middleware, and routes).\n- Debug, observe, and troubleshoot deployed agents using the Studio UI.", "tokens": 133, "node_type": "child"}
{"id": 71, "chunk_id": "c16acf8a553ac24f5d77e7d3a1c6e493", "parent_id": 66, "url": "", "namespace": "langchain", "title": "langsmith-docker", "headers": ["langsmith-docker"], "section_index": 0, "chunk_index": 0, "text": "# langsmith-docker\n\n> Source: https://docs.langchain.com/langsmith/docker\n\nSelf-hosting LangSmith is an add-on to the Enterprise Plan designed for our largest, most security-conscious customers. See our pricing page for more detail, and contact our sales team if you want to get a license key to trial LangSmith in your environment.\nNote that Docker Compose is limited to local development environments only and does not extend support to container services such as AWS Elastic Container Service, Azure Container Instances, and Google Cloud Run.\nPrerequisites\n-\nEnsure Docker is installed and running on your system. You can verify this by running:\nIf you don\u2019t see any server information in the output, make sure Docker is installed correctly and launch the Docker daemon.\n- Recommended: At least 4 vCPUs, 16GB Memory available on your machine.\n- You may need to tune resource requests/limits for all of our different services based off of organization size/usage\n- Disk Space: LangSmith can potentially require a lot of disk space. Ensure you have enough disk space available.\n- Recommended: At least 4 vCPUs, 16GB Memory available on your machine.\n-\nLangSmith License Key\n- You can get this from your LangChain representative. Contact our sales team for more information.\n-\nApi Key Salt\n- This is a secret key that you can generate. It should be a random string of characters.\n- You can generate this using the following command:\n-\nEgress to\nhttps://beacon.langchain.com\n(if not running in offline mode)- LangSmith requires egress to\nhttps://beacon.langchain.com\nfor license verification and usage reporting. This is required for LangSmith to function properly. You can find more information on egress requirements in the Egress section.\n- LangSmith requires egress to\n-\nConfiguration\n- There are several configuration options that you can set in the\n.env\nfile. You can find more information on the available configuration options in the Configuration section.\n- There are several configuration options that you can set in the\nRunning via Docker Compose\nThe following explains how to run the LangSmith using Docker Compose. This is the most flexible way to run LangSmith without Kubernetes. The default configuration for Docker Compose is intended for local testing only and not for instances where any services are exposed to the public internet. In production, we highly recommend using a secured Kubernetes environment.1. Fetch the LangSmith docker-compose.yml\nfile\nYou can find the docker-compose.yml\nfile and related files in the LangSmith SDK repository here: LangSmith Docker Compose File\nCopy the docker-compose.yml\nfile and all files in that directory from the LangSmith SDK to your project directory.\n- Ensure that you copy the\nusers.xml\nfile as well.\n2. Configure environment variables\n- Copy the\n.env.example\nfile from the LangSmith SDK to your project directory and rename it to.env\n. - Configure the appropriate values in the\n.env\nfile. You can find the available configuration options in the Configuration section.\ndocker-compose.yml\nfile directly or export them in your terminal. We recommend setting them in the .env\nfile.\n3. Start server\nStart the LangSmith application by executing the following command in your terminal:Validate your deployment:\n-\nCurl the exposed port of the\ncli-langchain-frontend-1\ncontainer: -\nVisit the exposed port of the\ncli-langchain-frontend-1\ncontainer on your browser The LangSmith UI should be visible/operational athttp://localhost:1980\nChecking the logs\nIf, at any point, you want to check if the server is running and see the logs, runStopping the server\nUsing LangSmith\nNow that LangSmith is running, you can start using it to trace your code. You can find more information on how to use self-hosted LangSmith in the self-hosted usage guide. Your LangSmith instance is now running but may not be fully setup yet. If you used one of the basic configs, you may have deployed a no-auth configuration. In this state, there is no authentication or concept of user accounts nor API keys and traces can be submitted directly without an API key so long as the hostname is passed to the LangChain tracer/LangSmith SDK. As a next step, it is strongly recommended you work with your infrastructure administrators to:- Setup DNS for your LangSmith instance to enable easier access\n- Configure SSL to ensure in-transit encryption of traces submitted to LangSmith\n- Configure LangSmith for oauth authentication or basic authentication to secure your LangSmith instance\n- Secure access to your Docker environment to limit access to only the LangSmith frontend and API\n- Connect LangSmith to secured Postgres and Redis instances", "tokens": 732, "node_type": "child"}
{"id": 72, "chunk_id": "c16acf8a553ac24f5d77e7d3a1c6e493", "parent_id": 67, "url": "", "namespace": "langchain", "title": "langsmith-double-texting", "headers": ["langsmith-double-texting"], "section_index": 0, "chunk_index": 0, "text": "# langsmith-double-texting\n\n> Source: https://docs.langchain.com/langsmith/double-texting\n\nMany times users might interact with your graph in unintended ways.\nFor instance, a user may send one message and before the graph has finished running send a second message.\nMore generally, users may invoke the graph a second time before the first run has finished.\nWe call this \u201cdouble texting\u201d.\nThis option rejects any additional incoming runs while a current run is in progress and prevents concurrent execution or double texting.For configuring the reject double text option, refer to the how-to guide.\nThis option allows the current run to finish before processing any new input. Incoming requests are queued and executed sequentially once prior runs complete.For configuring the enqueue double text option, refer to the how-to guide.\nThis option halts the current execution and preserves the progress made up to the interruption point. The new user input is then inserted, and execution continues from that state.When using this option, your graph must account for potential edge cases. For example, a tool call may have been initiated but not yet completed at the time of interruption. In these cases, handling or removing partial tool calls may be necessary to avoid unresolved operations.For configuring the interrupt double text option, refer to the how-to guide.\nThis option halts the current execution and reverts all progress\u2014including the initial run input\u2014before processing the new user input. The new input is treated as a fresh run, starting from the initial state.For configuring the rollback double text option, refer to the how-to guide.", "tokens": 252, "node_type": "child"}
{"id": 73, "chunk_id": "c16acf8a553ac24f5d77e7d3a1c6e493", "parent_id": 68, "url": "", "namespace": "langchain", "title": "langsmith-egress-metrics-metadata", "headers": ["langsmith-egress-metrics-metadata"], "section_index": 0, "chunk_index": 0, "text": "# langsmith-egress-metrics-metadata\n\n> Source: https://docs.langchain.com/langsmith/egress-metrics-metadata\n\nImportant: self-hosted only. This section only applies to customers who are not running in offline mode and assumes you are using a self-hosted LangSmith instance. This does not apply to cloud or hybrid deployments.\nhttps://beacon.langchain.com\n.\nIn the future, we will be introducing support diagnostics to help us ensure that LangSmith is running at an optimal level within your environment.\nThis will require egress to\nhttps://beacon.langchain.com\nfrom your network. If using an API key, you will also need to allow egress to https://api.smith.langchain.com\nor https://eu.api.smith.langchain.com\nfor API key verification. Refer to the allowlisting IP section for static IP addresses, if needed.- Subscription Metrics\n- Subscription metrics are used to determine level of access and utilization of LangSmith. This includes, but are not limited to:\n- Nodes Executed\n- Runs Executed\n- License Key Verification\n- Subscription metrics are used to determine level of access and utilization of LangSmith. This includes, but are not limited to:\n- Operational Metadata\n- This metadata will contain and collect the above subscription metrics to assist with remote support, allowing the LangChain team to diagnose and troubleshoot performance issues more effectively and proactively.\nExample Payloads\nIn an effort to maximize transparency, we provide sample payloads here:License Verification (Enterprise)\nEndpoint:POST beacon.langchain.com/v1/beacon/verify\nRequest:\nAPI key verification (LangSmith API key)\nEndpoint:POST api.smith.langchain.com/auth\nRequest:\nUsage Reporting\nEndpoint:POST beacon.langchain.com/v1/metadata/submit\nRequest:", "tokens": 225, "node_type": "child"}
{"id": 74, "chunk_id": "c16acf8a553ac24f5d77e7d3a1c6e493", "parent_id": 69, "url": "", "namespace": "langchain", "title": "langsmith-enqueue-concurrent", "headers": ["langsmith-enqueue-concurrent"], "section_index": 0, "chunk_index": 0, "text": "# langsmith-enqueue-concurrent\n\n> Source: https://docs.langchain.com/langsmith/enqueue-concurrent\n\nenqueue\noption for double texting, which adds the interruptions to a queue and executes them in the order they are received by the client. Below is a quick example of using the enqueue\noption.\nSetup\nFirst, we will define a quick helper function for printing out JS and CURL model outputs (you can skip this if using Python):- Javascript\n- CURL\nCopy\nfunction prettyPrint(m) {\nconst padded = \" \" + m['type'] + \" \";\nconst sepLen = Math.floor((80 - padded.length) / 2);\nconst sep = \"=\".repeat(sepLen);\nconst secondSep = sep + (padded.length % 2 ? \"=\" : \"\");\nconsole.log(`${sep}${padded}${secondSep}`);\nconsole.log(\"\\n\\n\");\nconsole.log(m.content);\n}\n- Python\n- Javascript\n- CURL\nCopy\nimport asyncio\nimport httpx\nfrom langchain_core.messages import convert_to_messages\nfrom langgraph_sdk import get_client\nclient = get_client(url=<DEPLOYMENT_URL>)\n# Using the graph deployed with the name \"agent\"\nassistant_id = \"agent\"\nthread = await client.threads.create()\nCreate runs\nNow let\u2019s start two runs, with the second interrupting the first one with a multitask strategy of \u201cenqueue\u201d:- Python\n- Javascript\n- CURL\nCopy\nfirst_run = await client.runs.create(\nthread[\"thread_id\"],\nassistant_id,\ninput={\"messages\": [{\"role\": \"user\", \"content\": \"what's the weather in sf?\"}]},\n)\nsecond_run = await client.runs.create(\nthread[\"thread_id\"],\nassistant_id,\ninput={\"messages\": [{\"role\": \"user\", \"content\": \"what's the weather in nyc?\"}]},\nmultitask_strategy=\"enqueue\",\n)\nView run results\nVerify that the thread has data from both runs:- Python\n- Javascript\n- CURL\nCopy\n# wait until the second run completes\nawait client.runs.join(thread[\"thread_id\"], second_run[\"run_id\"])\nstate = await client.threads.get_state(thread[\"thread_id\"])\nfor m in convert_to_messages(state[\"values\"][\"messages\"]):\nm.pretty_print()\nCopy\n================================ Human Message =================================\nwhat's the weather in sf?\n================================== Ai Message ==================================\n[{'id': 'toolu_01Dez1sJre4oA2Y7NsKJV6VT', 'input': {'query': 'weather in san francisco'}, 'name': 'tavily_search_results_json', 'type': 'tool_use'}]\nTool Calls:\ntavily_search_results_json (toolu_01Dez1sJre4oA2Y7NsKJV6VT)\nCall ID: toolu_01Dez1sJre4oA2Y7NsKJV6VT\nArgs:\nquery: weather in san francisco\n================================= Tool Message =================================\nName: tavily_search_results_json\n[{\"url\": \"https://www.accuweather.com/en/us/san-francisco/94103/weather-forecast/347629\", \"content\": \"Get the current and future weather conditions for San Francisco, CA, including temperature, precipitation, wind, air quality and more. See the hourly and 10-day outlook, radar maps, alerts and allergy information.\"}]\n================================== Ai Message ==================================\nAccording to AccuWeather, the current weather conditions in San Francisco are:\nTemperature: 57\u00b0F (14\u00b0C)\nConditions: Mostly Sunny\nWind: WSW 10 mph\nHumidity: 72%\nThe forecast for the next few days shows partly sunny skies with highs in the upper 50s to mid 60s F (14-18\u00b0C) and lows in the upper 40s to low 50s F (9-11\u00b0C). Typical mild, dry weather for San Francisco this time of year.\nSome key details from the AccuWeather forecast:\nToday: Mostly sunny, high of 62\u00b0F (17\u00b0C)\nTonight: Partly cloudy, low of 49\u00b0F (9\u00b0C)\nTomorrow: Partly sunny, high of 59\u00b0F (15\u00b0C)\nSaturday: Mostly sunny, high of 64\u00b0F (18\u00b0C)\nSunday: Partly sunny, high of 61\u00b0F (16\u00b0C)\nSo in summary, expect seasonable spring weather in San Francisco over the next several days, with a mix of sun and clouds and temperatures ranging from the upper 40s at night to the low 60s during the days. Typical dry conditions with no rain in the forecast.\n================================ Human Message =================================\nwhat's the weather in nyc?\n================================== Ai Message ==================================\n[{'text': 'Here are the current weather conditions and forecast for New York City:', 'type': 'text'}, {'id': 'toolu_01FFft5Sx9oS6AdVJuRWWcGp', 'input': {'query': 'weather in new york city'}, 'name': 'tavily_search_results_json', 'type': 'tool_use'}]\nTool Calls:\ntavily_search_results_json (toolu_01FFft5Sx9oS6AdVJuRWWcGp)\nCall ID: toolu_01FFft5Sx9oS6AdVJuRWWcGp\nArgs:\nquery: weather in new york city\n================================= Tool Message =================================\nName: tavily_search_results_json\n[{\"url\": \"https://www.weatherapi.com/\", \"content\": \"{'location': {'name': 'New York', 'region': 'New York', 'country': 'United States of America', 'lat': 40.71, 'lon': -74.01, 'tz_id': 'America/New_York', 'localtime_epoch': 1718734479, 'localtime': '2024-06-18 14:14'}, 'current': {'last_updated_epoch': 1718733600, 'last_updated': '2024-06-18 14:00', 'temp_c': 29.4, 'temp_f': 84.9, 'is_day': 1, 'condition': {'text': 'Sunny', 'icon': '//cdn.weatherapi.com/weather/64x64/day/113.png', 'code': 1000}, 'wind_mph': 2.2, 'wind_kph': 3.6, 'wind_degree': 158, 'wind_dir': 'SSE', 'pressure_mb': 1025.0, 'pressure_in': 30.26, 'precip_mm': 0.0, 'precip_in': 0.0, 'humidity': 63, 'cloud': 0, 'feelslike_c': 31.3, 'feelslike_f': 88.3, 'windchill_c': 28.3, 'windchill_f': 82.9, 'heatindex_c': 29.6, 'heatindex_f': 85.3, 'dewpoint_c': 18.4, 'dewpoint_f': 65.2, 'vis_km': 16.0, 'vis_miles': 9.0, 'uv': 7.0, 'gust_mph': 16.5, 'gust_kph': 26.5}}\"}]\n================================== Ai Message ==================================\nAccording to the weather data from WeatherAPI:\nCurrent Conditions in New York City (as of 2:00 PM local time):\n* Temperature: 85\u00b0F (29\u00b0C)\n* Conditions: Sunny\n* Wind: 2 mph (4 km/h) from the SSE\n* Humidity: 63%\n* Heat Index: 85\u00b0F (30\u00b0C)\nThe forecast shows sunny and warm conditions persisting over the next few days:\nToday: Sunny, high of 85\u00b0F (29\u00b0C)\nTonight: Clear, low of 68\u00b0F (20\u00b0C)\nTomorrow: Sunny, high of 88\u00b0F (31\u00b0C)\nThursday: Mostly sunny, high of 90\u00b0F (32\u00b0C)\nFriday: Partly cloudy, high of 87\u00b0F (31\u00b0C)\nSo New York City is experiencing beautiful sunny weather with seasonably warm temperatures in the mid-to-upper 80s Fahrenheit (around 30\u00b0C). Humidity is moderate in the 60% range. Overall, ideal late spring/early summer conditions for being outdoors in the city over the next several days.", "tokens": 769, "node_type": "child"}
{"id": 75, "chunk_id": "c16acf8a553ac24f5d77e7d3a1c6e493", "parent_id": 70, "url": "", "namespace": "langchain", "title": "langsmith-env-var", "headers": ["langsmith-env-var"], "section_index": 0, "chunk_index": 0, "text": "# langsmith-env-var > Source: https://docs.langchain.com/langsmith/env-var BG_JOB_ISOLATED_LOOPS Set BG_JOB_ISOLATED_LOOPS to True to execute background runs in an isolated event loop separate from the serving API event loop. This environment variable should be set to True if the implementation of a graph/node contains synchronous code. In this situation, the synchronous code will block the serving API event loop, which may cause the API to be unavailable. A symptom of an unavailable API is continuous application restarts due to failing health checks. Defaults to False . BG_JOB_SHUTDOWN_GRACE_PERIOD_SECS Specifies, in seconds, how long the server will wait for background jobs to finish after the queue receives a shutdown signal. After this period, the server will force termination. Defaults to 180 seconds. Set this to ensure jobs have enough time to complete cleanly during shutdown. Added in langgraph-api==0.2.16 . BG_JOB_TIMEOUT_SECS The timeout of a background run can be increased. However, the infrastructure for a Cloud deployment enforces a 1 hour timeout limit for API requests. This means the connection between client and server will timeout after 1 hour. This is not configurable. A background run can execute for longer than 1 hour, but a client must reconnect to the server (e.g. join stream via POST /threads/{thread_id}/runs/{run_id}/stream ) to retrieve output from the run if the run is taking longer than 1 hour. Defaults to 3600 . DD_API_KEY Specify DD_API_KEY (your Datadog API Key) to automatically enable Datadog tracing for the deployment. Specify other DD_* environment variables to configure the tracing instrumentation. If DD_API_KEY is specified, the application process is wrapped in the ddtrace-run command. Other DD_* environment variables (e.g. DD_SITE , DD_ENV , DD_SERVICE , DD_TRACE_ENABLED ) are typically needed to properly configure the tracing instrumentation. See DD_* environment variables for more details. You can enable DD_TRACE_DEBUG=true and set DD_LOG_LEVEL=debug to troubleshoot. Enabling DD_API_KEY (and thus ddtrace-run ) can override or interfere with other auto-instrumentation solutions (such as OpenTelemetry) that you may have instrumented into your application code.LANGCHAIN_TRACING_SAMPLING_RATE Sampling rate for traces sent to LangSmith. Valid values: Any float between 0 and 1 . For more details, refer to Set a sampling rate for traces. LANGGRAPH_AUTH_TYPE Type of authentication for the LangGraph Server deployment. Valid values: langsmith , noop . For deployments to LangSmith, this environment variable is set automatically. For local development or deployments where authentication is handled externally (e.g. self-hosted), set this environment variable to noop . LANGGRAPH_POSTGRES_POOL_MAX_SIZE Beginning with langgraph-api version 0.2.12 , the maximum size of the Postgres connection pool (per replica) can be controlled using the LANGGRAPH_POSTGRES_POOL_MAX_SIZE environment variable. By setting this variable, you can determine the upper bound on the number of simultaneous connections the server will establish with the Postgres database. For example, if a deployment is scaled up to 10 replicas and LANGGRAPH_POSTGRES_POOL_MAX_SIZE is configured to 150 , then up to 1500 connections to Postgres can be established. This is particularly useful for deployments where database resources are limited (or more available) or where you need to tune connection behavior for performance or scaling reasons. Defaults to 150 connections. LANGSMITH_API_KEY For deployments with self-hosted LangSmith only. To send traces to a self-hosted LangSmith instance, set LANGSMITH_API_KEY to an API key created from the self-hosted instance. LANGSMITH_ENDPOINT For deployments with self-hosted LangSmith only. To send traces to a self-hosted LangSmith instance, set LANGSMITH_ENDPOINT to the hostname of the self-hosted instance. LANGSMITH_TRACING Set LANGSMITH_TRACING to false to disable tracing to LangSmith. Defaults to true . LOG_COLOR This is mainly relevant in the context of using the dev server via the langgraph dev command. Set LOG_COLOR to true to enable ANSI-colored console output when using the default console renderer. Disabling color output by setting this variable to false produces monochrome logs. Defaults to true . LOG_LEVEL Configure log level. Defaults to INFO . LOG_JSON Set LOG_JSON to true to render all log messages as JSON objects using the configured JSONRenderer . This produces structured logs that can be easily parsed or ingested by log management systems. Defaults to false . MOUNT_PREFIX Only Allowed in Self-Hosted Deployments The MOUNT_PREFIX environment variable is only allowed in Self-Hosted Deployment models, LangSmith SaaS will not allow this environment variable.MOUNT_PREFIX to serve the LangGraph Server under a specific path prefix. This is useful for deployments where the server is behind a reverse proxy or load balancer that requires a specific path prefix. For example, if the server is to be served under https://example.com/langgraph , set MOUNT_PREFIX to /langgraph . N_JOBS_PER_WORKER Number of jobs per worker for the LangGraph Server task queue. Defaults to 10 . POSTGRES_URI_CUSTOM Specify POSTGRES_URI_CUSTOM to use a custom Postgres instance. The value of POSTGRES_URI_CUSTOM must be a valid Postgres connection URI. Postgres: - Version 15.8 or higher. - An initial database must be present and the connection URI must reference the database. - If POSTGRES_URI_CUSTOM is specified, the control plane will not provision a database for the server. - If POSTGRES_URI_CUSTOM is removed, the control plane will not provision a database for the server and will not delete the externally managed Postgres instance. - If POSTGRES_URI_CUSTOM is removed, deployment of the revision will not succeed. OncePOSTGRES_URI_CUSTOM is specified, it must always be set for the lifecycle of the deployment. - If the deployment is deleted, the control plane will not delete the externally managed Postgres instance. - The value of POSTGRES_URI_CUSTOM can be updated. For example, a password in the URI can be updated. - The custom Postgres instance must be accessible by the LangGraph Server. The user is responsible for ensuring connectivity. REDIS_CLUSTER This feature is in Alpha. Only Allowed in Self-Hosted Deployments Redis Cluster mode is only available in Self-Hosted Deployment models, LangSmith SaaS will provision a redis instance for you by default. REDIS_CLUSTER to True to enable Redis Cluster mode. When enabled, the system will connect to Redis using cluster mode. This is useful when connecting to a Redis Cluster deployment. Defaults to False . REDIS_KEY_PREFIX Available in API Server version 0.1.9+ This environment variable is supported in API Server version 0.1.9 and above. '' . REDIS_URI_CUSTOM Specify", "tokens": 1000, "node_type": "child"}
{"id": 76, "chunk_id": "9b0fd3b850698317bef57fa6e71a38ba", "parent_id": 70, "url": "", "namespace": "langchain", "title": "langsmith-env-var", "headers": ["langsmith-env-var"], "section_index": 0, "chunk_index": 1, "text": "be set for the lifecycle of the deployment. - If the deployment is deleted, the control plane will not delete the externally managed Postgres instance. - The value of POSTGRES_URI_CUSTOM can be updated. For example, a password in the URI can be updated. - The custom Postgres instance must be accessible by the LangGraph Server. The user is responsible for ensuring connectivity. REDIS_CLUSTER This feature is in Alpha. Only Allowed in Self-Hosted Deployments Redis Cluster mode is only available in Self-Hosted Deployment models, LangSmith SaaS will provision a redis instance for you by default. REDIS_CLUSTER to True to enable Redis Cluster mode. When enabled, the system will connect to Redis using cluster mode. This is useful when connecting to a Redis Cluster deployment. Defaults to False . REDIS_KEY_PREFIX Available in API Server version 0.1.9+ This environment variable is supported in API Server version 0.1.9 and above. '' . REDIS_URI_CUSTOM Specify REDIS_URI_CUSTOM to use a custom Redis instance. The value of REDIS_URI_CUSTOM must be a valid Redis connection URI. RESUMABLE_STREAM_TTL_SECONDS Time-to-live in seconds for resumable stream data in Redis. When a run is created and the output is streamed, the stream can be configured to be resumable (e.g. stream_resumable=True ). If a stream is resumable, output from the stream is temporarily stored in Redis. The TTL for this data can be configured by setting RESUMABLE_STREAM_TTL_SECONDS . See the Python and JS/TS SDKs for more details on how to implement resumable streams. Defaults to 120 seconds.", "tokens": 244, "node_type": "child"}
{"id": 77, "chunk_id": "c16acf8a553ac24f5d77e7d3a1c6e493", "parent_id": 71, "url": "", "namespace": "langchain", "title": "langsmith-evaluate-chatbot-tutorial", "headers": ["langsmith-evaluate-chatbot-tutorial"], "section_index": 0, "chunk_index": 0, "text": "# langsmith-evaluate-chatbot-tutorial\n\n> Source: https://docs.langchain.com/langsmith/evaluate-chatbot-tutorial\n\n- Create an initial golden dataset to measure performance\n- Define metrics to use to measure performance\n- Run evaluations on a few different prompts or models\n- Compare results manually\n- Track results over time\n- Set up automated testing to run in CI/CD\nSetup\nFirst install the required dependencies for this tutorial. We happen to use OpenAI, but LangSmith can be used with any model:Create a dataset\nThe first step when getting ready to test and evaluate your application is to define the datapoints you want to evaluate. There are a few aspects to consider here:- What should the schema of each datapoint be?\n- How many datapoints should I gather?\n- How should I gather those datapoints?\nQA Example Dataset\nin the Datasets & Testing\npage, when we click into it we should see that we have five new examples.\nDefine metrics\nAfter creating our dataset, we can now define some metrics to evaluate our responses on. Since we have an expected answer, we can compare to that as part of our evaluation. However, we do not expect our application to output those exact answers, but rather something that is similar. This makes our evaluation a little trickier. In addition to evaluating correctness, let\u2019s also make sure our answers are short and concise. This will be a little easier - we can define a simple Python function to measure the length of the response. Let\u2019s go ahead and define these two metrics. For the first, we will use an LLM to judge whether the output is correct (with respect to the expected output). This LLM-as-a-judge is relatively common for cases that are too complex to measure with a simple function. We can define our own prompt and LLM to use for evaluation here:Run Evaluations\nGreat! So now how do we run evaluations? Now that we have a dataset and evaluators, all that we need is our application! We will build a simple application that just has a system message with instructions on how to respond and then passes it to the LLM. We will build this using the OpenAI SDK directly:Experiments\ntab, we can now see a summary of our one run!\nLet\u2019s now try it out with a different model! Let\u2019s try gpt-4-turbo\nExperiments\ntab on the datasets page, we should see that all three runs now show up!\nComparing results\nAwesome, we\u2019ve evaluated three different runs. But how can we compare results? The first way we can do this is just by looking at the runs in theExperiments\ntab. If we do that, we can see a high level view of the metrics for each run:\nGreat! So we can tell that GPT-4 is better than GPT-3.5 at knowing who companies are, and we can see that the strict prompt helped a lot with the length. But what if we want to explore in more detail?\nIn order to do that, we can select all the runs we want to compare (in this case all three) and open them up in a comparison view. We immediately see all three tests side by side. Some of the cells are color coded - this is showing a regression of a certain metric compared to a certain baseline. We automatically choose defaults for the baseline and metric, but you can change those yourself. You can also choose which columns and which metrics you see by using the Display\ncontrol. You can also automatically filter to only see the runs that have improvements/regressions by clicking on the icons at the top.\nIf we want to see more information, we can also select the Expand\nbutton that appears when hovering over a row to open up a side panel with more detailed information:\nSet up automated testing to run in CI/CD\nNow that we\u2019ve run this in a one-off manner, we can set it to run in an automated fashion. We can do this pretty easily by just including it as a pytest file that we run in CI/CD. As part of this, we can either just log the results OR set up some criteria to determine if it passes or not. For example, if I wanted to ensure that we always got at least 80% of generated responses passing thelength\ncheck, we could set that up with a test like:\nTrack results over time\nNow that we\u2019ve got these experiments running in an automated fashion, we want to track these results over time. We can do this from the overallExperiments\ntab in the datasets page. By default, we show evaluation metrics over time (highlighted in red). We also automatically track git metrics, to easily associate it with the branch of your code (highlighted in yellow).\nConclusion\nThat\u2019s it for this tutorial! We\u2019ve gone over how to create an initial test set, define some evaluation metrics, run experiments, compare them manually, set up CI/CD, and track results over time. Hopefully this can help you iterate with confidence. This is just the start. As mentioned earlier, evaluation is an ongoing process. For example - the datapoints you will want to evaluate on will likely continue to change over time. There are many types of evaluators you may wish to explore. For information on this, check out the how-to guides. Additionally, there are other ways to evaluate data besides in this \u201coffline\u201d manner (e.g. you can evaluate production data). For more information on online evaluation, check out this guide.Reference code\nClick to see a consolidated code snippet\nClick to see a consolidated code snippet", "tokens": 927, "node_type": "child"}
{"id": 78, "chunk_id": "c16acf8a553ac24f5d77e7d3a1c6e493", "parent_id": 72, "url": "", "namespace": "langchain", "title": "langsmith-evaluate-complex-agent", "headers": ["langsmith-evaluate-complex-agent"], "section_index": 0, "chunk_index": 0, "text": "# langsmith-evaluate-complex-agent > Source: https://docs.langchain.com/langsmith/evaluate-complex-agent - Final response: Evaluate the agent\u2019s final response. - Trajectory: Evaluate whether the agent took the expected path (e.g., of tool calls) to arrive at the final answer. - Single step: Evaluate any agent step in isolation (e.g., whether it selects the appropriate first tool for a given step). Setup Configure the environment Let\u2019s install the required dependencies:Copy pip install -U langgraph langchain[openai] Copy import getpass import os def _set_env(var: str) -> None: if not os.environ.get(var): os.environ[var] = getpass.getpass(f\"Set {var}: \") os.environ[\"LANGSMITH_TRACING\"] = \"true\" _set_env(\"LANGSMITH_API_KEY\") _set_env(\"OPENAI_API_KEY\") Download the database We will create a SQLite database for this tutorial. SQLite is a lightweight database that is easy to set up and use. We will load thechinook database, which is a sample database that represents a digital media store. Find more information about the database here. For convenience, we have hosted the database in a public GCS bucket: Copy import requests url = \"https://storage.googleapis.com/benchmarks-artifacts/chinook/Chinook.db\" response = requests.get(url) if response.status_code == 200: # Open a local file in binary write mode with open(\"chinook.db\", \"wb\") as file: # Write the content of the response (the file) to the local file file.write(response.content) print(\"File downloaded and saved as Chinook.db\") else: print(f\"Failed to download the file. Status code: {response.status_code}\") Copy import sqlite3 # ... database connection and query code Copy [(1, 'AC/DC'), (2, 'Accept'), (3, 'Aerosmith'), (4, 'Alanis Morissette'), (5, 'Alice In Chains'), (6, 'Ant\u00f4nio Carlos Jobim'), (7, 'Apocalyptica'), (8, 'Audioslave'), (9, 'BackBeat'), (10, 'Billy Cobham')] Define the customer support agent We\u2019ll create a LangGraph agent with limited access to our database. For demo purposes, our agent will support two basic types of requests:- Lookup: The customer can look up song titles, artist names, and albums based on other identifying information. For example: \u201cWhat songs do you have by Jimi Hendrix?\u201d - Refund: The customer can request a refund on their past purchases. For example: \u201cMy name is Claude Shannon and I\u2019d like a refund on a purchase I made last week, could you help me?\u201d Refund agent Let\u2019s build the refund processing agent. This agent needs to:- Find the customer\u2019s purchase records in the database - Delete the relevant Invoice and InvoiceLine records to process the refund - A function to execute the refund by deleting records - A function to look up a customer\u2019s purchase history Copy import sqlite3 def _refund(invoice_id: int | None, invoice_line_ids: list[int] | None, mock: bool = False) -> float: ... def _lookup( ... - Extract customer and purchase information from the conversation - Route the request to one of three paths: - Refund path: If we have sufficient purchase details (Invoice ID or Invoice Line IDs) to process a refund - Lookup path: If we have enough customer information (name and phone) to search their purchase history - Response path: If we need more information, respond to the user requesting the specific details needed - The conversation history (messages between user and agent) - All customer and purchase information extracted from the conversation - The next message to send to the user (followup text) Copy from typing import Literal import json from langchain.chat_models import init_chat_model from langchain_core.runnables import RunnableConfig from langgraph.graph import END, StateGraph from langgraph.graph.message import AnyMessage, add_messages from langgraph.types import Command, interrupt from tabulate import tabulate from typing_extensions import Annotated, TypedDict # Graph state. class State(TypedDict): \"\"\"Agent state.\"\"\" messages: Annotated[list[AnyMessage], add_messages] followup: str | None invoice_id: int | None invoice_line_ids: list[int] | None customer_first_name: str | None customer_last_name: str | None customer_phone: str | None track_name: str | None album_title: str | None artist_name: str | None purchase_date_iso_8601: str | None # Instructions for extracting the user/purchase info from the conversation. gather_info_instructions = \"\"\"You are managing an online music store that sells song tracks. \\ Customers can buy multiple tracks at a time and these purchases are recorded in a database as \\ an Invoice per purchase and an associated set of Invoice Lines for each purchased track. Your task is to help customers who would like a refund for one or more of the tracks they've \\ purchased. In order for you to be able refund them, the customer must specify the Invoice ID \\ to get a refund on all the tracks they bought in a single transaction, or one or more Invoice \\ Line IDs if they would like refunds on individual tracks. Often a user will not know the specific Invoice ID(s) or Invoice Line ID(s) for which they \\ would like a refund. In this case you can help them look up their invoices by asking them to \\ specify: - Required: Their first name, last name, and phone number. - Optionally: The track name, artist name, album name, or purchase date. If the customer has not specified the required information (either Invoice/Invoice Line IDs \\ or first name, last name, phone) then please ask them to specify it.\"\"\" # Extraction schema, mirrors the graph state. class PurchaseInformation(TypedDict): \"\"\"All of the known information about the invoice / invoice lines the customer would like refunded. Do not make up values, leave fields as null if you don't know their value.\"\"\" invoice_id: int | None invoice_line_ids: list[int] | None customer_first_name: str | None customer_last_name: str | None customer_phone: str | None track_name: str | None album_title: str | None artist_name: str | None purchase_date_iso_8601: str | None followup: Annotated[ str | None, ..., \"If the user hasn't enough identifying information, please tell them what the required information is and ask them to specify it.\", ] # Model for performing extraction. info_llm = init_chat_model(\"gpt-4o-mini\").with_structured_output( PurchaseInformation, method=\"json_schema\", include_raw=True ) # Graph node for extracting user info and routing to lookup/refund/END. async def gather_info(state: State) -> Command[Literal[\"lookup\", \"refund\", END]]: info = await info_llm.ainvoke( [ {\"role\": \"system\", \"content\": gather_info_instructions}, *state[\"messages\"], ] ) parsed = info[\"parsed\"] if any(parsed[k] for k in (\"invoice_id\", \"invoice_line_ids\")): goto = \"refund\" elif all( parsed[k] for k in (\"customer_first_name\", \"customer_last_name\", \"customer_phone\") ): goto = \"lookup\" else: goto = END update = {\"messages\": [info[\"raw\"]], **parsed} return Command(update=update, goto=goto) # Graph node for", "tokens": 1000, "node_type": "child"}
{"id": 79, "chunk_id": "9b0fd3b850698317bef57fa6e71a38ba", "parent_id": 72, "url": "", "namespace": "langchain", "title": "langsmith-evaluate-complex-agent", "headers": ["langsmith-evaluate-complex-agent"], "section_index": 0, "chunk_index": 1, "text": "value.\"\"\" invoice_id: int | None invoice_line_ids: list[int] | None customer_first_name: str | None customer_last_name: str | None customer_phone: str | None track_name: str | None album_title: str | None artist_name: str | None purchase_date_iso_8601: str | None followup: Annotated[ str | None, ..., \"If the user hasn't enough identifying information, please tell them what the required information is and ask them to specify it.\", ] # Model for performing extraction. info_llm = init_chat_model(\"gpt-4o-mini\").with_structured_output( PurchaseInformation, method=\"json_schema\", include_raw=True ) # Graph node for extracting user info and routing to lookup/refund/END. async def gather_info(state: State) -> Command[Literal[\"lookup\", \"refund\", END]]: info = await info_llm.ainvoke( [ {\"role\": \"system\", \"content\": gather_info_instructions}, *state[\"messages\"], ] ) parsed = info[\"parsed\"] if any(parsed[k] for k in (\"invoice_id\", \"invoice_line_ids\")): goto = \"refund\" elif all( parsed[k] for k in (\"customer_first_name\", \"customer_last_name\", \"customer_phone\") ): goto = \"lookup\" else: goto = END update = {\"messages\": [info[\"raw\"]], **parsed} return Command(update=update, goto=goto) # Graph node for executing the refund. # Note that here we inspect the runtime config for an \"env\" variable. # If \"env\" is set to \"test\", then we don't actually delete any rows from our database. # This will become important when we're running our evaluations. def refund(state: State, config: RunnableConfig) -> dict: # Whether to mock the deletion. True if the configurable var 'env' is set to 'test'. mock = config.get(\"configurable\", {}).get(\"env\", \"prod\") == \"test\" refunded = _refund( invoice_id=state[\"invoice_id\"], invoice_line_ids=state[\"invoice_line_ids\"], mock=mock ) response = f\"You have been refunded a total of: ${refunded:.2f}. Is there anything else I can help with?\" return { \"messages\": [{\"role\": \"assistant\", \"content\": response}], \"followup\": response, } # Graph node for looking up the users purchases def lookup(state: State) -> dict: args = ( state[k] for k in ( \"customer_first_name\", \"customer_last_name\", \"customer_phone\", \"track_name\", \"album_title\", \"artist_name\", \"purchase_date_iso_8601\", ) ) results = _lookup(*args) if not results: response = \"We did not find any purchases associated with the information you've provided. Are you sure you've entered all of your information correctly?\" followup = response else: response = f\"Which of the following purchases would you like to be refunded for?\\n\\n```json{json.dumps(results, indent=2)}\\n```\" followup = f\"Which of the following purchases would you like to be refunded for?\\n\\n{tabulate(results, headers='keys')}\" return { \"messages\": [{\"role\": \"assistant\", \"content\": response}], \"followup\": followup, \"invoice_line_ids\": [res[\"invoice_line_id\"] for res in results], } # Building our graph graph_builder = StateGraph(State) graph_builder.add_node(gather_info) graph_builder.add_node(refund) graph_builder.add_node(lookup) graph_builder.set_entry_point(\"gather_info\") graph_builder.add_edge(\"lookup\", END) graph_builder.add_edge(\"refund\", END) refund_graph = graph_builder.compile() Copy # Assumes you're in an interactive Python environmentfrom IPython.display import Image, display ... Lookup agent For the lookup (i.e. question-answering) agent, we\u2019ll use a simple ReACT architecture and give the agent tools for looking up track names, artist names, and album names based on various filters. For example, you can look up albums by a particular artist, artists who released songs with a specific name, etc.Copy from langchain.embeddings import init_embeddings from langchain.tools import tool from langchain_core.vectorstores import InMemoryVectorStore from langchain.agents import create_agent # Our SQL queries will only work if we filter on the exact string values that are in the DB. # To ensure this, we'll create vectorstore indexes for all of the artists, tracks and albums # ahead of time and use those to disambiguate the user input. E.g. if a user searches for # songs by \"prince\" and our DB records the artist as \"Prince\", ideally when we query our # artist vectorstore for \"prince\" we'll get back the value \"Prince\", which we can then # use in our SQL queries. def index_fields() -> tuple[InMemoryVectorStore, InMemoryVectorStore, InMemoryVectorStore]: ... track_store, artist_store, album_store = index_fields() # Agent tools @tool def lookup_track( ... @tool def lookup_album( ... @tool def lookup_artist( ... # Agent model qa_llm = init_chat_model(\"claude-3-5-sonnet-latest\") # The prebuilt ReACT agent only expects State to have a 'messages' key, so the # state we defined for the refund agent can also be passed to our lookup agent. qa_graph = create_agent(qa_llm, tools=[lookup_track, lookup_artist, lookup_album]) Copy display(Image(qa_graph.get_graph(xray=True).draw_mermaid_png())) Parent agent Now let\u2019s define a parent agent that combines our two task-specific agents. The only job of the parent agent is to route to one of the sub-agents by classifying the user\u2019s current intent, and to compile the output into a followup message.Copy # Schema for routing user intent. # We'll use structured outputs to enforce that the model returns only # the desired output. class UserIntent(TypedDict): \"\"\"The user's current intent in the conversation\"\"\" intent: Literal[\"refund\", \"question_answering\"] # Routing model with structured output router_llm = init_chat_model(\"gpt-4o-mini\").with_structured_output( UserIntent, method=\"json_schema\", strict=True ) # Instructions for routing. route_instructions = \"\"\"You are managing an online music store that sells song tracks. \\ You can help customers in two types of ways: (1) answering general questions about \\ tracks sold at your store, (2) helping them get a refund on a purhcase they made at your store. Based on the following conversation, determine if the user is currently seeking general \\ information about song tracks or if they are trying to refund a specific purchase. Return 'refund' if they are trying to get a refund and 'question_answering' if they are \\ asking a general music question. Do NOT return anything else. Do NOT try to respond to \\ the user. \"\"\" # Node for routing. async def intent_classifier( state: State, ) -> Command[Literal[\"refund_agent\", \"question_answering_agent\"]]: response = router_llm.invoke( [{\"role\": \"system\", \"content\": route_instructions}, *state[\"messages\"]] ) return Command(goto=response[\"intent\"] + \"_agent\") # Node for making sure the 'followup' key is set before our agent run completes. def compile_followup(state: State) -> dict: \"\"\"Set the followup to be the last message if it hasn't explicitly been set.\"\"\" if not state.get(\"followup\"): return {\"followup\": state[\"messages\"][-1].content} return {} # Agent definition graph_builder = StateGraph(State) graph_builder.add_node(intent_classifier) # Since all of our subagents have compatible state, # we can add them as nodes directly. graph_builder.add_node(\"refund_agent\", refund_graph) graph_builder.add_node(\"question_answering_agent\", qa_graph) graph_builder.add_node(compile_followup) graph_builder.set_entry_point(\"intent_classifier\") graph_builder.add_edge(\"refund_agent\", \"compile_followup\") graph_builder.add_edge(\"question_answering_agent\", \"compile_followup\") graph_builder.add_edge(\"compile_followup\", END) graph = graph_builder.compile() Copy display(Image(graph.get_graph().draw_mermaid_png())) Try it out Let\u2019s give our custom support agent a whirl!Copy state = await graph.ainvoke( {\"messages\": [{\"role\": \"user\", \"content\": \"what james brown songs do you have\"}]} ) print(state[\"followup\"]) Copy I found 20 James Brown songs in the database, all from the album \"Sex Machine\". Here they are: ... Copy state", "tokens": 1000, "node_type": "child"}
{"id": 80, "chunk_id": "5345104ec252a41359fa4a3d2dfbec23", "parent_id": 72, "url": "", "namespace": "langchain", "title": "langsmith-evaluate-complex-agent", "headers": ["langsmith-evaluate-complex-agent"], "section_index": 0, "chunk_index": 2, "text": ") -> Command[Literal[\"refund_agent\", \"question_answering_agent\"]]: response = router_llm.invoke( [{\"role\": \"system\", \"content\": route_instructions}, *state[\"messages\"]] ) return Command(goto=response[\"intent\"] + \"_agent\") # Node for making sure the 'followup' key is set before our agent run completes. def compile_followup(state: State) -> dict: \"\"\"Set the followup to be the last message if it hasn't explicitly been set.\"\"\" if not state.get(\"followup\"): return {\"followup\": state[\"messages\"][-1].content} return {} # Agent definition graph_builder = StateGraph(State) graph_builder.add_node(intent_classifier) # Since all of our subagents have compatible state, # we can add them as nodes directly. graph_builder.add_node(\"refund_agent\", refund_graph) graph_builder.add_node(\"question_answering_agent\", qa_graph) graph_builder.add_node(compile_followup) graph_builder.set_entry_point(\"intent_classifier\") graph_builder.add_edge(\"refund_agent\", \"compile_followup\") graph_builder.add_edge(\"question_answering_agent\", \"compile_followup\") graph_builder.add_edge(\"compile_followup\", END) graph = graph_builder.compile() Copy display(Image(graph.get_graph().draw_mermaid_png())) Try it out Let\u2019s give our custom support agent a whirl!Copy state = await graph.ainvoke( {\"messages\": [{\"role\": \"user\", \"content\": \"what james brown songs do you have\"}]} ) print(state[\"followup\"]) Copy I found 20 James Brown songs in the database, all from the album \"Sex Machine\". Here they are: ... Copy state = await graph.ainvoke({\"messages\": [ { \"role\": \"user\", \"content\": \"my name is Aaron Mitchell and my number is +1 (204) 452-6452. I bought some songs by Led Zeppelin that i'd like refunded\", } ]}) print(state[\"followup\"]) Copy Which of the following purchases would you like to be refunded for? ... Evaluations Now that we\u2019ve got a testable version of our agent, let\u2019s run some evaluations. Agent evaluation can focus on at least 3 things:- Final response: The inputs are a prompt and an optional list of tools. The output is the final agent response. - Trajectory: As before, the inputs are a prompt and an optional list of tools. The output is the list of tool calls - Single step: As before, the inputs are a prompt and an optional list of tools. The output is the tool call. Final response evaluator First, let\u2019s create a dataset that evaluates end-to-end performance of the agent. For simplicity we\u2019ll use the same dataset for final response and trajectory evaluation, so we\u2019ll add both ground-truth responses and trajectories for each example question. We\u2019ll cover the trajectories in the next section.Copy from langsmith import Client client = Client() # Create a dataset examples = [ { \"inputs\": { \"question\": \"How many songs do you have by James Brown\", }, \"outputs\": { \"response\": \"We have 20 songs by James Brown\", \"trajectory\": [\"question_answering_agent\", \"lookup_track\"] } }, { \"inputs\": { \"question\": \"My name is Aaron Mitchell and I'd like a refund.\", }, \"outputs\": { \"response\": \"I need some more information to help you with the refund. Please specify your phone number, the invoice ID, or the line item IDs for the purchase you'd like refunded.\", \"trajectory\": [\"refund_agent\"], } }, { \"inputs\": { \"question\": \"My name is Aaron Mitchell and I'd like a refund on my Led Zeppelin purchases. My number is +1 (204) 452-6452\", }, \"outputs\": { \"response\": 'Which of the following purchases would you like to be refunded for?\\n\\n invoice_line_id track_name artist_name purchase_date quantity_purchased price_per_unit\\n----------------- -------------------------------- ------------- ------------------- -------------------- ----------------\\n 267 How Many More Times Led Zeppelin 2009-08-06 00:00:00 1 0.99\\n 268 What Is And What Should Never Be Led Zeppelin 2009-08-06 00:00:00 1 0.99', \"trajectory\": [\"refund_agent\", \"lookup\"], }, }, { \"inputs\": { \"question\": \"Who recorded Wish You Were Here again? What other albums of there's do you have?\", }, \"outputs\": { \"response\": \"Wish You Were Here is an album by Pink Floyd\", \"trajectory\": [\"question_answering_agent\", \"lookup_album\"], }, }, { \"inputs\": { \"question\": \"I want a full refund for invoice 237\", }, \"outputs\": { \"response\": \"You have been refunded $0.99.\", \"trajectory\": [\"refund_agent\", \"refund\"], } }, ] dataset_name = \"Chinook Customer Service Bot: E2E\" if not client.has_dataset(dataset_name=dataset_name): dataset = client.create_dataset(dataset_name=dataset_name) client.create_examples( dataset_id=dataset.id, examples=examples ) Copy # LLM-as-judge instructions grader_instructions = \"\"\"You are a teacher grading a quiz. You will be given a QUESTION, the GROUND TRUTH (correct) RESPONSE, and the STUDENT RESPONSE. Here is the grade criteria to follow: (1) Grade the student responses based ONLY on their factual accuracy relative to the ground truth answer. (2) Ensure that the student response does not contain any conflicting statements. (3) It is OK if the student response contains more information than the ground truth response, as long as it is factually accurate relative to the ground truth response. Correctness: True means that the student's response meets all of the criteria. False means that the student's response does not meet all of the criteria. Explain your reasoning in a step-by-step manner to ensure your reasoning and conclusion are correct.\"\"\" # LLM-as-judge output schema class Grade(TypedDict): \"\"\"Compare the expected and actual answers and grade the actual answer.\"\"\" reasoning: Annotated[str, ..., \"Explain your reasoning for whether the actual response is correct or not.\"] is_correct: Annotated[bool, ..., \"True if the student response is mostly or exactly correct, otherwise False.\"] # Judge LLM grader_llm = init_chat_model(\"gpt-4o-mini\", temperature=0).with_structured_output(Grade, method=\"json_schema\", strict=True) # Evaluator function async def final_answer_correct(inputs: dict, outputs: dict, reference_outputs: dict) -> bool: \"\"\"Evaluate if the final response is equivalent to reference response.\"\"\" # Note that we assume the outputs has a 'response' dictionary. We'll need to make sure # that the target function we define includes this key. user = f\"\"\"QUESTION: {inputs['question']} GROUND TRUTH RESPONSE: {reference_outputs['response']} STUDENT RESPONSE: {outputs['response']}\"\"\" grade = await grader_llm.ainvoke([{\"role\": \"system\", \"content\": grader_instructions}, {\"role\": \"user\", \"content\": user}]) return grade[\"is_correct\"] config={\"env\": \"test\"} , we would mock out the refunds without actually updating the DB. We\u2019ll use this configurable variable in our target run_graph method when invoking our graph: Copy # Target function async def run_graph(inputs: dict) -> dict: \"\"\"Run graph and track the trajectory it takes along with the final response.\"\"\" result = await graph.ainvoke({\"messages\": [ { \"role\": \"user\", \"content\": inputs['question']}, ]}, config={\"env\": \"test\"}) return {\"response\": result[\"followup\"]} # Evaluation job and results experiment_results = await client.aevaluate( run_graph, data=dataset_name, evaluators=[final_answer_correct], experiment_prefix=\"sql-agent-gpt4o-e2e\", num_repetitions=1, max_concurrency=4, ) experiment_results.to_pandas() Trajectory evaluator As agents become more complex, they have more potential points of failure. Rather than using simple pass/fail evaluations, it\u2019s often better to use evaluations that can give partial credit when an agent takes some correct steps, even if it doesn\u2019t reach the right final answer. This is where trajectory evaluations come in. A trajectory evaluation:- Compares the actual", "tokens": 1000, "node_type": "child"}
{"id": 81, "chunk_id": "0f7e5a6eec52bd2b622d96bc793bb7d8", "parent_id": 72, "url": "", "namespace": "langchain", "title": "langsmith-evaluate-complex-agent", "headers": ["langsmith-evaluate-complex-agent"], "section_index": 0, "chunk_index": 3, "text": "{\"role\": \"user\", \"content\": user}]) return grade[\"is_correct\"] config={\"env\": \"test\"} , we would mock out the refunds without actually updating the DB. We\u2019ll use this configurable variable in our target run_graph method when invoking our graph: Copy # Target function async def run_graph(inputs: dict) -> dict: \"\"\"Run graph and track the trajectory it takes along with the final response.\"\"\" result = await graph.ainvoke({\"messages\": [ { \"role\": \"user\", \"content\": inputs['question']}, ]}, config={\"env\": \"test\"}) return {\"response\": result[\"followup\"]} # Evaluation job and results experiment_results = await client.aevaluate( run_graph, data=dataset_name, evaluators=[final_answer_correct], experiment_prefix=\"sql-agent-gpt4o-e2e\", num_repetitions=1, max_concurrency=4, ) experiment_results.to_pandas() Trajectory evaluator As agents become more complex, they have more potential points of failure. Rather than using simple pass/fail evaluations, it\u2019s often better to use evaluations that can give partial credit when an agent takes some correct steps, even if it doesn\u2019t reach the right final answer. This is where trajectory evaluations come in. A trajectory evaluation:- Compares the actual sequence of steps the agent took against an expected sequence - Calculates a score based on how many of the expected steps were completed correctly Copy def trajectory_subsequence(outputs: dict, reference_outputs: dict) -> float: \"\"\"Check how many of the desired steps the agent took.\"\"\" if len(reference_outputs['trajectory']) > len(outputs['trajectory']): return False i = j = 0 while i < len(reference_outputs['trajectory']) and j < len(outputs['trajectory']): if reference_outputs['trajectory'][i] == outputs['trajectory'][j]: i += 1 j += 1 return i / len(reference_outputs['trajectory']) Copy async def run_graph(inputs: dict) -> dict: \"\"\"Run graph and track the trajectory it takes along with the final response.\"\"\" trajectory = [] # Set subgraph=True to stream events from subgraphs of the main graph: https://langchain-ai.github.io/langgraph/how-tos/streaming-subgraphs/ # Set stream_mode=\"debug\" to stream all possible events: https://langchain-ai.github.io/langgra/langsmith/observability-concepts/streaming async for namespace, chunk in graph.astream({\"messages\": [ { \"role\": \"user\", \"content\": inputs['question'], } ]}, subgraphs=True, stream_mode=\"debug\"): # Event type for entering a node if chunk['type'] == 'task': # Record the node name trajectory.append(chunk['payload']['name']) # Given how we defined our dataset, we also need to track when specific tools are # called by our question answering ReACT agent. These tool calls can be found # when the ToolsNode (named \"tools\") is invoked by looking at the AIMessage.tool_calls # of the latest input message. if chunk['payload']['name'] == 'tools' and chunk['type'] == 'task': for tc in chunk['payload']['input']['messages'][-1].tool_calls: trajectory.append(tc['name']) return {\"trajectory\": trajectory} experiment_results = await client.aevaluate( run_graph, data=dataset_name, evaluators=[trajectory_subsequence], experiment_prefix=\"sql-agent-gpt4o-trajectory\", num_repetitions=1, max_concurrency=4, ) experiment_results.to_pandas() Single step evaluators While end-to-end tests give you the most signal about your agents performance, for the sake of debugging and iterating on your agent it can be helpful to pinpoint specific steps that are difficult and evaluate them directly. In our case, a crucial part of our agent is that it routes the user\u2019s intention correctly into either the \u201crefund\u201d path or the \u201cquestion answering\u201d path. Let\u2019s create a dataset and run some evaluations to directly stress test this one component.Copy # Create dataset examples = [ { \"inputs\": {\"messages\": [{\"role\": \"user\", \"content\": \"i bought some tracks recently and i dont like them\"}]}, \"outputs\": {\"route\": \"refund_agent\"}, }, { \"inputs\": {\"messages\": [{\"role\": \"user\", \"content\": \"I was thinking of purchasing some Rolling Stones tunes, any recommendations?\"}]}, \"outputs\": {\"route\": \"question_answering_agent\"}, }, { \"inputs\": {\"messages\": [{\"role\": \"user\", \"content\": \"i want a refund on purchase 237\"}, {\"role\": \"assistant\", \"content\": \"I've refunded you a total of $1.98. How else can I help you today?\"}, {\"role\": \"user\", \"content\": \"did prince release any albums in 2000?\"}]}, \"outputs\": {\"route\": \"question_answering_agent\"}, }, { \"inputs\": {\"messages\": [{\"role\": \"user\", \"content\": \"i purchased a cover of Yesterday recently but can't remember who it was by, which versions of it do you have?\"}]}, \"outputs\": {\"route\": \"question_answering_agent\"}, }, ] dataset_name = \"Chinook Customer Service Bot: Intent Classifier\" if not client.has_dataset(dataset_name=dataset_name): dataset = client.create_dataset(dataset_name=dataset_name) client.create_examples( dataset_id=dataset.id, examples=examples ) # Evaluator def correct(outputs: dict, reference_outputs: dict) -> bool: \"\"\"Check if the agent chose the correct route.\"\"\" return outputs[\"route\"] == reference_outputs[\"route\"] # Target function for running the relevant step async def run_intent_classifier(inputs: dict) -> dict: # Note that we can access and run the intent_classifier node of our graph directly. command = await graph.nodes['intent_classifier'].ainvoke(inputs) return {\"route\": command.goto} # Run evaluation experiment_results = await client.aevaluate( run_intent_classifier, data=dataset_name, evaluators=[correct], experiment_prefix=\"sql-agent-gpt4o-intent-classifier\", max_concurrency=4, ) Reference code Here\u2019s a consolidated script with all the above code:Reference code Reference code Copy import json import sqlite3 from typing import Literal from langchain.chat_models import init_chat_model from langchain.embeddings import init_embeddings from langchain_core.runnables import RunnableConfig from langchain.tools import tool from langchain_core.vectorstores import InMemoryVectorStore from langgraph.graph import END, StateGraph from langgraph.graph.message import AnyMessage, add_messages from langchain.agents import create_agent from langgraph.types import Command, interrupt from langsmith import Client import requests from tabulate import tabulate from typing_extensions import Annotated, TypedDict url = \"https://storage.googleapis.com/benchmarks-artifacts/chinook/Chinook.db\" response = requests.get(url) if response.status_code == 200: # Open a local file in binary write mode with open(\"chinook.db\", \"wb\") as file: # Write the content of the response (the file) to the local file file.write(response.content) print(\"File downloaded and saved as Chinook.db\") else: print(f\"Failed to download the file. Status code: {response.status_code}\") def _refund( invoice_id: int | None, invoice_line_ids: list[int] | None, mock: bool = False ) -> float: \"\"\"Given an Invoice ID and/or Invoice Line IDs, delete the relevant Invoice/InvoiceLine records in the Chinook DB. Args: invoice_id: The Invoice to delete. invoice_line_ids: The Invoice Lines to delete. mock: If True, do not actually delete the specified Invoice/Invoice Lines. Used for testing purposes. Returns: float: The total dollar amount that was deleted (or mock deleted). \"\"\" if invoice_id is None and invoice_line_ids is None: return 0.0 # Connect to the Chinook database conn = sqlite3.connect(\"chinook.db\") cursor = conn.cursor() total_refund = 0.0 try: # If invoice_id is provided, delete entire invoice and its lines if invoice_id is not None: # First get the total amount for the invoice cursor.execute( \"\"\" SELECT Total FROM Invoice WHERE InvoiceId = ? \"\"\", (invoice_id,), ) result = cursor.fetchone() if result: total_refund += result[0] # Delete invoice lines first (due to foreign key constraints) if not mock: cursor.execute( \"\"\" DELETE FROM InvoiceLine WHERE InvoiceId = ? \"\"\", (invoice_id,), ) # Then delete the invoice cursor.execute( \"\"\" DELETE FROM Invoice WHERE InvoiceId = ? \"\"\", (invoice_id,), ) # If specific invoice lines are provided if", "tokens": 1000, "node_type": "child"}
{"id": 82, "chunk_id": "ce808d45926e624bd501771111d7be0d", "parent_id": 72, "url": "", "namespace": "langchain", "title": "langsmith-evaluate-complex-agent", "headers": ["langsmith-evaluate-complex-agent"], "section_index": 0, "chunk_index": 4, "text": "mock: If True, do not actually delete the specified Invoice/Invoice Lines. Used for testing purposes. Returns: float: The total dollar amount that was deleted (or mock deleted). \"\"\" if invoice_id is None and invoice_line_ids is None: return 0.0 # Connect to the Chinook database conn = sqlite3.connect(\"chinook.db\") cursor = conn.cursor() total_refund = 0.0 try: # If invoice_id is provided, delete entire invoice and its lines if invoice_id is not None: # First get the total amount for the invoice cursor.execute( \"\"\" SELECT Total FROM Invoice WHERE InvoiceId = ? \"\"\", (invoice_id,), ) result = cursor.fetchone() if result: total_refund += result[0] # Delete invoice lines first (due to foreign key constraints) if not mock: cursor.execute( \"\"\" DELETE FROM InvoiceLine WHERE InvoiceId = ? \"\"\", (invoice_id,), ) # Then delete the invoice cursor.execute( \"\"\" DELETE FROM Invoice WHERE InvoiceId = ? \"\"\", (invoice_id,), ) # If specific invoice lines are provided if invoice_line_ids is not None: # Get the total amount for the specified invoice lines placeholders = \",\".join([\"?\" for _ in invoice_line_ids]) cursor.execute( f\"\"\" SELECT SUM(UnitPrice * Quantity) FROM InvoiceLine WHERE InvoiceLineId IN ({placeholders}) \"\"\", invoice_line_ids, ) result = cursor.fetchone() if result and result[0]: total_refund += result[0] if not mock: # Delete the specified invoice lines cursor.execute( f\"\"\" DELETE FROM InvoiceLine WHERE InvoiceLineId IN ({placeholders}) \"\"\", invoice_line_ids, ) # Commit the changes conn.commit() except sqlite3.Error as e: # Roll back in case of error conn.rollback() raise e finally: # Close the connection conn.close() return float(total_refund) def _lookup( customer_first_name: str, customer_last_name: str, customer_phone: str, track_name: str | None, album_title: str | None, artist_name: str | None, purchase_date_iso_8601: str | None, ) -> list[dict]: \"\"\"Find all of the Invoice Line IDs in the Chinook DB for the given filters. Returns: a list of dictionaries that contain keys: { 'invoice_line_id', 'track_name', 'artist_name', 'purchase_date', 'quantity_purchased', 'price_per_unit' } \"\"\" # Connect to the database conn = sqlite3.connect(\"chinook.db\") cursor = conn.cursor() # Base query joining all necessary tables query = \"\"\" SELECT il.InvoiceLineId, t.Name as track_name, art.Name as artist_name, i.InvoiceDate as purchase_date, il.Quantity as quantity_purchased, il.UnitPrice as price_per_unit FROM InvoiceLine il JOIN Invoice i ON il.InvoiceId = i.InvoiceId JOIN Customer c ON i.CustomerId = c.CustomerId JOIN Track t ON il.TrackId = t.TrackId JOIN Album alb ON t.AlbumId = alb.AlbumId JOIN Artist art ON alb.ArtistId = art.ArtistId WHERE c.FirstName = ? AND c.LastName = ? AND c.Phone = ? \"\"\" # Parameters for the query params = [customer_first_name, customer_last_name, customer_phone] # Add optional filters if track_name: query += \" AND t.Name = ?\" params.append(track_name) if album_title: query += \" AND alb.Title = ?\" params.append(album_title) if artist_name: query += \" AND art.Name = ?\" params.append(artist_name) if purchase_date_iso_8601: query += \" AND date(i.InvoiceDate) = date(?)\" params.append(purchase_date_iso_8601) # Execute query cursor.execute(query, params) # Fetch results results = cursor.fetchall() # Convert results to list of dictionaries output = [] for row in results: output.append( { \"invoice_line_id\": row[0], \"track_name\": row[1], \"artist_name\": row[2], \"purchase_date\": row[3], \"quantity_purchased\": row[4], \"price_per_unit\": row[5], } ) # Close connection conn.close() return output # Graph state. class State(TypedDict): \"\"\"Agent state.\"\"\" messages: Annotated[list[AnyMessage], add_messages] followup: str | None invoice_id: int | None invoice_line_ids: list[int] | None customer_first_name: str | None customer_last_name: str | None customer_phone: str | None track_name: str | None album_title: str | None artist_name: str | None purchase_date_iso_8601: str | None # Instructions for extracting the user/purchase info from the conversation. gather_info_instructions = \"\"\"You are managing an online music store that sells song tracks. \\ Customers can buy multiple tracks at a time and these purchases are recorded in a database as \\ an Invoice per purchase and an associated set of Invoice Lines for each purchased track. Your task is to help customers who would like a refund for one or more of the tracks they've \\ purchased. In order for you to be able refund them, the customer must specify the Invoice ID \\ to get a refund on all the tracks they bought in a single transaction, or one or more Invoice \\ Line IDs if they would like refunds on individual tracks. Often a user will not know the specific Invoice ID(s) or Invoice Line ID(s) for which they \\ would like a refund. In this case you can help them look up their invoices by asking them to \\ specify: - Required: Their first name, last name, and phone number. - Optionally: The track name, artist name, album name, or purchase date. If the customer has not specified the required information (either Invoice/Invoice Line IDs \\ or first name, last name, phone) then please ask them to specify it.\"\"\" # Extraction schema, mirrors the graph state. class PurchaseInformation(TypedDict): \"\"\"All of the known information about the invoice / invoice lines the customer would like refunded. Do not make up values, leave fields as null if you don't know their value.\"\"\" invoice_id: int | None invoice_line_ids: list[int] | None customer_first_name: str | None customer_last_name: str | None customer_phone: str | None track_name: str | None album_title: str | None artist_name: str | None purchase_date_iso_8601: str | None followup: Annotated[ str | None, ..., \"If the user hasn't enough identifying information, please tell them what the required information is and ask them to specify it.\", ] # Model for performing extraction. info_llm = init_chat_model(\"gpt-4o-mini\").with_structured_output( PurchaseInformation, method=\"json_schema\", include_raw=True ) # Graph node for extracting user info and routing to lookup/refund/END. async def gather_info(state: State) -> Command[Literal[\"lookup\", \"refund\", END]]: info = await info_llm.ainvoke( [ {\"role\": \"system\", \"content\": gather_info_instructions}, *state[\"messages\"], ] ) parsed = info[\"parsed\"] if any(parsed[k] for k in (\"invoice_id\", \"invoice_line_ids\")): goto = \"refund\" elif all( parsed[k] for k in (\"customer_first_name\", \"customer_last_name\", \"customer_phone\") ): goto = \"lookup\" else: goto = END update = {\"messages\": [info[\"raw\"]], **parsed} return Command(update=update, goto=goto) # Graph node for executing the refund. # Note that here we inspect the runtime config for an \"env\" variable. # If \"env\" is set to \"test\", then we don't actually delete any rows from our database. # This will become important when we're running our evaluations. def refund(state: State, config: RunnableConfig) -> dict: # Whether to mock the", "tokens": 1000, "node_type": "child"}
{"id": 83, "chunk_id": "ce000c89325bde3d69294e46ebc44bcb", "parent_id": 72, "url": "", "namespace": "langchain", "title": "langsmith-evaluate-complex-agent", "headers": ["langsmith-evaluate-complex-agent"], "section_index": 0, "chunk_index": 5, "text": "required information is and ask them to specify it.\", ] # Model for performing extraction. info_llm = init_chat_model(\"gpt-4o-mini\").with_structured_output( PurchaseInformation, method=\"json_schema\", include_raw=True ) # Graph node for extracting user info and routing to lookup/refund/END. async def gather_info(state: State) -> Command[Literal[\"lookup\", \"refund\", END]]: info = await info_llm.ainvoke( [ {\"role\": \"system\", \"content\": gather_info_instructions}, *state[\"messages\"], ] ) parsed = info[\"parsed\"] if any(parsed[k] for k in (\"invoice_id\", \"invoice_line_ids\")): goto = \"refund\" elif all( parsed[k] for k in (\"customer_first_name\", \"customer_last_name\", \"customer_phone\") ): goto = \"lookup\" else: goto = END update = {\"messages\": [info[\"raw\"]], **parsed} return Command(update=update, goto=goto) # Graph node for executing the refund. # Note that here we inspect the runtime config for an \"env\" variable. # If \"env\" is set to \"test\", then we don't actually delete any rows from our database. # This will become important when we're running our evaluations. def refund(state: State, config: RunnableConfig) -> dict: # Whether to mock the deletion. True if the configurable var 'env' is set to 'test'. mock = config.get(\"configurable\", {}).get(\"env\", \"prod\") == \"test\" refunded = _refund( invoice_id=state[\"invoice_id\"], invoice_line_ids=state[\"invoice_line_ids\"], mock=mock, ) response = f\"You have been refunded a total of: ${refunded:.2f}. Is there anything else I can help with?\" return { \"messages\": [{\"role\": \"assistant\", \"content\": response}], \"followup\": response, } # Graph node for looking up the users purchases def lookup(state: State) -> dict: args = ( state[k] for k in ( \"customer_first_name\", \"customer_last_name\", \"customer_phone\", \"track_name\", \"album_title\", \"artist_name\", \"purchase_date_iso_8601\", ) ) results = _lookup(*args) if not results: response = \"We did not find any purchases associated with the information you've provided. Are you sure you've entered all of your information correctly?\" followup = response else: response = f\"Which of the following purchases would you like to be refunded for?\\n\\n```json{json.dumps(results, indent=2)}\\n```\" followup = f\"Which of the following purchases would you like to be refunded for?\\n\\n{tabulate(results, headers='keys')}\" return { \"messages\": [{\"role\": \"assistant\", \"content\": response}], \"followup\": followup, \"invoice_line_ids\": [res[\"invoice_line_id\"] for res in results], } # Building our graph graph_builder = StateGraph(State) graph_builder.add_node(gather_info) graph_builder.add_node(refund) graph_builder.add_node(lookup) graph_builder.set_entry_point(\"gather_info\") graph_builder.add_edge(\"lookup\", END) graph_builder.add_edge(\"refund\", END) refund_graph = graph_builder.compile() # Our SQL queries will only work if we filter on the exact string values that are in the DB. # To ensure this, we'll create vectorstore indexes for all of the artists, tracks and albums # ahead of time and use those to disambiguate the user input. E.g. if a user searches for # songs by \"prince\" and our DB records the artist as \"Prince\", ideally when we query our # artist vectorstore for \"prince\" we'll get back the value \"Prince\", which we can then # use in our SQL queries. def index_fields() -> ( tuple[InMemoryVectorStore, InMemoryVectorStore, InMemoryVectorStore] ): \"\"\"Create an index for all artists, an index for all albums, and an index for all songs.\"\"\" try: # Connect to the chinook database conn = sqlite3.connect(\"chinook.db\") cursor = conn.cursor() # Fetch all results tracks = cursor.execute(\"SELECT Name FROM Track\").fetchall() artists = cursor.execute(\"SELECT Name FROM Artist\").fetchall() albums = cursor.execute(\"SELECT Title FROM Album\").fetchall() finally: # Close the connection if conn: conn.close() embeddings = init_embeddings(\"openai:text-embedding-3-small\") track_store = InMemoryVectorStore(embeddings) artist_store = InMemoryVectorStore(embeddings) album_store = InMemoryVectorStore(embeddings) track_store.add_texts([t[0] for t in tracks]) artist_store.add_texts([a[0] for a in artists]) album_store.add_texts([a[0] for a in albums]) return track_store, artist_store, album_store track_store, artist_store, album_store = index_fields() # Agent tools @tool def lookup_track( track_name: str | None = None, album_title: str | None = None, artist_name: str | None = None, ) -> list[dict]: \"\"\"Lookup a track in Chinook DB based on identifying information about. Returns: a list of dictionaries per matching track that contain keys {'track_name', 'artist_name', 'album_name'} \"\"\" conn = sqlite3.connect(\"chinook.db\") cursor = conn.cursor() query = \"\"\" SELECT DISTINCT t.Name as track_name, ar.Name as artist_name, al.Title as album_name FROM Track t JOIN Album al ON t.AlbumId = al.AlbumId JOIN Artist ar ON al.ArtistId = ar.ArtistId WHERE 1=1 \"\"\" params = [] if track_name: track_name = track_store.similarity_search(track_name, k=1)[0].page_content query += \" AND t.Name LIKE ?\" params.append(f\"%{track_name}%\") if album_title: album_title = album_store.similarity_search(album_title, k=1)[0].page_content query += \" AND al.Title LIKE ?\" params.append(f\"%{album_title}%\") if artist_name: artist_name = artist_store.similarity_search(artist_name, k=1)[0].page_content query += \" AND ar.Name LIKE ?\" params.append(f\"%{artist_name}%\") cursor.execute(query, params) results = cursor.fetchall() tracks = [ {\"track_name\": row[0], \"artist_name\": row[1], \"album_name\": row[2]} for row in results ] conn.close() return tracks @tool def lookup_album( track_name: str | None = None, album_title: str | None = None, artist_name: str | None = None, ) -> list[dict]: \"\"\"Lookup an album in Chinook DB based on identifying information about. Returns: a list of dictionaries per matching album that contain keys {'album_name', 'artist_name'} \"\"\" conn = sqlite3.connect(\"chinook.db\") cursor = conn.cursor() query = \"\"\" SELECT DISTINCT al.Title as album_name, ar.Name as artist_name FROM Album al JOIN Artist ar ON al.ArtistId = ar.ArtistId LEFT JOIN Track t ON t.AlbumId = al.AlbumId WHERE 1=1 \"\"\" params = [] if track_name: query += \" AND t.Name LIKE ?\" params.append(f\"%{track_name}%\") if album_title: query += \" AND al.Title LIKE ?\" params.append(f\"%{album_title}%\") if artist_name: query += \" AND ar.Name LIKE ?\" params.append(f\"%{artist_name}%\") cursor.execute(query, params) results = cursor.fetchall() albums = [{\"album_name\": row[0], \"artist_name\": row[1]} for row in results] conn.close() return albums @tool def lookup_artist( track_name: str | None = None, album_title: str | None = None, artist_name: str | None = None, ) -> list[str]: \"\"\"Lookup an album in Chinook DB based on identifying information about. Returns: a list of matching artist names \"\"\" conn = sqlite3.connect(\"chinook.db\") cursor = conn.cursor() query = \"\"\" SELECT DISTINCT ar.Name as artist_name FROM Artist ar LEFT JOIN Album al ON al.ArtistId = ar.ArtistId LEFT JOIN Track t ON t.AlbumId = al.AlbumId WHERE 1=1 \"\"\" params = [] if track_name: query += \" AND t.Name LIKE ?\" params.append(f\"%{track_name}%\") if album_title: query += \" AND al.Title LIKE ?\" params.append(f\"%{album_title}%\") if artist_name: query += \" AND ar.Name LIKE ?\" params.append(f\"%{artist_name}%\") cursor.execute(query, params) results = cursor.fetchall() artists = [row[0] for row in results] conn.close() return artists # Agent model qa_llm = init_chat_model(\"claude-3-5-sonnet-latest\") # The prebuilt ReACT agent only expects State to have a 'messages' key, so the # state we defined for the refund agent can also be passed to our lookup agent. qa_graph = create_agent(qa_llm, [lookup_track, lookup_artist, lookup_album]) # Schema for routing", "tokens": 1000, "node_type": "child"}
{"id": 84, "chunk_id": "5b014c372b8fd1c9edae568e3e98258d", "parent_id": 72, "url": "", "namespace": "langchain", "title": "langsmith-evaluate-complex-agent", "headers": ["langsmith-evaluate-complex-agent"], "section_index": 0, "chunk_index": 6, "text": "\"\"\"Lookup an album in Chinook DB based on identifying information about. Returns: a list of matching artist names \"\"\" conn = sqlite3.connect(\"chinook.db\") cursor = conn.cursor() query = \"\"\" SELECT DISTINCT ar.Name as artist_name FROM Artist ar LEFT JOIN Album al ON al.ArtistId = ar.ArtistId LEFT JOIN Track t ON t.AlbumId = al.AlbumId WHERE 1=1 \"\"\" params = [] if track_name: query += \" AND t.Name LIKE ?\" params.append(f\"%{track_name}%\") if album_title: query += \" AND al.Title LIKE ?\" params.append(f\"%{album_title}%\") if artist_name: query += \" AND ar.Name LIKE ?\" params.append(f\"%{artist_name}%\") cursor.execute(query, params) results = cursor.fetchall() artists = [row[0] for row in results] conn.close() return artists # Agent model qa_llm = init_chat_model(\"claude-3-5-sonnet-latest\") # The prebuilt ReACT agent only expects State to have a 'messages' key, so the # state we defined for the refund agent can also be passed to our lookup agent. qa_graph = create_agent(qa_llm, [lookup_track, lookup_artist, lookup_album]) # Schema for routing user intent. # We'll use structured outputs to enforce that the model returns only # the desired output. class UserIntent(TypedDict): \"\"\"The user's current intent in the conversation\"\"\" intent: Literal[\"refund\", \"question_answering\"] # Routing model with structured output router_llm = init_chat_model(\"gpt-4o-mini\").with_structured_output( UserIntent, method=\"json_schema\", strict=True ) # Instructions for routing. route_instructions = \"\"\"You are managing an online music store that sells song tracks. \\ You can help customers in two types of ways: (1) answering general questions about \\ tracks sold at your store, (2) helping them get a refund on a purhcase they made at your store. Based on the following conversation, determine if the user is currently seeking general \\ information about song tracks or if they are trying to refund a specific purchase. Return 'refund' if they are trying to get a refund and 'question_answering' if they are \\ asking a general music question. Do NOT return anything else. Do NOT try to respond to \\ the user. \"\"\" # Node for routing. async def intent_classifier( state: State, ) -> Command[Literal[\"refund_agent\", \"question_answering_agent\"]]: response = router_llm.invoke( [{\"role\": \"system\", \"content\": route_instructions}, *state[\"messages\"]] ) return Command(goto=response[\"intent\"] + \"_agent\") # Node for making sure the 'followup' key is set before our agent run completes. def compile_followup(state: State) -> dict: \"\"\"Set the followup to be the last message if it hasn't explicitly been set.\"\"\" if not state.get(\"followup\"): return {\"followup\": state[\"messages\"][-1].content} return {} # Agent definition graph_builder = StateGraph(State) graph_builder.add_node(intent_classifier) # Since all of our subagents have compatible state, # we can add them as nodes directly. graph_builder.add_node(\"refund_agent\", refund_graph) graph_builder.add_node(\"question_answering_agent\", qa_graph) graph_builder.add_node(compile_followup) graph_builder.set_entry_point(\"intent_classifier\") graph_builder.add_edge(\"refund_agent\", \"compile_followup\") graph_builder.add_edge(\"question_answering_agent\", \"compile_followup\") graph_builder.add_edge(\"compile_followup\", END) graph = graph_builder.compile() client = Client() # Create a dataset examples = [ { \"inputs\": { \"question\": \"How many songs do you have by James Brown\" }, \"outputs\": { \"response\": \"We have 20 songs by James Brown\", \"trajectory\": [\"question_answering_agent\", \"lookup_tracks\"] }, }, { \"inputs\": { \"question\": \"My name is Aaron Mitchell and I'd like a refund.\", }, \"outputs\": { \"response\": \"I need some more information to help you with the refund. Please specify your phone number, the invoice ID, or the line item IDs for the purchase you'd like refunded.\", \"trajectory\": [\"refund_agent\"], } }, { \"inputs\": { \"question\": \"My name is Aaron Mitchell and I'd like a refund on my Led Zeppelin purchases. My number is +1 (204) 452-6452\", }, \"outputs\": { \"response\": \"Which of the following purchases would you like to be refunded for?\\n\\n invoice_line_id track_name artist_name purchase_date quantity_purchased price_per_unit\\n----------------- -------------------------------- ------------- ------------------- -------------------- ----------------\\n 267 How Many More Times Led Zeppelin 2009-08-06 00:00:00 1 0.99\\n 268 What Is And What Should Never Be Led Zeppelin 2009-08-06 00:00:00 1 0.99\", \"trajectory\": [\"refund_agent\", \"lookup\"], }, }, { \"inputs\": { \"question\": \"Who recorded Wish You Were Here again? What other albums of there's do you have?\", }, \"outputs\": { \"response\": \"Wish You Were Here is an album by Pink Floyd\", \"trajectory\": [\"question_answering_agent\", \"lookup_album\"], } }, { \"inputs\": { \"question\": \"I want a full refund for invoice 237\", }, \"outputs\": { \"response\": \"You have been refunded $2.97.\", \"trajectory\": [\"refund_agent\", \"refund\"], }, }, ] dataset_name = \"Chinook Customer Service Bot: E2E\" if not client.has_dataset(dataset_name=dataset_name): dataset = client.create_dataset(dataset_name=dataset_name) client.create_examples( dataset_id=dataset.id, examples=examples ) # LLM-as-judge instructions grader_instructions = \"\"\"You are a teacher grading a quiz. You will be given a QUESTION, the GROUND TRUTH (correct) RESPONSE, and the STUDENT RESPONSE. Here is the grade criteria to follow: (1) Grade the student responses based ONLY on their factual accuracy relative to the ground truth answer. (2) Ensure that the student response does not contain any conflicting statements. (3) It is OK if the student response contains more information than the ground truth response, as long as it is factually accurate relative to the ground truth response. Correctness: True means that the student's response meets all of the criteria. False means that the student's response does not meet all of the criteria. Explain your reasoning in a step-by-step manner to ensure your reasoning and conclusion are correct.\"\"\" # LLM-as-judge output schema class Grade(TypedDict): \"\"\"Compare the expected and actual answers and grade the actual answer.\"\"\" reasoning: Annotated[ str, ..., \"Explain your reasoning for whether the actual response is correct or not.\", ] is_correct: Annotated[ bool, ..., \"True if the student response is mostly or exactly correct, otherwise False.\", ] # Judge LLM grader_llm = init_chat_model(\"gpt-4o-mini\", temperature=0).with_structured_output( Grade, method=\"json_schema\", strict=True ) # Evaluator function async def final_answer_correct( inputs: dict, outputs: dict, reference_outputs: dict ) -> bool: \"\"\"Evaluate if the final response is equivalent to reference response.\"\"\" # Note that we assume the outputs has a 'response' dictionary. We'll need to make sure # that the target function we define includes this key. user = f\"\"\"QUESTION: {inputs['question']} GROUND TRUTH RESPONSE: {reference_outputs['response']} STUDENT RESPONSE: {outputs['response']}\"\"\" grade = await grader_llm.ainvoke( [ {\"role\": \"system\", \"content\": grader_instructions}, {\"role\": \"user\", \"content\": user}, ] ) return grade[\"is_correct\"] # Target function async def run_graph(inputs: dict) -> dict: \"\"\"Run graph and track the trajectory it takes along with the final response.\"\"\" result = await graph.ainvoke( { \"messages\": [ {\"role\": \"user\", \"content\": inputs[\"question\"]}, ] }, config={\"env\": \"test\"}, ) return {\"response\": result[\"followup\"]} # Evaluation job and results experiment_results = await client.aevaluate( run_graph, data=dataset_name, evaluators=[final_answer_correct], experiment_prefix=\"sql-agent-gpt4o-e2e\", num_repetitions=1, max_concurrency=4,", "tokens": 1000, "node_type": "child"}
{"id": 85, "chunk_id": "748c441ed44ed6d2cf2e60fe3a22c662", "parent_id": 72, "url": "", "namespace": "langchain", "title": "langsmith-evaluate-complex-agent", "headers": ["langsmith-evaluate-complex-agent"], "section_index": 0, "chunk_index": 7, "text": "correct, otherwise False.\", ] # Judge LLM grader_llm = init_chat_model(\"gpt-4o-mini\", temperature=0).with_structured_output( Grade, method=\"json_schema\", strict=True ) # Evaluator function async def final_answer_correct( inputs: dict, outputs: dict, reference_outputs: dict ) -> bool: \"\"\"Evaluate if the final response is equivalent to reference response.\"\"\" # Note that we assume the outputs has a 'response' dictionary. We'll need to make sure # that the target function we define includes this key. user = f\"\"\"QUESTION: {inputs['question']} GROUND TRUTH RESPONSE: {reference_outputs['response']} STUDENT RESPONSE: {outputs['response']}\"\"\" grade = await grader_llm.ainvoke( [ {\"role\": \"system\", \"content\": grader_instructions}, {\"role\": \"user\", \"content\": user}, ] ) return grade[\"is_correct\"] # Target function async def run_graph(inputs: dict) -> dict: \"\"\"Run graph and track the trajectory it takes along with the final response.\"\"\" result = await graph.ainvoke( { \"messages\": [ {\"role\": \"user\", \"content\": inputs[\"question\"]}, ] }, config={\"env\": \"test\"}, ) return {\"response\": result[\"followup\"]} # Evaluation job and results experiment_results = await client.aevaluate( run_graph, data=dataset_name, evaluators=[final_answer_correct], experiment_prefix=\"sql-agent-gpt4o-e2e\", num_repetitions=1, max_concurrency=4, ) experiment_results.to_pandas() def trajectory_subsequence(outputs: dict, reference_outputs: dict) -> float: \"\"\"Check how many of the desired steps the agent took.\"\"\" if len(reference_outputs[\"trajectory\"]) > len(outputs[\"trajectory\"]): return False i = j = 0 while i < len(reference_outputs[\"trajectory\"]) and j < len(outputs[\"trajectory\"]): if reference_outputs[\"trajectory\"][i] == outputs[\"trajectory\"][j]: i += 1 j += 1 return i / len(reference_outputs[\"trajectory\"]) async def run_graph(inputs: dict) -> dict: \"\"\"Run graph and track the trajectory it takes along with the final response.\"\"\" trajectory = [] # Set subgraph=True to stream events from subgraphs of the main graph: https://langchain-ai.github.io/langgraph/how-tos/streaming-subgraphs/ # Set stream_mode=\"debug\" to stream all possible events: https://langchain-ai.github.io/langgra/langsmith/observability-concepts/streaming async for namespace, chunk in graph.astream( { \"messages\": [ { \"role\": \"user\", \"content\": inputs[\"question\"], } ] }, subgraphs=True, stream_mode=\"debug\", ): # Event type for entering a node if chunk[\"type\"] == \"task\": # Record the node name trajectory.append(chunk[\"payload\"][\"name\"]) # Given how we defined our dataset, we also need to track when specific tools are # called by our question answering ReACT agent. These tool calls can be found # when the ToolsNode (named \"tools\") is invoked by looking at the AIMessage.tool_calls # of the latest input message. if chunk[\"payload\"][\"name\"] == \"tools\" and chunk[\"type\"] == \"task\": for tc in chunk[\"payload\"][\"input\"][\"messages\"][-1].tool_calls: trajectory.append(tc[\"name\"]) return {\"trajectory\": trajectory} experiment_results = await client.aevaluate( run_graph, data=dataset_name, evaluators=[trajectory_subsequence], experiment_prefix=\"sql-agent-gpt4o-trajectory\", num_repetitions=1, max_concurrency=4, ) experiment_results.to_pandas() # Create dataset examples = [ { \"inputs\": { \"messages\": [ { \"role\": \"user\", \"content\": \"i bought some tracks recently and i dont like them\", } ], } \"outputs\": {\"route\": \"refund_agent\"}, }, { \"inputs\": { \"messages\": [ { \"role\": \"user\", \"content\": \"I was thinking of purchasing some Rolling Stones tunes, any recommendations?\", } ], }, \"outputs\": {\"route\": \"question_answering_agent\"}, }, { \"inputs\": { \"messages\": [ {\"role\": \"user\", \"content\": \"i want a refund on purchase 237\"}, { \"role\": \"assistant\", \"content\": \"I've refunded you a total of $1.98. How else can I help you today?\", }, {\"role\": \"user\", \"content\": \"did prince release any albums in 2000?\"}, ], }, \"outputs\": {\"route\": \"question_answering_agent\"}, }, { \"inputs\": { \"messages\": [ { \"role\": \"user\", \"content\": \"i purchased a cover of Yesterday recently but can't remember who it was by, which versions of it do you have?\", } ], }, \"outputs\": {\"route\": \"question_answering_agent\"}, }, ] dataset_name = \"Chinook Customer Service Bot: Intent Classifier\" if not client.has_dataset(dataset_name=dataset_name): dataset = client.create_dataset(dataset_name=dataset_name) client.create_examples( dataset_id=dataset.id, examples=examples, ) # Evaluator def correct(outputs: dict, reference_outputs: dict) -> bool: \"\"\"Check if the agent chose the correct route.\"\"\" return outputs[\"route\"] == reference_outputs[\"route\"] # Target function for running the relevant step async def run_intent_classifier(inputs: dict) -> dict: # Note that we can access and run the intent_classifier node of our graph directly. command = await graph.nodes[\"intent_classifier\"].ainvoke(inputs) return {\"route\": command.goto} # Run evaluation experiment_results = await client.aevaluate( run_intent_classifier, data=dataset_name, evaluators=[correct], experiment_prefix=\"sql-agent-gpt4o-intent-classifier\", max_concurrency=4, ) experiment_results.to_pandas()", "tokens": 596, "node_type": "child"}
{"id": 86, "chunk_id": "c16acf8a553ac24f5d77e7d3a1c6e493", "parent_id": 74, "url": "", "namespace": "langchain", "title": "langsmith-evaluate-graph", "headers": ["langsmith-evaluate-graph"], "section_index": 0, "chunk_index": 0, "text": "# langsmith-evaluate-graph\n\n> Source: https://docs.langchain.com/langsmith/evaluate-graph\n\nlanggraph\nis a library for building stateful, multi-actor applications with LLMs, used to create agent and multi-agent workflows. Evaluating langgraph\ngraphs can be challenging because a single invocation can involve many LLM calls, and which LLM calls are made may depend on the outputs of preceding calls. In this guide we will focus on the mechanics of how to pass graphs and graph nodes to evaluate()\n/ aevaluate()\n. For evaluation techniques and best practices when building agents head to the langgraph docs.\nEnd-to-end evaluations\nThe most common type of evaluation is an end-to-end one, where we want to evaluate the final graph output for each example input.Define a graph\nLets construct a simple ReACT agent to start:Create a dataset\nLet\u2019s create a simple dataset of questions and expected responses:Create an evaluator\nAnd a simple evaluator: Requireslangsmith>=0.2.0\nRun evaluations\nNow we can run our evaluations and explore the results. We\u2019ll just need to wrap our graph function so that it can take inputs in the format they\u2019re stored on our example:If all of your graph nodes are defined as sync functions then you can use\nevaluate\nor aevaluate\n. If any of you nodes are defined as async, you\u2019ll need to use aevaluate\nlangsmith>=0.2.0\nEvaluating intermediate steps\nOften it is valuable to evaluate not only the final output of an agent but also the intermediate steps it has taken. What\u2019s nice aboutlanggraph\nis that the output of a graph is a state object that often already carries information about the intermediate steps taken. Usually we can evaluate whatever we\u2019re interested in just by looking at the messages in our state. For example, we can look at the messages to assert that the model invoked the \u2018search\u2019 tool upon as a first step.\nRequires langsmith>=0.2.0\nRunning and evaluating individual nodes\nSometimes you want to evaluate a single node directly to save time and costs.langgraph\nmakes it easy to do this. In this case we can even continue using the evaluators we\u2019ve been using.\nRelated\nReference code\nClick to see a consolidated code snippet\nClick to see a consolidated code snippet", "tokens": 354, "node_type": "child"}
{"id": 87, "chunk_id": "c16acf8a553ac24f5d77e7d3a1c6e493", "parent_id": 75, "url": "", "namespace": "langchain", "title": "langsmith-evaluate-llm-application", "headers": ["langsmith-evaluate-llm-application"], "section_index": 0, "chunk_index": 0, "text": "# langsmith-evaluate-llm-application\n\n> Source: https://docs.langchain.com/langsmith/evaluate-llm-application\n\nFor larger evaluation jobs in Python we recommend using aevaluate(), the asynchronous version of evaluate(). It is still worthwhile to read this guide first, as the two have identical interfaces, before reading the how-to guide on running an evaluation asynchronously.In JS/TS evaluate() is already asynchronous so no separate method is needed.It is also important to configure the\nmax_concurrency\n/maxConcurrency\narg when running large jobs. This parallelizes evaluation by effectively splitting the dataset across threads.Define an application\nFirst we need an application to evaluate. Let\u2019s create a simple toxicity classifier for this example.Create or select a dataset\nWe need a Dataset to evaluate our application on. Our dataset will contain labeled examples of toxic and non-toxic text. Requireslangsmith>=0.3.13\nDefine an evaluator\nEvaluators are functions for scoring your application\u2019s outputs. They take in the example inputs, actual outputs, and, when present, the reference outputs. Since we have labels for this task, our evaluator can directly check if the actual outputs match the reference outputs.\n- Python: Requires\nlangsmith>=0.3.13\n- TypeScript: Requires\nlangsmith>=0.2.9\nRun the evaluation\nWe\u2019ll use the evaluate() / aevaluate() methods to run the evaluation. The key arguments are:- a target function that takes an input dictionary and returns an output dictionary. The\nexample.inputs\nfield of each Example is what gets passed to the target function. In this case ourtoxicity_classifier\nis already set up to take in example inputs so we can use it directly. data\n- the name OR UUID of the LangSmith dataset to evaluate on, or an iterator of examplesevaluators\n- a list of evaluators to score the outputs of the function\nlangsmith>=0.3.13\nExplore the results\nEach invocation ofevaluate()\ncreates an Experiment which can be viewed in the LangSmith UI or queried via the SDK. Evaluation scores are stored against each actual output as feedback.\nIf you\u2019ve annotated your code for tracing, you can open the trace of each row in a side panel view.\nReference code\nClick to see a consolidated code snippet\nClick to see a consolidated code snippet\nRelated\n- Run an evaluation asynchronously\n- Run an evaluation via the REST API\n- Run an evaluation from the prompt playground", "tokens": 360, "node_type": "child"}
{"id": 88, "chunk_id": "c16acf8a553ac24f5d77e7d3a1c6e493", "parent_id": 76, "url": "", "namespace": "langchain", "title": "langsmith-evaluate-on-intermediate-steps", "headers": ["langsmith-evaluate-on-intermediate-steps"], "section_index": 0, "chunk_index": 0, "text": "# langsmith-evaluate-on-intermediate-steps\n\n> Source: https://docs.langchain.com/langsmith/evaluate-on-intermediate-steps\n\nHow to evaluate an application's intermediate steps\nWhile, in many scenarios, it is sufficient to evaluate the final output of your task, in some cases you might want to evaluate the intermediate steps of your pipeline.For example, for retrieval-augmented generation (RAG), you might want to\nEvaluate the retrieval step to ensure that the correct documents are retrieved w.r.t the input query.\nEvaluate the generation step to ensure that the correct answer is generated w.r.t the retrieved documents.\nIn this guide, we will use a simple, fully-custom evaluator for evaluating criteria 1 and an LLM-based evaluator for evaluating criteria 2 to highlight both scenarios.In order to evaluate the intermediate steps of your pipeline, your evaluator function should traverse and process the run/rootRun argument, which is a Run object that contains the intermediate steps of your pipeline.\nThe below RAG pipeline consists of 1) generating a Wikipedia query given the input question, 2) retrieving relevant documents from Wikipedia, and 3) generating an answer given the retrieved documents.\nCopy\npip install -U langsmith langchain[openai] wikipedia\nRequires langsmith>=0.3.13\nCopy\nimport wikipedia as wpfrom openai import OpenAIfrom langsmith import traceable, wrappersoai_client = wrappers.wrap_openai(OpenAI())@traceabledef generate_wiki_search(question: str) -> str: \"\"\"Generate the query to search in wikipedia.\"\"\" instructions = ( \"Generate a search query to pass into wikipedia to answer the user's question. \" \"Return only the search query and nothing more. \" \"This will passed in directly to the wikipedia search engine.\" ) messages = [ {\"role\": \"system\", \"content\": instructions}, {\"role\": \"user\", \"content\": question} ] result = oai_client.chat.completions.create( messages=messages, model=\"gpt-4o-mini\", temperature=0, ) return result.choices[0].message.content@traceable(run_type=\"retriever\")def retrieve(query: str) -> list: \"\"\"Get up to two search wikipedia results.\"\"\" results = [] for term in wp.search(query, results = 10): try: page = wp.page(term, auto_suggest=False) results.append({ \"page_content\": page.summary, \"type\": \"Document\", \"metadata\": {\"url\": page.url} }) except wp.DisambiguationError: pass if len(results) >= 2: return results@traceabledef generate_answer(question: str, context: str) -> str: \"\"\"Answer the question based on the retrieved information.\"\"\" instructions = f\"Answer the user's question based ONLY on the content below:\\n\\n{context}\" messages = [ {\"role\": \"system\", \"content\": instructions}, {\"role\": \"user\", \"content\": question} ] result = oai_client.chat.completions.create( messages=messages, model=\"gpt-4o-mini\", temperature=0 ) return result.choices[0].message.content@traceabledef qa_pipeline(question: str) -> str: \"\"\"The full pipeline.\"\"\" query = generate_wiki_search(question) context = \"\\n\\n\".join([doc[\"page_content\"] for doc in retrieve(query)]) return generate_answer(question, context)\nThis pipeline will produce a trace that looks something like:\nAs mentioned above, we will define two evaluators: one that evaluates the relevance of the retrieved documents w.r.t the input query and another that evaluates the hallucination of the generated answer w.r.t the retrieved documents. We will be using LangChain LLM wrappers, along with with_structured_output to define the evaluator for hallucination.The key here is that the evaluator function should traverse the run / rootRun argument to access the intermediate steps of the pipeline. The evaluator can then process the inputs and outputs of the intermediate steps to evaluate according to the desired criteria.Example uses langchain for convenience, this is not required.\nCopy\nfrom langchain.chat_models import init_chat_modelfrom langsmith.schemas import Runfrom pydantic import BaseModel, Fielddef document_relevance(run: Run) -> bool: \"\"\"Checks if retriever input exists in the retrieved docs.\"\"\" qa_pipeline_run = next( r for run in run.child_runs if r.name == \"qa_pipeline\" ) retrieve_run = next( r for run in qa_pipeline_run.child_runs if r.name == \"retrieve\" ) page_contents = \"\\n\\n\".join( doc[\"page_content\"] for doc in retrieve_run.outputs[\"output\"] ) return retrieve_run.inputs[\"query\"] in page_contents# Data modelclass GradeHallucinations(BaseModel): \"\"\"Binary score for hallucination present in generation answer.\"\"\" is_grounded: bool = Field(..., description=\"True if the answer is grounded in the facts, False otherwise.\")# LLM with structured outputs for grading hallucinations# For more see: https://python.langchain.com/docs/how_to/structured_output/grader_llm= init_chat_model(\"gpt-4o-mini\", temperature=0).with_structured_output( GradeHallucinations, method=\"json_schema\", strict=True,)def no_hallucination(run: Run) -> bool: \"\"\"Check if the answer is grounded in the documents. Return True if there is no hallucination, False otherwise. \"\"\" # Get documents and answer qa_pipeline_run = next( r for r in run.child_runs if r.name == \"qa_pipeline\" ) retrieve_run = next( r for r in qa_pipeline_run.child_runs if r.name == \"retrieve\" ) retrieved_content = \"\\n\\n\".join( doc[\"page_content\"] for doc in retrieve_run.outputs[\"output\"] ) # Construct prompt instructions = ( \"You are a grader assessing whether an LLM generation is grounded in / \" \"supported by a set of retrieved facts. Give a binary score 1 or 0, \" \"where 1 means that the answer is grounded in / supported by the set of facts.\" ) messages = [ {\"role\": \"system\", \"content\": instructions}, {\"role\": \"user\", \"content\": f\"Set of facts:\\n{retrieved_content}\\n\\nLLM generation: {run.outputs['answer']}\"}, ] grade = grader_llm.invoke(messages) return grade.is_grounded\nFinally, we\u2019ll run evaluate with the custom evaluators defined above.\nCopy\ndef qa_wrapper(inputs: dict) -> dict: \"\"\"Wrap the qa_pipeline so it can accept the Example.inputs dict as input.\"\"\" return {\"answer\": qa_pipeline(inputs[\"question\"])}experiment_results = ls_client.evaluate( qa_wrapper, data=dataset_name, evaluators=[document_relevance, no_hallucination], experiment_prefix=\"rag-wiki-oai\")\nThe experiment will contain the results of the evaluation, including the scores and comments from the evaluators:", "tokens": 781, "node_type": "child"}
{"id": 89, "chunk_id": "c16acf8a553ac24f5d77e7d3a1c6e493", "parent_id": 77, "url": "", "namespace": "langchain", "title": "langsmith-evaluate-pairwise", "headers": ["langsmith-evaluate-pairwise"], "section_index": 0, "chunk_index": 0, "text": "# langsmith-evaluate-pairwise\n\n> Source: https://docs.langchain.com/langsmith/evaluate-pairwise\n\nevaluate()\nwith two existing experiments to define an evaluator and run a pairwise evaluation. Finally, you\u2019ll use the LangSmith UI to view the pairwise experiments.\nPrerequisites\n- If you haven\u2019t already created experiments to compare, check out the quick start or the how-to guide to get started with evaluations.\n- This guide requires\nlangsmith\nPython version>=0.2.0\nor JS version>=0.2.9\n.\nevaluate()\ncomparative args\nAt its simplest, evaluate\n/ aevaluate\nfunction takes the following arguments:\n| Argument | Description |\n|---|---|\ntarget | A list of the two existing experiments you would like to evaluate against each other. These can be uuids or experiment names. |\nevaluators | A list of the pairwise evaluators that you would like to attach to this evaluation. See the section below for how to define these. |\n| Argument | Description |\n|---|---|\nrandomize_order / randomizeOrder | An optional boolean indicating whether the order of the outputs should be randomized for each evaluation. This is a strategy for minimizing positional bias in your prompt: often, the LLM will be biased towards one of the responses based on the order. This should mainly be addressed via prompt engineering, but this is another optional mitigation. Defaults to False. |\nexperiment_prefix / experimentPrefix | A prefix to be attached to the beginning of the pairwise experiment name. Defaults to None. |\ndescription | A description of the pairwise experiment. Defaults to None. |\nmax_concurrency / maxConcurrency | The maximum number of concurrent evaluations to run. Defaults to 5. |\nclient | The LangSmith client to use. Defaults to None. |\nmetadata | Metadata to attach to your pairwise experiment. Defaults to None. |\nload_nested / loadNested | Whether to load all child runs for the experiment. When False, only the root trace will be passed to your evaluator. Defaults to False. |\nDefine a pairwise evaluator\nPairwise evaluators are just functions with an expected signature.Evaluator args\nCustom evaluator functions must have specific argument names. They can take any subset of the following arguments:inputs: dict\n: A dictionary of the inputs corresponding to a single example in a dataset.outputs: list[dict]\n: A two-item list of the dict outputs produced by each experiment on the given inputs.reference_outputs\n/referenceOutputs: dict\n: A dictionary of the reference outputs associated with the example, if available.runs: list[Run]\n: A two-item list of the full Run objects generated by the two experiments on the given example. Use this if you need access to intermediate steps or metadata about each run.example: Example\n: The full dataset Example, including the example inputs, outputs (if available), and metadata (if available).\ninputs\n, outputs\n, and reference_outputs\n/ referenceOutputs\n. runs\nand example\nare useful only if you need some extra trace or example metadata outside of the actual inputs and outputs of the application.\nEvaluator output\nCustom evaluators are expected to return one of the following types: Python and JS/TS-\ndict\n: dictionary with keys:key\n, which represents the feedback key that will be loggedscores\n, which is a mapping from run ID to score for that run.comment\n, which is a string. Most commonly used for model reasoning.\nlist[int | float | bool]\n: a two-item list of scores. The list is assumed to have the same order as theruns\n/outputs\nevaluator args. The evaluator function name is used for the feedback key.\npairwise_\nor ranked_\n.\nRun a pairwise evaluation\nThe following example uses a prompt which asks the LLM to decide which is better between two AI assistant responses. It uses structured output to parse the AI\u2019s response: 0, 1, or 2.In the Python example below, we are pulling this structured prompt from the LangChain Hub and using it with a LangChain chat model wrapper.Usage of LangChain is totally optional. To illustrate this point, the TypeScript example uses the OpenAI SDK directly.\n- Python: Requires\nlangsmith>=0.2.0\n- TypeScript: Requires\nlangsmith>=0.2.9", "tokens": 650, "node_type": "child"}
{"id": 90, "chunk_id": "c16acf8a553ac24f5d77e7d3a1c6e493", "parent_id": 78, "url": "", "namespace": "langchain", "title": "langsmith-evaluate-rag-tutorial", "headers": ["langsmith-evaluate-rag-tutorial"], "section_index": 0, "chunk_index": 0, "text": "# langsmith-evaluate-rag-tutorial\n\n> Source: https://docs.langchain.com/langsmith/evaluate-rag-tutorial\n\n- How to create test datasets\n- How to run your RAG application on those datasets\n- How to measure your application\u2019s performance using different evaluation metrics\nOverview\nA typical RAG evaluation workflow consists of three main steps:- Creating a dataset with questions and their expected answers\n- Running your RAG application on those questions\n-\nUsing evaluators to measure how well your application performed, looking at factors like:\n- Answer relevance\n- Answer accuracy\n- Retrieval quality\nSetup\nEnvironment\nFirst, let\u2019s set our environment variables:Application\nWhile this tutorial uses LangChain, the evaluation techniques and LangSmith functionality demonstrated here work with any framework. Feel free to use your preferred tools and libraries.\n- Indexing: chunks and indexes a few of Lilian Weng\u2019s blogs in a vector store\n- Retrieval: retrieves those chunks based on the user question\n- Generation: passes the question and retrieved docs to an LLM.\nIndexing and retrieval\nFirst, lets load the blog posts we want to build a chatbot for and index them.Generation\nWe can now define the generative pipeline.Dataset\nNow that we\u2019ve got our application, let\u2019s build a dataset to evaluate it. Our dataset will be very simple in this case: we\u2019ll have example questions and reference answers.Evaluators\nOne way to think about different types of RAG evaluators is as a tuple of what is being evaluated X what its being evaluated against:- Correctness: Response vs reference answer\nGoal\n: Measure \u201chow similar/correct is the RAG chain answer, relative to a ground-truth answer\u201dMode\n: Requires a ground truth (reference) answer supplied through a datasetEvaluator\n: Use LLM-as-judge to assess answer correctness.\n- Relevance: Response vs input\nGoal\n: Measure \u201chow well does the generated response address the initial user input\u201dMode\n: Does not require reference answer, because it will compare the answer to the input questionEvaluator\n: Use LLM-as-judge to assess answer relevance, helpfulness, etc.\n- Groundedness: Response vs retrieved docs\nGoal\n: Measure \u201cto what extent does the generated response agree with the retrieved context\u201dMode\n: Does not require reference answer, because it will compare the answer to the retrieved contextEvaluator\n: Use LLM-as-judge to assess faithfulness, hallucinations, etc.\n- Retrieval relevance: Retrieved docs vs input\nGoal\n: Measure \u201chow relevant are my retrieved results for this query\u201dMode\n: Does not require reference answer, because it will compare the question to the retrieved contextEvaluator\n: Use LLM-as-judge to assess relevance\nCorrectness: Response vs reference answer\nRelevance: Response vs input\nThe flow is similar to above, but we simply look at theinputs\nand outputs\nwithout needing the reference_outputs\n. Without a reference answer we can\u2019t grade accuracy, but can still grade relevance\u2014as in, did the model address the user\u2019s question or not.\nGroundedness: Response vs retrieved docs\nAnother useful way to evaluate responses without needing reference answers is to check if the response is justified by (or \u201cgrounded in\u201d) the retrieved documents.Retrieval relevance: Retrieved docs vs input\nRun evaluation\nWe can now kick off our evaluation job with all of our different evaluators.Reference code\nHere's a consolidated script with all the above code:\nHere's a consolidated script with all the above code:", "tokens": 522, "node_type": "child"}
{"id": 91, "chunk_id": "c16acf8a553ac24f5d77e7d3a1c6e493", "parent_id": 79, "url": "", "namespace": "langchain", "title": "langsmith-evaluate-with-attachments", "headers": ["langsmith-evaluate-with-attachments"], "section_index": 0, "chunk_index": 0, "text": "# langsmith-evaluate-with-attachments\n\n> Source: https://docs.langchain.com/langsmith/evaluate-with-attachments\n\n- Faster upload and download speeds due to more efficient binary file transfers\n- Enhanced visualization of different file types in the LangSmith UI\nSDK\n1. Create examples with attachments\nTo upload examples with attachments using the SDK, use the create_examples / update_examples Python methods or the uploadExamplesMultipart / updateExamplesMultipart TypeScript methods.Python\nRequireslangsmith>=0.3.13\nTypeScript\nRequires version >= 0.2.13 You can use theuploadExamplesMultipart\nmethod to upload examples with attachments.\nNote that this is a different method from the standard createExamples\nmethod, which currently does not support attachments. Each attachment requires either a Uint8Array\nor an ArrayBuffer\nas the data type.\nUint8Array\n: Useful for handling binary data directly.ArrayBuffer\n: Represents fixed-length binary data, which can be converted toUint8Array\nas needed.\nAlong with being passed in as bytes, attachments can be specified as paths to local files. To do so pass in a path for the attachment\ndata\nvalue and specify arg dangerously_allow_filesystem=True\n:2. Run evaluations\nDefine a target function\nNow that we have a dataset that includes examples with attachments, we can define a target function to run over these examples. The following example simply uses OpenAI\u2019s GPT-4o model to answer questions about an image and an audio clip.Python\nThe target function you are evaluating must have two positional arguments in order to consume the attachments associated with the example, the first must be calledinputs\nand the second must be called attachments\n.\n- The\ninputs\nargument is a dictionary that contains the input data for the example, excluding the attachments. - The\nattachments\nargument is a dictionary that maps the attachment name to a dictionary containing a presigned url, mime_type, and a reader of the bytes content of the file. You can use either the presigned url or the reader to get the file contents. Each value in the attachments dictionary is a dictionary with the following structure:\nTypeScript\nIn the TypeScript SDK, theconfig\nargument is used to pass in the attachments to the target function if includeAttachments\nis set to true\n.\nThe config\nwill contain attachments\nwhich is an object mapping the attachment name to an object of the form:\nDefine custom evaluators\nThe exact same rules apply as above to determine whether the evaluator should receive attachments. The evaluator below uses an LLM to judge if the reasoning and the answer are consistent. To learn more about how to define llm-based evaluators, please see this guide.Update examples with attachments\nIn the code above, we showed how to add examples with attachments to a dataset. It is also possible to update these same examples using the SDK. As with existing examples, datasets are versioned when you update them with attachments. Therefore, you can navigate to the dataset version history to see the changes made to each example. To learn more, please see this guide. When updating an example with attachments, you can update attachments in a few different ways:- Pass in new attachments\n- Rename existing attachments\n- Delete existing attachments\n- Any existing attachments that are not explicitly renamed or retained will be deleted.\n- An error will be raised if you pass in a non-existent attachment name to\nretain\norrename\n. - New attachments take precedence over existing attachments in case the same attachment name appears in the\nattachments\nandattachment_operations\nfields.\nUI\n1. Create examples with attachments\nYou can add examples with attachments to a dataset in a few different ways.From existing runs\nWhen adding runs to a LangSmith dataset, attachments can be selectively propagated from the source run to the destination example. To learn more, please see this guide.From scratch\nYou can create examples with attachments directly from the LangSmith UI. Click the+ Example\nbutton in the Examples\ntab of the dataset UI. Then upload attachments using the \u201cUpload Files\u201d button:\nOnce uploaded, you can view examples with attachments in the LangSmith UI. Each attachment will be rendered with a preview for easy inspection.\n2. Create a multimodal prompt\nThe LangSmith UI allows you to include attachments in your prompts when evaluating multimodal models: First, click the file icon in the message where you want to add multimodal content. Next, add a template variable for the attachment(s) you want to include for each example.- For a single attachment type: Use the suggested variable name. Note: all examples must have an attachment with this name.\n- For multiple attachments or if your attachments have varying names from one example to another: Use the\nAll attachments\nvariable to include all available attachments for each example.\nDefine custom evaluators\nThe LangSmith playground does not currently support pulling multimodal content into evaluators. If this would be helpful for your use case, please let us know in the LangChain Forum (sign up here if you\u2019re not already a member)!\n- OCR \u2192 text correction: Use a vision model to extract text from a document, then evaluate the accuracy of the extracted output.\n- Speech-to-text \u2192 transcription quality: Use a voice model to transcribe audio to text, then evaluate the transcription against your reference.\nUpdate examples with attachments\nAttachments are limited to 20MB in size in the UI.\n- Upload new attachments\n- Rename and delete attachments\n- Reset attachments to their previous state using the quick reset button", "tokens": 873, "node_type": "child"}
{"id": 92, "chunk_id": "c16acf8a553ac24f5d77e7d3a1c6e493", "parent_id": 80, "url": "", "namespace": "langchain", "title": "langsmith-evaluation-approaches", "headers": ["langsmith-evaluation-approaches"], "section_index": 0, "chunk_index": 0, "text": "# langsmith-evaluation-approaches > Source: https://docs.langchain.com/langsmith/evaluation-approaches Agents LLM-powered autonomous agents combine three components (1) Tool calling, (2) Memory, and (3) Planning. Agents use tool calling with planning (e.g., often via prompting) and memory (e.g., often short-term message history) to generate responses. Tool calling allows a model to respond to a given prompt by generating two things: (1) a tool to invoke and (2) the input arguments required. Below is a tool-calling agent in LangGraph. Theassistant node is an LLM that determines whether to invoke a tool based upon the input. The tool condition sees if a tool was selected by the assistant node and, if so, routes to the tool node . The tool node executes the tool and returns the output as a tool message to the assistant node . This loop continues until as long as the assistant node selects a tool. If no tool is selected, then the agent directly returns the LLM response. This sets up three general types of agent evaluations that users are often interested in: Final Response : Evaluate the agent\u2019s final response.Single step : Evaluate any agent step in isolation (e.g., whether it selects the appropriate tool).Trajectory : Evaluate whether the agent took the expected path (e.g., of tool calls) to arrive at the final answer. Evaluating an agent\u2019s final response One way to evaluate an agent is to assess its overall performance on a task. This basically involves treating the agent as a black box and simply evaluating whether or not it gets the job done. The inputs should be the user input and (optionally) a list of tools. In some cases, tool are hardcoded as part of the agent and they don\u2019t need to be passed in. In other cases, the agent is more generic, meaning it does not have a fixed set of tools and tools need to be passed in at run time. The output should be the agent\u2019s final response. The evaluator varies depending on the task you are asking the agent to do. Many agents perform a relatively complex set of steps and the output a final text response. Similar to RAG, LLM-as-judge evaluators are often effective for evaluation in these cases because they can assess whether the agent got a job done directly from the text response. However, there are several downsides to this type of evaluation. First, it usually takes a while to run. Second, you are not evaluating anything that happens inside the agent, so it can be hard to debug when failures occur. Third, it can sometimes be hard to define appropriate evaluation metrics.Evaluating a single step of an agent Agents generally perform multiple actions. While it is useful to evaluate them end-to-end, it can also be useful to evaluate these individual actions. This generally involves evaluating a single step of the agent - the LLM call where it decides what to do. The inputs should be the input to a single step. Depending on what you are testing, this could just be the raw user input (e.g., a prompt and / or a set of tools) or it can also include previously completed steps. The outputs are just the output of that step, which is usually the LLM response. The LLM response often contains tool calls, indicating what action the agent should take next. The evaluator for this is usually some binary score for whether the correct tool call was selected, as well as some heuristic for whether the input to the tool was correct. The reference tool can be simply specified as a string. There are several benefits to this type of evaluation. It allows you to evaluate individual actions, which lets you hone in where your application may be failing. They are also relatively fast to run (because they only involve a single LLM call) and evaluation often uses simple heuristic evaluation of the selected tool relative to the reference tool. One downside is that they don\u2019t capture the full agent - only one particular step. Another downside is that dataset creation can be challenging, particular if you want to include past history in the agent input. It is pretty easy to generate a dataset for steps early on in an agent\u2019s trajectory (e.g., this may only include the input prompt), but it can be difficult to generate a dataset for steps later on in the trajectory (e.g., including numerous prior agent actions and responses).Evaluating an agent\u2019s trajectory Evaluating an agent\u2019s trajectory involves evaluating all the steps an agent took. The inputs are again the inputs to the overall agent (the user input, and optionally a list of tools). The outputs are a list of tool calls, which can be formulated as an \u201cexact\u201d trajectory (e.g., an expected sequence of tool calls) or simply a set of tool calls that are expected (in any order). The evaluator here is some function over the steps taken. Assessing the \u201cexact\u201d trajectory can use a single binary score that confirms an exact match for each tool name in the sequence. This is simple, but has some flaws. Sometimes there can be multiple correct paths. This evaluation also does not capture the difference between a trajectory being off by a single step versus being completely wrong. To address these flaws, evaluation metrics can focused on the number of \u201cincorrect\u201d steps taken, which better accounts for trajectories that are close versus ones that deviate significantly. Evaluation metrics can also focus on whether all of the expected tools are called in any order. However, none of these approaches evaluate the input to the tools; they only focus on the tools selected. In order to account for this, another evaluation technique is to pass the full agent\u2019s trajectory (along with a reference trajectory) as a set of messages (e.g., all LLM responses and tool calls) an LLM-as-judge. This can evaluate the complete behavior of the agent, but it is the most challenging reference to compile (luckily, using a framework like LangGraph can help with this!). Another downside", "tokens": 1000, "node_type": "child"}
{"id": 93, "chunk_id": "9b0fd3b850698317bef57fa6e71a38ba", "parent_id": 80, "url": "", "namespace": "langchain", "title": "langsmith-evaluation-approaches", "headers": ["langsmith-evaluation-approaches"], "section_index": 0, "chunk_index": 1, "text": "paths. This evaluation also does not capture the difference between a trajectory being off by a single step versus being completely wrong. To address these flaws, evaluation metrics can focused on the number of \u201cincorrect\u201d steps taken, which better accounts for trajectories that are close versus ones that deviate significantly. Evaluation metrics can also focus on whether all of the expected tools are called in any order. However, none of these approaches evaluate the input to the tools; they only focus on the tools selected. In order to account for this, another evaluation technique is to pass the full agent\u2019s trajectory (along with a reference trajectory) as a set of messages (e.g., all LLM responses and tool calls) an LLM-as-judge. This can evaluate the complete behavior of the agent, but it is the most challenging reference to compile (luckily, using a framework like LangGraph can help with this!). Another downside is that evaluation metrics can be somewhat tricky to come up with.Retrieval augmented generation (RAG) Retrieval Augmented Generation (RAG) is a powerful technique that involves retrieving relevant documents based on a user\u2019s input and passing them to a language model for processing. RAG enables AI applications to generate more informed and context-aware responses by leveraging external knowledge.Dataset When evaluating RAG applications, a key consideration is whether you have (or can easily obtain) reference answers for each input question. Reference answers serve as ground truth for assessing the correctness of the generated responses. However, even in the absence of reference answers, various evaluations can still be performed using reference-free RAG evaluation prompts (examples provided below).Evaluator LLM-as-judge is a commonly used evaluator for RAG because it\u2019s an effective way to evaluate factual accuracy or consistency between texts. When evaluating RAG applications, you can have evaluators that require reference outputs and those that don\u2019t: - Require reference output: Compare the RAG chain\u2019s generated answer or retrievals against a reference answer (or retrievals) to assess its correctness. - Don\u2019t require reference output: Perform self-consistency checks using prompts that don\u2019t require a reference answer (represented by orange, green, and red in the above figure). Applying RAG Evaluation When applying RAG evaluation, consider the following approaches:- Offline evaluation : Use offline evaluation for any prompts that rely on a reference answer. This is most commonly used for RAG answer correctness evaluation, where the reference is a ground truth (correct) answer. - Online evaluation : Employ online evaluation for any reference-free prompts. This allows you to assess the RAG application\u2019s performance in real-time scenarios. - Pairwise evaluation : Utilize pairwise evaluation to compare answers produced by different RAG chains. This evaluation focuses on user-specified criteria (e.g., answer format or style) rather than correctness, which can be evaluated using self-consistency or a ground truth reference. RAG evaluation summary | Evaluator | Detail | Needs reference output | LLM-as-judge? | Pairwise relevant | |---|---|---|---|---| | Document relevance | Are documents relevant to the question? | No | Yes - prompt | No | | Answer faithfulness | Is the answer grounded in the documents? | No | Yes - prompt | No | | Answer helpfulness | Does the answer help address the question? | No | Yes - prompt | No | | Answer correctness | Is the answer consistent with a reference answer? | Yes | Yes - prompt | No | | Pairwise comparison | How do multiple answer versions compare? | No | Yes - prompt | Yes | Summarization Summarization is one specific type of free-form writing. The evaluation aim is typically to examine the writing (summary) relative to a set of criteria.Developer curated examples of texts to summarize are commonly used for evaluation (see a dataset example here). However, user logs from a production (summarization) app can be used for online evaluation with any of the Reference-free evaluation prompts below. LLM-as-judge is typically used for evaluation of summarization (as well as other types of writing) using Reference-free prompts that follow provided criteria to grade a summary. It is less common to provide a particular Reference summary, because summarization is a creative task and there are many possible correct answers. Online or Offline evaluation are feasible because of the Reference-free prompt used. Pairwise evaluation is also a powerful way to perform comparisons between different summarization chains (e.g., different summarization prompts or LLMs): | Use Case | Detail | Needs reference output | LLM-as-judge? | Pairwise relevant | |---|---|---|---|---| | Factual accuracy | Is the summary accurate relative to the source documents? | No | Yes - prompt | Yes | | Faithfulness | Is the summary grounded in the source documents (e.g., no hallucinations)? | No | Yes - prompt | Yes | | Helpfulness | Is summary helpful relative to user need? | No | Yes - prompt | Yes | Classification and tagging Classification and tagging apply a label to a given input (e.g., for toxicity detection, sentiment analysis, etc). Classification/tagging evaluation typically employs the following components, which we will review in detail below: A central consideration for classification/tagging evaluation is whether you have a dataset withreference labels or not. If not, users frequently want to define an evaluator that uses criteria to apply label (e.g., toxicity, etc) to an input (e.g., text, user-question, etc). However, if ground truth class labels are provided, then the evaluation objective is focused on scoring a classification/tagging chain relative to the ground truth class label (e.g., using metrics such as precision, recall, etc). If ground truth reference labels are provided, then it\u2019s common to simply define a custom heuristic evaluator to compare ground truth labels to the chain output. However, it is increasingly common given the emergence of LLMs simply use LLM-as-judge to perform the classification/tagging of an input based upon specified criteria (without a ground truth reference). Online or Offline evaluation is feasible when using LLM-as-judge with the Reference-free prompt used. In particular, this is well suited to Online evaluation when a user wants to tag / classify application input (e.g., for toxicity, etc). |", "tokens": 1000, "node_type": "child"}
{"id": 94, "chunk_id": "5345104ec252a41359fa4a3d2dfbec23", "parent_id": 80, "url": "", "namespace": "langchain", "title": "langsmith-evaluation-approaches", "headers": ["langsmith-evaluation-approaches"], "section_index": 0, "chunk_index": 2, "text": "users frequently want to define an evaluator that uses criteria to apply label (e.g., toxicity, etc) to an input (e.g., text, user-question, etc). However, if ground truth class labels are provided, then the evaluation objective is focused on scoring a classification/tagging chain relative to the ground truth class label (e.g., using metrics such as precision, recall, etc). If ground truth reference labels are provided, then it\u2019s common to simply define a custom heuristic evaluator to compare ground truth labels to the chain output. However, it is increasingly common given the emergence of LLMs simply use LLM-as-judge to perform the classification/tagging of an input based upon specified criteria (without a ground truth reference). Online or Offline evaluation is feasible when using LLM-as-judge with the Reference-free prompt used. In particular, this is well suited to Online evaluation when a user wants to tag / classify application input (e.g., for toxicity, etc). | Use Case | Detail | Needs reference output | LLM-as-judge? | Pairwise relevant | |---|---|---|---|---| | Accuracy | Standard definition | Yes | No | No | | Precision | Standard definition | Yes | No | No | | Recall | Standard definition | Yes | No | No |", "tokens": 201, "node_type": "child"}
{"id": 95, "chunk_id": "c16acf8a553ac24f5d77e7d3a1c6e493", "parent_id": 81, "url": "", "namespace": "langchain", "title": "langsmith-evaluation-async", "headers": ["langsmith-evaluation-async"], "section_index": 0, "chunk_index": 0, "text": "# langsmith-evaluation-async\n\n> Source: https://docs.langchain.com/langsmith/evaluation-async\n\nWe can run evaluations asynchronously via the SDK using aevaluate(), which accepts all of the same arguments as evaluate() but expects the application function to be asynchronous. You can learn more about how to use the evaluate() function here.\nThis guide is only relevant when using the Python SDK. In JS/TS the evaluate() function is already async. You can see how to use it here.\nfrom langsmith import wrappers, Clientfrom openai import AsyncOpenAI# Optionally wrap the OpenAI client to trace all model calls.oai_client = wrappers.wrap_openai(AsyncOpenAI())# Optionally add the 'traceable' decorator to trace the inputs/outputs of this function.@traceableasync def researcher_app(inputs: dict) -> str: instructions = \"\"\"You are an excellent researcher. Given a high-level research idea, \\list 5 concrete questions that should be investigated to determine if the idea is worth pursuing.\"\"\" response = await oai_client.chat.completions.create( model=\"gpt-4o-mini\", messages=[ {\"role\": \"system\", \"content\": instructions}, {\"role\": \"user\", \"content\": inputs[\"idea\"]}, ], ) return response.choices[0].message.content# Evaluator functions can be sync or asyncdef concise(inputs: dict, outputs: dict) -> bool: return len(outputs[\"output\"]) < 3 * len(inputs[\"idea\"])ls_client = Client()ideas = [ \"universal basic income\", \"nuclear fusion\", \"hyperloop\", \"nuclear powered rockets\",]dataset = ls_client.create_dataset(\"research ideas\")ls_client.create_examples( dataset_name=dataset.name, examples=[{\"inputs\": {\"idea\": i}} for i in ideas],)# Can equivalently use the 'aevaluate' function directly:# from langsmith import aevaluate# await aevaluate(...)results = await ls_client.aevaluate( researcher_app, data=dataset, evaluators=[concise], # Optional, add concurrency. max_concurrency=2, # Optional, add concurrency. experiment_prefix=\"gpt-4o-mini-baseline\" # Optional, random by default.)", "tokens": 230, "node_type": "child"}
{"id": 96, "chunk_id": "c16acf8a553ac24f5d77e7d3a1c6e493", "parent_id": 82, "url": "", "namespace": "langchain", "title": "langsmith-evaluation-concepts", "headers": ["langsmith-evaluation-concepts"], "section_index": 0, "chunk_index": 0, "text": "# langsmith-evaluation-concepts > Source: https://docs.langchain.com/langsmith/evaluation-concepts LangSmith makes building high-quality evaluations easy. This guide explains the key concepts of the LangSmith evaluation framework. The building blocks of the LangSmith framework are: Datasets: Collections of test inputs and reference outputs. This is how we typically recommend people get started creating datasets. From building your application, you probably have some idea of what types of inputs you expect your application to be able to handle, and what \u201cgood\u201d responses may be. You probably want to cover a few different common edge cases or situations you can imagine. Even 10-20 high-quality, manually-curated examples can go a long way. Once you have an application in production, you start getting valuable information: how are users actually using it? These real-world runs make for great examples because they\u2019re, well, the most realistic!If you\u2019re getting a lot of traffic, how can you determine which runs are valuable to add to a dataset? There are a few techniques you can use: User feedback: If possible - try to collect end user feedback. You can then see which datapoints got negative feedback. That is super valuable! These are spots where your application did not perform well. You should add these to your dataset to test against in the future. Heuristics: You can also use other heuristics to identify \u201cinteresting\u201d datapoints. For example, runs that took a long time to complete could be interesting to look at and add to a dataset. LLM feedback: You can use another LLM to detect noteworthy runs. For example, you could use an LLM to label chatbot conversations where the user had to rephrase their question or correct the model in some way, indicating the chatbot did not initially respond correctly. Once you have a few examples, you can try to artificially generate some more. It\u2019s generally advised to have a few good hand-crafted examples before this, as this synthetic data will often resemble them in some way. This can be a useful way to get a lot of datapoints, quickly. When setting up your evaluation, you may want to partition your dataset into different splits. For example, you might use a smaller split for many rapid and cheap iterations and a larger split for your final evaluation. In addition, splits can be important for the interpretability of your experiments. For example, if you have a RAG application, you may want your dataset splits to focus on different types of questions (e.g., factual, opinion, etc) and to evaluate your application on each split separately.Learn how to create and manage dataset splits. Datasets are versioned such that every time you add, update, or delete examples in your dataset, a new version of the dataset is created. This makes it easy to inspect and revert changes to your dataset in case you make a mistake. You can also tag versions of your dataset to give them a more human-readable name. This can be useful for marking important milestones in your dataset\u2019s history.You can run evaluations on specific versions of a dataset. This can be useful when running evaluations in CI, to make sure that a dataset update doesn\u2019t accidentally break your CI pipelines. There are a number of ways to define and run evaluators: Custom code: Define custom evaluators as Python or TypeScript functions and run them client-side using the SDKs or server-side via the UI. Built-in evaluators: LangSmith has a number of built-in evaluators that you can configure and run via the UI. You can run evaluators using the LangSmith SDK (Python and TypeScript), via the Prompt Playground, or by configuring Rules to automatically run them on particular tracing projects or datasets. Human evaluation is often a great starting point for evaluation. LangSmith makes it easy to review your LLM application outputs as well as the traces (all intermediate steps).LangSmith\u2019s annotation queues make it easy to get human feedback on your application\u2019s outputs. Heuristic evaluators are deterministic, rule-based functions. These are good for simple checks like making sure that a chatbot\u2019s response isn\u2019t empty, that a snippet of generated code can be compiled, or that a classification is exactly correct. LLM-as-judge evaluators use LLMs to score the application\u2019s output. To use them, you typically encode the grading rules / criteria in the LLM prompt. They can be reference-free (e.g., check if system output contains offensive content or adheres to specific criteria). Or, they can compare task output to a reference output (e.g., check if the output is factually accurate relative to the reference).With LLM-as-judge evaluators, it is important to carefully review the resulting scores and tune the grader prompt if needed. Often it is helpful to write these as few-shot evaluators, where you provide examples of inputs, outputs, and expected grades as part of the grader prompt.Learn about how to define an LLM-as-a-judge evaluator. Pairwise evaluators allow you to compare the outputs of two versions of an application. Think LMSYS Chatbot Arena - this is the same concept, but applied to AI applications more generally, not just models! This can use either a heuristic (\u201cwhich response is longer\u201d), an LLM (with a specific pairwise prompt), or human (asking them to manually annotate examples).When should you use pairwise evaluation?Pairwise evaluation is helpful when it is difficult to directly score an LLM output, but easier to compare two outputs. This can be the case for tasks like summarization - it may be hard to give a summary an absolute score, but easy to choose which of two summaries is more informative.Learn how run pairwise evaluations. Each time we evaluate an application on a dataset, we are conducting an experiment. An experiment contains the results of running a specific version of your application on the dataset. To understand how to use the LangSmith experiment view, see how to analyze experiment results.Typically, we will run multiple experiments on a given dataset, testing different configurations of our application (e.g., different prompts or LLMs). In LangSmith, you can easily view all the experiments associated with your dataset. Additionally, you can compare multiple", "tokens": 1000, "node_type": "child"}
{"id": 97, "chunk_id": "9b0fd3b850698317bef57fa6e71a38ba", "parent_id": 82, "url": "", "namespace": "langchain", "title": "langsmith-evaluation-concepts", "headers": ["langsmith-evaluation-concepts"], "section_index": 0, "chunk_index": 1, "text": "human (asking them to manually annotate examples).When should you use pairwise evaluation?Pairwise evaluation is helpful when it is difficult to directly score an LLM output, but easier to compare two outputs. This can be the case for tasks like summarization - it may be hard to give a summary an absolute score, but easy to choose which of two summaries is more informative.Learn how run pairwise evaluations. Each time we evaluate an application on a dataset, we are conducting an experiment. An experiment contains the results of running a specific version of your application on the dataset. To understand how to use the LangSmith experiment view, see how to analyze experiment results.Typically, we will run multiple experiments on a given dataset, testing different configurations of our application (e.g., different prompts or LLMs). In LangSmith, you can easily view all the experiments associated with your dataset. Additionally, you can compare multiple experiments in a comparison view. Running an experiment multiple times can be helpful since LLM outputs are not deterministic and can differ from one repetition to the next. By running multiple repetitions, you can get a more accurate estimate of the performance of your system.Repetitions can be configured by passing the num_repetitions argument to evaluate / aevaluate (Python, TypeScript). Repeating the experiment involves both re-running the target function to generate outputs and re-running the evaluators.To learn more about running repetitions on experiments, read the how-to-guide. By passing the max_concurrency argument to evaluate / aevaluate, you can specify the concurrency of your experiment. The max_concurrency argument has slightly different semantics depending on whether you are using evaluate or aevaluate. evaluate The max_concurrency argument to evaluate specifies the maximum number of concurrent threads to use when running the experiment. This is both for when running your target function as well as your evaluators. aevaluate The max_concurrency argument to aevaluate is fairly similar to evaluate, but instead uses a semaphore to limit the number of concurrent tasks that can run at once. aevaluate works by creating a task for each example in the dataset. Each task consists of running the target function as well as all of the evaluators on that specific example. The max_concurrency argument specifies the maximum number of concurrent tasks, or put another way - examples, to run at once. Lastly, you can also cache the API calls made in your experiment by setting the LANGSMITH_TEST_CACHE to a valid folder on your device with write access. This will cause the API calls made in your experiment to be cached to disk, meaning future experiments that make the same API calls will be greatly sped up. Human feedback is often the most valuable feedback you can gather on your application. With annotation queues you can flag runs of your application for annotation. Human annotators then have a streamlined view to review and provide feedback on the runs in a queue. Often (some subset of) these annotated runs are then transferred to a dataset for future evaluations. While you can always annotate runs inline, annotation queues provide another option to group runs together, specify annotation criteria, and configure permissions.Learn more about annotation queues and human feedback. Evaluating an application on a dataset is what we call \u201coffline\u201d evaluation. It is offline because we\u2019re evaluating on a pre-compiled set of data. An online evaluation, on the other hand, is one in which we evaluate a deployed application\u2019s outputs on real traffic, in near realtime. Offline evaluations are used for testing a version(s) of your application pre-deployment.You can run offline evaluations client-side using the LangSmith SDK (Python and TypeScript). You can run them server-side via the Prompt Playground or by configuring automations to run certain evaluators on every new experiment against a specific dataset. Perhaps the most common type of offline evaluation is one in which we curate a dataset of representative inputs, define the key performance metrics, and benchmark multiple versions of our application to find the best one. Benchmarking can be laborious because for many use cases you have to curate a dataset with gold-standard reference outputs and design good metrics for comparing experimental outputs to them. For a RAG Q&A bot this might look like a dataset of questions and reference answers, and an LLM-as-judge evaluator that determines if the actual answer is semantically equivalent to the reference answer. For a ReACT agent this might look like a dataset of user requests and a reference set of all the tool calls the model is supposed to make, and a heuristic evaluator that checks if all of the reference tool calls were made. Unit tests are used in software development to verify the correctness of individual system components. Unit tests in the context of LLMs are often rule-based assertions on LLM inputs or outputs (e.g., checking that LLM-generated code can be compiled, JSON can be loaded, etc.) that validate basic functionality.Unit tests are often written with the expectation that they should always pass. These types of tests are nice to run as part of CI. Note that when doing so it is useful to set up a cache to minimize LLM calls (because those can quickly rack up!). Regression tests are used to measure performance across versions of your application over time. They are used to, at the very least, ensure that a new app version does not regress on examples that your current version correctly handles, and ideally to measure how much better your new version is relative to the current. Often these are triggered when you are making app updates (e.g. updating models or architectures) that are expected to influence the user experience.LangSmith\u2019s comparison view has native support for regression testing, allowing you to quickly see examples that have changed relative to the baseline. Regressions are highlighted red, improvements green. Backtesting is an approach that combines dataset creation (discussed above) with evaluation. If you have a collection of production logs, you can turn them into a dataset. Then, you can re-run those production examples with newer application versions. This allows", "tokens": 1000, "node_type": "child"}
{"id": 98, "chunk_id": "5345104ec252a41359fa4a3d2dfbec23", "parent_id": 82, "url": "", "namespace": "langchain", "title": "langsmith-evaluation-concepts", "headers": ["langsmith-evaluation-concepts"], "section_index": 0, "chunk_index": 2, "text": "(because those can quickly rack up!). Regression tests are used to measure performance across versions of your application over time. They are used to, at the very least, ensure that a new app version does not regress on examples that your current version correctly handles, and ideally to measure how much better your new version is relative to the current. Often these are triggered when you are making app updates (e.g. updating models or architectures) that are expected to influence the user experience.LangSmith\u2019s comparison view has native support for regression testing, allowing you to quickly see examples that have changed relative to the baseline. Regressions are highlighted red, improvements green. Backtesting is an approach that combines dataset creation (discussed above) with evaluation. If you have a collection of production logs, you can turn them into a dataset. Then, you can re-run those production examples with newer application versions. This allows you to assess performance on past and realistic user inputs.This is commonly used to evaluate new model versions. Anthropic dropped a new model? No problem! Grab the 1000 most recent runs through your application and pass them through the new model. Then compare those results to what actually happened in production. For some tasks it is easier for a human or LLM grader to determine if \u201cversion A is better than B\u201d than to assign an absolute score to either A or B. Pairwise evaluations are just this \u2014 a scoring of the outputs of two versions against each other as opposed to against some reference output or absolute criteria. Pairwise evaluations are often useful when using LLM-as-judge evaluators on more general tasks. For example, if you have a summarizer application, it may be easier for an LLM-as-judge to determine \u201cWhich of these two summaries is more clear and concise?\u201d than to give an absolute score like \u201cGive this summary a score of 1-10 in terms of clarity and concision.\u201dLearn how run pairwise evaluations. Evaluating a deployed application\u2019s outputs in (roughly) realtime is what we call \u201conline\u201d evaluation. In this case there is no dataset involved and no possibility of reference outputs \u2014 we\u2019re running evaluators on real inputs and real outputs as they\u2019re produced. This is useful for monitoring your application and flagging unintended behavior. Online evaluation can also work hand-in-hand with offline evaluation: for example, an online evaluator can be used to classify input questions into a set of categories that can later be used to curate a dataset for offline evaluation.Online evaluators are generally intended to be run server-side. LangSmith has built-in LLM-as-judge evaluators that you can configure, or you can define custom code evaluators that are also run within LangSmith. Testing and evaluation are very similar and overlapping concepts that often get confused.An evaluation measures performance according to a metric(s). Evaluation metrics can be fuzzy or subjective, and are more useful in relative terms than absolute ones. That is, they\u2019re often used to compare two systems against each other rather than to assert something about an individual system.Testing asserts correctness. A system can only be deployed if it passes all tests.Evaluation metrics can be turned into tests. For example, you can write regression tests to assert that any new version of a system must outperform some baseline version of the system on the relevant evaluation metrics.It can also be more resource efficient to run tests and evaluations together if your system is expensive to run and you have overlapping datasets for your tests and evaluations.You can also choose to write evaluations using standard software testing tools like pytest or vitest/jest out of convenience. The LangSmith SDKs come with integrations for pytest and Vitest/Jest. These make it easy to: Track test results in LangSmith Write evaluations as tests Tracking test results in LangSmith makes it easy to share results, compare systems, and debug failing tests.Writing evaluations as tests can be useful when each example you want to evaluate on requires custom logic for running the application and/or evaluators. The standard evaluation flows assume that you can run your application and evaluators in the same way on every example in a dataset. But for more complex systems or comprehensive evals, you may want to evaluate specific subsets of your system with specific types of inputs and metrics. These types of heterogenous evals are much easier to write as a suite of distinct test cases that all get tracked together rather than using the standard evaluate flow.Using testing tools is also helpful when you want to both evaluate your system\u2019s outputs and assert some basic things about them.", "tokens": 759, "node_type": "child"}
{"id": 99, "chunk_id": "c16acf8a553ac24f5d77e7d3a1c6e493", "parent_id": 83, "url": "", "namespace": "langchain", "title": "langsmith-evaluation-quickstart", "headers": ["langsmith-evaluation-quickstart"], "section_index": 0, "chunk_index": 0, "text": "# langsmith-evaluation-quickstart\n\n> Source: https://docs.langchain.com/langsmith/evaluation-quickstart\n\n- Dataset: A set of test inputs (and optionally, expected outputs).\n- Target function: The part of your application you want to test\u2014this might be a single LLM call with a new prompt, one module, or your entire workflow.\n- Evaluators: Functions that score your target function\u2019s outputs.\nPrerequisites\nBefore you begin, make sure you have:- A LangSmith account: Sign up or log in at smith.langchain.com.\n- A LangSmith API key: Follow the Create an API key guide.\n- An OpenAI API key: Generate this from the OpenAI dashboard.\n- UI\n- SDK\n1. Set workspace secrets\nIn the LangSmith UI, ensure that your OpenAI API key is set as a workspace secret.- Navigate to Settings and then move to the Secrets tab.\n- Select Add secret and enter the\nOPENAI_API_KEY\nand your API key as the Value. - Select Save secret.\nWhen adding workspace secrets in the LangSmith UI, make sure the secret keys match the environment variable names expected by your model provider.\n2. Create a prompt\nLangSmith\u2019s Prompt Playground makes it possible to run evaluations over different prompts, new models, or test different model configurations.- In the LangSmith UI, navigate to the Playground under Prompt Engineering.\n-\nUnder the Prompts panel, modify the system prompt to:\nLeave the Human message as is:\n{question}\n.\n3. Create a dataset\n- Click Set up Evaluation, which will open a New Experiment table at the bottom of the page.\n-\nIn the Select or create a new dataset dropdown, click the + New button to create a new dataset.\n-\nAdd the following examples to the dataset:\nInputs Reference Outputs question: Which country is Mount Kilimanjaro located in? output: Mount Kilimanjaro is located in Tanzania. question: What is Earth\u2019s lowest point? output: Earth\u2019s lowest point is The Dead Sea. - Click Save and enter a name to save your newly created dataset.\n4. Add an evaluator\n- Click + Evaluator and select Correctness from the Pre-built Evaluator options.\n- In the Correctness panel, click Save.\n5. Run your evaluation\n-\nSelect Start on the top right to run your evaluation. This will create an experiment with a preview in the New Experiment table. You can view in full by clicking the experiment name.\nNext steps\n- For more details on evaluations, refer to the Evaluation documentation.\n- Learn how to create and manage datasets in the UI.\n- Learn how to run an evaluation from the prompt playground.", "tokens": 413, "node_type": "child"}
{"id": 100, "chunk_id": "c16acf8a553ac24f5d77e7d3a1c6e493", "parent_id": 85, "url": "", "namespace": "langchain", "title": "langsmith-example-data-format", "headers": ["langsmith-example-data-format"], "section_index": 0, "chunk_index": 0, "text": "# langsmith-example-data-format\n\n> Source: https://docs.langchain.com/langsmith/example-data-format\n\nLangSmith stores examples in datasets as follows:\n| Field Name | Type | Description |\n|---|---|---|\n| id | UUID | Unique identifier for the example. |\n| name | string | The name of the example. |\n| created_at | datetime | The time this example was created |\n| modified_at | datetime | The last time this example was modified |\n| inputs | object | A map of inputs for the example. |\n| outputs | object | A map or set of outputs generated by the run. |\n| dataset_id | UUID | The dataset the example belongs to |\n| source_run_id | UUID | If this example was created from a LangSmith Run , the ID of said run |\n| metadata | object | A map of additional, user or SDK defined information that can be stored on an example. |", "tokens": 152, "node_type": "child"}
{"id": 101, "chunk_id": "c16acf8a553ac24f5d77e7d3a1c6e493", "parent_id": 86, "url": "", "namespace": "langchain", "title": "langsmith-export-backend", "headers": ["langsmith-export-backend"], "section_index": 0, "chunk_index": 0, "text": "# langsmith-export-backend\n\n> Source: https://docs.langchain.com/langsmith/export-backend\n\nThis section is only applicable for Kubernetes deployments.\n- Collectors, such as OpenTelemetry, FluentBit or Prometheus.\n- Observability backends, such as Datadog or the Grafana ecosystem.\nLogs: OTel Example\nAll services that are part of the LangSmith self-hosted deployment write logs to their node\u2019s filesystem and to stdout. In order to access these logs, you need to set up your collector to read from either the filesystem or stdout. Most popular collectors support reading logs from filesystems.- OpenTelemetry: File Log Receiver\n- FluentBit: Tail Input\n- Datadog: Kubernetes Log Collection\nMetrics: OTel Example\nLangSmith Services\nThe following LangSmith services expose metrics at an endpoint, in the Prometheus metrics format. The frontend does not currently expose metrics.- Backend:\nhttp://<langsmith_release_name>-backend.<namespace>.svc.cluster.local:1984/metrics\n- Platform Backend:\nhttp://<langsmith_release_name>-platform-backend.<namespace>.svc.cluster.local:1986/metrics\n- Playground:\nhttp://<langsmith_release_name>-playground.<namespace>.svc.cluster.local:1988/metrics\n- (LangSmith Control Plane only) Host Backend:\nhttp://<langsmith_release_name>-host-backend.<namespace>.svc.cluster.local:1985/metrics\nFrontend Nginx\nThe frontend service exposes its Nginx metrics at the following endpoint:langsmith-frontend.langsmith.svc.cluster.local:80/nginx_status\n. You can either scrape them yourself, or bring up a Prometheus Nginx exporter using the LangSmith Observability Helm Chart\nThe following sections apply for in-cluster databases only. If you are using external databases, you will need to configure exposing and fetching metrics.\nPostgres + Redis\nIf you are using in-cluster Postgres/Redis instances, you can use a Prometheus exporter to expose metrics from your instance. You can deploy your own, or if you would like, you can use the LangSmith Observability Helm Chart to deploy an exporter for you.Clickhouse\nThe in-cluster Clickhouse is configured to expose metrics without the need for an exporter. You can use your collector to scrape metrics athttp://<langsmith_release_name>-clickhouse.<namespace>.svc.cluster.local:9363/metrics\nTraces: OTel Example\nThe LangSmith Backend, Platform Backend, Playground and LangSmith Queue deployments have been instrumented to emit Otel traces. Tracing is toggled off by default, and can be enabled for all LangSmith services with the following in yourlangsmith_config.yaml\n(or equivalent) file:", "tokens": 305, "node_type": "child"}
{"id": 102, "chunk_id": "c16acf8a553ac24f5d77e7d3a1c6e493", "parent_id": 87, "url": "", "namespace": "langchain", "title": "langsmith-export-traces", "headers": ["langsmith-export-traces"], "section_index": 0, "chunk_index": 0, "text": "# langsmith-export-traces\n\n> Source: https://docs.langchain.com/langsmith/export-traces\n\nIf you are looking to export a large volume of traces, we recommend that you use the Bulk Data Export functionality, as it will better handle large data volumes and will support automatic retries and parallelization across partitions.\nlist_runs\nmethod in the SDK or /runs/query\nendpoint in the API.\nLangSmith stores traces in a simple format that is specified in the Run (span) data format.\nUse filter arguments\nFor simple queries, you don\u2019t have to rely on our query syntax. You can use the filter arguments specified in the filter arguments reference.PrerequisitesInitialize the client before running the below code snippets.\nList all runs in a project\nList LLM and Chat runs in the last 24 hours\nList root runs in a project\nRoot runs are runs that have no parents. These are assigned a value ofTrue\nfor is_root\n. You can use this to filter for root runs.\nList runs without errors\nList runs by run ID\nIgnores Other ArgumentsIf you provide a list of run IDs in the way described above, it will ignore all other filtering arguments like\nproject_name\n, run_type\n, etc. and directly return the runs matching the given IDs.Use filter query language\nFor more complex queries, you can use the query language described in the filter query language reference.List all root runs in a conversational thread\nThis is the way to fetch runs in a conversational thread. For more information on setting up threads, refer to our how-to guide on setting up threads. Threads are grouped by setting a shared thread ID. The LangSmith UI lets you use any one of the following three metadata keys:session_id\n, conversation_id\n, or thread_id\n. The session ID is also known as the tracing project ID. The following query matches on any of them.\nList all runs called \u201cextractor\u201d whose root of the trace was assigned feedback \u201cuser_score\u201d score of 1\nList runs with \u201cstar_rating\u201d key whose score is greater than 4\nList runs that took longer than 5 seconds to complete\nList all runs that have \u201cerror\u201d not equal to null\nList all runs where start_time is greater than a specific timestamp\nList all runs that contain the string \u201csubstring\u201d\nList all runs that are tagged with the git hash \u201c2aa1cf4\u201d\nList all runs that started after a specific timestamp and either have \u201cerror\u201d not equal to null or a \u201cCorrectness\u201d feedback score equal to 0\nComplex query: List all runs where tags include \u201cexperimental\u201d or \u201cbeta\u201d and latency is greater than 2 seconds\nSearch trace trees by full text\nYou can use thesearch()\nfunction without any specific field to do a full text search across all string fields in a run. This allows you to quickly find traces that match a search term.\nCheck for presence of metadata\nIf you want to check for the presence of metadata, you can use theeq\noperator, optionally with an and\nstatement to match by value. This is useful if you want to log more structured information about your runs.\nCheck for environment details in metadata\nA common pattern is to add environment information to your traces via metadata. If you want to filter for runs containing environment metadata, you can use the same pattern as above:Check for conversation ID in metadata\nAnother common way to associate traces in the same conversation is by using a shared conversation ID. If you want to filter runs based on a conversation ID in this way, you can search for that ID in the metadata.Negative filtering on key-value pairs\nYou can use negative filtering on metadata, input, and output key-value pairs to exclude specific runs from your results. Here are some examples for metadata key-value pairs but the same logic applies to input and output key-value pairs.Combine multiple filters\nIf you want to combine multiple conditions to refine your search, you can use theand\noperator along with other filtering functions. Here\u2019s how you can search for runs named \u201cChatOpenAI\u201d that also have a specific conversation_id\nin their metadata:\nTree Filter\nList all runs named \u201cRetrieveDocs\u201d whose root run has a \u201cuser_score\u201d feedback of 1 and any run in the full trace is named \u201cExpandQuery\u201d. This type of query is useful if you want to extract a specific run conditional on various states or steps being reached within the trace.Advanced: export flattened trace view with child tool usage\nThe following Python example demonstrates how to export a flattened view of traces, including information on the tools (from nested runs) used by the agent within each trace. This can be used to analyze the behavior of your agents across multiple traces. This example queries all tool runs within a specified number of days and groups them by their parent (root) run ID. It then fetches the relevant information for each root run, such as the run name, inputs, outputs, and combines that information with the child run information. To optimize the query, the example:- Selects only the necessary fields when querying tool runs to reduce query time.\n- Fetches root runs in batches while processing tool runs concurrently.", "tokens": 848, "node_type": "child"}
{"id": 103, "chunk_id": "c16acf8a553ac24f5d77e7d3a1c6e493", "parent_id": 88, "url": "", "namespace": "langchain", "title": "langsmith-faq", "headers": ["langsmith-faq"], "section_index": 0, "chunk_index": 0, "text": "# langsmith-faq > Source: https://docs.langchain.com/langsmith/faq Observability I can\u2019t create API keys or manage users in the UI, what\u2019s wrong? - You have likely deployed LangSmith without setting up SSO. LangSmith requires SSO to manage users and API keys. You can find more information on setting up SSO in the configuration section. How does load balancing/ingress work? - You will need to expose the frontend container/service to your applications/users. This will handle routing to all downstream services. - You will need to terminate SSL at the ingress level. We recommend using a managed service like AWS ALB, GCP Load Balancer, or Nginx. How can we authenticate to the application? - Currently, our self-hosted solution supports SSO with OAuth2.0 and OIDC as an authn solution. Note, we do offer a no-auth solution but highly recommend setting up oauth before moving into production. Can I use external storage services? - You can configure LangSmith to use external versions of all storage services. In a production setting, we strongly recommend using external storage services. Check out the configuration section for more information. Does my application need egress to function properly? Our deployment only needs egress for a few things (most of which can reside within your VPC):- Fetching images (If mirroring your images, this may not be needed) - Talking to any LLM endpoints - Talking to any external storage services you may have configured - Fetching OAuth information - Subscription Metrics and Operational Metadata (if not running in offline mode) - Requires egress to https://beacon.langchain.com - See Egress for more information - Requires egress to X-Organization-Id and X-Tenant-Id headers to be allowed to be passed through to the backend service. These are used to determine which organization and workspace (previously called \u201ctenant\u201d) the request is for. Resource requirements for the application? - In kubernetes, we recommend a minimum helm configuration which can be found in here. For docker, we recommend a minimum of 16GB of RAM and 4 CPUs. - For Postgres, we recommend a minimum of 8GB of RAM and 2 CPUs. - For Redis, we recommend 4GB of RAM and 2 CPUs. - For Clickhouse, we recommend 32GB of RAM and 8 CPUs. SAML SSO FAQs How do I change a SAML SSO user\u2019s email address? Some identity providers retain the originalUser ID through an email change while others do not, so we recommend that you follow these steps to avoid duplicate users in LangSmith: - Remove the user from the organization (see here) - Change their email address in the IdP - Have them login to LangSmith again via SAML SSO - this will trigger the usual JIT provisioning flow with their new email address How do I fix \u201c405 method not allowed\u201d? Ensure you\u2019re using the correct ACS URL: https://auth.langchain.com/auth/v1/sso/saml/acsSCIM FAQs Can I use SCIM without SAML SSO? - Cloud: No, SAML SSO is required for SCIM in cloud deployments - Self-hosted: Yes, SCIM works with OAuth with Client Secret authentication mode What happens if I have both JIT provisioning and SCIM enabled? JIT provisioning and SCIM can conflict with each other. We recommend disabling JIT provisioning before enabling SCIM to ensure consistent user provisioning behavior.How do I change a user\u2019s role or workspace access? Update the user\u2019s group membership in your IdP. The changes will be synchronized to LangSmith according to the role precedence rules.What happens when a user is removed from all groups? The user will be deprovisioned from your LangSmith organization according to your IdP\u2019s deprovisioning settings.Can I use custom group names? Yes. If your identity provider supports syncing alternate fields to thedisplayName group attribute, you may use an alternate attribute (like description ) as the displayName in LangSmith and retain full customizability of the identity provider group name. Otherwise, groups must follow the specific naming convention described in the Group Naming Convention section to properly map to LangSmith roles and workspaces. Why is my Okta integration not working? See Okta\u2019s troubleshooting guide here: https://help.okta.com/en-us/content/topics/users-groups-profiles/usgp-group-push-troubleshoot.htm.Deployment Do I need to use LangChain to use LangGraph? What\u2019s the difference? No. LangGraph is an orchestration framework for complex agentic systems and is more low-level and controllable than LangChain agents. LangChain provides a standard interface to interact with models and other components, useful for straight-forward chains and retrieval flows.How is LangGraph different from other agent frameworks? Other agentic frameworks can work for simple, generic tasks but fall short for complex tasks bespoke to a company\u2019s needs. LangGraph provides a more expressive framework to handle companies\u2019 unique tasks without restricting users to a single black-box cognitive architecture.Does LangGraph impact the performance of my app? LangGraph will not add any overhead to your code and is specifically designed with streaming workflows in mind.Is LangGraph open source? Is it free? Yes. LangGraph is an MIT-licensed open-source library and is free to use.How are LangGraph and LangSmith different? LangGraph is a stateful, orchestration framework that brings added control to agent workflows. LangSmith is a service for deploying and scaling agentic applications, with an opinionated API for building agent UXs, plus an integrated developer UI.| Features | LangGraph (open source) | LangSmith | |---|---|---| | Description | Stateful orchestration framework for agentic applications | Scalable infrastructure for deploying LangGraph applications | | SDKs | Python and JavaScript | Python and JavaScript | | HTTP APIs | None | Yes - useful for retrieving & updating state or long-term memory, or creating a configurable assistant | | Streaming | Basic | Dedicated mode for token-by-token messages | | Checkpointer | Community contributed | Supported out-of-the-box | | Persistence Layer | Self-managed | Managed Postgres with efficient storage | | Deployment | Self-managed | \u2022 Cloud \u2022 Free self-hosted \u2022 Enterprise (paid self-hosted) | | Scalability | Self-managed | Auto-scaling of task queues and servers | | Fault-tolerance | Self-managed | Automated retries | | Concurrency Control | Simple threading | Supports double-texting | | Scheduling | None | Cron scheduling | | Monitoring | None | Integrated with LangSmith for observability | | IDE integration |", "tokens": 1000, "node_type": "child"}
{"id": 104, "chunk_id": "9b0fd3b850698317bef57fa6e71a38ba", "parent_id": 88, "url": "", "namespace": "langchain", "title": "langsmith-faq", "headers": ["langsmith-faq"], "section_index": 0, "chunk_index": 1, "text": "Description | Stateful orchestration framework for agentic applications | Scalable infrastructure for deploying LangGraph applications | | SDKs | Python and JavaScript | Python and JavaScript | | HTTP APIs | None | Yes - useful for retrieving & updating state or long-term memory, or creating a configurable assistant | | Streaming | Basic | Dedicated mode for token-by-token messages | | Checkpointer | Community contributed | Supported out-of-the-box | | Persistence Layer | Self-managed | Managed Postgres with efficient storage | | Deployment | Self-managed | \u2022 Cloud \u2022 Free self-hosted \u2022 Enterprise (paid self-hosted) | | Scalability | Self-managed | Auto-scaling of task queues and servers | | Fault-tolerance | Self-managed | Automated retries | | Concurrency Control | Simple threading | Supports double-texting | | Scheduling | None | Cron scheduling | | Monitoring | None | Integrated with LangSmith for observability | | IDE integration | Studio | Studio | Is LangSmith open source? No. LangSmith is proprietary software. There is a free, self-hosted version of LangSmith with access to basic features. The Cloud deployment option and the Self-Hosted deployment options are paid services. Contact our sales team to learn more. For more information, see our LangSmith pricing page.Does LangGraph work with LLMs that don\u2019t support tool calling? Yes! You can use LangGraph with any LLMs. The main reason we use LLMs that support tool calling is that this is often the most convenient way to have the LLM make its decision about what to do. If your LLM does not support tool calling, you can still use it - you just need to write a bit of logic to convert the raw LLM string response to a decision about what to do.Does LangGraph work with OSS LLMs? Yes! LangGraph is totally ambivalent to what LLMs are used under the hood. The main reason we use closed LLMs in most of the tutorials is that they seamlessly support tool calling, while OSS LLMs often don\u2019t. But tool calling is not necessary (see this section) so you can totally use LangGraph with OSS LLMs.Can I use Studio without logging in to LangSmith? Yes! You can use the development version of LangGraph Server to run the backend locally. This will connect to the Studio frontend hosted as part of LangSmith. If you set an environment variable ofLANGSMITH_TRACING=false , then no traces will be sent to LangSmith.", "tokens": 398, "node_type": "child"}
{"id": 105, "chunk_id": "c16acf8a553ac24f5d77e7d3a1c6e493", "parent_id": 89, "url": "", "namespace": "langchain", "title": "langsmith-feedback-data-format", "headers": ["langsmith-feedback-data-format"], "section_index": 0, "chunk_index": 0, "text": "# langsmith-feedback-data-format\n\n> Source: https://docs.langchain.com/langsmith/feedback-data-format\n\nFeedback is LangSmith\u2019s way of storing the criteria and scores from evaluation on a particular trace or intermediate run (span). Feedback can be produced from a variety of ways, such as:\n- Sent up along with a trace from the LLM application\n- Generated by a user in the app inline or in an annotation queue\n- Generated by an automatic evaluator during offline evaluation\n- Generated by an online evaluator\n| Field Name | Type | Description |\n|---|---|---|\n| id | UUID | Unique identifier for the record itself |\n| created_at | datetime | Timestamp when the record was created |\n| modified_at | datetime | Timestamp when the record was last modified |\n| session_id | UUID | Unique identifier for the experiment or tracing project the run was a part of |\n| run_id | UUID | Unique identifier for a specific run within a session |\n| key | string | A key describing the criteria of the feedback, eg \u201ccorrectness\u201d |\n| score | number | Numerical score associated with the feedback key |\n| value | string | Reserved for storing a value associated with the score. Useful for categorical feedback. |\n| comment | string | Any comment or annotation associated with the record. This can be a justification for the score given. |\n| correction | object | Reserved for storing correction details, if any |\n| feedback_source | object | Object containing information about the feedback source |\n| feedback_source.type | string | The type of source where the feedback originated, eg \u201capi\u201d, \u201capp\u201d, \u201cevaluator\u201d |\n| feedback_source.metadata | object | Reserved for additional metadata, currently |\n| feedback_source.user_id | UUID | Unique identifier for the user providing feedback |", "tokens": 296, "node_type": "child"}
{"id": 106, "chunk_id": "c16acf8a553ac24f5d77e7d3a1c6e493", "parent_id": 90, "url": "", "namespace": "langchain", "title": "langsmith-fetch-perf-metrics-experiment", "headers": ["langsmith-fetch-perf-metrics-experiment"], "section_index": 0, "chunk_index": 0, "text": "# langsmith-fetch-perf-metrics-experiment\n\n> Source: https://docs.langchain.com/langsmith/fetch-perf-metrics-experiment\n\nTracing projects and experiments use the same underlying data structure in our backend, which is called a \u201csession.\u201dYou might see these terms interchangeably in our documentation, but they all refer to the same underlying data structure.We are working on unifying the terminology across our documentation and APIs.\nevaluate\nwith the Python or TypeScript SDK, you can fetch the performance metrics for the experiment using the read_project\n/readProject\nmethods.\nThe payload for experiment details includes the following values:\nlatency_p50\n: The 50th percentile latency in seconds.latency_p99\n: The 99th percentile latency in seconds.total_tokens\n: The total number of tokens used.prompt_tokens\n: The number of prompt tokens used.completion_tokens\n: The number of completion tokens used.total_cost\n: The total cost of the experiment.prompt_cost\n: The cost of the prompt tokens.completion_cost\n: The cost of the completion tokens.feedback_stats\n: The feedback statistics for the experiment.error_rate\n: The error rate for the experiment.first_token_p50\n: The 50th percentile latency for the time to generate the first token (if using streaming).first_token_p99\n: The 99th percentile latency for the time to generate the first token (if using streaming).\nevaluate\n, then fetch the performance metrics for the experiment.", "tokens": 195, "node_type": "child"}
{"id": 107, "chunk_id": "c16acf8a553ac24f5d77e7d3a1c6e493", "parent_id": 91, "url": "", "namespace": "langchain", "title": "langsmith-filter-experiments-ui", "headers": ["langsmith-filter-experiments-ui"], "section_index": 0, "chunk_index": 0, "text": "# langsmith-filter-experiments-ui\n\n> Source: https://docs.langchain.com/langsmith/filter-experiments-ui\n\nWhen you run an experiment in the SDK, you can attach metadata to make it easier to filter in UI. This is helpful if you know what axes you want to drill down into when running experiments.In our example, we are going to attach metadata to our experiment around the model used, the model provider, and a known ID of the prompt:\nCopy\nmodels = { \"openai-gpt-4o\": ChatOpenAI(model=\"gpt-4o\", temperature=0), \"openai-gpt-4o-mini\": ChatOpenAI(model=\"gpt-4o-mini\", temperature=0), \"anthropic-claude-3-sonnet-20240229\": ChatAnthropic(temperature=0, model_name=\"claude-3-sonnet-20240229\")}prompts = { \"singleminded\": \"always answer questions with the word banana.\", \"fruitminded\": \"always discuss fruit in your answers.\", \"basic\": \"you are a chatbot.\"}def answer_evaluator(run, example) -> dict: llm = ChatOpenAI(model=\"gpt-4o\", temperature=0) answer_grader = hub.pull(\"langchain-ai/rag-answer-vs-reference\") | llm score = answer_grader.invoke( { \"question\": example.inputs[\"question\"], \"correct_answer\": example.outputs[\"answer\"], \"student_answer\": run.outputs, } ) return {\"key\": \"correctness\", \"score\": score[\"Score\"]}dataset_name = \"Filterable Dataset\"for model_type, model in models.items(): for prompt_type, prompt in prompts.items(): def predict(example): return model.invoke( [(\"system\", prompt), (\"user\", example[\"question\"])] ) model_provider = model_type.split(\"-\")[0] model_name = model_type[len(model_provider) + 1:] evaluate( predict, data=dataset_name, evaluators=[answer_evaluator], # ADD IN METADATA HERE!! metadata={ \"model_provider\": model_provider, \"model_name\": model_name, \"prompt_id\": prompt_type } )\nIn the UI, we see all experiments that have been run by default.If we, say, have a preference for openai models, we can easily filter down and see scores within just openai models first:We can stack filters, allowing us to filter out low scores on correctness to make sure we only compare relevant experiments:Finally, we can clear and reset filters. For example, if we see there is clear there\u2019s a winner with the singleminded prompt, we can change filtering settings to see if any other model providers\u2019 models work as well with it:", "tokens": 272, "node_type": "child"}
{"id": 108, "chunk_id": "c16acf8a553ac24f5d77e7d3a1c6e493", "parent_id": 92, "url": "", "namespace": "langchain", "title": "langsmith-filter-traces-in-application", "headers": ["langsmith-filter-traces-in-application"], "section_index": 0, "chunk_index": 0, "text": "# langsmith-filter-traces-in-application > Source: https://docs.langchain.com/langsmith/filter-traces-in-application Tracing projects can contain a significant amount of data. Filters are used for effectively navigating and analyzing this data, allowing you to: - Have focused investigations: Quickly narrow down to specific runs for ad-hoc analysis - Debug and analyze: Identify and examine errors, failed runs, and performance bottlenecks Creating and Applying Filters Filtering by run attributes There are two ways to filter runs in a tracing project:- Filters: Located towards the top-left of the tracing projects page. This is where you construct and manage detailed filter criteria. - Filter Shortcuts: Positioned on the right sidebar of the tracing projects page. The filter shortcuts bar provides quick access to filters based on the most frequently occurring attributes in your project\u2019s runs. Default filterBy default, the IsTrace is true filter is applied. This displays only top-level traces. Removing this filter will show all runs, including intermediate spans, in the project.Filtering by time range In addition to filtering by run attributes, you can also filter runs within a specific time range. This option is available towards the top-left of the tracing projects page.Filter operators The available filter operators depend on the data type of the attribute you are filtering on. Here\u2019s an overview of common operators:- is: Exact match on the filter value - is not: Negative match on the filter value - contains: Partial match on the filter value - does not contain: Negative partial match on the filter value - is one of: Match on any of the values in the list > /< : Available for numeric fields Specific Filtering Techniques Filter for intermediate runs (spans) In order to filter for intermediate runs (spans), you first need to remove the defaultIsTrace is true filter. For example, you would do this if you wanted to filter by run name for sub runs or filter by run type . Run metadata and tags are also powerful to filter on. These rely on good tagging across all parts of your pipeline. To learn more, you can check out this guide. Filter based on inputs and outputs You can filter runs based on the content in the inputs and outputs of the run. To filter either inputs or outputs, you can use theFull-Text Search filter which will match keywords in either field. For more targeted search, you can use the Input or Output filters which will only match content based on the respective field. You can also specify multiple matches, either by including multiple terms separated by whitespace, or adding multiple filters - which will try to match all terms provided. Note that keyword search is done by splitting the text and finding any partial matches on the search keywords, so it is not done in specific order. We exclude common stop words from the search (from the nltk stop word list along with a few other common JSON keywords). Based on the filters above, the system will search for python and tensorflow in either inputs or outputs, and embedding in the inputs along with fine and tune in the outputs. Filter based on input / output key-value pairs In addition to full-text search, you can filter runs based on specific key-value pairs in the inputs and outputs. This allows for more precise filtering, especially when dealing with structured data.We index up to 100 unique keys to keep your data organized and searchable. Each key also has a character limit of 250 characters per value. If your data exceeds either of these limits, the text won\u2019t be indexed. This helps us ensure fast, reliable performance. Input Key or Output Key filter from the filters dropdown. For example, to match the following input: Filters , Add Filter to bring up the filtering options. Then select Input Key , enter input as the key and enter What is the capital of France? as the value. You can also match nested keys by using dot notation to select the nested key name. For example, to match nested keys in the output: Output Key , enter documents.page_content as the key and enter The capital of France is Paris as the value. This will match the nested key documents.page_content with the specified value. You can add multiple key-value filters to create more complex queries. You can also use the Filter Shortcuts on the right side to quickly filter based on common key-value pairs as shown below: Example: Filtering for tool calls It\u2019s common to want to search for traces that contain specific tool calls. Tool calls are typically indicated in the output of an LLM run. To filter for tool calls, you would use theOutput Key filter. While this example will show you how to filter for tool calls, the same logic can be applied to filter for any key-value pair in the output. In this case, let\u2019s assume this is the output you want to filter for: | Key | Value | |---|---| generations.type | ChatGeneration | generations.message.type | constructor | generations.message.kwargs.type | ai | generations.message.kwargs.id | run-ca7f7531-f4de-4790-9c3e-960be7f8b109 | generations.message.kwargs.tool_calls.name | Plan | generations.message.kwargs.tool_calls.args.steps | Research LangGraph's node configuration capabilities | generations.message.kwargs.tool_calls.args.steps | Investigate how to add a Python code execution node | generations.message.kwargs.tool_calls.args.steps | Find an example or create a sample implementation of a code execution node | generations.message.kwargs.tool_calls.id | toolu_01XexPzAVknT3gRmUB5PK5BP | generations.message.kwargs.tool_calls.type | tool_call | type | LLMResult | generations.message.kwargs.tool_calls.name = Plan This will match root and non-root runs where the tool_calls name is Plan . Negative filtering on key-value pairs Different types of negative filtering can be applied toMetadata , Input Key , and Output Key fields to exclude specific runs from your results. For example, to find all runs where the metadata key phone is not equal to 1234567890 , set the Metadata Key operator to is and Key field to phone , then set the Value operator to is not and the Value field to 1234567890 . This will match all runs that have a metadata key phone with any value except 1234567890 . To find runs that", "tokens": 1000, "node_type": "child"}
{"id": 109, "chunk_id": "9b0fd3b850698317bef57fa6e71a38ba", "parent_id": 92, "url": "", "namespace": "langchain", "title": "langsmith-filter-traces-in-application", "headers": ["langsmith-filter-traces-in-application"], "section_index": 0, "chunk_index": 1, "text": "Investigate how to add a Python code execution node | generations.message.kwargs.tool_calls.args.steps | Find an example or create a sample implementation of a code execution node | generations.message.kwargs.tool_calls.id | toolu_01XexPzAVknT3gRmUB5PK5BP | generations.message.kwargs.tool_calls.type | tool_call | type | LLMResult | generations.message.kwargs.tool_calls.name = Plan This will match root and non-root runs where the tool_calls name is Plan . Negative filtering on key-value pairs Different types of negative filtering can be applied toMetadata , Input Key , and Output Key fields to exclude specific runs from your results. For example, to find all runs where the metadata key phone is not equal to 1234567890 , set the Metadata Key operator to is and Key field to phone , then set the Value operator to is not and the Value field to 1234567890 . This will match all runs that have a metadata key phone with any value except 1234567890 . To find runs that don\u2019t have a specific metadata key, set the Key operator to is not . For example, setting the Key operator to is not with phone as the key will match all runs that don\u2019t have a phone field in their metadata. You can also filter for runs that neither have a specific key nor a specific value. To find runs where the metadata has neither the key phone nor any field with the value 1234567890 , set the Key operator to is not with key phone , and the Value operator to is not with value 1234567890 . Finally, you can also filter for runs that do not have a specific key but have a specific value. To find runs where there is no phone key but there is a value of 1234567890 for some other key, set the Key operator to is not with key phone , and the Value operator to is with value 1234567890 . Note that you can use the does not contain operator instead of is not to perform a substring match. Save a filter Saving filters allows you to store and reuse frequently used filter configurations. Saved filters are specific to a tracing project.Save a filter In the filter box, click the Save filter button after you have constructed your filter. This will bring up a dialog to specify the name and a description of the filter.Use a saved filter After saving a filter, it is available in the filter bar as a quick filter for you to use. If you have more than three saved filters, only two will be displayed directly, with the rest accessible via a \u201cmore\u201d menu. You can use the settings icon in the saved filter bar to optionally hide default saved filters.Update a saved filter With the filter selected, make any changes to filter parameters. Then click Update filter > Update to update the filter. In the same menu, you can also create a new saved filter by clicking Update filter > Create new.Delete a saved filter Click the settings icon in the saved filter bar, and delete a filter using the trash icon.Copy a filter You can copy a constructed filter to share it with colleagues, reuse it later, or query runs programmatically in the API or SDK. In order to copy the filter, you can first create it in the UI. From there, you can click the copy button in the upper right hand corner. If you have constructed tree or trace filters, you can also copy those. This will give you a string representing the filter in the LangSmith query language. For example:and(eq(is_root, true), and(eq(feedback_key, \"user_score\"), eq(feedback_score, 1))) . For more information on the query language syntax, please refer to this reference. Filtering runs within the trace view You can also apply filters directly within the trace view, which is useful for sifting through traces with a large number of runs. The same filters available in the main runs table view can be applied here. By default, only the runs that match the filters will be shown. To see the matched runs within the broader context of the trace tree, switch the view option from \u201cFiltered Only\u201d to \u201cShow All\u201d or \u201cMost relevant\u201d.Manually specify a raw query in LangSmith query language If you have copied a previously constructed filter, you may want to manually apply this raw query in a future session. In order to do this, you can click on Advanced filters on the bottom of the filters popover. From there you can paste a raw query into the text box. Note that this will add that query to the existing queries, not overwrite it.Use an AI Query to auto-generate a query (Experimental) Sometimes figuring out the exact query to specify can be difficult! In order to make it easier, we\u2019ve added anAI Query functionality. With this, you can type in the filter you want to construct in natural language and it will convert it into a valid query. For example: \u201cAll runs longer than 10 seconds\u201d Advanced filters Filter for intermediate runs (spans) on properties of the root A common concept is to filter for intermediate runs which are part of a trace whose root run has some attribute. An example is filtering for intermediate runs of a particular type whose root run has positive (or negative) feedback associated with it. In order to do this, first set up a filter for intermediate runs (per the above section). After that, you can then add another filter rule. You can then click theAdvanced Filters link all the way at the bottom of the filter. This will open up a new modal where you can add Trace filters . These filters will apply to the traces of all the parent runs of the individual runs you\u2019ve already filtered for. Filter for runs (spans) whose child runs have some attribute This is the opposite of the above. You may want to search for runs who have specific types of sub runs. An example of this could be searching for all traces that had a sub run", "tokens": 1000, "node_type": "child"}
{"id": 110, "chunk_id": "5345104ec252a41359fa4a3d2dfbec23", "parent_id": 92, "url": "", "namespace": "langchain", "title": "langsmith-filter-traces-in-application", "headers": ["langsmith-filter-traces-in-application"], "section_index": 0, "chunk_index": 2, "text": "whose root run has some attribute. An example is filtering for intermediate runs of a particular type whose root run has positive (or negative) feedback associated with it. In order to do this, first set up a filter for intermediate runs (per the above section). After that, you can then add another filter rule. You can then click theAdvanced Filters link all the way at the bottom of the filter. This will open up a new modal where you can add Trace filters . These filters will apply to the traces of all the parent runs of the individual runs you\u2019ve already filtered for. Filter for runs (spans) whose child runs have some attribute This is the opposite of the above. You may want to search for runs who have specific types of sub runs. An example of this could be searching for all traces that had a sub run with nameFoo . This is useful when Foo is not always called, but you want to analyze the cases where it is. In order to do this, you can click on the Advanced Filters link all the way at the bottom of the filter. This will open up a new modal where you can add Tree filters . This will make the rule you specify apply to all child runs of the individual runs you\u2019ve already filtered for.", "tokens": 228, "node_type": "child"}
{"id": 111, "chunk_id": "c16acf8a553ac24f5d77e7d3a1c6e493", "parent_id": 93, "url": "", "namespace": "langchain", "title": "langsmith-generative-ui-react", "headers": ["langsmith-generative-ui-react"], "section_index": 0, "chunk_index": 0, "text": "# langsmith-generative-ui-react\n\n> Source: https://docs.langchain.com/langsmith/generative-ui-react\n\nTutorial\n1. Define and configure UI components\nFirst, create your first UI component. For each component you need to provide an unique identifier that will be used to reference the component in your graph code.src/agent/ui.tsx\nlanggraph.json\nconfiguration:\nui\nsection points to the UI components that will be used by graphs. By default, we recommend using the same key as the graph name, but you can split out the components however you like, see Customise the namespace of UI components for more details.\nLangSmith will automatically bundle your UI components code and styles and serve them as external assets that can be loaded by the LoadExternalComponent\ncomponent. Some dependencies such as react\nand react-dom\nwill be automatically excluded from the bundle.\nCSS and Tailwind 4.x is also supported out of the box, so you can freely use Tailwind classes as well as shadcn/ui\nin your UI components.\n- src/agent/ui.tsx\n- src/agent/styles.css\n2. Send the UI components in your graph\n- Python\n- JS\nsrc/agent.py\n3. Handle UI elements in your React application\nOn the client side, you can useuseStream()\nand LoadExternalComponent\nto display the UI elements.\nsrc/app/page.tsx\nLoadExternalComponent\nwill fetch the JS and CSS for the UI components from LangSmith and render them in a shadow DOM, thus ensuring style isolation from the rest of your application.\nHow-to guides\nProvide custom components on the client side\nIf you already have the components loaded in your client application, you can provide a map of such components to be rendered directly without fetching the UI code from LangSmith.Show loading UI when components are loading\nYou can provide a fallback UI to be rendered when the components are loading.Customise the namespace of UI components.\nBy defaultLoadExternalComponent\nwill use the assistantId\nfrom useStream()\nhook to fetch the code for UI components. You can customise this by providing a namespace\nprop to the LoadExternalComponent\ncomponent.\n- src/app/page.tsx\n- langgraph.json\nAccess and interact with the thread state from the UI component\nYou can access the thread state inside the UI component by using theuseStreamContext\nhook.\nPass additional context to the client components\nYou can pass additional context to the client components by providing ameta\nprop to the LoadExternalComponent\ncomponent.\nmeta\nprop in the UI component by using the useStreamContext\nhook.\nStreaming UI messages from the server\nYou can stream UI messages before the node execution is finished by using theonCustomEvent\ncallback of the useStream()\nhook. This is especially useful when updating the UI component as the LLM is generating the response.\nui.push()\n/ push_ui_message()\nwith the same ID as the UI message you wish to update.\n- Python\n- JS\n- ui.tsx\nRemove UI messages from state\nSimilar to how messages can be removed from the state by appending a RemoveMessage you can remove an UI message from the state by callingremove_ui_message\n/ ui.delete\nwith the ID of the UI message.\n- Python\n- JS", "tokens": 483, "node_type": "child"}
{"id": 112, "chunk_id": "c16acf8a553ac24f5d77e7d3a1c6e493", "parent_id": 94, "url": "", "namespace": "langchain", "title": "langsmith-graph-rebuild", "headers": ["langsmith-graph-rebuild"], "section_index": 0, "chunk_index": 0, "text": "# langsmith-graph-rebuild\n\n> Source: https://docs.langchain.com/langsmith/graph-rebuild\n\nNote\nIn most cases, customizing behavior based on the config should be handled by a single graph where each node can read a config and change its behavior based on it\nPrerequisites\nMake sure to check out this how-to guide on setting up your app for deployment first.Define graphs\nLet\u2019s say you have an app with a simple graph that calls an LLM and returns the response to the user. The app file directory looks like the following:openai_agent.py\n.\nNo rebuild\nIn the standard LangGraph API configuration, the server uses the compiled graph instance that\u2019s defined at the top level ofopenai_agent.py\n, which looks like the following:\nCompiledStateGraph\ninstance in your LangGraph API configuration (langgraph.json\n), e.g.:\nRebuild\nTo make your graph rebuild on each new run with custom configuration, you need to rewriteopenai_agent.py\nto instead provide a function that takes a config and returns a graph (or compiled graph) instance. Let\u2019s say we want to return our existing graph for user ID \u20181\u2019, and a tool-calling agent for other users. We can modify openai_agent.py\nas follows:\nmake_graph\n) in langgraph.json\n:", "tokens": 187, "node_type": "child"}
{"id": 113, "chunk_id": "c16acf8a553ac24f5d77e7d3a1c6e493", "parent_id": 95, "url": "", "namespace": "langchain", "title": "langsmith-home", "headers": ["langsmith-home"], "section_index": 0, "chunk_index": 0, "text": "# langsmith-home\n\n> Source: https://docs.langchain.com/langsmith/home\n\nlangchain\nand langgraph\n.\nPrototype locally, then move to production with integrated monitoring and evaluation to build more reliable AI systems.\nGet started\nObservability\nGain visibility into every step your application takes to debug faster and improve reliability.\nEvaluation\nMeasure and track quality over time to ensure your AI applications are consistent and trustworthy.\nDeployment\nDeploy your agents as LangGraph Servers, ready to scale in production.\nPrompt Testing\nIterate on prompts with built-in versioning and collaboration to ship improvements faster.\nStudio\nUse a visual interface to design, test, and refine applications end-to-end.\nHosting\nHost LangSmith in the cloud, in your environment, or hybrid to match your infrastructure and compliance needs.", "tokens": 115, "node_type": "child"}
{"id": 114, "chunk_id": "c16acf8a553ac24f5d77e7d3a1c6e493", "parent_id": 96, "url": "", "namespace": "langchain", "title": "langsmith-hosting", "headers": ["langsmith-hosting"], "section_index": 0, "chunk_index": 0, "text": "# langsmith-hosting\n\n> Source: https://docs.langchain.com/langsmith/hosting\n\nChoose your hosting option\nYou\u2019ll deploy LangSmith in one of three modes:- Cloud: fully managed by LangChain\n- Hybrid: LangChain manages the ; you host the\n- Self-hosted: you manage the full stack within your infrastructure\nCloud\nRun all components fully managed in LangChain\u2019s cloud.\nHybrid\n(Enterprise) Manage the data plane running in your cloud while LangChain manages the control plane.\nSelf-hosted\n(Enterprise) Run the full LangSmith or run standalone LangGraph Servers without the control plane UI.\nComparison\nRefer to the following table for a comparison of hosting options:| Feature | Cloud | Hybrid | Self-Hosted |\n|---|---|---|---|\n| Infrastructure location | LangChain\u2019s cloud | Split: Control plane in LangChain cloud, data plane in your cloud | Your cloud |\n| Who manages updates | LangChain | LangChain (control plane), You (data plane) | You |\n| Who manages CI/CD for your apps | LangChain | You | You |\n| Can deploy applications? | \u2705 Yes | \u2705 Yes | \u2705 Yes (with full platform option) |\n| Observability data location | LangChain cloud | LangChain cloud | Your cloud |\n| Pricing | Plus tier | Enterprise | Enterprise |\n| Best for | Quick setup, managed infrastructure | Data residency requirements + managed control plane | Full control, data isolation |", "tokens": 219, "node_type": "child"}
{"id": 115, "chunk_id": "c16acf8a553ac24f5d77e7d3a1c6e493", "parent_id": 97, "url": "", "namespace": "langchain", "title": "langsmith-human-in-the-loop-time-travel", "headers": ["langsmith-human-in-the-loop-time-travel"], "section_index": 0, "chunk_index": 0, "text": "# langsmith-human-in-the-loop-time-travel\n\n> Source: https://docs.langchain.com/langsmith/human-in-the-loop-time-travel\n\n- Run the graph with initial inputs using LangGraph SDK\u2019s client.runs.wait or client.runs.stream APIs.\n- Identify a checkpoint in an existing thread: Use client.threads.get_history method to retrieve the execution history for a specific\nthread_id\nand locate the desiredcheckpoint_id\n. Alternatively, set a breakpoint before the node(s) where you want execution to pause. You can then find the most recent checkpoint recorded up to that breakpoint. - (Optional) modify the graph state: Use the client.threads.update_state method to modify the graph\u2019s state at the checkpoint and resume execution from alternative state.\n- Resume execution from the checkpoint: Use the client.runs.wait or client.runs.stream APIs with an input of\nNone\nand the appropriatethread_id\nandcheckpoint_id\n.\nUse time travel in a workflow\nExample graph\nExample graph\n1. Run the graph\n- Python\n- JavaScript\n- cURL\n2. Identify a checkpoint\n- Python\n- JavaScript\n- cURL\n3. Update the state\nupdate_state\nwill create a new checkpoint. The new checkpoint will be associated with the same thread, but a new checkpoint ID.\n- Python\n- JavaScript\n- cURL\n4. Resume execution from the checkpoint\n- Python\n- JavaScript\n- cURL\nLearn more\n- LangGraph time travel guide: learn more about using time travel in LangGraph.", "tokens": 204, "node_type": "child"}
{"id": 116, "chunk_id": "c16acf8a553ac24f5d77e7d3a1c6e493", "parent_id": 98, "url": "", "namespace": "langchain", "title": "langsmith-hybrid", "headers": ["langsmith-hybrid"], "section_index": 0, "chunk_index": 0, "text": "# langsmith-hybrid\n\n> Source: https://docs.langchain.com/langsmith/hybrid\n\n- Control plane (LangSmith UI, APIs, and orchestration) runs in LangChain\u2019s cloud, managed by LangChain.\n- Data plane (your and agent workloads) runs in your cloud, managed by you.\n| Component | Responsibilities | Where it runs | Who manages it |\n|---|---|---|---|\n| LangChain\u2019s cloud | LangChain | |\n| Your cloud | You |\nWorkflow\n- Use the\nlanggraph-cli\nor Studio to test your graph locally. - Build a Docker image using the\nlanggraph build\ncommand. - Deploy your LangGraph Server from the control plane UI.\nArchitecture\nCompute Platforms\n- Kubernetes: Hybrid supports running the data plane on any Kubernetes cluster.\nEgress to LangSmith and the control plane\nIn the hybrid deployment model, your self-hosted data plane will send network requests to the control plane to poll for changes that need to be implemented in the data plane. Traces from data plane deployments also get sent to the LangSmith instance integrated with the control plane. This traffic to the control plane is encrypted, over HTTPS. The data plane authenticates with the control plane with a LangSmith API key. In order to enable this egress, you may need to update internal firewall rules or cloud resources (such as Security Groups) to allow certain IP addresses.AWS/Azure PrivateLink or GCP Private Service Connect is currently not supported. This traffic will go over the internet.\nListeners\nIn the hybrid option, one or more \u201clistener\u201d applications can run depending on how your LangSmith workspaces and Kubernetes clusters are organized.Kubernetes cluster organization\n- One or more listeners can run in a Kubernetes cluster.\n- A listener can deploy into one or more namespaces in that cluster.\n- Cluster owners are responsible for planning listener layout and LangGraph Server deployments.\nLangSmith workspace organization\n- A workspace can be associated with one or more listeners.\n- A workspace can only deploy to Kubernetes clusters where all of its listeners are deployed.\nUse Cases\nHere are some common listener configurations (not strict requirements):Each LangSmith workspace \u2192 separate Kubernetes cluster\n- Cluster\nalpha\nruns workspaceA\n- Cluster\nbeta\nruns workspaceB\nSeparate clusters, with shared \u201cdev\u201d cluster\n- Cluster\nalpha\nruns workspaceA\n- Cluster\nbeta\nruns workspaceB\n- Cluster\ndev\nruns workspacesA\nandB\n- Both workspaces have two listeners; cluster\ndev\nhas two listener deployments\nOne cluster, one namespace per workspace\n- Cluster\nalpha\n, namespace1\nruns workspaceA\n- Cluster\nalpha\n, namespace2\nruns workspaceB\nOne cluster, single namespace for multiple workspaces\n- Cluster\nalpha\nruns workspaceA\n- Cluster\nalpha\nruns workspaceB", "tokens": 418, "node_type": "child"}
{"id": 117, "chunk_id": "c16acf8a553ac24f5d77e7d3a1c6e493", "parent_id": 99, "url": "", "namespace": "langchain", "title": "langsmith-improve-judge-evaluator-feedback", "headers": ["langsmith-improve-judge-evaluator-feedback"], "section_index": 0, "chunk_index": 0, "text": "# langsmith-improve-judge-evaluator-feedback\n\n> Source: https://docs.langchain.com/langsmith/improve-judge-evaluator-feedback\n\nReliable LLM-as-a-judge evaluators are critical for making informed decisions about your AI applications (e.g., prompt, model, architecture changes). Defining the evaluator prompt correctly can be difficult, but it directly affects the trustworthiness of your evaluations.\nThis guide describes how to align your LLM-as-a-judge evaluator using human feedback to improve your evaluator\u2019s quality and help you build reliable AI applications.\nHow it works\nLangSmith\u2019s Align Evaluator feature has a series of steps that help you align your LLM-as-a-judge evaluator with human expert feedback. You can use this feature to align evaluators that run on a dataset for offline evaluations or for online evaluations. In either case, the steps are similar:- Select experiments or runs that contain outputs from your application.\n- Add the selected experiments or runs to an annotation queue where a human expert can label the data.\n- Test your LLM-as-a-judge evaluator prompt against the labeled examples. Check the cases where your evaluator result is not aligned with the labeled data. This indicates areas where your evaluator prompt needs improvement.\n- Refine and repeat to improve evaluator alignment. Update your LLM-as-a-judge evaluator prompt and test again.\nPrerequisites\nYou\u2019ll need the following before starting this guide for offline evaluations or online evaluations:Offline evaluations\n- A dataset with at least one experiment.\n- You\u2019ll need to upload or create datasets via the SDK or the UI and run an experiment via the SDK or the Playground.\nOnline evaluations\n- An application that\u2019s already sending traces to LangSmith.\n- Configure this with one of the tracing integrations to start.\nGetting started\nYou can enter the alignment flow for both new and existing evaluators in datasets and tracing projects.| Dataset Evaluators | Tracing Project Evaluators | |\n|---|---|---|\n| Create an aligned evaluator from scratch | 1. Datasets & Experiments and select your dataset 2. Click + Evaluator > Create from labeled data 3. Enter a descriptive feedback key name (e.g. correctness , hallucination ) | 1. Projects and select your project 2. Click + New > Evaluator > Create from labeled data 3. Enter a descriptive feedback\u2011key name (e.g. correctness , hallucination ) |\n| Align an existing evaluator | 1. Datasets & Experiments > select your dataset > Evaluators tab 2. In the Align Evaluator with experiment data box, click Select Experiments | 1. Projects > select your project > Evaluators tab 2. In the Align Evaluator with experiment data box, click Select Experiments |\n1. Select experiments or runs\nSelect one or more experiments (or runs) to send for human labeling. This will add runs to an annotation queue. To add any new experiments/runs to an existing annotation queue, head to the Evaluators tab, select the evaluator you are aligning and click Add to Queue.Datasets should be representative of inputs and outputs you expect to see in production.While you don\u2019t need to cover every possible scenario, it\u2019s important to include examples across the full range of expected use cases. For example, if you\u2019re building a sports bot that answers questions about baseball, basketball, and football, your dataset should include at least one labeled example from each sport.\n2. Label examples\nLabel examples in the annotation queue by adding a feedback score. Once you\u2019ve labeled an example, click Add to Reference Dataset.If you have a large number of examples in your experiments, you don\u2019t need to label every example to get started. We recommend starting with at least 20 examples, you can always add more later. We recommend that the examples that you label are diverse (balanced in both 0 and 1 labels) to ensure that you\u2019re building a well rounded evaluator prompt.\n3. Test your evaluator prompt against the labeled examples\nOnce you have labeled examples, the next step is iterating on your evaluator prompt to mimic the labeled data as well as possible. This iteration is done in the Evaluator Playground. To go to the evaluator playground: Click the View evaluator button on the top right of the evaluator queue. This will take you to the detail page of the evaluator you are aligning. Click the Evaluator Playground button to access the playground. In the evaluator playground you can create or edit your evaluator prompt and click Start Alignment to run it over the set of labeled examples that you created in Step 2. After running your evaluator, you\u2019ll see how its generated scores compare to your human labels. The alignment score is the percentage of examples where the evaluator\u2019s judgment matches that of the human expert.4. Repeat to improve evaluator alignment\nIterate by updating your prompt and testing again to improve evaluator alignment.Updates to your evaluator prompt are not saved by default. We reccomend saving your evaluator prompt regularly, and especially after you see your alignment score improve.The evaluator playground will show the alignment score for the most recently saved version of your evaluator prompt for comparison when you\u2019re iterating on your prompt.", "tokens": 820, "node_type": "child"}
{"id": 118, "chunk_id": "c16acf8a553ac24f5d77e7d3a1c6e493", "parent_id": 100, "url": "", "namespace": "langchain", "title": "langsmith-index-datasets-for-dynamic-few-shot-example-selection", "headers": ["langsmith-index-datasets-for-dynamic-few-shot-example-selection"], "section_index": 0, "chunk_index": 0, "text": "# langsmith-index-datasets-for-dynamic-few-shot-example-selection\n\n> Source: https://docs.langchain.com/langsmith/index-datasets-for-dynamic-few-shot-example-selection\n\nConfigure your datasets so that you can search for few shot examples based on an incoming request.\nPre-conditions\n- Your dataset must use the KV store data type (we do not currently support chat model or LLM type datasets)\n- You must have an input schema defined for your dataset. See our docs on setting up schema validation in our UI for details.\n- You must be on a paid team plan (e.g. Plus plan)\n- You must be on LangSmith cloud\nIndex your dataset for few shot search\nNavigate to the datasets UI, and click the newFew-Shot search\ntab. Hit the Start sync\nbutton, which will create a new index on your dataset to make it searchable.\nBy default, we sync to the latest version of your dataset. That means when new examples are added to your dataset, they will automatically be added to your index. This process runs every few minutes, so there should be a very short delay for indexing new examples. You can see whether your index is up to date under Few-shot index\non the lefthand side of the screen in the next section.\nTest search quality in the few shot playground\nNow that you have turned on indexing for your dataset, you will see the new few shot playground. You can type in a sample input, and check which results would be returned by our search API. Each result will have a score and a link to the example in the dataset. The scoring system works such that 0 is a completely random result, and higher scores are better. Results will be sorted in descending order according to score.Search uses a BM25-like algorithm for keyword based similarity scores. The actual score is subject to change as we improve the search algorithm, so we recommend not relying on the scores themselves, as their meaning may evolve over time. They are simply used for convenience in vibe-testing outputs in the playground.\nAdding few shot search to your application\nClick theGet Code Snippet\nbutton in the previous diagram, you\u2019ll be taken to a screen that has code snippets from our LangSmith SDK in different languages.\nFor code samples on using few shot search in LangChain python applications, please see our how-to guide in the LangChain docs.\nCode snippets\nPlease ensure you are using the python SDK with version >= 1.101 or the typescript SDK with version >= 1.43", "tokens": 406, "node_type": "child"}
{"id": 119, "chunk_id": "c16acf8a553ac24f5d77e7d3a1c6e493", "parent_id": 101, "url": "", "namespace": "langchain", "title": "langsmith-insights", "headers": ["langsmith-insights"], "section_index": 0, "chunk_index": 0, "text": "# langsmith-insights\n\n> Source: https://docs.langchain.com/langsmith/insights\n\nPrerequisites\nAn OpenAI API key \u2014 generate one from the OpenAI dashboard.Run your first Insights job\nFrom the LangSmith UI:- Navigate to Tracing Projects in the left-hand menu and select a tracing project.\n- Click +New in the top right corner then New Insights Job to kick off a new Insights job.\n- Enter a name for your job.\n- Click the icon in the top right of the job creation pane to set your OpenAI API key as a workspace secret. If your workspace already has an OpenAI API key set, you can skip this step.\n- Click Create.\nGenerating insights over 1,000 runs typically costs $0.50-$1.00 in OpenAI API calls. The cost grows linearly in the number of runs and the size of each run.\nUnderstand the results\nOnce your job has completed, you can navigate to the Insights tab where you\u2019ll see a table of Insights jobs. Each job contains insights generated over a specific sample of runs from the tracing project.Insights jobs for a single tracing project\nCommon topics of conversations with the https://chat.langchain.com chatbot\nTop-level categories\nYour traces are automatically grouped into top-level categories that represent the broadest patterns in your data. The distribution bars show how frequently each pattern occurs, making it easy to spot behaviors that happen more or less than expected. Each category has a brief description and displays aggregated metrics over the traces it contains, including:- Typical runs stats (like error rates, latency, cost)\n- Feedback scores from your evaluators\n- Attributes extracted as part of the job\nSubcategories\nClicking on any category shows a breakdown into subcategories, which gives you a more granular understanding of interaction patterns in that category of traces. In the Chat Langchain example pictured above, under \u201cData & Retrieval\u201d there are subcategories like \u201cVector Stores\u201d and \u201cData Ingestion\u201d.Individual traces\nYou can view the traces assigned to each category or subcategory by clicking through to see the runs table. From there, you can click into any trace to see the full conversation details.Configure a job\nWhen kicking off an Insights job, you can configure the following:Select runs\n- Sample size: The maximum number of traces to analyze. Currently capped at 1,000\n- Time range: Traces are sampled from this time range\n- Filters: Additional run filters. As you adjust filters, you\u2019ll see how many traces match your criteria\nCategories\nBy default, top-level categories are automatically generated bottom-up from the underlying traces. In some instances, you know specific categories you\u2019re interested in upfront and want the job to bucket traces into those predefined categories. The Categories section of the config lets you do this by enumerating the names and descriptions of the top-level categories you want to be used. Subcategories are still auto-generated by the algorithm within the predefined top-level categories.Summary prompt\nThe first step of the job is to create a brief summary of every trace \u2014 it is these summaries that are then categorized. Extracting the right information in the summary is essential for getting useful categories. The prompt used to generate these summaries can be edited. The two things to think about when editing the prompt are:- Summarization instructions: Any information that isn\u2019t in the trace summary won\u2019t affect the categories that get generated, so make sure to provide clear instructions on what information is important to extract from each trace.\n- Trace content: Use mustache formatting to specify which parts of each trace are passed to the summarizer. Large traces with lots of inputs and outputs can be expensive and noisy. Reducing the prompt to only include the most relevant parts of the trace can improve your results.\n{{run.inputs}}\nand the outputs via {{run.outputs}}\n. For example, the prompt \"Summarize this: {{run.inputs}}\"\nwill include (a JSON serialization of) all of the run inputs. The prompt \"Summarize this: {{run.inputs.foo.bar}}\"\nwill include only the \u201cbar\u201d value within the \u201cfoo\u201d value of the run inputs.\nAttributes\nAlong with a summary, you can define additional categorical, numerical, and boolean attributes to be extracted from each trace. These attributes will influence the categorization step \u2014 traces with similar attribute values will tend to be categorized together. You can also see aggregations of these attributes per category. As an example, you might want to extract the attributeuser_satisfied: boolean\nfrom each trace to steer the algorithm towards categories that split up positive and negative user experiences, and to see the average user satisfaction per category.", "tokens": 739, "node_type": "child"}
{"id": 120, "chunk_id": "c16acf8a553ac24f5d77e7d3a1c6e493", "parent_id": 102, "url": "", "namespace": "langchain", "title": "langsmith-interrupt-concurrent", "headers": ["langsmith-interrupt-concurrent"], "section_index": 0, "chunk_index": 0, "text": "# langsmith-interrupt-concurrent\n\n> Source: https://docs.langchain.com/langsmith/interrupt-concurrent\n\nThis guide assumes knowledge of what double-texting is, which you can learn about in the double-texting conceptual guide.The guide covers the interrupt option for double texting, which interrupts the prior run of the graph and starts a new one with the double-text. This option does not delete the first run, but rather keeps it in the database but sets its status to interrupted. Below is a quick example of using the interrupt option.\nNow, let\u2019s import our required packages and instantiate our client, assistant, and thread.\nPython\nJavascript\nCURL\nCopy\nimport asynciofrom langchain_core.messages import convert_to_messagesfrom langgraph_sdk import get_clientclient = get_client(url=<DEPLOYMENT_URL>)# Using the graph deployed with the name \"agent\"assistant_id = \"agent\"thread = await client.threads.create()\nNow we can start our two runs and join the second one until it has completed:\nPython\nJavascript\nCURL\nCopy\n# the first run will be interruptedinterrupted_run = await client.runs.create( thread[\"thread_id\"], assistant_id, input={\"messages\": [{\"role\": \"user\", \"content\": \"what's the weather in sf?\"}]},)# sleep a bit to get partial outputs from the first runawait asyncio.sleep(2)run = await client.runs.create( thread[\"thread_id\"], assistant_id, input={\"messages\": [{\"role\": \"user\", \"content\": \"what's the weather in nyc?\"}]}, multitask_strategy=\"interrupt\",)# wait until the second run completesawait client.runs.join(thread[\"thread_id\"], run[\"run_id\"])\nWe can see that the thread has partial data from the first run + data from the second run\nPython\nJavascript\nCURL\nCopy\nstate = await client.threads.get_state(thread[\"thread_id\"])for m in convert_to_messages(state[\"values\"][\"messages\"]): m.pretty_print()\nOutput:\nCopy\n================================ Human Message =================================what's the weather in sf?================================== Ai Message ==================================[{'id': 'toolu_01MjNtVJwEcpujRGrf3x6Pih', 'input': {'query': 'weather in san francisco'}, 'name': 'tavily_search_results_json', 'type': 'tool_use'}]Tool Calls:tavily_search_results_json (toolu_01MjNtVJwEcpujRGrf3x6Pih)Call ID: toolu_01MjNtVJwEcpujRGrf3x6PihArgs:query: weather in san francisco================================= Tool Message =================================Name: tavily_search_results_json[{\"url\": \"https://www.wunderground.com/hourly/us/ca/san-francisco/KCASANFR2002/date/2024-6-18\", \"content\": \"High 64F. Winds W at 10 to 20 mph. A few clouds from time to time. Low 49F. Winds W at 10 to 20 mph. Temp. San Francisco Weather Forecasts. Weather Underground provides local & long-range weather ...\"}]================================ Human Message =================================what's the weather in nyc?================================== Ai Message ==================================[{'id': 'toolu_01KtE1m1ifPLQAx4fQLyZL9Q', 'input': {'query': 'weather in new york city'}, 'name': 'tavily_search_results_json', 'type': 'tool_use'}]Tool Calls:tavily_search_results_json (toolu_01KtE1m1ifPLQAx4fQLyZL9Q)Call ID: toolu_01KtE1m1ifPLQAx4fQLyZL9QArgs:query: weather in new york city================================= Tool Message =================================Name: tavily_search_results_json[{\"url\": \"https://www.accuweather.com/en/us/new-york/10021/june-weather/349727\", \"content\": \"Get the monthly weather forecast for New York, NY, including daily high/low, historical averages, to help you plan ahead.\"}]================================== Ai Message ==================================The search results provide weather forecasts and information for New York City. Based on the top result from AccuWeather, here are some key details about the weather in NYC:* This is a monthly weather forecast for New York City for the month of June.* It includes daily high and low temperatures to help plan ahead.* Historical averages for June in NYC are also provided as a reference point.* More detailed daily or hourly forecasts with precipitation chances, humidity, wind, etc. can be found by visiting the AccuWeather page.So in summary, the search provides a convenient overview of the expected weather conditions in New York City over the next month to give you an idea of what to prepare for if traveling or making plans there. Let me know if you need any other details!\nVerify that the original, interrupted run was interrupted", "tokens": 501, "node_type": "child"}
{"id": 121, "chunk_id": "c16acf8a553ac24f5d77e7d3a1c6e493", "parent_id": 103, "url": "", "namespace": "langchain", "title": "langsmith-kubernetes", "headers": ["langsmith-kubernetes"], "section_index": 0, "chunk_index": 0, "text": "# langsmith-kubernetes > Source: https://docs.langchain.com/langsmith/kubernetes Self-hosting LangSmith is an add-on to the Enterprise Plan designed for our largest, most security-conscious customers. See our pricing page for more detail, and contact our sales team if you want to get a license key to trial LangSmith in your environment. This guide installs the base LangSmith platform which includes observability and evaluation, but not the deployment management features. Review the self-hosted options if you\u2019re unsure which you need. - \u2705 LangSmith UI and APIs: for observability, tracing, and evaluation. - \u2705 Backend services: (queue, playground, ACE). - \u2705 Datastores: (PostgreSQL, Redis, ClickHouse, optional blob storage). - \u274c Deployment management: To add deployment capabilities, complete this guide first, then follow Self-host LangSmith with deployment. - Google Kubernetes Engine (GKE) - Amazon Elastic Kubernetes Service (EKS) - Azure Kubernetes Service (AKS) - OpenShift (4.14+) - Minikube and Kind (for development purposes) We have several Terraform modules the help in the provisioning of resources for LangSmith. You can find those in our public Terraform repo.Supported cloud providers include:You can click on the links above to see the documentation for each module. These modules are designed to help you quickly set up the necessary infrastructure for LangSmith, including Kubernetes clusters, storage, and networking. Prerequisites Ensure you have the following tools/items ready. Some items are marked optional:- LangSmith License Key - You can get this from your LangChain representative. Contact our sales team for more information. - Api Key Salt - This is a secret key that you can generate. It should be a random string of characters. - You can generate this using the following command: - JWT Secret (Optional but used for basic auth) - This is a secret key that you can generate. It should be a random string of characters. - You can generate this using the following command: Databases LangSmith uses a PostgreSQL database, a Redis cache, and a ClickHouse database to store traces. By default, these services are installed inside your Kubernetes cluster. However, we highly recommend using external databases instead. For PostgreSQL and Redis, the best option is your cloud provider\u2019s managed services. For more information, refer to the following setup guides for external services:Kubernetes cluster requirements - You will need a working Kubernetes cluster that you can access via kubectl . Your cluster should have the following minimum requirements:- Recommended: At least 16 vCPUs, 64GB Memory available - You may need to tune resource requests/limits for all of our different services based off of organization size/usage. Our recommendations can be found here. - We recommend using a cluster autoscaler to handle scaling up/down of nodes based on resource usage. - We recommend setting up the metrics server so that autoscaling can be turned on. - If you are running Clickhouse in-cluster, you must have a node with at least 4 vCPUs and 16GB of memory allocatable as ClickHouse will request this amount of resources by default. - Valid Dynamic PV provisioner or PVs available on your cluster (required only if you are running databases in-cluster) - To enable persistence, we will try to provision volumes for any database running in-cluster. - If using PVs in your cluster, we highly recommend setting up backups in a production environment. - We strongly encourage using a storage class backed by SSDs for better performance. We recommend 7000 IOPS and 1000 MiB/s throughput. - On EKS, you may need to ensure you have the ebs-csi-driver installed and configured for dynamic provisioning. Refer to the EBS CSI Driver documentation for more information. The output should show at least one storage class with a provisioner that supports dynamic provisioning. For example:Refer to the Kubernetes documentation for more information on storage classes.We highly recommend using a storage class that supports volume expansion. This is because traces can potentially require a lot of disk space and your volumes may need to be resized over time. - Recommended: At least 16 vCPUs, 64GB Memory available - Helm - To install helm refer to the Helm documentation - To install - Egress to https://beacon.langchain.com (if not running in offline mode)- LangSmith requires egress to https://beacon.langchain.com for license verification and usage reporting. This is required for LangSmith to function properly. You can find more information on egress requirements in the Egress section. - LangSmith requires egress to Configure your Helm Charts: - Create a new file called langsmith_config.yaml with the configuration options from the previous step.- There are several configuration options that you can set in the langsmith_config.yaml file. You can find more information on specific configuration options in the Configuration section. - If you are new to Kubernetes or Helm, we\u2019d recommend starting with one of the example configurations in the examples directory of the Helm Chart repository here: LangSmith helm chart examples. - You can see a full list of configuration options in the values.yaml file in the Helm Chart repository here: LangSmith Helm Chart - There are several configuration options that you can set in the - At a minimum, you will need to set the following configuration options (using basic auth): Deploying to Kubernetes: - Verify that you can connect to your Kubernetes cluster(note: We highly suggest installing into an empty namespace) - Run kubectl get pods Output should look something like: If you are using a namespace other than the default namespace, you will need to specify the namespace in thehelm andkubectl commands by using the-n <namespace> flag. - Run - Ensure you have the LangChain Helm repo added. (skip this step if you are using local charts) - Find the latest version of the chart. You can find the available versions in the Helm Chart repository. - We generally recommend using the latest version. - You can also run helm search repo langchain/langsmith --versions to see the available versions. The output will look something like this: - Run helm upgrade -i langsmith langchain/langsmith --values langsmith_config.yaml --version <version> -n <namespace> --wait --debug - Replace <namespace> with the namespace you want to deploy LangSmith to. - Replace", "tokens": 1000, "node_type": "child"}
{"id": 122, "chunk_id": "9b0fd3b850698317bef57fa6e71a38ba", "parent_id": 103, "url": "", "namespace": "langchain", "title": "langsmith-kubernetes", "headers": ["langsmith-kubernetes"], "section_index": 0, "chunk_index": 1, "text": "you can connect to your Kubernetes cluster(note: We highly suggest installing into an empty namespace) - Run kubectl get pods Output should look something like: If you are using a namespace other than the default namespace, you will need to specify the namespace in thehelm andkubectl commands by using the-n <namespace> flag. - Run - Ensure you have the LangChain Helm repo added. (skip this step if you are using local charts) - Find the latest version of the chart. You can find the available versions in the Helm Chart repository. - We generally recommend using the latest version. - You can also run helm search repo langchain/langsmith --versions to see the available versions. The output will look something like this: - Run helm upgrade -i langsmith langchain/langsmith --values langsmith_config.yaml --version <version> -n <namespace> --wait --debug - Replace <namespace> with the namespace you want to deploy LangSmith to. - Replace <version> with the version of LangSmith you want to install from the previous step. Most users should install the latest version available. helm install command runs and finishes successfully, you should see output similar to this:This may take a few minutes to complete as it will create several Kubernetes resources and run several jobs to initialize the database and other services. - Replace - Run kubectl get pods Output should now look something like this (note the exact pod names may vary based on the version and configuration you used): Validate your deployment: - Run kubectl get services Output should look something like: - Curl the external ip of the langsmith-frontend service:Expected output: - Visit the external ip for the langsmith-frontend service on your browser The LangSmith UI should be visible/operational Using LangSmith Now that LangSmith is running, you can start using it to trace your code. You can find more information on how to use self-hosted LangSmith in the self-hosted usage guide. Your LangSmith instance is now running but may not be fully setup yet. If you used one of the basic configs, you will have a default admin user account created for you. You can log in with the email address and password you specified in thelangsmith_config.yaml file. As a next step, it is strongly recommended you work with your infrastructure administrators to: - Setup DNS for your LangSmith instance to enable easier access - Configure SSL to ensure in-transit encryption of traces submitted to LangSmith - Configure LangSmith with Single Sign-On to secure your LangSmith instance - Connect LangSmith to external Postgres and Redis instances - Set up Blob Storage for storing large files", "tokens": 427, "node_type": "child"}
{"id": 123, "chunk_id": "c16acf8a553ac24f5d77e7d3a1c6e493", "parent_id": 104, "url": "", "namespace": "langchain", "title": "langsmith-langchain-runnable", "headers": ["langsmith-langchain-runnable"], "section_index": 0, "chunk_index": 0, "text": "# langsmith-langchain-runnable\n\n> Source: https://docs.langchain.com/langsmith/langchain-runnable\n\nLet\u2019s define a simple chain to evaluate. First, install all the required packages:\nCopy\npip install -U langsmith langchain[openai]\nNow define a chain:\nCopy\nfrom langchain.chat_models import init_chat_modelfrom langchain_core.prompts import ChatPromptTemplatefrom langchain_core.output_parsers import StrOutputParserinstructions = ( \"Please review the user query below and determine if it contains any form \" \"of toxic behavior, such as insults, threats, or highly negative comments. \" \"Respond with 'Toxic' if it does, and 'Not toxic' if it doesn't.\")prompt = ChatPromptTemplate( [(\"system\", instructions), (\"user\", \"{text}\")],)llm = init_chat_model(\"gpt-4o\")chain = prompt | llm | StrOutputParser()\nTo evaluate our chain we can pass it directly to the evaluate() / aevaluate() method. Note that the input variables of the chain must match the keys of the example inputs. In this case, the example inputs should have the form {\"text\": \"...\"}.\nCopy\nfrom langsmith import aevaluate, Clientclient = Client()# Clone a dataset of texts with toxicity labels.# Each example input has a \"text\" key and each output has a \"label\" key.dataset = client.clone_public_dataset( \"https://smith.langchain.com/public/3d6831e6-1680-4c88-94df-618c8e01fc55/d\")def correct(outputs: dict, reference_outputs: dict) -> bool: # Since our chain outputs a string not a dict, this string # gets stored under the default \"output\" key in the outputs dict: actual = outputs[\"output\"] expected = reference_outputs[\"label\"] return actual == expectedresults = await aevaluate( chain, data=dataset, evaluators=[correct], experiment_prefix=\"gpt-4o, baseline\",)\nThe runnable is traced appropriately for each output.", "tokens": 225, "node_type": "child"}
{"id": 124, "chunk_id": "c16acf8a553ac24f5d77e7d3a1c6e493", "parent_id": 105, "url": "", "namespace": "langchain", "title": "langsmith-langgraph-server-changelog", "headers": ["langsmith-langgraph-server-changelog"], "section_index": 0, "chunk_index": 0, "text": "# langsmith-langgraph-server-changelog\n\n> Source: https://docs.langchain.com/langsmith/langgraph-server-changelog\n\nLangGraph Server is an API platform for creating and managing agent-based applications. It provides built-in persistence, a task queue, and supports deploying, configuring, and running assistants (agentic workflows) at scale. This changelog documents all notable updates, features, and fixes to LangGraph Server releases.\nResolved a timezone issue in the core API, ensuring accurate time data retrieval.\nIntroduced a new middleware_order setting to apply authentication middleware before custom middleware, allowing finer control over protected route configurations.\nLogged the Redis URL when errors occur during Redis client creation.\nImproved Go engine/runtime context propagation to ensure consistent execution flow.\nRemoved the unnecessary assistants.put call from the executor entrypoint to streamline the process.\nAdded support for context when using stream_mode=\"events\" and included new tests for this functionality.\nAdded support for overriding the server port using $LANGGRAPH_SERVER_PORT and removed an unnecessary Dockerfile ARG for cleaner configuration.\nApplied authorization filters to all table references in thread delete CTE to enhance security.\nIntroduced self-hosted metrics ingestion capability, allowing metrics to be sent to an OTLP collector every minute when the corresponding environment variables are set.\nEnsured that the set_latest function properly updates the name and description of the version.\nAdded a format parameter to the queue metrics server for enhanced customization.\nCorrected MOUNT_PREFIX environment variable usage in CLI for consistency with documentation and to prevent confusion.\nAdded a feature to log warnings when messages are dropped due to no subscribers, controllable via a feature flag.\nAdded support for Bookworm and Bullseye distributions in Node images.\nConsolidated executor definitions by moving them from the langgraph-go repository, improving manageability and updating the checkpointer setup method for server migrations.\nEnsured correct response headers are sent for a2a, improving compatibility and communication.\nConsolidated PostgreSQL checkpoint implementation, added CI testing for the /core directory, fixed RemoteStore test errors, and enhanced the Store implementation with transactions.\nAdded PostgreSQL migrations to the queue server to prevent errors from graphs being added before migrations are performed.\nAdded timeouts to specific Redis calls to prevent workers from being left active.\nUpdated the Golang runtime and added pytest skips for unsupported functionalities, including initial support for passing store to node and message streaming.\nIntroduced a reverse proxy setup for serving combined Python and Node.js graphs, with nginx handling server routing, to facilitate a Postgres/Redis backend for the Node.js API server.\nSet a default 15-minute statement timeout and implemented monitoring for long-running queries to ensure system efficiency.\nStop propagating run configurable values to the thread configuration, because this can cause issues on subsequent runs if you are specifying a checkpoint_id. This is a slight breaking change in behavior, since the thread value will no longer automatically reflect the unioned configuration of the most recent run. We believe this behavior is more intuitive, however.\nEnhanced compatibility with older worker versions by handling event data in channel names within ops.py.\nAdded a feature flag (FF_RICH_THREADS=false) to disable thread updates on run creation, reducing lock contention and simplifying thread status handling.\nUtilized existing connections for aput and apwrite operations to improve performance.\nImproved error handling for decoding issues to enhance data processing reliability.\nExcluded headers from logs to improve security while maintaining runtime functionality.\nFixed an error that prevented mapping slots to a single node.\nAdded debug logs to track node execution in JS deployments for improved issue diagnosis.\nChanged the default multitask strategy to enqueue, improving throughput by eliminating the need to fetch inflight runs during new run insertions.\nOptimized database operations for Runs.next and Runs.sweep to reduce redundant queries and improve efficiency.\nImproved run creation speed by skipping unnecessary inflight runs queries.\nRestored the original streaming behavior of runs, ensuring consistent inclusion of interrupt events based on stream_mode settings.\nOptimized Runs.next query to reduce average execution time from ~14.43ms to ~2.42ms, improving performance.\nAdded support for stream mode \u201ctasks\u201d and \u201ccheckpoints\u201d, normalized the UI namespace, and upgraded @langchain/langgraph-api for enhanced functionality.\nAdded a composite index on threads for faster searches with owner-based authentication and updated the default sort order to updated_at for improved query performance.\nImproved interoperability with the ckpt ingestion worker on the main loop to prevent task scheduling issues.\nDelayed queue worker startup until after migrations are completed to prevent premature execution.\nEnhanced thread state error handling by adding specific metadata and improved response codes for better clarity when state updates fail during creation.\nExposed the interrupt ID when retrieving the thread state to improve API transparency.\nReduced writes to the checkpoint_blobs table by inlining small values (null, numeric, str, etc.). This means we don\u2019t need to store extra values for channels that haven\u2019t been updated.\nHandled CancelledError by marking tasks as ready to retry, improving error management in worker processes.\nAdded LG API version and request ID to metadata and logs for better tracking.\nAdded LG API version and request ID to metadata and logs to improve traceability.\nImproved database performance by creating indexes concurrently.\nEnsured postgres write is committed only after the Redis running marker is set to prevent race conditions.\nEnhanced query efficiency and reliability by adding a unique index on thread_id/running, optimizing row locks, and ensuring deterministic run selection.\nResolved a race condition by ensuring Postgres updates only occur after the Redis running marker is set.", "tokens": 868, "node_type": "child"}
{"id": 125, "chunk_id": "c16acf8a553ac24f5d77e7d3a1c6e493", "parent_id": 106, "url": "", "namespace": "langchain", "title": "langsmith-langgraph-server", "headers": ["langsmith-langgraph-server"], "section_index": 0, "chunk_index": 0, "text": "# langsmith-langgraph-server\n\n> Source: https://docs.langchain.com/langsmith/langgraph-server\n\nTo use the\nEnterprise\nversion of the LangGraph Server, you must acquire a license key that you will need to specify when running the Docker image. To acquire a license key, contact our sales team.\nYou can run the Enterprise\nversion of the LangGraph Server on the following deployment options:\n- Cloud\n- Hybrid\n- Self-hosted\nApplication structure\nTo deploy a LangGraph Server application, you need to specify the graph(s) you want to deploy, as well as any relevant configuration settings, such as dependencies and environment variables. Read the application structure guide to learn how to structure your LangGraph application for deployment.Parts of a deployment\nWhen you deploy LangGraph Server, you are deploying one or more graphs, a database for persistence, and a task queue.Graphs\nWhen you deploy a graph with LangGraph Server, you are deploying a \u201cblueprint\u201d for an Assistant. An Assistant is a graph paired with specific configuration settings. You can create multiple assistants per graph, each with unique settings to accommodate different use cases that can be served by the same graph. Upon deployment, LangGraph Server will automatically create a default assistant for each graph using the graph\u2019s default configuration settings.We often think of a graph as implementing an agent, but a graph does not necessarily need to implement an agent. For example, a graph could implement a simple\nchatbot that only supports back-and-forth conversation, without the ability to influence any application control flow. In reality, as applications get more complex, a graph will often implement a more complex flow that may use multiple agents working in tandem.\nPersistence and task queue\nLangGraph Server leverages a database for persistence and a task queue. PostgreSQL is supported as a database for LangGraph Server and Redis as the task queue. If you\u2019re deploying using LangSmith cloud, these components are managed for you. If you\u2019re deploying LangGraph Server on your own infrastructure, you\u2019ll need to set up and manage these components yourself. For more information on how these components are set up and managed, review the hosting options guide.Learn more\n- LangGraph Application Structure guide explains how to structure your LangGraph application for deployment.\n- The API Reference provides detailed information on the API endpoints and data models.", "tokens": 373, "node_type": "child"}
{"id": 126, "chunk_id": "c16acf8a553ac24f5d77e7d3a1c6e493", "parent_id": 107, "url": "", "namespace": "langchain", "title": "langsmith-langsmith-collector", "headers": ["langsmith-langsmith-collector"], "section_index": 0, "chunk_index": 0, "text": "# langsmith-langsmith-collector\n\n> Source: https://docs.langchain.com/langsmith/langsmith-collector\n\nThis section is only applicable for Kubernetes deployments.\nReceivers\nLogs\nThis is an example for a Sidecar collector to read logs from its own pod, excluding logs from non domain-specific containers. A Sidecar configuration is useful here because we require access to every container\u2019s filesystem. A DaemonSet can also be used.This configuration requires \u2018get\u2019, \u2018list\u2019, and \u2018watch\u2019 permissions on pods in the given namespace.\nMetrics\nMetrics can be scraped using the Prometheus endpoints. A single instance Gateway collector can be be used to avoid duplication of queries when fetching metrics. The following config scrapes all of the default named LangSmith services:This configuration requires \u2018get\u2019, \u2018list\u2019, and \u2018watch\u2019 permissions on pods, services and endpoints in the given namespace.\nTraces\nFor traces, you need to enable the OTLP receiver. The following configuration can be used to listen to HTTP traces on port 4318, and GRPC on port 4317:Processors\nRecommended OTEL Processors\nThe following processors are recommended when using the OTel collector:- Batch Processor: Groups the data into batches before sending to exporters.\n- Memory Limiter: Prevents the collector from using too much memory and crashing. When the soft limit is crossed, the collector stops accepting new data.\n- Kubernetes Attributes Processor: Adds Kubernetes metadata such as pod name into the telemetry data.", "tokens": 215, "node_type": "child"}
{"id": 127, "chunk_id": "c16acf8a553ac24f5d77e7d3a1c6e493", "parent_id": 108, "url": "", "namespace": "langchain", "title": "langsmith-langsmith-managed-clickhouse", "headers": ["langsmith-langsmith-managed-clickhouse"], "section_index": 0, "chunk_index": 0, "text": "# langsmith-langsmith-managed-clickhouse\n\n> Source: https://docs.langchain.com/langsmith/langsmith-managed-clickhouse\n\nLangSmith uses ClickHouse as the primary storage engine for traces and feedback. For easier management and scaling, it is recommended to connect a self-hosted LangSmith instance to an external ClickHouse instance. LangSmith-managed ClickHouse is an option that allows you to use a fully managed ClickHouse instance that is monitored and maintained by the LangSmith team.\nArchitecture Overview\nThe architecture of using LangSmith-managed ClickHouse with your self-hosted LangSmith instance is similar to using a fully self-hosted ClickHouse instance, with a few key differences:- You will need to set up a private network connection between your LangSmith instance and the LangSmith-managed ClickHouse instance. This is to ensure that your data is secure and that you can connect to the ClickHouse instance from your self-hosted LangSmith instance.\n- With this option, sensitive information (inputs and outputs) of your traces will be stored in cloud object storage (S3 or GCS) within your cloud instead of ClickHouse to ensure that sensitive information doesn\u2019t leave your VPC. For more details on where particular data fields are stored, refer to Data storage.\n- The LangSmith team will monitor your ClickHouse instance and ensure that it is running smoothly. This allows us to track metrics like run-ingestion delay and query performance.\nRequirements\n- You must use a supported blob storage option. Read the blob storage guide for more information.\n- To use private endpoints, ensure that your VPC is in a ClickHouse Cloud supported region. Otherwise, you will need to use a public endpoint we will secure with firewall rules. Your VPC will need to have a NAT gateway to allow us to allowlist your traffic.\n- You must have a VPC that can connect to the LangSmith-managed ClickHouse service. You will need to work with our team to set up the necessary networking.\n- You must have a LangSmith self-hosted instance running. You can use our managed ClickHouse service with both Kubernetes and Docker installations.\nData storage\nClickHouse stores runs and feedback data, specifically:- All feedback data fields.\n- Some run data fields.\ninputs\n, outputs\n, errors\n, manifests\n, extras\n, and events\nof a run, since these fields may contain LLM prompts and completions. With LangSmith-managed ClickHouse, these sensitive fields are stored in cloud object storage (S3 or GCS) within your cloud, while the rest of the run data is stored in ClickHouse, ensuring sensitive information never leaves your VPC.\nStored feedback data fields\nBecause all feedback data is stored in ClickHouse, do not send sensitive information in feedback (scores and annotations/comments) or in any other run fields that are mentioned in Stored run data fields.\n| Field Name | Type | Description |\n|---|---|---|\n| id | UUID | Unique identifier for the record itself |\n| created_at | datetime | Timestamp when the record was created |\n| modified_at | datetime | Timestamp when the record was last modified |\n| session_id | UUID | Unique identifier for the experiment or tracing project the run was a part of |\n| run_id | UUID | Unique identifier for a specific run within a session |\n| key | string | A key describing the criteria of the feedback, eg \u201ccorrectness\u201d |\n| score | number | Numerical score associated with the feedback key |\n| value | string | Reserved for storing a value associated with the score. Useful for categorical feedback. |\n| comment | string | Any comment or annotation associated with the record. This can be a justification for the score given. |\n| correction | object | Reserved for storing correction details, if any |\n| feedback_source | object | Object containing information about the feedback source |\n| feedback_source.type | string | The type of source where the feedback originated, eg \u201capi\u201d, \u201capp\u201d, \u201cevaluator\u201d |\n| feedback_source.metadata | object | Reserved for additional metadata, currently |\n| feedback_source.user_id | UUID | Unique identifier for the user providing feedback |\nStored run data fields\nRun data fields are split between the managed ClickHouse database and your cloud object storage (e.g., S3 or GCS).For run fields stored in object storage, only a reference or pointer is kept in ClickHouse. For example,\ninputs\nand outputs\ncontent are offloaded to S3/GCS, with the ClickHouse record storing corresponding S3 URLs in the inputs_s3_urls\nand outputs_s3_urls\nfields.| Field | Storage Location |\n|---|---|\nid | ClickHouse |\nname | ClickHouse |\ninputs | Object Storage |\nrun_type | ClickHouse |\nstart_time | ClickHouse |\nend_time | ClickHouse |\nextra | Object Storage |\nerror | Object Storage |\noutputs | Object Storage |\nevents | Object Storage |\ntags | ClickHouse |\ntrace_id | ClickHouse |\ndotted_order | ClickHouse |\nstatus | ClickHouse |\nchild_run_ids | ClickHouse |\ndirect_child_run_ids | ClickHouse |\nparent_run_ids | ClickHouse |\nfeedback_stats | ClickHouse |\nreference_example_id | ClickHouse |\ntotal_tokens | ClickHouse |\nprompt_tokens | ClickHouse |\ncompletion_tokens | ClickHouse |\ntotal_cost | ClickHouse |\nprompt_cost | ClickHouse |\ncompletion_cost | ClickHouse |\nfirst_token_time | ClickHouse |\nsession_id | ClickHouse |\nin_dataset | ClickHouse |\nparent_run_id | ClickHouse |\nexecution_order (deprecated) | ClickHouse |\nserialized | ClickHouse |\nmanifest_id (deprecated) | ClickHouse |\nmanifest_s3_id | ClickHouse |\ninputs_s3_urls | ClickHouse |\noutputs_s3_urls | ClickHouse |\nprice_model_id | ClickHouse |\napp_path | ClickHouse |\nlast_queued_at | ClickHouse |\nshare_token | ClickHouse |", "tokens": 888, "node_type": "child"}
{"id": 128, "chunk_id": "c16acf8a553ac24f5d77e7d3a1c6e493", "parent_id": 109, "url": "", "namespace": "langchain", "title": "langsmith-llm-as-judge", "headers": ["langsmith-llm-as-judge"], "section_index": 0, "chunk_index": 0, "text": "# langsmith-llm-as-judge\n\n> Source: https://docs.langchain.com/langsmith/llm-as-judge\n\nLLM applications can be challenging to evaluate since they often generate conversational text with no single correct answer.This guide shows you how to define an LLM-as-a-judge evaluator for offline evaluation using either the LangSmith SDK or the UI. Note: To run evaluations in real-time on your production traces, refer to setting up online evaluations.\nPre-built evaluators are a useful starting point for setting up evaluations. Refer to pre-built evaluators for how to use pre-built evaluators with LangSmith.\nFor complete control of evaluator logic, create your own LLM-as-a-judge evaluator and run it using the LangSmith SDK (Python / TypeScript).Requires langsmith>=0.2.0\nCopy\nfrom langsmith import evaluate, traceable, wrappers, Clientfrom openai import OpenAI# Assumes you've installed pydanticfrom pydantic import BaseModel# Optionally wrap the OpenAI client to trace all model calls.oai_client = wrappers.wrap_openai(OpenAI())def valid_reasoning(inputs: dict, outputs: dict) -> bool: \"\"\"Use an LLM to judge if the reasoning and the answer are consistent.\"\"\" instructions = \"\"\"Given the following question, answer, and reasoning, determine if the reasoningfor the answer is logically valid and consistent with the question and the answer.\"\"\" class Response(BaseModel): reasoning_is_valid: bool msg = f\"Question: {inputs['question']}\\nAnswer: {outputs['answer']}\\nReasoning: {outputs['reasoning']}\" response = oai_client.beta.chat.completions.parse( model=\"gpt-4o\", messages=[{\"role\": \"system\", \"content\": instructions,}, {\"role\": \"user\", \"content\": msg}], response_format=Response ) return response.choices[0].message.parsed.reasoning_is_valid# Optionally add the 'traceable' decorator to trace the inputs/outputs of this function.@traceabledef dummy_app(inputs: dict) -> dict: return {\"answer\": \"hmm i'm not sure\", \"reasoning\": \"i didn't understand the question\"}ls_client = Client()dataset = ls_client.create_dataset(\"big questions\")examples = [ {\"inputs\": {\"question\": \"how will the universe end\"}}, {\"inputs\": {\"question\": \"are we alone\"}},]ls_client.create_examples(dataset_id=dataset.id, examples=examples)results = evaluate( dummy_app, data=dataset, evaluators=[valid_reasoning])\nSee here for more on how to write a custom evaluator.\nAdd specific instructions for your LLM-as-a-judge evalutor prompt and configure which parts of the input/output/reference output should be passed to the evaluator.\nCreate a new prompt, or choose an existing prompt from the prompt hub.\nCreate your own prompt: Create a custom prompt inline.\nPull a prompt from the prompt hub: Use the Select a prompt dropdown to select from an existing prompt. You can\u2019t edit these prompts directly within the prompt editor, but you can view the prompt and the schema it uses. To make changes, edit the prompt in the playground and commit the version, and then pull in your new prompt in the evaluator.\nUse variable mapping to indicate the variables that are passed into your evaluator prompt from your run or example. To aid with variable mapping, an example (or run) is provided for reference. Click on the the variables in your prompt and use the dropdown to map them to the relevant parts of the input, output, or reference output.To add prompt variables type the variable with double curly brackets {{prompt_var}} if using mustache formatting (the default) or single curly brackets {prompt_var} if using f-string formatting.You may remove variables as needed. For example if you are evaluating a metric such as conciseness, you typically don\u2019t need a reference output so you may remove that variable.\nTo better align the LLM-as-a-judge evaluator to human preferences, LangSmith allows you to collect human corrections on evaluator scores. With this selection enabled, corrections are then inserted automatically as few-shot examples into your prompt.Learn how to set up few-shot examples and make corrections.\nFeedback configuration is the scoring criteria that your LLM-as-a-judge evaluator will use. Think of this as the rubric that your evaluator will grade based on. Scores will be added as feedback to a run or example. Defining feedback for your evaluator:\nName the feedback key: This is the name that will appear when viewing evaluation results. Names should be unique across experiments.\nAdd a description: Describe what the feedback represents.\nChoose a feedback type:\nBoolean: True/false feedback.\nCategorical: Select from predefined categories.\nContinuous: Numerical scoring within a specified range.\nBehind the scenes, feedback configuration is added as structured output to the LLM-as-a-judge prompt. If you\u2019re using an existing prompt from the hub, you must add an output schema to the prompt before configuring an evaluator to use it. Each top-level key in the output schema will be treated as a separate piece of feedback.", "tokens": 676, "node_type": "child"}
{"id": 129, "chunk_id": "c16acf8a553ac24f5d77e7d3a1c6e493", "parent_id": 110, "url": "", "namespace": "langchain", "title": "langsmith-local-server", "headers": ["langsmith-local-server"], "section_index": 0, "chunk_index": 0, "text": "# langsmith-local-server\n\n> Source: https://docs.langchain.com/langsmith/local-server\n\nlanggraph new path/to/your/app --template new-langgraph-project-python\nAdditional templates\nIf you use langgraph new without specifying a template, you will be presented with an interactive menu that will allow you to choose from a list of available templates.\nYou will find a .env.example in the root of your new LangGraph app. Create a .env file in the root of your new LangGraph app and copy the contents of the .env.example file into it, filling in the necessary API keys:\nfrom langgraph_sdk import get_clientimport asyncioclient = get_client(url=\"http://localhost:2024\")async def main(): async for chunk in client.runs.stream( None, # Threadless run \"agent\", # Name of assistant. Defined in langgraph.json. input={ \"messages\": [{ \"role\": \"human\", \"content\": \"What is LangGraph?\", }], }, ): print(f\"Receiving new event of type: {chunk.event}...\") print(chunk.data) print(\"\\n\\n\")asyncio.run(main())", "tokens": 128, "node_type": "child"}
{"id": 130, "chunk_id": "c16acf8a553ac24f5d77e7d3a1c6e493", "parent_id": 111, "url": "", "namespace": "langchain", "title": "langsmith-local", "headers": ["langsmith-local"], "section_index": 0, "chunk_index": 0, "text": "# langsmith-local\n\n> Source: https://docs.langchain.com/langsmith/local\n\nupload_results=False\nto evaluate()\n/ aevaluate()\n.\nThis will run you application and evaluators exactly as it always does and return the same output, but nothing will be recorded to LangSmith. This includes not just the experiment results but also the application and evaluator traces.\nExample\nLet\u2019s take a look at an example: Requireslangsmith>=0.2.0\n. Example also uses pandas\n.\n| inputs.question | outputs.answer | reference.answer | feedback.is_concise | |\n|---|---|---|---|---|\n| 0 | What is the largest mammal? | What is the largest mammal? is a good question. I don\u2019t know the answer. | The blue whale | False |\n| 1 | What do mammals and birds have in common? | What do mammals and birds have in common? is a good question. I don\u2019t know the answer. | They are both warm-blooded | False |", "tokens": 142, "node_type": "child"}
{"id": 131, "chunk_id": "c16acf8a553ac24f5d77e7d3a1c6e493", "parent_id": 112, "url": "", "namespace": "langchain", "title": "langsmith-log-llm-trace", "headers": ["langsmith-log-llm-trace"], "section_index": 0, "chunk_index": 0, "text": "# langsmith-log-llm-trace\n\n> Source: https://docs.langchain.com/langsmith/log-llm-trace\n\n- Rich, structured rendering of message lists\n- Token and cost tracking per LLM call, per trace and across traces over time\nMessages Format\nWhen tracing a custom model or a custom input/output format, it must either follow the LangChain format, OpenAI completions format or Anthropic messages format. For more details, refer to the OpenAI Chat Completions or Anthropic Messages documentation. The LangChain format is:Examples\nConverting custom I/O formats into LangSmith compatible formats\nIf you\u2019re using a custom input or output format, you can convert it to a LangSmith compatible format usingprocess_inputs\n/processInputs\nand process_outputs\n/processOutputs\nfunctions on the @traceable\ndecorator (Python) or traceable\nfunction (TS).\nprocess_inputs\n/processInputs\nand process_outputs\n/processOutputs\naccept functions that allow you to transform the inputs and outputs of a specific trace before they are logged to LangSmith. They have access to the trace\u2019s inputs and outputs, and can return a new dictionary with the processed data.\nHere\u2019s a boilerplate example of how to use process_inputs\nand process_outputs\nto convert a custom I/O format into a LangSmith compatible format:\nIdentifying a custom model in traces\nWhen using a custom model, it is recommended to also provide the followingmetadata\nfields to identify the model when viewing traces and when filtering.\nls_provider\n: The provider of the model, eg \u201copenai\u201d, \u201canthropic\u201d, etc.ls_model_name\n: The name of the model, eg \u201cgpt-4o-mini\u201d, \u201cclaude-3-opus-20240307\u201d, etc.\nIf you implement a custom streaming chat_model, you can \u201creduce\u201d the outputs into the same format as the non-streaming version. This is currently only supported in Python.\nIf\nls_model_name\nis not present in extra.metadata\n, other fields might be used from the extra.metadata\nfor estimating token counts. The following fields are used in the order of precedence:metadata.ls_model_name\ninputs.model\ninputs.model_name\nmetadata\nfields, refer to the Add metadata and tags guide.\nProvide token and cost information\nBy default, LangSmith uses tiktoken to count tokens, utilizing a best guess at the model\u2019s tokenizer based on thels_model_name\nprovided. It also calculates costs automatically by using the model pricing table. To learn how LangSmith calculates token-based costs, see this guide.\nHowever, many models already include exact token counts as part of the response. If you have this information, you can override the default token calculation in LangSmith in one of two ways:\n- Extract usage within your traced function and set a\nusage_metadata\nfield on the run\u2019s metadata. - Return a\nusage_metadata\nfield in your traced function outputs.\nYou cannot set any fields other than the ones listed below. You do not need to include all fields.\nSetting run metadata\nYou can modify the current run\u2019s metadata with usage information within your traced function. The advantage of this approach is that you do not need to change your traced function\u2019s runtime outputs. Here\u2019s an example:Requires\nlangsmith>=0.3.43\n(Python) and langsmith>=0.3.30\n(JS/TS).Setting run outputs\nYou can add ausage_metadata\nkey to the function\u2019s response to set manual token counts and costs.\nTime-to-first-token\nIf you are usingtraceable\nor one of our SDK wrappers, LangSmith will automatically populate time-to-first-token for streaming LLM runs.\nHowever, if you are using the RunTree\nAPI directly, you will need to add a new_token\nevent to the run tree in order to properly populate time-to-first-token.\nHere\u2019s an example:", "tokens": 532, "node_type": "child"}
{"id": 132, "chunk_id": "c16acf8a553ac24f5d77e7d3a1c6e493", "parent_id": 114, "url": "", "namespace": "langchain", "title": "langsmith-log-retriever-trace", "headers": ["langsmith-log-retriever-trace"], "section_index": 0, "chunk_index": 0, "text": "# langsmith-log-retriever-trace\n\n> Source: https://docs.langchain.com/langsmith/log-retriever-trace\n\nNothing will break if you don\u2019t log retriever traces in the correct format and data will still be logged. However, the data will not be rendered in a way that is specific to retriever steps.\nMany LLM applications require looking up documents from vector databases, knowledge graphs, or other types of indexes. Retriever traces are a way to log the documents that are retrieved by the retriever. LangSmith provides special rendering for retrieval steps in traces to make it easier to understand and diagnose retrieval issues. In order for retrieval steps to be rendered correctly, a few small steps need to be taken.\n-\nAnnotate the retriever step with\nrun_type=\"retriever\"\n.\n-\nReturn a list of Python dictionaries or TypeScript objects from the retriever step. Each dictionary should contain the following keys:\npage_content\n: The text of the document.\ntype\n: This should always be \u201cDocument\u201d.\nmetadata\n: A python dictionary or TypeScript object containing metadata about the document. This metadata will be displayed in the trace.\nThe following code snippets show how to log a retrieval steps in Python and TypeScript.\nThe following image shows how a retriever step is rendered in a trace. The contents along with the metadata are displayed with each document.", "tokens": 211, "node_type": "child"}
{"id": 133, "chunk_id": "c16acf8a553ac24f5d77e7d3a1c6e493", "parent_id": 115, "url": "", "namespace": "langchain", "title": "langsmith-log-traces-to-project", "headers": ["langsmith-log-traces-to-project"], "section_index": 0, "chunk_index": 0, "text": "# langsmith-log-traces-to-project\n\n> Source: https://docs.langchain.com/langsmith/log-traces-to-project\n\nSet the destination project statically\nAs mentioned in the Tracing Concepts section, LangSmith uses the concept of aProject\nto group traces. If left unspecified, the project is set to default\n. You can set the LANGSMITH_PROJECT\nenvironment variable to configure a custom project name for an entire application run. This should be done before executing your application.\nThe\nLANGSMITH_PROJECT\nflag is only supported in JS SDK versions >= 0.2.16, use LANGCHAIN_PROJECT\ninstead if you are using an older version.Set the destination project dynamically\nYou can also set the project name at program runtime in various ways, depending on how you are annotating your code for tracing. This is useful when you want to log traces to different projects within the same application.Setting the project name dynamically using one of the below methods overrides the project name set by the\nLANGSMITH_PROJECT\nenvironment variable.", "tokens": 147, "node_type": "child"}
{"id": 134, "chunk_id": "c16acf8a553ac24f5d77e7d3a1c6e493", "parent_id": 116, "url": "", "namespace": "langchain", "title": "langsmith-manage-datasets-in-application", "headers": ["langsmith-manage-datasets-in-application"], "section_index": 0, "chunk_index": 0, "text": "# langsmith-manage-datasets-in-application > Source: https://docs.langchain.com/langsmith/manage-datasets-in-application Create a dataset and add examples The following sections explain the different ways you can create a dataset in LangSmith and add examples to it. Depending on your workflow, you can manually curate examples, automatically capture them from tracing, import files, or even generate synthetic data:- Manually from a tracing project - Automatically from a tracing project - From examples in an Annotation Queue - From the Prompt Playground - Import a dataset from a CSV or JSONL file - Create a new dataset from the dataset page - Add synthetic examples created by an LLM via the Datasets UI Manually from a tracing project A common pattern for constructing datasets is to convert notable traces from your application into dataset examples. This approach requires that you have configured tracing to LangSmith.A technique to build datasets is to filter the most interesting traces, such as traces that were tagged with poor user feedback, and add them to a dataset. For tips on how to filter traces, refer to Filter traces guide. - Multi-select runs from the runs table. On the Runs tab, multi-select runs. At the bottom of the page, click Add to Dataset. - On the Runs tab, select a run from the table. On the individual run details page, select Add to -> Dataset in the top right corner. When you select a dataset from the run details page, a modal will pop up letting you know if any transformations were applied or if schema validation failed. For example, the screenshot below shows a dataset that is using transformations to optimize for collecting LLM runs. You can then optionally edit the run before adding it to the dataset. Automatically from a tracing project You can use run rules to automatically add traces to a dataset based on certain conditions. For example, you could add all traces that are tagged with a specific use case or have a low feedback score.From examples in an Annotation Queue If you rely on subject matter experts to build meaningful datasets, use annotation queues to provide a streamlined view for reviewers. Human reviewers can optionally modify the inputs/outputs/reference outputs from a trace before it is added to the dataset. D to add the run to it. Any modifications you make to the run in your annotation queue will carry over to the dataset, and all metadata associated with the run will also be copied. Note you can also set up rules to add runs that meet specific criteria to an annotation queue using automation rules. From the Prompt Playground On the Prompt Playground page, select Set up Evaluation, click +New if you\u2019re starting a new dataset or select from an existing dataset. To edit the examples: - Use +Row to add a new example to the dataset - Delete an example using the \u22ee dropdown on the right hand side of the table - If you\u2019re creating a reference-free dataset remove the \u201cReference Output\u201d column using the x button in the column. Note: this action is not reversible. Import a dataset from a CSV or JSONL file On the Datasets & Experiments page, click +New Dataset, then Import an existing dataset from CSV or JSONL file.Create a new dataset from the Datasets & Experiments page - Navigate to the Datasets & Experiments page from the left-hand menu. - Click + New Dataset. - On the New Dataset page, select the Create from scratch tab. - Add a name and description for the dataset. - (Optional) Create a dataset schema to validate your dataset. - Click Create, which will create an empty dataset. - To add examples inline, on the dataset\u2019s page, go to the Examples tab. Click + Example. - Define examples in JSON and click Submit. For more details on dataset splits, refer to Create and manage dataset splits. Add synthetic examples created by an LLM If you have existing examples and a schema defined on your dataset, when you click + Example there is an option to Add AI-Generated Examples. This will use an LLM to create synthetic examples. In Generate examples, do the following:- Click API Key in the top right of the pane to set your OpenAI API key as a workspace secret. If your workspace already has an OpenAI API key set, you can skip this step. - Select : Toggle Automatic or Manual reference examples. You can select these examples manually from your dataset or use the automatic selection option. - Enter the number of synthetic examples you want to generate. - Click Generate. - The examples will appear on the Select generated examples page. Choose which examples to add to your dataset, with the option to edit them before finalizing. Click Save Examples. - Each example will be validated against your specified dataset schema and tagged as synthetic in the source metadata. Manage a dataset Create a dataset schema LangSmith datasets store arbitrary JSON objects. We recommend (but do not require) that you define a schema for your dataset to ensure that they conform to a specific JSON schema. Dataset schemas are defined with standard JSON schema, with the addition of a few prebuilt types that make it easier to type common primitives like messages and tools. Certain fields in your schema have a+ Transformations option. Transformations are preprocessing steps that, if enabled, update your examples when you add them to the dataset. For example the convert to OpenAI messages transformation will convert message-like objects, like LangChain messages, to OpenAI message format. For the full list of available transformations, see our reference. If you plan to collect production traces in your dataset from LangChain ChatModels or from OpenAI calls using the LangSmith OpenAI wrapper, we offer a prebuilt Chat Model schema that converts messages and tools into industry standard openai formats that can be used downstream with any model for testing. You can also customize the template settings to match your use case.Please see the dataset transformations reference for more information.", "tokens": 1000, "node_type": "child"}
{"id": 135, "chunk_id": "9b0fd3b850698317bef57fa6e71a38ba", "parent_id": 116, "url": "", "namespace": "langchain", "title": "langsmith-manage-datasets-in-application", "headers": ["langsmith-manage-datasets-in-application"], "section_index": 0, "chunk_index": 1, "text": "are defined with standard JSON schema, with the addition of a few prebuilt types that make it easier to type common primitives like messages and tools. Certain fields in your schema have a+ Transformations option. Transformations are preprocessing steps that, if enabled, update your examples when you add them to the dataset. For example the convert to OpenAI messages transformation will convert message-like objects, like LangChain messages, to OpenAI message format. For the full list of available transformations, see our reference. If you plan to collect production traces in your dataset from LangChain ChatModels or from OpenAI calls using the LangSmith OpenAI wrapper, we offer a prebuilt Chat Model schema that converts messages and tools into industry standard openai formats that can be used downstream with any model for testing. You can also customize the template settings to match your use case.Please see the dataset transformations reference for more information. Create and manage dataset splits Dataset splits are divisions of your dataset that you can use to segment your data. For example, it is common in machine learning workflows to split datasets into training, validation, and test sets. This can be useful to prevent overfitting - where a model performs well on the training data but poorly on unseen data. In evaluation workflows, it can be useful to do this when you have a dataset with multiple categories that you may want to evaluate separately; or if you are testing a new use case that you may want to include in your dataset in the future, but want to keep separate for now. Note that the same effect can be achieved manually via metadata - but we expect splits to be used for higher level organization of your dataset to split it into separate groups for evaluation, whereas metadata would be used more for storing information on your examples like tags and information about its origin. In machine learning, it is best practice to keep your splits separate (each example belongs to exactly one split). However, we allow you to select multiple splits for the same example in LangSmith because it can make sense for some evaluation workflows - for example, if an example falls into multiple categories on which you may want to evaluate your application. In order to create and manage splits in the app, you can select some examples in your dataset and click \u201cAdd to Split\u201d. From the resulting popup menu, you can select and unselect splits for the selected examples, or create a new split.Edit example metadata You can add metadata to your examples by clicking on an example and then clicking \u201cEdit\u201d on the top righthand side of the popover. From this page, you can update/delete existing metadata, or add new metadata. You may use this to store information about your examples, such as tags or version info, which you can then group by when analyzing experiment results or filter by when you calllist_examples in the SDK. Filter examples You can filter examples by split, metadata key/value or perform full-text search over examples. These filtering options are available to the top left of the examples table.- Filter by split: Select split > Select a split to filter by - Filter by metadata: Filters > Select \u201cMetadata\u201d from the dropdown > Select the metadata key and value to filter on - Full-text search: Filters > Select \u201cFull Text\u201d from the dropdown > Enter your search criteria", "tokens": 571, "node_type": "child"}
{"id": 136, "chunk_id": "c16acf8a553ac24f5d77e7d3a1c6e493", "parent_id": 117, "url": "", "namespace": "langchain", "title": "langsmith-manage-datasets-programmatically", "headers": ["langsmith-manage-datasets-programmatically"], "section_index": 0, "chunk_index": 0, "text": "# langsmith-manage-datasets-programmatically\n\n> Source: https://docs.langchain.com/langsmith/manage-datasets-programmatically\n\nCreate a dataset\nCreate a dataset from list of values\nThe most flexible way to make a dataset using the client is by creating examples from a list of inputs and optional outputs. Below is an example. Note that you can add arbitrary metadata to each example, such as a note or a source. The metadata is stored as a dictionary.If you have many examples to create, consider using the\ncreate_examples\n/createExamples\nmethod to create multiple examples in a single request. If creating a single example, you can use the create_example\n/createExample\nmethod.Create a dataset from traces\nTo create datasets from the runs (spans) of your traces, you can use the same approach. For many more examples of how to fetch and filter runs, see the export traces guide. Below is an example:Create a dataset from a CSV file\nIn this section, we will demonstrate how you can create a dataset by uploading a CSV file. First, ensure your CSV file is properly formatted with columns that represent your input and output keys. These keys will be utilized to map your data properly during the upload. You can specify an optional name and description for your dataset. Otherwise, the file name will be used as the dataset name and no description will be provided.Create a dataset from pandas DataFrame (Python only)\nThe python client offers an additional convenience method to upload a dataset from a pandas dataframe.Fetch datasets\nYou can programmatically fetch datasets from LangSmith using thelist_datasets\n/listDatasets\nmethod in the Python and TypeScript SDKs. Below are some common calls.\nInitialize the client before running the below code snippets.\nQuery all datasets\nList datasets by name\nIf you want to search by the exact name, you can do the following:List datasets by type\nYou can filter datasets by type. Below is an example querying for chat datasets.Fetch examples\nYou can programmatically fetch examples from LangSmith using thelist_examples\n/listExamples\nmethod in the Python and TypeScript SDKs. Below are some common calls.\nInitialize the client before running the below code snippets.\nList all examples for a dataset\nYou can filter by dataset ID:List examples by id\nYou can also list multiple examples all by ID.List examples by metadata\nYou can also filter examples by metadata. Below is an example querying for examples with a specific metadata key-value pair. Under the hood, we check to see if the example\u2019s metadata contains the key-value pair(s) you specify. For example, if you have an example with metadata{\"foo\": \"bar\", \"baz\": \"qux\"}\n, both {foo: bar}\nand {baz: qux}\nwould match, as would {foo: bar, baz: qux}\n.\nList examples by structured filter\nSimilar to how you can use the structured filter query language to fetch runs, you can use it to fetch examples.This is currently only available in v0.1.83 and later of the Python SDK and v0.1.35 and later of the TypeScript SDK.Additionally, the structured filter query language is only supported for\nmetadata\nfields.has\noperator to fetch examples with metadata fields that contain specific key/value pairs and the exists\noperator to fetch examples with metadata fields that contain a specific key. Additionally, you can also chain multiple filters together using the and\noperator and negate a filter using the not\noperator.\nUpdate examples\nUpdate single example\nYou can programmatically update examples from LangSmith using theupdate_example\n/updateExample\nmethod in the Python and TypeScript SDKs. Below is an example.\nBulk update examples\nYou can also programmatically update multiple examples in a single request with theupdate_examples\n/updateExamples\nmethod in the Python and TypeScript SDKs. Below is an example.", "tokens": 594, "node_type": "child"}
{"id": 137, "chunk_id": "c16acf8a553ac24f5d77e7d3a1c6e493", "parent_id": 118, "url": "", "namespace": "langchain", "title": "langsmith-manage-datasets", "headers": ["langsmith-manage-datasets"], "section_index": 0, "chunk_index": 0, "text": "# langsmith-manage-datasets\n\n> Source: https://docs.langchain.com/langsmith/manage-datasets\n\n- Versioning datasets to track changes over time.\n- Filtering and splitting datasets for evaluation.\n- Sharing datasets publicly.\n- Exporting datasets in various formats.\nVersion a dataset\nIn LangSmith, datasets are versioned. This means that every time you add, update, or delete examples in your dataset, a new version of the dataset is created.Create a new version of a dataset\nAny time you add, update, or delete examples in your dataset, a new version of your dataset is created. This allows you to track changes to your dataset over time and understand how your dataset has evolved. By default, the version is defined by the timestamp of the change. When you click on a particular version of a dataset (by timestamp) in the Examples tab, you will find the state of the dataset at that point in time. Note that examples are read-only when viewing a past version of the dataset. You will also see the operations that were between this version of the dataset and the latest version of the dataset.By default, the latest version of the dataset is shown in the Examples tab and experiments from all versions are shown in the Tests tab.\nTag a version\nYou can also tag versions of your dataset to give them a more human-readable name, which can be useful for marking important milestones in your dataset\u2019s history. For example, you might tag a version of your dataset as \u201cprod\u201d and use it to run tests against your LLM pipeline. You can tag a version of your dataset in the UI by clicking on + Tag this version in the Examples tab. You can also tag versions of your dataset using the SDK. Here\u2019s an example of how to tag a version of a dataset using the Python SDK:Evaluate on a specific dataset version\nUse list_examples\nYou can use evaluate\n/ aevaluate\nto pass in an iterable of examples to evaluate on a particular version of a dataset. Use list_examples\n/ listExamples\nto fetch examples from a particular version tag using as_of\n/ asOf\nand pass that into the data\nargument.\nEvaluate on a split / filtered view of a dataset\nEvaluate on a filtered view of a dataset\nYou can use thelist_examples\n/ listExamples\nmethod to fetch a subset of examples from a dataset to evaluate on.\nOne common workflow is to fetch examples that have a certain metadata key-value pair.\nEvaluate on a dataset split\nYou can use thelist_examples\n/ listExamples\nmethod to evaluate on one or multiple splits of your dataset. The splits\nparameter takes a list of the splits you would like to evaluate.\nShare a dataset\nShare a dataset publicly\nSharing a dataset publicly will make the dataset examples, experiments and associated runs, and feedback on this dataset accessible to anyone with the link, even if they don\u2019t have a LangSmith account. Make sure you\u2019re not sharing sensitive information.This feature is only available in the cloud-hosted version of LangSmith.\nUnshare a dataset\n- Click on Unshare by clicking on Public in the upper right hand corner of any publicly shared dataset, then Unshare in the dialog.\n- Navigate to your organization\u2019s list of publicly shared datasets, by clicking on Settings -> Shared URLs or this link, then click on Unshare next to the dataset you want to unshare.", "tokens": 557, "node_type": "child"}
{"id": 138, "chunk_id": "c16acf8a553ac24f5d77e7d3a1c6e493", "parent_id": 119, "url": "", "namespace": "langchain", "title": "langsmith-manage-organization-by-api", "headers": ["langsmith-manage-organization-by-api"], "section_index": 0, "chunk_index": 0, "text": "# langsmith-manage-organization-by-api\n\n> Source: https://docs.langchain.com/langsmith/manage-organization-by-api\n\nThere are a few limitations that will be lifted soon:\n- The LangSmith SDKs do not support these organization management actions yet.\n- Organization-scoped service keys with Organization Admin permission may be used for these actions.\nUse the\nX-Tenant-Id\nheader to specify which workspace to target. If the header is not present, operations will default to the workspace the API key was initially created in if it is not organization-scoped.If X-Tenant-Id\nis not specified when accessing workspace-scoped resources with an organization-scoped API key, the request will fail with 403 Forbidden\n.X-Organization-Id\nheader should be present on all requests, and X-Tenant-Id\nheader should be present on requests that are scoped to a particular workspace.\nWorkspaces\nUser management\nRBAC\nMembership management\nList roles\nunder RBAC should be used for retrieving role IDs of these operations. List [organization|workspace] members\nendpoints (below) response \"id\"\ns should be used as identity_id\nin these operations.\nOrganization level:\n- List active organization members\n- List pending organization members\n- Invite a user to the organization and one or more workspaces. This should be used when the user is not already a member in the organization.\n- Update a user\u2019s organization role\n- Remove someone from the organization\n- List workspace members\n- Add a member to a workspace that is already part of the organization\n- Update a user\u2019s workspace role\n- Remove someone from a workspace\nAPI keys\nSecurity settings\nUpdating these settings affects all resources in the organization.\n- Update organization sharing settings\n- use\nunshare_all\nto unshare ALL shared resources in the organization - usedisable_public_sharing\nto prevent future sharing of resources\n- use\nUser-only endpoints\nThese endpoints are user-scoped and require a logged-in user\u2019s JWT, so they should only be executed through the UI./api-key/current\nendpoints: these are related a user\u2019s PATs/sso/email-verification/send\n(Cloud-only): this endpoint is related to SAML SSO\nSample code\nThe sample code below goes through a few common workflows related to organization management. Make sure to make necessary replacements wherever<replace_me>\nis in the code.", "tokens": 337, "node_type": "child"}
{"id": 139, "chunk_id": "c16acf8a553ac24f5d77e7d3a1c6e493", "parent_id": 120, "url": "", "namespace": "langchain", "title": "langsmith-manage-prompts-programmatically", "headers": ["langsmith-manage-prompts-programmatically"], "section_index": 0, "chunk_index": 0, "text": "# langsmith-manage-prompts-programmatically\n\n> Source: https://docs.langchain.com/langsmith/manage-prompts-programmatically\n\nPreviously this functionality lived in the\nlangchainhub\npackage which is now deprecated. All functionality going forward will live in the langsmith\npackage.Install packages\nIn Python, you can directly use the LangSmith SDK (recommended, full functionality) or you can use through the LangChain package (limited to pushing and pulling prompts). In TypeScript, you must use the LangChain npm package for pulling prompts (it also allows pushing). For all other functionality, use the LangSmith package.Configure environment variables\nIf you already haveLANGSMITH_API_KEY\nset to your current workspace\u2019s api key from LangSmith, you can skip this step.\nOtherwise, get an API key for your workspace by navigating to Settings > API Keys > Create API Key\nin LangSmith.\nSet your environment variable.\nWhat we refer to as \u201cprompts\u201d used to be called \u201crepos\u201d, so any references to \u201crepo\u201d in the code are referring to a prompt.\nPush a prompt\nTo create a new prompt or update an existing prompt, you can use thepush prompt\nmethod.\nPull a prompt\nTo pull a prompt, you can use thepull prompt\nmethod, which returns a the prompt as a langchain PromptTemplate\n.\nTo pull a private prompt you do not need to specify the owner handle (though you can, if you have one set).\nTo pull a public prompt from the LangChain Hub, you need to specify the handle of the prompt\u2019s author.\nFor pulling prompts, if you are using Node.js or an environment that supports dynamic imports, we recommend using the\nlangchain/hub/node\nentrypoint, as it handles deserialization of models associated with your prompt configuration automatically.If you are in a non-Node environment, \u201cincludeModel\u201d is not supported for non-OpenAI models and you should use the base langchain/hub\nentrypoint.Use a prompt without LangChain\nIf you want to store your prompts in LangSmith but use them directly with a model provider\u2019s API, you can use our conversion methods. These convert your prompt into the payload required for the OpenAI or Anthropic API. These conversion methods rely on logic from within LangChain integration packages, and you will need to install the appropriate package as a dependency in addition to your official SDK of choice. Here are some examples:OpenAI\nAnthropic\nList, delete, and like prompts\nYou can also list, delete, and like/unlike prompts using thelist prompts\n, delete prompt\n, like prompt\nand unlike prompt\nmethods. See the LangSmith SDK client for extensive documentation on these methods.", "tokens": 399, "node_type": "child"}
{"id": 140, "chunk_id": "c16acf8a553ac24f5d77e7d3a1c6e493", "parent_id": 121, "url": "", "namespace": "langchain", "title": "langsmith-manage-prompts", "headers": ["langsmith-manage-prompts"], "section_index": 0, "chunk_index": 0, "text": "# langsmith-manage-prompts\n\n> Source: https://docs.langchain.com/langsmith/manage-prompts\n\n- Commit tags for version control and environment management.\n- Webhook triggers for automating workflows when prompts are updated.\n- Public prompt hub for discovering and using community-created prompts.\nCommit tags\nCommit tags are labels that reference a specific commit in your prompt\u2019s version history. They help you mark significant versions and control which versions run in different environments. By referencing tags rather than commit IDs in your code, you can update which version is being used without modifying the code itself. Each tag references exactly one commit, though you can reassign a tag to point to a different commit. Use cases for commit tags can include:- Environment-specific tags: Mark commits for\nproduction\norstaging\nenvironments, which allows you to switch between different versions without changing your code. - Version control: Mark stable versions of your prompts, for example,\nv1\n,v2\n, which lets you reference specific versions in your code and track changes over time. - Collaboration: Mark versions ready for review, which enables you to share specific versions with collaborators and get feedback.\nCreate a tag\nTo create a tag, navigate to the Commits tab for a prompt. Click on the tag icon next to the commit you want to tag. Click New Tag and enter a name for the tag.Move a tag\nTo point a tag to a different commit, click on the tag icon next to the destination commit, and select the tag you want to move. This will automatically update the tag to point to the new commit.Delete a tag\nTo delete a tag, click on the delete icon next to the tag you want to delete. This will delete the tag altogether and it will no longer be associated with any commit.Use tags in code\nTags provide a stable way to reference specific versions of your prompts in code. Instead of using commit hashes directly, you can reference tags that can be updated without changing your code. Here is an example of pulling a prompt by tag in Python:Trigger a webhook on prompt commit\nYou can configure a webhook to be triggered whenever a commit is made to a prompt. Some common use cases of this include:- Triggering a CI/CD pipeline when prompts are updated.\n- Synchronizing prompts with a GitHub repository.\n- Notifying team members about prompt modifications.\nConfigure a webhook\nNavigate to the Prompts section in the left-hand sidebar or from the application homepage. In the top right corner, click on the+ Webhook\nbutton.\nAdd a webhook URL and any required headers.\nTo test out your webhook, click the Send test notification button. This will send a test notification to the webhook URL you provided with a sample payload.\nThe sample payload is a JSON object with the following fields:\nprompt_id\n: The ID of the prompt that was committed.prompt_name\n: The name of the prompt that was committed.commit_hash\n: The commit hash of the prompt.created_at\n: The date of the commit.created_by\n: The author of the commit.manifest\n: The manifest of the prompt.\nTrigger the webhook\nCommit to a prompt to trigger the webhook you\u2019ve configured.Use the Playground\nIf you do this in the Playground, you\u2019ll be prompted to deselect the webhooks you\u2019d like to avoid triggering.Using the API\nIf you commit via the API, you can specify to skip triggering the webhook by setting theskip_webhooks\nparameter to true\nor to an array of webhook ids to ignore. Refer to the API docs for more information.\nPublic prompt hub\nLangSmith\u2019s public prompt hub is a collection of prompts that have been created by the LangChain community that you can use for reference.\nNavigate to the Prompts section of the left-hand sidebar and click on Browse all Public Prompts in the LangChain Hub.\nHere you\u2019ll find all of the publicly listed prompts in the LangChain Hub. You can search for prompts by name, handle, use cases, descriptions, or models. You can fork prompts to your personal organization, view the prompt\u2019s details, and run the prompt in the Playground. You can pull any public prompt into your code using the SDK.\nTo view prompts tied to your workspace, visit the Prompts tab in the sidebar.", "tokens": 696, "node_type": "child"}
{"id": 141, "chunk_id": "c16acf8a553ac24f5d77e7d3a1c6e493", "parent_id": 122, "url": "", "namespace": "langchain", "title": "langsmith-managing-model-configurations", "headers": ["langsmith-managing-model-configurations"], "section_index": 0, "chunk_index": 0, "text": "# langsmith-managing-model-configurations\n\n> Source: https://docs.langchain.com/langsmith/managing-model-configurations\n\nModel configurations define the parameters your prompt runs against. In the LangSmith Playground, you can save and manage these configurations, which allows you to reuse your preferred settings across prompts and sessions. For details on specific settings, refer to your model provider\u2019s documentation (for example, Anthropic, OpenAI).\nIn the Model Configurations tab, adjust the model configuration as needed\u2014you can select a saved configuration to edit.\nClick the Save As button in the top bar.\nEnter a name and optional description for your configuration and confirm.\nNow that you\u2019ve saved the configuration, anyone in your organization\u2019s workspace can access it. All saved configurations are available in the Model Configuration dropdown.\nOnce you have created a saved configuration, you can set it as your default, so any new prompt you create will automatically use this configuration. To set a configuration as your default, click the Set as default icon next to the model name in the dropdown.\nThe Extra Parameters field allows you to pass additional model parameters that aren\u2019t directly supported in the LangSmith interface. This is particularly useful in two scenarios:\nWhen model providers release new parameters that haven\u2019t yet been integrated into the LangSmith interface. You can specify these parameters in JSON format to use them right away. For example:\nCopy\n{ \"reasoning_effort\": \"medium\"}\nWhen troubleshooting parameter-related errors in the playground, such as:\nCopy\nTypeError: AsyncCompletions.create() got an unexpected keyword argument 'max_concurrency'\nIf you receive an error about unnecessary parameters (which is more common when using LangChain JS for run tracing), you can use this field to remove the extra parameters.\nTools enable your LLM to perform tasks like searching the web, looking up information, and so on. In the Tools Settings tab, you can manage the ways your LLM uses and accesses the tools you have defined in your prompt, including:\nParallel Tool Calls: Calling multiple tools in parallel when appropriate. This allows the model to gather information from different sources simultaneously. (Dependent on model support for parallel execution.)\nTool Choice: Select the tools that the model can access. For more details, refer to Use tools in a prompt.", "tokens": 356, "node_type": "child"}
{"id": 142, "chunk_id": "c16acf8a553ac24f5d77e7d3a1c6e493", "parent_id": 123, "url": "", "namespace": "langchain", "title": "langsmith-mask-inputs-outputs", "headers": ["langsmith-mask-inputs-outputs"], "section_index": 0, "chunk_index": 0, "text": "# langsmith-mask-inputs-outputs\n\n> Source: https://docs.langchain.com/langsmith/mask-inputs-outputs\n\nClient\ninstance. This can be done by setting the hide_inputs\nand hide_outputs\nparameters on the Client\nobject (hideInputs\nand hideOutputs\nin TypeScript).\nFor the example below, we will simply return an empty object for both hide_inputs\nand hide_outputs\n, but you can customize this to your needs.\nRule-based masking of inputs and outputs\nThis feature is available in the following LangSmith SDK versions:\n- Python: 0.1.81 and above\n- TypeScript: 0.1.33 and above\ncreate_anonymizer\n/ createAnonymizer\nfunction and pass the newly created anonymizer when instantiating the client. The anonymizer can be either constructed from a list of regex patterns and the replacement values or from a function that accepts and returns a string value.\nThe anonymizer will be skipped for inputs if LANGSMITH_HIDE_INPUTS = true\n. Same applies for outputs if LANGSMITH_HIDE_OUTPUTS = true\n.\nHowever, if inputs or outputs are to be sent to client, the anonymizer\nmethod will take precedence over functions found in hide_inputs\nand hide_outputs\n. By default, the create_anonymizer\nwill only look at maximum of 10 nesting levels deep, which can be configured via the max_depth\nparameter.\nOlder versions of LangSmith SDKs can use the\nhide_inputs\nand hide_outputs\nparameters to achieve the same effect. You can also use these parameters to process the inputs and outputs more efficiently as well.\nProcessing Inputs & Outputs for a Single Function\nThe\nprocess_outputs\nparameter is available in LangSmith SDK version 0.1.98 and above for Python.process_inputs\nand process_outputs\nparameters of the @traceable\ndecorator.\nThese parameters accept functions that allow you to transform the inputs and outputs of a specific function before they are logged to LangSmith. This is useful for reducing payload size, removing sensitive information, or customizing how an object should be serialized and represented in LangSmith for a particular function.\nHere\u2019s an example of how to use process_inputs\nand process_outputs\n:\nprocess_inputs\ncreates a new dictionary with processed input data, and process_outputs\ntransforms the output into a specific format before logging to LangSmith.\nIt\u2019s recommended to avoid mutating the source objects in the processor functions. Instead, create and return new objects with the processed data.\nhide_inputs\nand hide_outputs\n) when both are defined.\nQuick starts\nYou can combine rule-based masking with various anonymizers to scrub sensitive information from inputs and outputs. In this how-to-guide, we\u2019ll cover working with regex, Microsoft Presidio, and Amazon Comprehend.Regex\nThe implementation below is not exhaustive and may miss some formats or edge cases. Test any implementation thoroughly before using it in production.\nMicrosoft Presidio\nThe implementation below provides a general example of how to anonymize sensitive information in messages exchanged between a user and an LLM. It is not exhaustive and does not account for all cases. Test any implementation thoroughly before using it in production.\nAmazon Comprehend\nThe implementation below provides a general example of how to anonymize sensitive information in messages exchanged between a user and an LLM. It is not exhaustive and does not account for all cases. Test any implementation thoroughly before using it in production.", "tokens": 503, "node_type": "child"}
{"id": 143, "chunk_id": "c16acf8a553ac24f5d77e7d3a1c6e493", "parent_id": 124, "url": "", "namespace": "langchain", "title": "langsmith-metric-type", "headers": ["langsmith-metric-type"], "section_index": 0, "chunk_index": 0, "text": "# langsmith-metric-type\n\n> Source: https://docs.langchain.com/langsmith/metric-type\n\nLangSmith supports both categorical and numerical metrics, and you can return either when writing a custom evaluator.For an evaluator result to be logged as a numerical metric, it must returned as:\n(Python only) an int, float, or bool\na dict of the form {\"key\": \"metric_name\", \"score\": int | float | bool}\nFor an evaluator result to be logged as a categorical metric, it must be returned as:\n(Python only) a str\na dict of the form {\"key\": \"metric_name\", \"value\": str | int | float | bool}\nHere are some examples:\nPython: Requires langsmith>=0.2.0\nTypeScript: Support for multiple scores is available in langsmith@0.1.32 and higher", "tokens": 109, "node_type": "child"}
{"id": 144, "chunk_id": "c16acf8a553ac24f5d77e7d3a1c6e493", "parent_id": 125, "url": "", "namespace": "langchain", "title": "langsmith-monorepo-support", "headers": ["langsmith-monorepo-support"], "section_index": 0, "chunk_index": 0, "text": "# langsmith-monorepo-support\n\n> Source: https://docs.langchain.com/langsmith/monorepo-support\n\nLangSmith supports deploying agents from monorepo setups where your agent code may depend on shared packages located elsewhere in the repository. This guide shows how to structure your monorepo and configure your langgraph.json file to work with shared dependencies.\nKeep agent configs in agent directories: Place langgraph.json files in the specific agent directories, not at the monorepo root. This allows you to support multiple agents in the same monorepo, without having to deploy them all in the same LangGraph platform deployment.\nUse relative paths for Python: For Python monorepos, use relative paths like \"../../shared-package\" in the dependencies array.\nLeverage workspace features for JS: For JavaScript/TypeScript, use your package manager\u2019s workspace features to manage dependencies between packages.\nTest locally first: Always test your build locally before deploying to ensure all dependencies are correctly resolved.\nEnvironment variables: Keep environment files (.env) in your agent directories for environment-specific configuration.", "tokens": 151, "node_type": "child"}
{"id": 145, "chunk_id": "c16acf8a553ac24f5d77e7d3a1c6e493", "parent_id": 126, "url": "", "namespace": "langchain", "title": "langsmith-multi-turn-simulation", "headers": ["langsmith-multi-turn-simulation"], "section_index": 0, "chunk_index": 0, "text": "# langsmith-multi-turn-simulation\n\n> Source: https://docs.langchain.com/langsmith/multi-turn-simulation\n\n- Ease of getting started vs. an evaluation over a full dataset of pre-existing trajectories\n- End-to-end coverage from an initial query until a successful or unsuccessful resolution\n- The ability to detect repetitive behavior or context loss over several iterations of your app\nopenevals\npackage, which contains prebuilt evaluators and other convenient resources for evaluating your AI apps. It will also use OpenAI models, though you can use other providers as well.\nSetup\nFirst, ensure you have the required dependencies installed:If you are using\nyarn\nas your package manager, you will also need to manually install @langchain/core\nas a peer dependency of openevals\n. This is not required for LangSmith evals in general.Running a simulation\nThere are two primary components you\u2019ll need to get started:app\n: Your application, or a function wrapping it. Must accept a single chat message (dict with \u201crole\u201d and \u201ccontent\u201d keys) as an input arg and athread_id\nas a kwarg. Should accept other kwargs as more may be added in future releases. Returns a chat message as output with at least role and content keys.user\n: The simulated user. In this guide, we will use an imported prebuilt function namedcreate_llm_simulated_user\nwhich uses an LLM to generate user responses, though you can create your own too.\nopenevals\npasses a single chat message to your app\nfrom the user\nfor each turn. Therefore you should statefully track the current history internally based on thread_id\nif needed.\nHere\u2019s an example that simulates a multi-turn customer support interaction. This guide uses a simple chat app that wraps a single call to the OpenAI chat completions API, however this is where you would call your application or agent. In this example, our simulated user is playing the role of a particularly aggressive customer:\nuser\n, then passes response chat messages back and forth until it reaches max_turns\n(you can alternatively pass a stopping_condition\nthat takes the current trajectory and returns True\nor False\n- see the OpenEvals README for more information). The return value is the final list of chat messages that make up the converation\u2019s trajectory.\nThere are several ways to configure the simulated user, such as having it return fixed responses for the first turns of your simulation, as well as the simulation as a whole. For full details, check out the OpenEvals README.\napp\nand user\ninterleaved:\nCongrats! You just ran your first multi-turn simulation. Next, we\u2019ll cover how to run it in a LangSmith experiment.\nRunning in LangSmith experiments\nYou can use the results of multi-turn simulations as part of a LangSmith experiment to track performance and progress over time. For these sections, it helps to be familiar with at least one of LangSmith\u2019spytest\n(Python-only), Vitest\n/Jest\n(JS only), or evaluate\nrunners.\nUsing pytest\nor Vitest/Jest\nIf you are using one of the LangSmith test framework integrations, you can pass in an array of OpenEvals evaluators as a\ntrajectory_evaluators\nparam when running the simulation. These evaluators will run at the end of the simulation, taking the final list of chat messages as an outputs\nkwarg. Your passed trajectory_evaluator\nmust therefore accept this kwarg.\nHere\u2019s an example:\ntrajectory_evaluators\n, adding it to the experiment. Note also that the test case uses the fixed_responses\nparam on the simulated user to start the conversation with a specific input, which you can log and make part of your stored dataset.\nYou may also find it convenient to have the simulated user\u2019s system prompt to be part of your logged dataset as well.\nUsing evaluate\nYou can also use the evaluate\nrunner to evaluate simulated multi-turn interactions. This will be a little bit different from the pytest\n/Vitest\n/Jest\nexample in the following ways:\n- The simulation should be part of your\ntarget\nfunction, and your target function should return the final trajectory.- This will make the trajectory the\noutputs\nthat LangSmith will pass to your evaluators.\n- This will make the trajectory the\n- Instead of using the\ntrajectory_evaluators\nparam, you should pass your evaluators as a param into theevaluate()\nmethod. - You will need an existing dataset of inputs and (optionally) reference trajectories.\nModifying the simulated user persona\nThe above examples run using the same simulated user persona for all input examples, defined by thesystem\nparameter passed into create_llm_simulated_user\n. If you would like to use a different persona for specific items in your dataset, you can update your dataset examples to also contain an extra field with the desired system\nprompt, then pass that field in when creating your simulated user like this:\nNext Steps\nYou\u2019ve just seen some techniques for simulating multi-turn interactions and running them in LangSmith evals. Here are some topics you might want to explore next:- Trace multiturn conversations across different traces\n- Use multiple messages in the playground UI\n- Return multiple metrics in one evaluator", "tokens": 808, "node_type": "child"}
{"id": 146, "chunk_id": "c16acf8a553ac24f5d77e7d3a1c6e493", "parent_id": 127, "url": "", "namespace": "langchain", "title": "langsmith-multimodal-content", "headers": ["langsmith-multimodal-content"], "section_index": 0, "chunk_index": 0, "text": "# langsmith-multimodal-content\n\n> Source: https://docs.langchain.com/langsmith/multimodal-content\n\n- Inline content: Embed static files (images, PDFs, audio) directly in your prompt. This is ideal when you want to consistently include the same multimodal content across all uses of the prompt. For example, you might include a reference image that helps ground the model\u2019s responses.\n-\nTemplate variables: Create dynamic placeholders for attachments that can be populated with different content each time. This approach offers more flexibility, allowing you to:\n- Test how the model handles different inputs\n- Create reusable prompts that work with varying content\nNot all models support multimodal content. Before using multimodal features in the playground, make sure your selected model supports the file types you want to use.\nInline content\nClick the file icon in the message where you want to add multimodal content. Under theUpload content\ntab, you can upload a file and include it inline in the prompt.\nTemplate variables\nClick the file icon in the message where you want to add multimodal content. Under theTemplate variables\ntab, you can create a template variable for a specific attachment type. Currently, only images, PDFs, and audio files (.wav, .mp3) are supported.\nPopulate the template variable\nOnce you\u2019ve added a template variable, you can provide content for it using the panel on the right side of the screen. Simply click the+\nbutton to upload or select content that will be used to populate the template variable.", "tokens": 237, "node_type": "child"}
{"id": 147, "chunk_id": "c16acf8a553ac24f5d77e7d3a1c6e493", "parent_id": 128, "url": "", "namespace": "langchain", "title": "langsmith-multiple-messages", "headers": ["langsmith-multiple-messages"], "section_index": 0, "chunk_index": 0, "text": "# langsmith-multiple-messages\n\n> Source: https://docs.langchain.com/langsmith/multiple-messages\n\nThis how-to guide walks you through the various ways you can set up the playground for multi-turn conversations, which will allow you to test different tool configurations and system prompts against longer threads of messages.\nFirst, ensure you have properly traced a multi-turn conversation, and then navigate to your tracing project. Once you get to your tracing project simply open the run, select the LLM call, and open it in the playground as follows:You can then edit the system prompt, tweak the tools and/or output schema and observe how the output of the multi-turn conversation changes.\nBefore starting, make sure you have set up your dataset. Since you want to evaluate multi-turn conversations, make sure there is a key in your inputs that contains a list of messages.Once you have created your dataset, head to the playground and load your dataset to evaluate.Then, add a messages list variable to your prompt, making sure to name it the same as the key in your inputs that contains the list of messages:When you run your prompt, the messages from each example will be added as a list in place of the \u2018Messages List\u2019 variable.\nThere are two ways to manually create multi-turn conversations. The first way is by simply appending messages to the prompt:This is helpful for quick iteration, but is rigid since the multi-turn conversation is hardcoded. Instead, if you want your prompt to work with any multi-turn conversation you can add a \u2018Messages List\u2019 variable and add your multi-turn conversation there:This allows you to just tweak the system prompt or the tools, while allowing any multi-turn conversation to take the place of the Messages List variable, allowing you to reuse this prompt across various runs.\nNow that you know how to set up the playground for multi-turn interactions, you can either manually inspect and judge the outputs, or you can add evaluators to classify results.You can also read these how-to guides to learn more about how to use the playground to run evaluations.", "tokens": 338, "node_type": "child"}
{"id": 148, "chunk_id": "c16acf8a553ac24f5d77e7d3a1c6e493", "parent_id": 129, "url": "", "namespace": "langchain", "title": "langsmith-multiple-scores", "headers": ["langsmith-multiple-scores"], "section_index": 0, "chunk_index": 0, "text": "# langsmith-multiple-scores\n\n> Source: https://docs.langchain.com/langsmith/multiple-scores\n\nSometimes it is useful for a custom evaluator or summary evaluator to return multiple metrics. For example, if you have multiple metrics being generated by an LLM judge, you can save time and money by making a single LLM call that generates multiple metrics instead of making multiple LLM calls.To return multiple scores using the Python SDK, simply return a list of dictionaries/objects of the following form:\nCopy\n[ # 'key' is the metric name # 'score' is the value of a numerical metric {\"key\": string, \"score\": number}, # 'value' is the value of a categorical metric {\"key\": string, \"value\": string}, ... # You may log as many as you wish]\nTo do so with the JS/TS SDK, return an object with a \u2018results\u2019 key and then a list of the above form\nCopy\n{results: [{ key: string, score: number }, ...]};\nEach of these dictionaries can contain any or all of the feedback fields; check out the linked document for more information.Example:\nPython: Requires langsmith>=0.2.0\nTypeScript: Support for multiple scores is available in langsmith@0.1.32 and higher", "tokens": 182, "node_type": "child"}
{"id": 149, "chunk_id": "c16acf8a553ac24f5d77e7d3a1c6e493", "parent_id": 130, "url": "", "namespace": "langchain", "title": "langsmith-nest-traces", "headers": ["langsmith-nest-traces"], "section_index": 0, "chunk_index": 0, "text": "# langsmith-nest-traces\n\n> Source: https://docs.langchain.com/langsmith/nest-traces\n\nWhen tracing with the LangSmith SDK, LangGraph, and LangChain, tracing should automatically propagate the correct context so that code executed within a parent trace will be rendered in the expected location in the UI.If you see a child run go to a separate trace (and appear on the top level), it may be caused by one of the following known \u201cedge cases\u201d.\nWhen using async calls (especially with streaming) in Python versions < 3.11, you may encounter issues with trace nesting. This is because Python\u2019s asyncio only added full support for passing context in version 3.11.\nLangChain and LangSmith SDK use contextvars to propagate tracing information implicitly. In Python 3.11 and above, this works seamlessly. However, in earlier versions (3.8, 3.9, 3.10), asyncio tasks lack proper contextvar support, which can lead to disconnected traces.\nUpgrade Python Version (Recommended) If possible, upgrade to Python 3.11 or later for automatic context propagation.\nManual Context Propagation If upgrading isn\u2019t an option, you\u2019ll need to manually propagate the tracing context. The method varies depending on your setup:a) Using LangGraph or LangChain Pass the parent config to the child call:\nCopy\nimport asynciofrom langchain_core.runnables import RunnableConfig, RunnableLambda@RunnableLambdaasync def my_child_runnable( inputs: str, # The config arg (present in parent_runnable below) is optional): yield \"A\" yield \"response\"@RunnableLambdaasync def parent_runnable(inputs: str, config: RunnableConfig): async for chunk in my_child_runnable.astream(inputs, config): yield chunkasync def main(): return [val async for val in parent_runnable.astream(\"call\")]asyncio.run(main())\nb) Using LangSmith Directly Pass the run tree directly:\nCopy\nimport asyncioimport langsmith as ls@ls.traceableasync def my_child_function(inputs: str): yield \"A\" yield \"response\"@ls.traceableasync def parent_function( inputs: str, # The run tree can be auto-populated by the decorator run_tree: ls.RunTree,): async for chunk in my_child_function(inputs, langsmith_extra={\"parent\": run_tree}): yield chunkasync def main(): return [val async for val in parent_function(\"call\")]asyncio.run(main())\nc) Combining Decorated Code with LangGraph/LangChain Use a combination of techniques for manual handoff:\nCopy\nimport asyncioimport langsmith as lsfrom langchain_core.runnables import RunnableConfig, RunnableLambda@RunnableLambdaasync def my_child_runnable(inputs: str): yield \"A\" yield \"response\"@ls.traceableasync def my_child_function(inputs: str, run_tree: ls.RunTree): with ls.tracing_context(parent=run_tree): async for chunk in my_child_runnable.astream(inputs): yield chunk@RunnableLambdaasync def parent_runnable(inputs: str, config: RunnableConfig): # @traceable decorated functions can directly accept a RunnableConfig when passed in via \"config\" async for chunk in my_child_function(inputs, langsmith_extra={\"config\": config}): yield chunk@ls.traceableasync def parent_function(inputs: str, run_tree: ls.RunTree): # You can set the tracing context manually with ls.tracing_context(parent=run_tree): async for chunk in parent_runnable.astream(inputs): yield chunkasync def main(): return [val async for val in parent_function(\"call\")]asyncio.run(main())\nIt\u2019s common to start tracing and want to apply some parallelism on child tasks all within a single trace. Python\u2019s stdlib ThreadPoolExecutor by default breaks tracing.\nUsing LangSmith\u2019s ContextThreadPoolExecutorLangSmith provides a ContextThreadPoolExecutor that automatically handles context propagation:\nCopy\nfrom langsmith.utils import ContextThreadPoolExecutorfrom langsmith import traceable@traceabledef outer_func(): with ContextThreadPoolExecutor() as executor: inputs = [1, 2] r = list(executor.map(inner_func, inputs))@traceabledef inner_func(x): print(x)outer_func()\nManually providing the parent run treeAlternatively, you can manually pass the parent run tree to the inner function:\nCopy\nfrom langsmith import traceable, get_current_run_treefrom concurrent.futures import ThreadPoolExecutor@traceabledef outer_func(): rt = get_current_run_tree() with ThreadPoolExecutor() as executor: r = list( executor.map( lambda x: inner_func(x, langsmith_extra={\"parent\": rt}), [1, 2] ) )@traceabledef inner_func(x): print(x)outer_func()\nIn this approach, we use get_current_run_tree() to obtain the current run tree and pass it to the inner function using the langsmith_extra parameter.Both methods ensure that the inner function calls are correctly aggregated under the initial trace stack, even when executed in separate threads.", "tokens": 551, "node_type": "child"}
{"id": 150, "chunk_id": "c16acf8a553ac24f5d77e7d3a1c6e493", "parent_id": 131, "url": "", "namespace": "langchain", "title": "langsmith-observability-concepts", "headers": ["langsmith-observability-concepts"], "section_index": 0, "chunk_index": 0, "text": "# langsmith-observability-concepts\n\n> Source: https://docs.langchain.com/langsmith/observability-concepts\n\nRuns\nA run is a span representing a single unit of work or operation within your LLM application. This could be anything from a single call to an LLM or chain, to a prompt formatting call, to a runnable lambda invocation. If you are familiar with OpenTelemetry, you can think of a run as a span.Traces\nA trace is a collection of runs for a single operation. For example, if you have a user request that triggers a chain, and that chain makes a call to an LLM, then to an output parser, and so on, all of these runs would be part of the same trace. If you are familiar with OpenTelemetry, you can think of a LangSmith trace as a collection of spans. Runs are bound to a trace by a unique trace ID.Projects\nA project is a collection of traces. You can think of a project as a container for all the traces that are related to a single application or service. You can have multiple projects, and each project can have multiple traces.Feedback\nFeedback allows you to score an individual run based on certain criteria. Each feedback entry consists of a feedback tag and feedback score, and is bound to a run by a unique run ID. Feedback can be continuous or discrete (categorical), and you can reuse feedback tags across different runs within an organization. You can collect feedback on runs in a number of ways:- Sent up along with a trace from the LLM application.\n- Generated by a user in the app inline or in an annotation queue.\n- Generated by an automatic evaluator during offline evaluation.\n- Generated by an online evaluator.\nTags\nTags are collections of strings that can be attached to runs. You can use tags to do the following in the LangSmith UI:- Categorize runs for easier search.\n- Filter runs.\n- Group runs together for analysis.\nMetadata\nMetadata is a collection of key-value pairs that you can attach to runs. You can use metadata to store additional information about a run, such as the version of the application that generated the run, the environment in which the run was generated, or any other information that you want to associate with a run. Similarly to tags, you can use metadata to filter runs in the LangSmith UI or group runs together for analysis. Learn how to add metadata to your traces.Data storage and retention\nFor traces ingested on or after Wednesday, May 22, 2024, LangSmith (SaaS) retains trace data for a maximum of 400 days past the date and time the trace was inserted into the LangSmith trace database. After 400 days, the traces are permanently deleted from LangSmith, with a limited amount of metadata retained for the purpose of showing accurate statistics, such as historic usage and cost.If you wish to keep tracing data longer than the data retention period, you can add it to a dataset. A dataset allows you to store the trace inputs and outputs (e.g., as a key-value dataset), and will persist indefinitely, even after the trace gets deleted.\nDeleting traces from LangSmith\nIf you need to remove a trace from LangSmith before its expiration date, you can do so by deleting the project that contains it. You can delete a project with one of the following ways:- In the LangSmith UI, select the Delete option on the project\u2019s overflow menu.\n- With the\ndelete_tracer_sessions\nAPI endpoint - With the\ndelete_project()\n(Python) ordeleteProject()\n(JS/TS) in the LangSmith SDK.", "tokens": 589, "node_type": "child"}
{"id": 151, "chunk_id": "c16acf8a553ac24f5d77e7d3a1c6e493", "parent_id": 132, "url": "", "namespace": "langchain", "title": "langsmith-observability-llm-tutorial", "headers": ["langsmith-observability-llm-tutorial"], "section_index": 0, "chunk_index": 0, "text": "# langsmith-observability-llm-tutorial\n\n> Source: https://docs.langchain.com/langsmith/observability-llm-tutorial\n\nPrototyping\nHaving observability set up from the start can help you iterate much more quickly than you would otherwise be able to. It allows you to have great visibility into your application as you are rapidly iterating on the prompt, or changing the data and models you are using. In this section we\u2019ll walk through how to set up observability so you can have maximal observability as you are prototyping.Set up your environment\nFirst, create an API key by navigating to the settings page. Next, install the LangSmith SDK:default\nproject (though you can easily change that).\nYou may see these variables referenced as\nLANGCHAIN_*\nin other places. These are all equivalent, however the best practice is to use LANGSMITH_TRACING\n, LANGSMITH_API_KEY\n, LANGSMITH_PROJECT\n.The LANGSMITH_PROJECT\nflag is only supported in JS SDK versions >= 0.2.16, use LANGCHAIN_PROJECT\ninstead if you are using an older version.Trace your LLM calls\nThe first thing you might want to trace is all your OpenAI calls. After all, this is where the LLM is actually being called, so it is the most important part! We\u2019ve tried to make this as easy as possible with LangSmith by introducing a dead-simple OpenAI wrapper. All you have to do is modify your code to look something like:from langsmith.wrappers import wrap_openai\nand use it to wrap the OpenAI client (openai_client = wrap_openai(OpenAI())\n).\nWhat happens if you call it in the following way?\nTrace the whole chain\nGreat - we\u2019ve traced the LLM call. But it\u2019s often very informative to trace more than that. LangSmith is built for tracing the entire LLM pipeline - so let\u2019s do that! We can do this by modifying the code to now look something like this:from langsmith import traceable\nand use it decorate the overall function (@traceable\n).\nWhat happens if you call it in the following way?\nBeta Testing\nThe next stage of LLM application development is beta testing your application. This is when you release it to a few initial users. Having good observability set up here is crucial as often you don\u2019t know exactly how users will actually use your application, so this allows you get insights into how they do so. This also means that you probably want to make some changes to your tracing set up to better allow for that. This extends the observability you set up in the previous sectionCollecting Feedback\nA huge part of having good observability during beta testing is collecting feedback. What feedback you collect is often application specific - but at the very least a simple thumbs up/down is a good start. After logging that feedback, you need to be able to easily associate it with the run that caused that. Luckily LangSmith makes it easy to do that. First, you need to log the feedback from your app. An easy way to do this is to keep track of a run ID for each run, and then use that to log feedback. Keeping track of the run ID would look something like:Metadata\ntab when inspecting the run. It should look something like this\nYou can also query for all runs with positive (or negative) feedback by using the filtering logic in the runs table. You can do this by creating a filter like the following:\nLogging Metadata\nIt is also a good idea to start logging metadata. This allows you to start keep track of different attributes of your app. This is important in allowing you to know what version or variant of your app was used to produce a given result. For this example, we will log the LLM used. Oftentimes you may be experimenting with different LLMs, so having that information as metadata can be useful for filtering. In order to do that, we can add it as such:@traceable(metadata={\"llm\": \"gpt-4o-mini\"})\nto the rag\nfunction.\nKeeping track of metadata in this way assumes that it is known ahead of time. This is fine for LLM types, but less desirable for other types of information - like a User ID. In order to log information that, we can pass it in at run time with the run ID.\nProduction\nGreat - you\u2019ve used this newfound observability to iterate quickly and gain confidence that your app is performing well. Time to ship it to production! What new observability do you need to add? First of all, let\u2019s note that the same observability you\u2019ve already added will keep on providing value in production. You will continue to be able to drill down into particular runs. In production you likely have a LOT more traffic. So you don\u2019t really want to be stuck looking at datapoints one at a time. Luckily, LangSmith has a set of tools to help with observability in production.Monitoring\nIf you click on theMonitor\ntab in a project, you will see a series of monitoring charts. Here we track lots of LLM specific statistics - number of traces, feedback, time-to-first-token, etc. You can view these over time across a few different time bins.\nA/B Testing\nGroup-by functionality for A/B testing requires at least 2 different values to exist for a given metadata key.\nllm\n. We can group the monitoring charts by ANY metadata attribute, and instantly get grouped charts over time. This allows us to experiment with different LLMs (or prompts, or other) and track their performance over time.\nIn order to do this, we just need to click on the Metadata\nbutton at the top. This will give us a drop down of options to choose from to group by:\nOnce we select this, we will start to see charts grouped by this attribute:", "tokens": 941, "node_type": "child"}
{"id": 152, "chunk_id": "c16acf8a553ac24f5d77e7d3a1c6e493", "parent_id": 133, "url": "", "namespace": "langchain", "title": "langsmith-observability-quickstart", "headers": ["langsmith-observability-quickstart"], "section_index": 0, "chunk_index": 0, "text": "# langsmith-observability-quickstart\n\n> Source: https://docs.langchain.com/langsmith/observability-quickstart\n\nObservability is a critical requirement for applications built with large language models (LLMs). LLMs are non-deterministic, which means that the same prompt can produce different responses. This behavior makes debugging and monitoring more challenging than with traditional software.LangSmith addresses this by providing end-to-end visibility into how your application handles a request. Each request generates a trace, which captures the full record of what happened. Within a trace are individual runs, the specific operations your application performed, such as an LLM call or a retrieval step. Tracing runs allows you to inspect, debug, and validate your application\u2019s behavior.In this quickstart, you will set up a minimal Retrieval Augmented Generation (RAG) application and add tracing with LangSmith. You will:\nConfigure your environment.\nCreate an application that retrieves context and calls an LLM.\nEnable tracing to capture both the retrieval step and the LLM call.\nView the resulting traces in the LangSmith UI.\nIf you prefer to watch a video on getting started with tracing, refer to the quickstart Video guide.\nThe example app in this quickstart will use OpenAI as the LLM provider. You can adapt the example for your app\u2019s LLM provider.\nIf you\u2019re building an application with LangChain or LangGraph, you can enable LangSmith tracing with a single environment variable. Get started by reading the guides for tracing with LangChain or tracing with LangGraph.\nYou can use the example app code outlined in this step to instrument a RAG application. Or, you can use your own application code that includes an LLM call.This is a minimal RAG app that uses the OpenAI SDK directly without any LangSmith tracing added yet. It has three main parts:\nRetriever function: Simulates document retrieval that always returns the same string.\nOpenAI client: Instantiates a plain OpenAI client to send a chat completion request.\nRAG function: Combines the retrieved documents with the user\u2019s question to form a system prompt, calls the chat.completions.create() endpoint with gpt-4o-mini, and returns the assistant\u2019s response.\nAdd the following code into your app file (e.g., app.py or app.ts):\nCopy\nfrom openai import OpenAIdef retriever(query: str): # Minimal example retriever return [\"Harrison worked at Kensho\"]# OpenAI client call (no wrapping yet)client = OpenAI()def rag(question: str) -> str: docs = retriever(question) system_message = ( \"Answer the user's question using only the provided information below:\\n\" + \"\\n\".join(docs) ) # This call is not traced yet resp = client.chat.completions.create( model=\"gpt-4o-mini\", messages=[ {\"role\": \"system\", \"content\": system_message}, {\"role\": \"user\", \"content\": question}, ], ) return resp.choices[0].message.contentif __name__ == \"__main__\": print(rag(\"Where did Harrison work?\"))\nThis snippet wraps the OpenAI client so that every subsequent model call is logged automatically as a traced child run in LangSmith.\nInclude the highlighted lines in your app file:\nCopy\nfrom openai import OpenAIfrom langsmith.wrappers import wrap_openai # traces openai callsdef retriever(query: str): return [\"Harrison worked at Kensho\"]client = wrap_openai(OpenAI()) # log traces by wrapping the model callsdef rag(question: str) -> str: docs = retriever(question) system_message = ( \"Answer the user's question using only the provided information below:\\n\" + \"\\n\".join(docs) ) resp = client.chat.completions.create( model=\"gpt-4o-mini\", messages=[ {\"role\": \"system\", \"content\": system_message}, {\"role\": \"user\", \"content\": question}, ], ) return resp.choices[0].message.contentif __name__ == \"__main__\": print(rag(\"Where did Harrison work?\"))\nCall your application:\nCopy\npython app.py\nYou\u2019ll receive the following output:\nCopy\nHarrison worked at Kensho.\nIn the LangSmith UI, navigate to the default Tracing Project for your workspace (or the workspace you specified in Step 2). You\u2019ll see the OpenAI call you just instrumented.\nYou can also use the traceable decorator for Python or TypeScript to trace your entire application instead of just the LLM calls.\nInclude the highlighted code in your app file:\nCopy\nfrom openai import OpenAIfrom langsmith.wrappers import wrap_openaifrom langsmith import traceabledef retriever(query: str): return [\"Harrison worked at Kensho\"]client = wrap_openai(OpenAI()) # keep this to capture the prompt and response from the LLM@traceabledef rag(question: str) -> str: docs = retriever(question) system_message = ( \"Answer the user's question using only the provided information below:\\n\" + \"\\n\".join(docs) ) resp = client.chat.completions.create( model=\"gpt-4o-mini\", messages=[ {\"role\": \"system\", \"content\": system_message}, {\"role\": \"user\", \"content\": question}, ], ) return resp.choices[0].message.contentif __name__ == \"__main__\": print(rag(\"Where did Harrison work?\"))\nCall the application again to create a run:\nCopy\npython app.py\nReturn to the LangSmith UI, navigate to the default Tracing Project for your workspace (or the workspace you specified in Step 2). You\u2019ll find a trace of the entire app pipeline with the rag step and the ChatOpenAI LLM call.", "tokens": 731, "node_type": "child"}
{"id": 153, "chunk_id": "c16acf8a553ac24f5d77e7d3a1c6e493", "parent_id": 134, "url": "", "namespace": "langchain", "title": "langsmith-observability-stack", "headers": ["langsmith-observability-stack"], "section_index": 0, "chunk_index": 0, "text": "# langsmith-observability-stack\n\n> Source: https://docs.langchain.com/langsmith/observability-stack\n\nThis section is only applicable for Kubernetes deployments.\nSection 1: Prometheus Exporters\nUse this section if you would like to only deploy metrics exporters for the components in your self hosted deployment, which you can then scrape using your telemetry. If you would like a full observability stack deployed for you, go to the End-to-End Deployment Section. The helm chart provides a set of Prometheus exporters to expose metrics from Redis, Postgres, Nginx, and Kube state metrics.- Create a local file called\nlangsmith_obs_config.yaml\n- Copy over the values from this file into\nlangsmith_obs_config.yaml\n, making sure to modify the values to match your LangSmith deployment. - Find the latest version of the chart by running\nhelm search repo langchain/langsmith-observability --versions\n. - Grab the latest version number, and run\nhelm install langsmith-observability langchain/langsmith-observability --values langsmith_obs_config.yaml --version <version> -n <namespace> --wait --debug\n- Postgres:\nlangsmith-observability-postgres-exporter:9187/metrics\n- Redis:\nlangsmith-observability-redis-exporter:9121/metrics\n- Nginx:\nlangsmith-observability-nginx-exporter:9113/metrics\n- KubeStateMetrics:\nlangsmith-observability-kube-state-metrics:8080/metrics\nkubectl get pods -n langsmith-observability\n, you should see:\nSection 2: Full Observability Stack\nThis is not a production observability stack. Use this to gain quick insight into logs, metrics and traces for your deployment. This is only made to handle a few dozen GB of data per day.\nPrerequisites\n1. Compute Resources\nThe resource requests and limits for each part of the stack can be modified in the helm chart. Here are the current allocations (request/limit):- Loki:\n2vCPU/3vCPU + 2Gi/4Gi\n- Mimir:\n1vCPU/2vCPU + 2Gi/4Gi\n- Tempo:\n1vCPU/2vCPU + 4Gi/6Gi\n2. Cert-Manager\nThe helm chart uses the OpenTelemetry Operator to provision collectors. The operator require that you have cert-manager installed in your Kubernetes cluster. If you do not have it installed, you can run the following commands:3. OpenTelemetry Operator\nUse the following to install the OpenTelemetry Operator:Installation\nThe following instructions will bring up OTel collectors, the LGTM stack, Grafana and Prometheus exporters.- Create a local file called\nlangsmith_obs_config.yaml\n- Copy over the values from this file into\nlangsmith_obs_config.yaml\n, making sure to modify the values to match your LangSmith deployment. - Find the latest version of the chart by running\nhelm search repo langchain/langsmith-observability --versions\n. - Grab the latest version number, and run\nhelm install langsmith-observability langchain/langsmith-observability --values langsmith_obs_config.yaml --version <version> -n <namespace> --wait --debug\nYou can selectively collect logs, metrics or traces by modifying the boolean values under\notelCollector\nin your config file. You can also selectively bring up each respective piece of the backend (Loki, Mimir, Tempo).kubectl get pods -n langsmith-observability\n, you should see:\nPost-Installation\nEnable Logs and Traces in LangSmith\nOnce you have installed the observability helm chart, you need to set the following values in your LangSmith helm configuration file to enable collection of logs and traces.- To get\n${LANGSMITH_OTEL_CRD_NAME}\n, you can runkubectl get opentelemetrycollectors -n ${LANGSMITH_OBS_NAMESPACE}\nand select the name of the one with MODE =sidecar\n- To get\n${GATEWAY_COLLECTOR_SERVICE_NAME}\nname, runkubectl get services -n ${LANGSMITH_OBS_NAMESPACE}\nand select the one with Ports 4317/4318 AND a ClusterIP set. It should be something likelangsmith-observability-collector-gateway-collector\nhelm upgrade langsmith langchain/langsmith --values langsmith_config.yaml -n <langsmith-namespace> --wait --debug\nOnce upgraded, if you run kubectl get pods -n <langsmith-namespace>\nyou should see the following (note the 2/2 for sidecar collectors):\nGrafana Usage\nOnce everything is installed, do the following: to get your Grafana password:langsmith-observability-grafana\ncontainer at port 3000, and open your browser as localhost:3000\n. Use the username admin\nand the password from the secret above to log into Grafana.\nOnce in Grafana, you can use the UI to monitor logs, metrics and traces. Grafana also comes pre-packaged with sets of dashboards for monitoring the main components of your deployment.", "tokens": 600, "node_type": "child"}
{"id": 154, "chunk_id": "c16acf8a553ac24f5d77e7d3a1c6e493", "parent_id": 135, "url": "", "namespace": "langchain", "title": "langsmith-observability-studio", "headers": ["langsmith-observability-studio"], "section_index": 0, "chunk_index": 0, "text": "# langsmith-observability-studio\n\n> Source: https://docs.langchain.com/langsmith/observability-studio\n\n- Iterate on prompts: Modify prompts inside graph nodes directly or with the LangSmith playground.\n- Run experiments over a dataset: Execute your assistant over a LangSmith dataset to score and compare results.\n- Debug LangSmith traces: Import traced runs into Studio and optionally clone them into your local agent.\n- Add a node to a dataset: Turn parts of thread history into dataset examples for evaluation or further analysis.\nIterate on prompts\nStudio supports the following methods for modifying prompts in your graph:Direct node editing\nStudio allows you to edit prompts used inside individual nodes, directly from the graph interface.Graph Configuration\nDefine your configuration to specify prompt fields and their associated nodes usinglanggraph_nodes\nand langgraph_type\nkeys.\nlanggraph_nodes\n- Description: Specifies which nodes of the graph a configuration field is associated with.\n- Value Type: Array of strings, where each string is the name of a node in your graph.\n- Usage Context: Include in the\njson_schema_extra\ndictionary for Pydantic models or themetadata[\"json_schema_extra\"]\ndictionary for dataclasses. - Example:\nlanggraph_type\n- Description: Specifies the type of configuration field, which determines how it\u2019s handled in the UI.\n- Value Type: String\n- Supported Values:\n\"prompt\"\n: Indicates the field contains prompt text that should be treated specially in the UI.\n- Usage Context: Include in the\njson_schema_extra\ndictionary for Pydantic models or themetadata[\"json_schema_extra\"]\ndictionary for dataclasses. - Example:\nFull example configuration\nFull example configuration\nEditing prompts in the UI\n- Locate the gear icon on nodes with associated configuration fields.\n- Click to open the configuration modal.\n- Edit the values.\n- Save to update the current assistant version or create a new one.\nPlayground\nThe playground interface allows testing individual LLM calls without running the full graph:- Select a thread.\n- Click View LLM Runs on a node. This lists all the LLM calls (if any) made inside the node.\n- Select an LLM run to open in the playground.\n- Modify prompts and test different model and tool settings.\n- Copy updated prompts back to your graph.\nRun experiments over a dataset\nStudio lets you run evaluations by executing your assistant against a predefined LangSmith dataset. This allows you to test performance across a variety of inputs, compare outputs to reference answers, and score results with configured evaluators. This guide shows you how to run a full end-to-end experiment directly from Studio.Prerequisites\nBefore running an experiment, ensure you have the following:- A LangSmith dataset: Your dataset should contain the inputs you want to test and optionally, reference outputs for comparison. The schema for the inputs must match the required input schema for the assistant. For more information on schemas, see here. For more on creating datasets, refer to How to Manage Datasets.\n- (Optional) Evaluators: You can attach evaluators (e.g., LLM-as-a-Judge, heuristics, or custom functions) to your dataset in LangSmith. These will run automatically after the graph has processed all inputs.\n- A running application: The experiment can be run against:\n- An application deployed on LangSmith.\n- A locally running application started via the langgraph-cli.\nExperiment setup\n- Launch the experiment. Click the Run experiment button in the top right corner of the Studio page.\n- Select your dataset. In the modal that appears, select the dataset (or a specific dataset split) to use for the experiment and click Start.\n- Monitor the progress. All of the inputs in the dataset will now be run against the active assistant. Monitor the experiment\u2019s progress via the badge in the top right corner.\n- You can continue to work in Studio while the experiment runs in the background. Click the arrow icon button at any time to navigate to LangSmith and view the detailed experiment results.\nDebug LangSmith traces\nThis guide explains how to open LangSmith traces in Studio for interactive investigation and debugging.Open deployed threads\n- Open the LangSmith trace, selecting the root run.\n- Click Run in Studio.\nTesting local agents with remote traces\nThis section explains how to test a local agent against remote traces from LangSmith. This enables you to use production traces as input for local testing, allowing you to debug and verify agent modifications in your development environment.Prerequisites\n- A LangSmith traced thread\n- A locally running agent.\nLocal agent requirements\n- langgraph>=0.3.18\n- langgraph-api>=0.0.32\n- Contains the same set of nodes present in the remote trace\nClone thread\n- Open the LangSmith trace, selecting the root run.\n- Click the dropdown next to Run in Studio.\n- Enter your local agent\u2019s URL.\n- Select Clone thread locally.\n- If multiple graphs exist, select the target graph.\nAdd node to dataset\nAdd examples to LangSmith datasets from nodes in the thread log. This is useful to evaluate individual steps of the agent.- Select a thread.\n- Click Add to Dataset.\n- Select nodes whose input/output you want to add to a dataset.\n- For each selected node, select the target dataset to create the example in. By default a dataset for the specific assistant and node will be selected. If this dataset does not yet exist, it will be created.\n- Edit the example\u2019s input/output as needed before adding it to the dataset.\n- Select Add to dataset at the bottom of the page to add all selected nodes to their respective datasets.", "tokens": 883, "node_type": "child"}
{"id": 155, "chunk_id": "c16acf8a553ac24f5d77e7d3a1c6e493", "parent_id": 137, "url": "", "namespace": "langchain", "title": "langsmith-online-evaluations", "headers": ["langsmith-online-evaluations"], "section_index": 0, "chunk_index": 0, "text": "# langsmith-online-evaluations\n\n> Source: https://docs.langchain.com/langsmith/online-evaluations\n\nOnline evaluations provide real-time feedback on your production traces. This is useful to continuously monitor the performance of your application\u2014to identify issues, measure improvements, and ensure consistent quality over time.\nThere are two types of online evaluations supported in LangSmith:\n- LLM-as-a-judge: Use an LLM to evaluate your traces. Used as a scalable way to provide human-like judgement to your output (e.g. toxicity, hallucination, correctness, etc.).\n- Custom Code: Write an evaluator in Python directly in LangSmith. Often used for validating structure or statistical properties of your data.\nView online evaluators\nHead to the Tracing Projects tab and select a tracing project. To view existing online evaluators for that project, click on the Evaluators tab.Configure online evaluators\n1. Navigate to online evaluators\nHead to the Tracing Projects tab and select a tracing project. Click on + New in the top right corner of the tracing project page, then click on New Evaluator.2. Name your evaluator\n3. Create a filter\nFor example, you may want to apply specific evaluators based on:- Runs where a user left feedback indicating the response was unsatisfactory.\n- Runs that invoke a specific tool call. See filtering for tool calls for more information.\n- Runs that match a particular piece of metadata (e.g. if you log traces with a\nplan_type\nand only want to run evaluations on traces from your enterprise customers). See adding metadata to your traces for more information.\nIt\u2019s often helpful to inspect runs as you\u2019re creating a filter for your evaluator. With the evaluator configuration panel open, you can inspect runs and apply filters to them. Any filters you apply to the runs table will automatically be reflected in filters on your evaluator.\n4. (Optional) Configure a sampling rate\nConfigure a sampling rate to control the percentage of filtered runs that trigger the automation action. For example, to control costs, you may want to set a filter to only apply the evaluator to 10% of traces. In order to do this, you would set the sampling rate to 0.1.5. (Optional) Apply rule to past runs\nApply rule to past runs by toggling the Apply to past runs and entering a \u201cBackfill from\u201d date. This is only possible upon rule creation. Note: the backfill is processed as a background job, so you will not see the results immediately. In order to track progress of the backfill, you can view logs for your evaluator by heading to the Evaluators tab within a tracing project and clicking the Logs button for the evaluator you created. Online evaluator logs are similar to automation rule logs.- Add an evaluator name\n- Optionally filter runs that you would like to apply your evaluator on or configure a sampling rate.\n- Select Apply Evaluator\n6. Select evaluator type\n- Configuring LLM-as-a-judge evaluators\n- Configuring custom code evaluators\nConfigure a LLM-as-a-judge online evaluator\nView this guide to configure an LLM-as-a-judge evaluator.Configure a custom code evaluator\nSelect custom code evaluator.Write your evaluation function\nCustom code evaluators restrictions.Allowed Libraries: You can import all standard library functions, as well as the following public packages:Network Access: You cannot access the internet from a custom code evaluator.\n- A\nRun\n(reference). This represents the sampled run to evaluate.\n- Feedback(s) Dictionary: A dictionary whose keys are the type of feedback you want to return, and values are the score you will give for that feedback key. For example,\n{\"correctness\": 1, \"silliness\": 0}\nwould create two types of feedback on the run, one saying it is correct, and the other saying it is not silly.", "tokens": 594, "node_type": "child"}
{"id": 156, "chunk_id": "c16acf8a553ac24f5d77e7d3a1c6e493", "parent_id": 138, "url": "", "namespace": "langchain", "title": "langsmith-openapi-security", "headers": ["langsmith-openapi-security"], "section_index": 0, "chunk_index": 0, "text": "# langsmith-openapi-security\n\n> Source: https://docs.langchain.com/langsmith/openapi-security\n\nThis guide applies to all LangSmith deployments (Cloud and self-hosted). It does not apply to usage of the LangGraph open source library if you are not using LangSmith.\nDefault Schema\nThe default security scheme varies by deployment type:- LangSmith\nx-api-key\nheader:\n- Self-hosted\nCustom Security Schema\nTo customize the security schema in your OpenAPI documentation, add anopenapi\nfield to your auth\nconfiguration in langgraph.json\n. Remember that this only updates the API documentation - you must also implement the corresponding authentication logic as shown in How to add custom authentication.\nNote that LangSmith does not provide authentication endpoints - you\u2019ll need to handle user authentication in your client application and pass the resulting credentials to the LangGraph API.\n- OAuth2 with Bearer Token\n- API Key\nTesting\nAfter updating your configuration:- Deploy your application\n- Visit\n/docs\nto see the updated OpenAPI documentation - Try out the endpoints using credentials from your authentication server (make sure you\u2019ve implemented the authentication logic first)", "tokens": 167, "node_type": "child"}
{"id": 157, "chunk_id": "c16acf8a553ac24f5d77e7d3a1c6e493", "parent_id": 139, "url": "", "namespace": "langchain", "title": "langsmith-optimize-classifier", "headers": ["langsmith-optimize-classifier"], "section_index": 0, "chunk_index": 0, "text": "# langsmith-optimize-classifier\n\n> Source: https://docs.langchain.com/langsmith/optimize-classifier\n\nThis tutorial walks through optimizing a classifier based on user a feedback. Classifiers are great to optimize because its generally pretty simple to collect the desired output, which makes it easy to create few shot examples based on user feedback. That is exactly what we will do in this example.\nIn this example, we will build a bot that classify GitHub issues based on their title. It will take in a title and classify it into one of many different classes. Then, we will start to collect user feedback and use that to shape how this classifier performs.\nWe can then create our initial application. This will be a really simple function that just takes in a GitHub issue title and tries to label it.\nCopy\nimport openaifrom langsmith import traceable, Clientimport uuidclient = openai.Client()available_topics = [ \"bug\", \"improvement\", \"new_feature\", \"documentation\", \"integration\",]prompt_template = \"\"\"Classify the type of the issue as one of {topics}.Issue: {text}\"\"\"@traceable( run_type=\"chain\", name=\"Classifier\",)def topic_classifier( topic: str): return client.chat.completions.create( model=\"gpt-4o-mini\", temperature=0, messages=[ { \"role\": \"user\", \"content\": prompt_template.format( topics=','.join(available_topics), text=topic, ) } ], ).choices[0].message.content\nWe can then start to interact with it. When interacting with it, we will generate the LangSmith run id ahead of time and pass that into this function. We do this so we can attach feedback later on.Here\u2019s how we can invoke the application:\nCopy\nrun_id = uuid.uuid4()topic_classifier( \"fix bug in LCEL\", langsmith_extra={\"run_id\": run_id})\nHere\u2019s how we can attach feedback after. We can collect feedback in two forms.First, we can collect \u201cpositive\u201d feedback - this is for examples that the model got right.\nNext, we can focus on collecting feedback that corresponds to a \u201ccorrection\u201d to the generation. In this example the model will classify it as a bug, whereas I really want this to be classified as documentation.\nWe can now set up automations to move examples with feedback of some form into a dataset. We will set up two automations, one for positive feedback and the other for negative feedback.The first will take all runs with positive feedback and automatically add them to a dataset. The logic behind this is that any run with positive feedback we can use as a good example in future iterations. Let\u2019s create a dataset called classifier-github-issues to add this data to.The second will take all runs with a correction and use a webhook to add them to a dataset. When creating this webhook, we will select the option to \u201cUse Corrections\u201d. This option will make it so that when creating a dataset from a run, rather than using the output of the run as the gold-truth output of the datapoint, it will use the correction.\nWe can now update our code to pull down the dataset we are sending runs to. Once we pull it down, we can create a string with the examples in it. We can then put this string as part of the prompt!\nCopy\n### NEW CODE #### Initialize the LangSmith Client so we can use to get the datasetls_client = Client()# Create a function that will take in a list of examples and format them into a stringdef create_example_string(examples): final_strings = [] for e in examples: final_strings.append(f\"Input: {e.inputs['topic']}\\n> {e.outputs['output']}\") return \"\\n\\n\".join(final_strings)### NEW CODE ###client = openai.Client()available_topics = [ \"bug\", \"improvement\", \"new_feature\", \"documentation\", \"integration\",]prompt_template = \"\"\"Classify the type of the issue as one of {topics}.Here are some examples:{examples}Begin!Issue: {text}>\"\"\"@traceable( run_type=\"chain\", name=\"Classifier\",)def topic_classifier( topic: str): # We can now pull down the examples from the dataset # We do this inside the function so it always get the most up-to-date examples, # But this can be done outside and cached for speed if desired examples = list(ls_client.list_examples(dataset_name=\"classifier-github-issues\")) # <- New Code example_string = create_example_string(examples) return client.chat.completions.create( model=\"gpt-4o-mini\", temperature=0, messages=[ { \"role\": \"user\", \"content\": prompt_template.format( topics=','.join(available_topics), text=topic, examples=example_string, ) } ], ).choices[0].message.content\nIf now run the application with a similar input as before, we can see that it correctly learns that anything related to docs (even if a bug) should be classified as documentation\nOne additional thing we can do is only use the most semantically similar examples. This is useful when you start to build up a lot of examples.In order to do this, we can first define an example to find the k most similar examples:\nCopy\nimport numpy as npdef find_similar(examples, topic, k=5): inputs = [e.inputs['topic'] for e in examples] + [topic] vectors = client.embeddings.create(input=inputs, model=\"text-embedding-3-small\") vectors = [e.embedding for e in vectors.data] vectors = np.array(vectors) args = np.argsort(-vectors.dot(vectors[-1])[:-1])[:5] examples = [examples[i] for i in args] return examples\nWe can then use that in the application\nCopy\nls_client = Client()def create_example_string(examples): final_strings = [] for e in examples: final_strings.append(f\"Input: {e.inputs['topic']}\\n> {e.outputs['output']}\") return \"\\n\\n\".join(final_strings)client = openai.Client()available_topics = [ \"bug\", \"improvement\", \"new_feature\", \"documentation\", \"integration\",]prompt_template = \"\"\"Classify the type of the issue as one of {topics}.Here are some examples:{examples}Begin!Issue: {text}>\"\"\"@traceable( run_type=\"chain\", name=\"Classifier\",)def topic_classifier( topic: str): examples = list(ls_client.list_examples(dataset_name=\"classifier-github-issues\")) examples = find_similar(examples, topic) example_string = create_example_string(examples) return client.chat.completions.create( model=\"gpt-4o-mini\", temperature=0, messages=[ { \"role\": \"user\", \"content\": prompt_template.format( topics=','.join(available_topics), text=topic, examples=example_string, ) } ], ).choices[0].message.content", "tokens": 832, "node_type": "child"}
{"id": 158, "chunk_id": "c16acf8a553ac24f5d77e7d3a1c6e493", "parent_id": 141, "url": "", "namespace": "langchain", "title": "langsmith-prebuilt-evaluators", "headers": ["langsmith-prebuilt-evaluators"], "section_index": 0, "chunk_index": 0, "text": "# langsmith-prebuilt-evaluators\n\n> Source: https://docs.langchain.com/langsmith/prebuilt-evaluators\n\nThis how-to guide will demonstrate how to set up and run one type of evaluator (LLM-as-a-judge). For a complete list of prebuilt evaluators with usage examples, refer to the openevals and agentevals repos.\nSetup\nYou\u2019ll need to install theopenevals\npackage to use the pre-built LLM-as-a-judge evaluator.\nopenevals\nalso integrates seamlessly with the evaluate\nmethod as well. See the appropriate guides for setup instructions.\nRunning an evaluator\nThe general flow is simple: import the evaluator or factory function fromopenevals\n, then run it within your test file with inputs, outputs, and reference outputs. LangSmith will automatically log the evaluator\u2019s results as feedback.\nNote that not all evaluators will require each parameter (the exact match evaluator only requires outputs and reference outputs, for example). Additionally, if your LLM-as-a-judge prompt requires additional variables, passing them in as kwargs will format them into the prompt.\nSet up your test file like this:\nfeedback_key\n/feedbackKey\nparameter will be used as the name of the feedback in your experiment.\nRunning the eval in your terminal will result in something like the following:\nYou can also pass prebuilt evaluators directly into the evaluate\nmethod if you have already created a dataset in LangSmith. If using Python, this requires langsmith>=0.3.11\n:", "tokens": 208, "node_type": "child"}
{"id": 159, "chunk_id": "c16acf8a553ac24f5d77e7d3a1c6e493", "parent_id": 142, "url": "", "namespace": "langchain", "title": "langsmith-pricing-faq", "headers": ["langsmith-pricing-faq"], "section_index": 0, "chunk_index": 0, "text": "# langsmith-pricing-faq\n\n> Source: https://docs.langchain.com/langsmith/pricing-faq\n\nQuestions and Answers\nI\u2019ve been using LangSmith since before pricing took effect for new users. When will pricing go into effect for my account?\nIf you\u2019ve been using LangSmith already, your usage will be billable starting in July 2024. At that point if you want to add seats or use more than the monthly allotment of free traces, you will need to add a credit card to LangSmith or contact sales. If you are interested in the Enterprise plan with higher rate limits and special deployment options, you can learn more or make a purchase by contacting our sales team.Which plan is right for me?\nIf you\u2019re an individual developer, the Developer plan is a great choice for small projects. For teams that want to collaborate in LangSmith, check out the Plus plan. If you are an early-stage startup building an AI application, you may be eligible for our Startup plan with discounted prices and a generous free monthly trace allotment. Please reach out via our Startup Contact Form for more details. If you need more advanced administration, authentication and authorization, deployment options, support, or annual invoicing, the Enterprise plan is right for you. Please reach out via our Sales Contact Form for more details.What is a seat?\nA seat is a distinct user inside your organization. We consider the total number of users (including invited users) to determine the number of seats to bill.What is a trace?\nA trace is one complete invocation of your application chain or agent, evaluator run, or playground run. Here is an example of a single trace.What is an ingested event?\nAn ingested event is any distinct, trace-related data sent to LangSmith. This includes:- Inputs, outputs and metadata sent at the start of a run step within a trace\n- Inputs, outputs and metadata sent at the end of a run step within a trace\n- Feedback on run steps or traces\nI\u2019ve hit my rate or usage limits. What can I do?\nWhen you first sign up for a LangSmith account, you get a Personal organization that is limited to 5000 monthly traces. To continue sending traces after reaching this limit, upgrade to the Developer or Plus plans by adding a credit card. Head to Plans and Billing to upgrade. Similarly, if you\u2019ve hit the rate limits on your current plan, you can upgrade to a higher plan to get higher limits, or reach out to support@langchain.dev with questions.I have a developer account, can I upgrade my account to the Plus or Enterprise plan?\nYes, Developer plan users can easily upgrade to the Plus plan on the Plans and Billing page. For the Enterprise plan, please contact our sales team to discuss your needs.How does billing work?\nSeats Seats are billed monthly on the first of the month. Additional seats purchased mid-month are pro-rated and billed within one day of the purchase. Seats removed mid-month will not be credited. Traces As long as you have a card on file in your account, we\u2019ll service your traces and bill you on the first of the month for traces that you submitted in the previous month. You will be able to set usage limits if you so choose to limit the maximum charges you could incur in any given month.Can I limit how much I spend on tracing?\nYou can set limits on the number of traces that can be sent to LangSmith per month on the Usage configuration page.While we do show you the dollar value of your usage limit for convenience, this limit evaluated in terms of number of traces instead of dollar amount. For example, if you are approved for our startup plan tier where you are given a generous allotment of free traces, your usage limit will not automatically change.You are not currently able to set a spend limit in the product.", "tokens": 646, "node_type": "child"}
{"id": 160, "chunk_id": "c16acf8a553ac24f5d77e7d3a1c6e493", "parent_id": 143, "url": "", "namespace": "langchain", "title": "langsmith-prompt-commit", "headers": ["langsmith-prompt-commit"], "section_index": 0, "chunk_index": 0, "text": "# langsmith-prompt-commit > Source: https://docs.langchain.com/langsmith/prompt-commit - Version Control: Keep your prompts versioned alongside your application code in a familiar system. - CI/CD Integration: Trigger automated staging or production deployments when critical prompts change. Prerequisites Before we begin, ensure you have the following set up:- GitHub Account: A standard GitHub account. - GitHub Repository: Create a new (or choose an existing) repository where your LangSmith prompt manifests will be stored. This could be the same repository as your application code or a dedicated one for prompts. - GitHub Personal Access Token (PAT): - LangSmith webhooks don\u2019t directly interact with GitHub\u2014they call an intermediary server that you create. - This server requires a GitHub PAT to authenticate and make commits to your repository. - Must include the repo scope (public_repo is sufficient for public repositories). - Go to GitHub > Settings > Developer settings > Personal access tokens > Tokens (classic). - Click Generate new token (classic). - Name it (e.g., \u201cLangSmith Prompt Sync\u201d), set an expiration, and select the required scopes. - Click Generate token and copy it immediately \u2014 it won\u2019t be shown again. - Store the token securely and provide it as an environment variable to your server. Understanding LangSmith \u201cPrompt Commits\u201d and Webhooks In LangSmith, when you save changes to a prompt, you\u2019re essentially creating a new version or a \u201cPrompt Commit.\u201d These commits are what can trigger webhooks. The webhook will send a JSON payload containing the new prompt manifest.Sample Webhook Payload Sample Webhook Payload It\u2019s important to understand that LangSmith webhooks for prompt commits are generally triggered at the workspace level. This means if any prompt within your LangSmith workspace is modified and a \u201cprompt commit\u201d is saved, the webhook will fire and send the updated manifest of the prompt. The payloads are identifiable by prompt id. Your receiving server should be designed with this in mind. Implementing a FastAPI Server for Webhook Reception To effectively process webhook notifications from LangSmith when prompts are updated, an intermediary server application is necessary. This server will act as the receiver for HTTP POST requests sent by LangSmith. For demonstration purposes in this guide, we will outline the creation of a simple FastAPI application to fulfill this role. This publicly accessible server will be responsible for:- Receiving Webhook Requests: Listening for incoming HTTP POST requests. - Parsing Payloads: Extracting and interpreting the JSON-formatted prompt manifest from the request body. - Committing to GitHub: Programmatically creating a new commit in your specified GitHub repository, containing the updated prompt manifest. This ensures your prompts remain version-controlled and synchronized with changes made in LangSmith. Minimal FastAPI Server Code () Minimal FastAPI Server Code () main.py This server will listen for incoming webhooks from LangSmith and commit the received prompt manifest to your GitHub repository.- Configuration ( .env ): It expects a.env file with yourGITHUB_TOKEN ,GITHUB_REPO_OWNER , andGITHUB_REPO_NAME . You can also customizeGITHUB_FILE_PATH (default:LangSmith_prompt_manifest.json ) andGITHUB_BRANCH (default:main ). - GitHub Interaction: The commit_manifest_to_github function handles the logic of fetching the current file\u2019s SHA (to update it) and then committing the new manifest content. - Webhook Endpoint ( /webhook/commit ): This is the URL path your LangSmith webhook will target. - Error Handling: Basic error handling for GitHub API interactions is included. https://prompt-commit-webhook.onrender.com ).Configuring the Webhook in LangSmith Once your FastAPI server is deployed and you have its public URL, you can configure the webhook in LangSmith:- Navigate to your LangSmith workspace. - Go to the Prompts section. Here you\u2019ll see a list of your prompts. - On the top right of the Prompts page, click the + Webhook button. - You\u2019ll be presented with a form to configure your webhook: - Webhook URL: Enter the full public URL of your deployed FastAPI server\u2019s endpoint. For our example server, this would be https://prompt-commit-webhook.onrender.com/webhook/commit . - Headers (Optional): - You can add custom headers that LangSmith will send with each webhook request. - Webhook URL: Enter the full public URL of your deployed FastAPI server\u2019s endpoint. For our example server, this would be - Test the Webhook: LangSmith provides a \u201cSend Test Notification\u201d button. Use this to send a sample payload to your server. Check your server logs (e.g., on Render) to ensure it receives the request and processes it successfully (or to debug any issues). - Save the webhook configuration. The Workflow in Action Now, with everything set up, here\u2019s what happens:- Prompt Modification: A user (developer or non-technical team member) modifies a prompt in the LangSmith UI and saves it, creating a new \u201cprompt commit.\u201d - Webhook Trigger: LangSmith detects this new prompt commit and triggers the configured webhook. - HTTP Request: LangSmith sends an HTTP POST request to the public URL of your FastAPI server (e.g., https://prompt-commit-webhook.onrender.com/webhook/commit ). The body of this request contains the JSON prompt manifest for the entire workspace. - Server Receives Payload: Your FastAPI server\u2019s endpoint receives the request. - GitHub Commit: The server parses the JSON manifest from the request body. It then uses the configured GitHub Personal Access Token, repository owner, repository name, file path, and branch to: - Check if the manifest file already exists in the repository on the specified branch to get its SHA (this is necessary for updating an existing file). - Create a new commit with the latest prompt manifest, either creating the file or updating it if it already exists. The commit message will indicate that it\u2019s an update from LangSmith. - Confirmation: You should see the new commit appear in your GitHub repository. Beyond a Simple Commit Our example FastAPI server performs a direct commit of the entire prompt manifest. However, this is just the starting point. You can extend the server\u2019s functionality to perform more sophisticated actions:- Granular Commits: Parse the manifest and commit changes to individual prompt files if you prefer a more granular structure in your repository. - Trigger CI/CD: Instead of (or in addition to) committing, have the server trigger a CI/CD pipeline (e.g., Jenkins, GitHub Actions, GitLab CI) to deploy a staging environment, run", "tokens": 1000, "node_type": "child"}
{"id": 161, "chunk_id": "9b0fd3b850698317bef57fa6e71a38ba", "parent_id": 143, "url": "", "namespace": "langchain", "title": "langsmith-prompt-commit", "headers": ["langsmith-prompt-commit"], "section_index": 0, "chunk_index": 1, "text": "in the repository on the specified branch to get its SHA (this is necessary for updating an existing file). - Create a new commit with the latest prompt manifest, either creating the file or updating it if it already exists. The commit message will indicate that it\u2019s an update from LangSmith. - Confirmation: You should see the new commit appear in your GitHub repository. Beyond a Simple Commit Our example FastAPI server performs a direct commit of the entire prompt manifest. However, this is just the starting point. You can extend the server\u2019s functionality to perform more sophisticated actions:- Granular Commits: Parse the manifest and commit changes to individual prompt files if you prefer a more granular structure in your repository. - Trigger CI/CD: Instead of (or in addition to) committing, have the server trigger a CI/CD pipeline (e.g., Jenkins, GitHub Actions, GitLab CI) to deploy a staging environment, run tests, or build new application versions. - Update Databases/Caches: If your application loads prompts from a database or cache, update these stores directly. - Notifications: Send notifications to Slack, email, or other communication channels about prompt changes. - Selective Processing: Based on metadata within the LangSmith payload (if available, e.g., which specific prompt changed or by whom), you could apply different logic.", "tokens": 212, "node_type": "child"}
{"id": 162, "chunk_id": "c16acf8a553ac24f5d77e7d3a1c6e493", "parent_id": 144, "url": "", "namespace": "langchain", "title": "langsmith-prompt-engineering-concepts", "headers": ["langsmith-prompt-engineering-concepts"], "section_index": 0, "chunk_index": 0, "text": "# langsmith-prompt-engineering-concepts\n\n> Source: https://docs.langchain.com/langsmith/prompt-engineering-concepts\n\nWhy prompt engineering?\nA prompt sets the stage for the model, like an audience member at an improv show directing the actor\u2019s next performance - it guides the model\u2019s behavior without changing its underlying capabilities. Just as telling an actor to \u201cbe a pirate\u201d determines how they act, a prompt provides instructions, examples, and context that shape how the model responds. Prompt engineering is important because it allows you to change the way the model behaves. While there are other ways to change the model\u2019s behavior (like fine-tuning), prompt engineering is usually the simplest to get started with and often provides the highest ROI. We often see that prompt engineering is multi-disciplinary. Sometimes the best prompt engineer is not the software engineer who is building the application, but rather the product manager or another domain expert. It is important to have the proper tooling and infrastructure to support this cross-disciplinary building.Prompts vs. prompt templates\nAlthough we often use these terms interchangably, it is important to understand the difference between \u201cprompts\u201d and \u201cprompt templates\u201d. Prompts refer to the messages that are passed into the language model. Prompt Templates refer to a way of formatting information to get that prompt to hold the information that you want. Prompt templates can include variables for few shot examples, outside context, or any other external data that is needed in your prompt.Prompts in LangSmith\nYou can store and version prompts templates in LangSmith. There are few key aspects of a prompt template to understand.Chat vs Completion\nThere are two different types of prompts:chat\nstyle prompts and completion\nstyle prompts.\nChat style prompts are a list of messages. This is the prompting style supported by most model APIs these days, and so this should generally be preferred.\nCompletion style prompts are just a string. This is an older style of prompting, and so mostly exists for legacy reasons.\nF-string vs. mustache\nYou can format your prompt with input variables using either f-string or mustache format. Here is an example prompt with f-string format:- The playground UI will pick up\nis_logged_in\nvariable, but any nested variables you\u2019ll need to specify yourself. Paste the following into inputs to ensure the above conditional prompt works:\nThe LangSmith Playground uses\nf-string\nas the default template format, but you can switch to mustache\nformat in the prompt settings/template format section. mustache\ngives you more flexibility around conditional variables, loops, and nested keys. For conditional variables, you\u2019ll need to manually add json variables in the \u2018inputs\u2019 section. Read the documentationTools\nTools are interfaces the LLM can use to interact with the outside world. Tools consist of a name, description, and JSON schema of arguments used to call the tool.Structured output\nStructured output is a feature of most state of the art LLMs, wherein instead of producing raw text as output they stick to a specified schema. This may or may not use Tools under the hood.Structured outputs are similar to tools, but different in a few key ways. With tools, the LLM choose which tool to call (or may choose not to call any); with structured output, the LLM always responds in this format. With tools, the LLM may select multiple tools; with structured output, only one response is generate.\nModel\nOptionally, you can store a model configuration alongside a prompt template. This includes the name of the model and any other parameters (temperature, etc).Prompt versioning\nVerisioning is a key part of iterating and collaborating on your different prompts.Commits\nEvery saved update to a prompt creates a new commit. You can view previous commits, making it easy to review earlier prompt versions or revert to a previous state if needed. In the SDK, you can access a specific commit of a prompt by specifying the commit hash along with the prompt name (e.g.prompt_name:commit_hash\n).\nIn the UI, you can compare a commit with its previous version by toggling the \u201cdiff\u201d button in the top-right corner of the Commits tab.\nTags\nYou may want to tag prompt commits with a human-readable tag so that you can refer to it even as new commits are added. Common use cases include tagging a prompt withdev\nor prod\ntags. This allows you to track which versions of prompts are used where.\nPrompt playground\nThe prompt playground makes the process of iterating and testing your prompts seamless. You can enter the playground from the sidebar or directly from a saved prompt. In the playground you can:- Change the model being used\n- Change prompt template being used\n- Change the output schema\n- Change the tools available\n- Enter the input variables to run through the prompt template\n- Run the prompt through the model\n- Observe the outputs", "tokens": 787, "node_type": "child"}
{"id": 163, "chunk_id": "c16acf8a553ac24f5d77e7d3a1c6e493", "parent_id": 145, "url": "", "namespace": "langchain", "title": "langsmith-prompt-engineering-quickstart", "headers": ["langsmith-prompt-engineering-quickstart"], "section_index": 0, "chunk_index": 0, "text": "# langsmith-prompt-engineering-quickstart\n\n> Source: https://docs.langchain.com/langsmith/prompt-engineering-quickstart\n\nPrerequisites\nBefore you begin, make sure you have:- A LangSmith account: Sign up or log in at smith.langchain.com.\n- A LangSmith API key: Follow the Create an API key guide.\n- An OpenAI API key: Generate this from the OpenAI dashboard.\n- UI\n- SDK\n1. Set workspace secret\nIn the LangSmith UI, ensure that your OpenAI API key is set as a workspace secret.- Navigate to Settings and then move to the Secrets tab.\n- Select Add secret and enter the\nOPENAI_API_KEY\nand your API key as the Value. - Select Save secret.\nWhen adding workspace secrets in the LangSmith UI, make sure the secret keys match the environment variable names expected by your model provider.\n2. Create a prompt\n- In the LangSmith UI, navigate to the Prompts section in the left-hand menu.\n- Click on + Prompt to create a prompt.\n- Modify the prompt by editing or adding prompts and input variables as needed.\n3. Test a prompt\n- Under the Prompts heading select the gear icon next to the model name, which will launch the Prompt Settings window on the Model Configuration tab.\n-\nSet the model configuration you want to use. The Provider and Model you select will determine the parameters that are configurable on this configuration page. Once set, click Save as.\n-\nSpecify the input variables you would like to test in the Inputs box and then click Start.\nTo learn about more options for configuring your prompt in the Playground, refer to Configure prompt settings.\n- After testing and refining your prompt, click Save to store it for future use.\n4. Iterate on a prompt\nLangSmith allows for team-based prompt iteration. Workspace members can experiment with prompts in the playground and save their changes as a new commit when ready.To improve your prompts:- Reference the documentation provided by your model provider for best practices in prompt creation, such as:\n- Build and refine your prompts with the Prompt Canvas\u2014an interactive tool in LangSmith. Learn more in the Prompt Canvas guide.\n-\nTag specific commits to mark important moments in your commit history.\n- To create a commit, navigate to the Playground and select Commit. Choose the prompt to commit changes to and then Commit.\n- Navigate to Prompts in the left-hand menu. Select the prompt. Once on the prompt\u2019s detail page, move to the Commits tab. Find the tag icon to Add a Commit Tag.\nNext steps\n- Learn more about how to store and manage prompts using the Prompt Hub in the Create a prompt guide.\n- Learn how to set up the Playground to Test multi-turn conversations in this tutorial.\n- Learn how to test your prompt\u2019s performance over a dataset instead of individual examples, refer to Run an evaluation from the Prompt Playground.", "tokens": 470, "node_type": "child"}
{"id": 164, "chunk_id": "c16acf8a553ac24f5d77e7d3a1c6e493", "parent_id": 146, "url": "", "namespace": "langchain", "title": "langsmith-prompt-engineering", "headers": ["langsmith-prompt-engineering"], "section_index": 0, "chunk_index": 0, "text": "# langsmith-prompt-engineering\n\n> Source: https://docs.langchain.com/langsmith/prompt-engineering\n\nReview core concepts\nRead definitions and key terminology for prompt engineering in LangSmith.\nCreate and update prompts\nBuild prompts via the UI or SDK, configure settings, use tools, add multimodal inputs, and connect model providers.\nManage prompts\nOrganize with tags, commit changes, trigger webhooks, and share through the public prompt hub.\nExplore the prompt hub\nBrowse and manage prompt tags and discover community prompts from the LangChain Hub.\nOpen the prompt playground\nTest and experiment with prompts using custom endpoints and model configurations.\nFollow tutorials\nLearn step-by-step techniques, like optimizing classifiers and advanced prompt engineering.", "tokens": 100, "node_type": "child"}
{"id": 165, "chunk_id": "c16acf8a553ac24f5d77e7d3a1c6e493", "parent_id": 147, "url": "", "namespace": "langchain", "title": "langsmith-pytest", "headers": ["langsmith-pytest"], "section_index": 0, "chunk_index": 0, "text": "# langsmith-pytest > Source: https://docs.langchain.com/langsmith/pytest - Each example requires different evaluation logic - You want to assert binary expectations, and both track these assertions in LangSmith and raise assertion errors locally (e.g. in CI pipelines) - You want pytest-like terminal outputs - You already use pytest to test your app and want to add LangSmith tracking The pytest integration is in beta and is subject to change in upcoming releases. Installation This functionality requires Python SDK versionlangsmith>=0.3.4 . For extra features like rich terminal outputs and test caching install: Define and run tests The pytest integration lets you define datasets and evaluators as test cases. To track a test in LangSmith add the@pytest.mark.langsmith decorator. Every decorated test case will be synced to a dataset example. When you run the test suite, the dataset will be updated and a new experiment will be created with one result for each test case. pass boolean feedback key based on the test case passing / failing. It will also track any inputs, outputs, and reference (expected) outputs that you log. Use pytest as you normally would to run the tests: - creates a dataset for each test file. If a dataset for this test file already exists it will be updated - creates an experiment in each created/updated dataset - creates an experiment row for each test case, with the inputs, outputs, reference outputs and feedback you\u2019ve logged - collects the pass/fail rate under the pass feedback key for each test case Log inputs, outputs, and reference outputs Every time we run a test we\u2019re syncing it to a dataset example and tracing it as a run. There\u2019s a few different ways that we can trace the example inputs and reference outputs and the run outputs. The simplest is to use thelog_inputs , log_outputs , and log_reference_outputs methods. You can run these any time in a test to update the example and run for that test: {\"a\": 1, \"b\": 2} , reference outputs {\"foo\": \"bar\"} and trace a run with outputs {\"foo\": \"baz\"} . NOTE: If you run log_inputs , log_outputs , or log_reference_outputs twice, the previous values will be overwritten. Another way to define example inputs and reference outputs is via pytest fixtures/parametrizations. By default any arguments to your test function will be logged as inputs on the corresponding example. If certain arguments are meant to represet reference outputs, you can specify that they should be logged as such using @pytest.mark.langsmith(output_keys=[\"name_of_ref_output_arg\"]) : {\"c\": 5} and reference outputs {\"d\": 6} , and run output {\"d\": 10} . Log feedback By default LangSmith collects the pass/fail rate under thepass feedback key for each test case. You can add additional feedback with log_feedback . trace_feedback() context manager. This makes it so that the LLM-as-judge call is traced separately from the rest of the test case. Instead of showing up in the main test case run it will instead show up in the trace for the correct feedback key. NOTE: Make sure that the log_feedback call associated with the feedback trace occurs inside the trace_feedback context. This way we\u2019ll be able to associate the feedback with the trace, and when seeing the feedback in the UI you\u2019ll be able to click on it to see the trace that generated it. Trace intermediate calls LangSmith will automatically trace any traceable intermediate calls that happen in the course of test case execution.Grouping tests into a test suite By default, all tests within a given file will be grouped as a single \u201ctest suite\u201d with a corresponding dataset. You can configure which test suite a test belongs to by passing thetest_suite_name parameter to @pytest.mark.langsmith for case-by-case grouping, or you can set the LANGSMITH_TEST_SUITE env var to group all tests from an execution into a single test suite: LANGSMITH_TEST_SUITE to get a consolidated view of all of your results. Naming experiments You can name an experiment using theLANGSMITH_EXPERIMENT env var: Caching LLMs on every commit in CI can get expensive. To save time and resources, LangSmith lets you cache HTTP requests to disk. To enable caching, install withlangsmith[pytest] and set an env var: LANGSMITH_TEST_CACHE=/my/cache/path : tests/cassettes and loaded from there on subsequent runs. If you check this in to your repository, your CI will be able to use the cache as well. In langsmith>=0.4.10 , you may selectively enable caching for requests to individual URLs or hostnames like this: pytest features @pytest.mark.langsmith is designed to stay out of your way and works well with familiar pytest features. Parametrize with pytest.mark.parametrize You can use the parametrize decorator as before. This will create a new test case for each parametrized instance of the test. evaluate() instead. This parallelizes the evaluation and makes it easier to control individual experiments and the corresponding dataset. Parallelize with pytest-xdist You can use pytest-xdist as you normally would to parallelize test execution: Async tests with pytest-asyncio @pytest.mark.langsmith works with sync or async tests, so you can run async tests exactly as before. Watch mode with pytest-watch Use watch mode to quickly iterate on your tests. We highly recommend ony using this with test caching (see below) enabled to avoid unnecessary LLM calls: Rich outputs If you\u2019d like to see a rich display of the LangSmith results of your test run you can specify--langsmith-output : --output=langsmith in langsmith<=0.3.3 but was updated to avoid collisions with other pytest plugins. You\u2019ll get a nice table per test suite that updates live as the results are uploaded to LangSmith: Some important notes for using this feature: - Make sure you\u2019ve installed pip install -U \"langsmith[pytest]\" - Rich outputs do not currently work with pytest-xdist Dry-run mode If you want to run the tests without syncing the results to LangSmith, you can setLANGSMITH_TEST_TRACKING=false in your environment. Expectations LangSmith provides an expect utility to help define expectations about your LLM output. For example:assert ing that the expectation is met possibly triggering a test failure. expect also provides \u201cfuzzy match\u201d methods. For example: - The embedding_distance between the prediction and the expectation - The binary", "tokens": 1000, "node_type": "child"}
{"id": 166, "chunk_id": "9b0fd3b850698317bef57fa6e71a38ba", "parent_id": 147, "url": "", "namespace": "langchain", "title": "langsmith-pytest", "headers": ["langsmith-pytest"], "section_index": 0, "chunk_index": 1, "text": "avoid unnecessary LLM calls: Rich outputs If you\u2019d like to see a rich display of the LangSmith results of your test run you can specify--langsmith-output : --output=langsmith in langsmith<=0.3.3 but was updated to avoid collisions with other pytest plugins. You\u2019ll get a nice table per test suite that updates live as the results are uploaded to LangSmith: Some important notes for using this feature: - Make sure you\u2019ve installed pip install -U \"langsmith[pytest]\" - Rich outputs do not currently work with pytest-xdist Dry-run mode If you want to run the tests without syncing the results to LangSmith, you can setLANGSMITH_TEST_TRACKING=false in your environment. Expectations LangSmith provides an expect utility to help define expectations about your LLM output. For example:assert ing that the expectation is met possibly triggering a test failure. expect also provides \u201cfuzzy match\u201d methods. For example: - The embedding_distance between the prediction and the expectation - The binary expectation score (1 if cosine distance is less than 0.5, 0 if not) - The edit_distance between the prediction and the expectation - The overall test pass/fail score (binary) expect utility is modeled off of Jest\u2019s expect API, with some off-the-shelf functionality to make it easier to grade your LLMs. Legacy @test / @unit decorator The legacy method for marking test cases is using the @test or @unit decorators:", "tokens": 219, "node_type": "child"}
{"id": 167, "chunk_id": "c16acf8a553ac24f5d77e7d3a1c6e493", "parent_id": 148, "url": "", "namespace": "langchain", "title": "langsmith-quick-start-studio", "headers": ["langsmith-quick-start-studio"], "section_index": 0, "chunk_index": 0, "text": "# langsmith-quick-start-studio\n\n> Source: https://docs.langchain.com/langsmith/quick-start-studio\n\n- Graphs deployed on cloud or self-hosted.\n- Graphs running locally with LangGraph server.\nDeployed graphs\nStudio is accessed in the LangSmith UI from the Deployments navigation. For applications that are deployed, you can access Studio as part of that deployment. To do so, navigate to the deployment in the UI and select Studio. This will load Studio connected to your live deployment, allowing you to create, read, and update the threads, assistants, and memory in that deployment.Local development server\nPrerequisites\nTo test your application locally using Studio:- Follow the local application quickstart first.\n- If you don\u2019t want data traced to LangSmith, set\nLANGSMITH_TRACING=false\nin your application\u2019s.env\nfile. With tracing disabled, no data leaves your local server.\nSetup\n-\nInstall the LangGraph CLI:\nThis will start the LangGraph Server locally, running in-memory. The server will run in watch mode, listening for and automatically restarting on code changes. Read this reference to learn about all the options for starting the API server. You will see the following logs:Browser Compatibility Safari blocks\nlocalhost\nconnections to Studio. To work around this, run the command with--tunnel\nto access Studio via a secure tunnel.Once running, you will automatically be directed to Studio. -\nFor a running server, access the Dbugger with one of the following:\n- Directly navigate to the following URL:\nhttps://smith.langchain.com/studio/?baseUrl=http://127.0.0.1:2024\n. - Navigate to Deployments in the UI, click the Studio button on a deployment, enter\nhttp://127.0.0.1:2024\nand click Connect.\nbaseUrl\nto match. - Directly navigate to the following URL:\n(Optional) Attach a debugger\nFor step-by-step debugging with breakpoints and variable inspection, run the following:- VS Code\n- PyCharm\nAdd this configuration to\nlaunch.json\n:Next steps\nFor more information on how to run Studio, refer to the following guides:- Run application\n- Manage assistants\n- Manage threads\n- Iterate on prompts\n- Debug LangSmith traces\n- Add node to dataset", "tokens": 314, "node_type": "child"}
{"id": 168, "chunk_id": "c16acf8a553ac24f5d77e7d3a1c6e493", "parent_id": 149, "url": "", "namespace": "langchain", "title": "langsmith-rate-limiting", "headers": ["langsmith-rate-limiting"], "section_index": 0, "chunk_index": 0, "text": "# langsmith-rate-limiting\n\n> Source: https://docs.langchain.com/langsmith/rate-limiting\n\nA common issue when running large evaluation jobs is running into third-party API rate limits, usually from model providers. There are a few ways to deal with rate limits.\nIf you\u2019re using langchain Python ChatModels in your application or evaluators, you can add rate limiters to your model(s) that will add client-side control of the frequency with which requests are sent to the model provider API to avoid rate limit errors.\nCopy\nfrom langchain.chat_models import init_chat_modelfrom langchain_core.rate_limiters import InMemoryRateLimiterrate_limiter = InMemoryRateLimiter( requests_per_second=0.1, # <-- Super slow! We can only make a request once every 10 seconds!! check_every_n_seconds=0.1, # Wake up every 100 ms to check whether allowed to make a request, max_bucket_size=10, # Controls the maximum burst size.)llm = init_chat_model(\"gpt-4o\", rate_limiter=rate_limiter)def app(inputs: dict) -> dict: response = llm.invoke(...) ...def evaluator(inputs: dict, outputs: dict, reference_outputs: dict) -> dict: response = llm.invoke(...) ...\nSee the langchain documentation for more on how to configure rate limiters.\nA very common way to deal with rate limit errors is retrying with exponential backoff. Retrying with exponential backoff means repeatedly retrying failed requests with an (exponentially) increasing wait time between each retry. This continues until either the request succeeds or a maximum number of requests is made.\nIf you\u2019re not using langchain you can use other libraries like tenacity (Python) or backoff (Python) to implement retries with exponential backoff, or you can implement it from scratch. See some examples of how to do this in the OpenAI docs.\nLimiting the number of concurrent calls you\u2019re making to your application and evaluators is another way to decrease the frequency of model calls you\u2019re making, and in that way avoid rate limit errors. max_concurrency can be set directly on the evaluate() / aevaluate() functions. This parallelizes evaluation by effectively splitting the dataset across threads.\nCopy\nfrom langsmith import aevaluateresults = await aevaluate( ... max_concurrency=4,)", "tokens": 311, "node_type": "child"}
{"id": 169, "chunk_id": "c16acf8a553ac24f5d77e7d3a1c6e493", "parent_id": 150, "url": "", "namespace": "langchain", "title": "langsmith-regions-faq", "headers": ["langsmith-regions-faq"], "section_index": 0, "chunk_index": 0, "text": "# langsmith-regions-faq\n\n> Source: https://docs.langchain.com/langsmith/regions-faq\n\nWhat privacy and data protection frameworks does LangSmith, including its EU instance, comply with?\nLangSmith complies with the General Data Protection Regulation (GDPR) and other laws and regulations applicable to the LangSmith service. We are also SOC 2 Type 2 certified and are HIPAA compliant. You can request more information about our security policies and posture at trust.langchain.com. If you would like to sign a Data Processing Addendum (DPA) with us, please reach out to support@langchain.dev. Please note we only enter into Business Associate Agreements (BAAs) with customers on our Enterprise plan.\nAre there any functional differences between US and EU cloud-managed LangSmith?\nThere may be a small delay between launches to each region depending on the feature. Besides that, they are functionally equivalent - all features supported in the US are supported in the EU and vice versa.", "tokens": 144, "node_type": "child"}
{"id": 170, "chunk_id": "c16acf8a553ac24f5d77e7d3a1c6e493", "parent_id": 151, "url": "", "namespace": "langchain", "title": "langsmith-reject-concurrent", "headers": ["langsmith-reject-concurrent"], "section_index": 0, "chunk_index": 0, "text": "# langsmith-reject-concurrent\n\n> Source: https://docs.langchain.com/langsmith/reject-concurrent\n\nThis guide assumes knowledge of what double-texting is, which you can learn about in the double-texting conceptual guide.The guide covers the reject option for double texting, which rejects the new run of the graph by throwing an error and continues with the original run until completion. Below is a quick example of using the reject option.\nNow, let\u2019s import our required packages and instantiate our client, assistant, and thread.\nPython\nJavascript\nCURL\nCopy\nimport httpxfrom langchain_core.messages import convert_to_messagesfrom langgraph_sdk import get_clientclient = get_client(url=<DEPLOYMENT_URL>)# Using the graph deployed with the name \"agent\"assistant_id = \"agent\"thread = await client.threads.create()\nNow we can run a thread and try to run a second one with the \u201creject\u201d option, which should fail since we have already started a run:\nPython\nJavascript\nCURL\nCopy\nrun = await client.runs.create( thread[\"thread_id\"], assistant_id, input={\"messages\": [{\"role\": \"user\", \"content\": \"what's the weather in sf?\"}]},)try: await client.runs.create( thread[\"thread_id\"], assistant_id, input={ \"messages\": [{\"role\": \"user\", \"content\": \"what's the weather in nyc?\"}] }, multitask_strategy=\"reject\", )except httpx.HTTPStatusError as e: print(\"Failed to start concurrent run\", e)\nOutput:\nCopy\nFailed to start concurrent run Client error '409 Conflict' for url 'http://localhost:8123/threads/f9e7088b-8028-4e5c-88d2-9cc9a2870e50/runs'For more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/409\nWe can verify that the original thread finished executing:\nPython\nJavascript\nCURL\nCopy\n# wait until the original run completesawait client.runs.join(thread[\"thread_id\"], run[\"run_id\"])state = await client.threads.get_state(thread[\"thread_id\"])for m in convert_to_messages(state[\"values\"][\"messages\"]): m.pretty_print()\nOutput:\nCopy\n================================ Human Message =================================what's the weather in sf?================================== Ai Message ==================================[{'id': 'toolu_01CyewEifV2Kmi7EFKHbMDr1', 'input': {'query': 'weather in san francisco'}, 'name': 'tavily_search_results_json', 'type': 'tool_use'}]Tool Calls:tavily_search_results_json (toolu_01CyewEifV2Kmi7EFKHbMDr1)Call ID: toolu_01CyewEifV2Kmi7EFKHbMDr1Args:query: weather in san francisco================================= Tool Message =================================Name: tavily_search_results_json[{\"url\": \"https://www.accuweather.com/en/us/san-francisco/94103/june-weather/347629\", \"content\": \"Get the monthly weather forecast for San Francisco, CA, including daily high/low, historical averages, to help you plan ahead.\"}]================================== Ai Message ==================================According to the search results from Tavily, the current weather in San Francisco is:The average high temperature in San Francisco in June is around 65\u00b0F (18\u00b0C), with average lows around 54\u00b0F (12\u00b0C). June tends to be one of the cooler and foggier months in San Francisco due to the marine layer of fog that often blankets the city during the summer months.Some key points about the typical June weather in San Francisco:* Mild temperatures with highs in the 60s F and lows in the 50s F* Foggy mornings that often burn off to sunny afternoons* Little to no rainfall, as June falls in the dry season* Breezy conditions, with winds off the Pacific Ocean* Layers are recommended for changing weather conditionsSo in summary, you can expect mild, foggy mornings giving way to sunny but cool afternoons in San Francisco this time of year. The marine layer keeps temperatures moderate compared to other parts of California in June.", "tokens": 436, "node_type": "child"}
{"id": 171, "chunk_id": "c16acf8a553ac24f5d77e7d3a1c6e493", "parent_id": 152, "url": "", "namespace": "langchain", "title": "langsmith-release-versions", "headers": ["langsmith-release-versions"], "section_index": 0, "chunk_index": 0, "text": "# langsmith-release-versions\n\n> Source: https://docs.langchain.com/langsmith/release-versions\n\nSkip to main content\nprovides different support levels for different versions, which may include new features, bug fixes, or security patches.\nSupport levels\nThere are four support levels:\nActive\nCritical\nEnd of life (EOL)\nDeprecated\nActive\nWhere N represents the latest minor version (e.g., 0.3, 0.4, etc.).\nThe current minor version (N) receives full support, including:\nNew features and capabilities\nBug fixes and regressions\nSecurity patches\nQuality-of-life improvements\nHigh confidence changes that are narrowly scoped\nCritical\nThe previous minor version (N-1) receives limited support:\nCritical security fixes\nInstallation fixes\nNo new features or general bug fixes\nTransitioned from Active when a newer minor version is released\nEnd of life (EOL)\nVersions older than N-2 (N-2, N-3, etc.) receive no support:\nNo new patch releases\nNo bug fixes, including known bugs\nNo security updates\nUsers should upgrade to a supported version\nDeprecated\nVersions that are no longer maintained:\nAll versions prior to the first stable release\nVersions that have been explicitly deprecated\nNo support or maintenance provided\nVersion support policy\nfollows an N-2 support policy for minor versions:\nN (Current) : Active support\nN-1 : Critical support\nN-2 and older : End of Life\nMinor version support\nMinor versions include new features and capabilities and are supported according to the N-2 policy. When we refer to a minor version, such as v0.3, we always mean its latest available patch release (v0.3.x).\nPatch releases\nDuring the support window for each version:\nActive Support : Regular patch releases with bug fixes, regressions, and new features\nCritical Support : Security-only releases for critical fixes related to security and installation\nEnd of Life : No new patches released\nRecommendations\nStay Current : We recommend upgrading to the latest minor version to receive full support and access to new features\nPlan Upgrades : Monitor the changelog for upcoming version changes and plan upgrades accordingly\nSecurity : Critical security fixes are only provided for Active and Critical support versions\nTesting : Test your applications with newer versions before upgrading in production\nVersion compatibility\nWhen upgrading between minor versions:\nReview the changelog for breaking changes\nTest your applications thoroughly\nFollow the upgrade guides provided in the documentation\nConsider the support timeline for your current version\nCurrent version support\nTo check the current supported versions and their support levels, refer to the LangGraph Server Changelog for the latest release information.", "tokens": 396, "node_type": "child"}
{"id": 172, "chunk_id": "c16acf8a553ac24f5d77e7d3a1c6e493", "parent_id": 153, "url": "", "namespace": "langchain", "title": "langsmith-repetition", "headers": ["langsmith-repetition"], "section_index": 0, "chunk_index": 0, "text": "# langsmith-repetition\n\n> Source: https://docs.langchain.com/langsmith/repetition\n\nRunning multiple repetitions can give a more accurate estimate of the performance of your system since LLM outputs are not deterministic. Outputs can differ from one repetition to the next. Repetitions are a way to reduce noise in systems prone to high variability, such as agents.\nAdd the optional num_repetitions param to the evaluate / aevaluate function (Python, TypeScript) to specify how many times to evaluate over each example in your dataset. For instance, if you have 5 examples in the dataset and set num_repetitions=5, each example will be run 5 times, for a total of 25 runs.\nViewing results of experiments run with repetitions\nIf you\u2019ve run your experiment with repetitions, there will be arrows in the output results column so you can view outputs in the table. To view each run from the repetition, hover over the output cell and click the expanded view. When you run an experiment with repetitions, LangSmith displays the average for each feedback score in the table. Click on the feedback score to view the feedback scores from individual runs, or to view the standard deviation across repetitions.", "tokens": 190, "node_type": "child"}
{"id": 173, "chunk_id": "c16acf8a553ac24f5d77e7d3a1c6e493", "parent_id": 154, "url": "", "namespace": "langchain", "title": "langsmith-resource-auth", "headers": ["langsmith-resource-auth"], "section_index": 0, "chunk_index": 0, "text": "# langsmith-resource-auth\n\n> Source: https://docs.langchain.com/langsmith/resource-auth\n\nPrerequisites\nBefore you start this tutorial, ensure you have the bot from the first tutorial running without errors.1. Add resource authorization\nRecall that in the last tutorial, theAuth\nobject lets you register an authentication function, which LangSmith uses to validate the bearer tokens in incoming requests. Now you\u2019ll use it to register an authorization handler.\nAuthorization handlers are functions that run after authentication succeeds. These handlers can add metadata to resources (like who owns them) and filter what each user can see.\nUpdate your src/security/auth.py\nand add one authorization handler to run on every request:\nsrc/security/auth.py\nctx\n(AuthContext): contains info about the currentuser\n, the user\u2019spermissions\n, theresource\n(\u201cthreads\u201d, \u201ccrons\u201d, \u201cassistants\u201d), and theaction\nbeing taken (\u201ccreate\u201d, \u201cread\u201d, \u201cupdate\u201d, \u201cdelete\u201d, \u201csearch\u201d, \u201ccreate_run\u201d)value\n(dict\n): data that is being created or accessed. The contents of this dict depend on the resource and action being accessed. See adding scoped authorization handlers below for information on how to get more tightly scoped access control.\n- Adds the user\u2019s ID to the resource\u2019s metadata.\n- Returns a metadata filter so users only see resources they own.\n2. Test private conversations\nTest your authorization. If you have set things up correctly, you will see all \u2705 messages. Be sure to have your development server running (runlanggraph dev\n):\n- Each user can create and chat in their own threads\n- Users can\u2019t see each other\u2019s threads\n- Listing threads only shows your own\n3. Add scoped authorization handlers\nThe broad@auth.on\nhandler matches on all authorization events. This is concise, but it means the contents of the value\ndict are not well-scoped, and the same user-level access control is applied to every resource. If you want to be more fine-grained, you can also control specific actions on resources.\nUpdate src/security/auth.py\nto add handlers for specific resource types:\n- Creating threads\n- Reading threads\n- Accessing assistants\n@auth.on.assistants\n) matches any action on the assistants\nresource. For each request, LangGraph will run the most specific handler that matches the resource and action being accessed. This means that the four handlers above will run rather than the broadly scoped \u201c@auth.on\n\u201d handler.\nTry adding the following test code to your test file:\nWhat\u2019s Next?\nNow that you can control access to resources, you might want to:- Move on to Connect an authentication provider to add real user accounts.\n- Read more about authorization patterns.\n- Check out the API reference for details about the interfaces and methods used in this tutorial.", "tokens": 417, "node_type": "child"}
{"id": 174, "chunk_id": "c16acf8a553ac24f5d77e7d3a1c6e493", "parent_id": 155, "url": "", "namespace": "langchain", "title": "langsmith-rollback-concurrent", "headers": ["langsmith-rollback-concurrent"], "section_index": 0, "chunk_index": 0, "text": "# langsmith-rollback-concurrent\n\n> Source: https://docs.langchain.com/langsmith/rollback-concurrent\n\nThis guide assumes knowledge of what double-texting is, which you can learn about in the double-texting conceptual guide.The guide covers the rollback option for double texting, which interrupts the prior run of the graph and starts a new one with the double-text. This option is very similar to the interrupt option, but in this case the first run is completely deleted from the database and cannot be restarted. Below is a quick example of using the rollback option.\nNow, let\u2019s import our required packages and instantiate our client, assistant, and thread.\nPython\nJavascript\nCURL\nCopy\nimport asyncioimport httpxfrom langchain_core.messages import convert_to_messagesfrom langgraph_sdk import get_clientclient = get_client(url=<DEPLOYMENT_URL>)# Using the graph deployed with the name \"agent\"assistant_id = \"agent\"thread = await client.threads.create()\nNow let\u2019s run a thread with the multitask parameter set to \u201crollback\u201d:\nPython\nJavascript\nCURL\nCopy\n# the first run will be rolled backrolled_back_run = await client.runs.create( thread[\"thread_id\"], assistant_id, input={\"messages\": [{\"role\": \"user\", \"content\": \"what's the weather in sf?\"}]},)run = await client.runs.create( thread[\"thread_id\"], assistant_id, input={\"messages\": [{\"role\": \"user\", \"content\": \"what's the weather in nyc?\"}]}, multitask_strategy=\"rollback\",)# wait until the second run completesawait client.runs.join(thread[\"thread_id\"], run[\"run_id\"])\nWe can see that the thread has data only from the second run\nPython\nJavascript\nCURL\nCopy\nstate = await client.threads.get_state(thread[\"thread_id\"])for m in convert_to_messages(state[\"values\"][\"messages\"]): m.pretty_print()\nOutput:\nCopy\n================================ Human Message =================================what's the weather in nyc?================================== Ai Message ==================================[{'id': 'toolu_01JzPqefao1gxwajHQ3Yh3JD', 'input': {'query': 'weather in nyc'}, 'name': 'tavily_search_results_json', 'type': 'tool_use'}]Tool Calls:tavily_search_results_json (toolu_01JzPqefao1gxwajHQ3Yh3JD)Call ID: toolu_01JzPqefao1gxwajHQ3Yh3JDArgs:query: weather in nyc================================= Tool Message =================================Name: tavily_search_results_json[{\"url\": \"https://www.weatherapi.com/\", \"content\": \"{'location': {'name': 'New York', 'region': 'New York', 'country': 'United States of America', 'lat': 40.71, 'lon': -74.01, 'tz_id': 'America/New_York', 'localtime_epoch': 1718734479, 'localtime': '2024-06-18 14:14'}, 'current': {'last_updated_epoch': 1718733600, 'last_updated': '2024-06-18 14:00', 'temp_c': 29.4, 'temp_f': 84.9, 'is_day': 1, 'condition': {'text': 'Sunny', 'icon': '//cdn.weatherapi.com/weather/64x64/day/113.png', 'code': 1000}, 'wind_mph': 2.2, 'wind_kph': 3.6, 'wind_degree': 158, 'wind_dir': 'SSE', 'pressure_mb': 1025.0, 'pressure_in': 30.26, 'precip_mm': 0.0, 'precip_in': 0.0, 'humidity': 63, 'cloud': 0, 'feelslike_c': 31.3, 'feelslike_f': 88.3, 'windchill_c': 28.3, 'windchill_f': 82.9, 'heatindex_c': 29.6, 'heatindex_f': 85.3, 'dewpoint_c': 18.4, 'dewpoint_f': 65.2, 'vis_km': 16.0, 'vis_miles': 9.0, 'uv': 7.0, 'gust_mph': 16.5, 'gust_kph': 26.5}}\"}]================================== Ai Message ==================================The weather API results show that the current weather in New York City is sunny with a temperature of around 85\u00b0F (29\u00b0C). The wind is light at around 2-3 mph from the south-southeast. Overall it looks like a nice sunny summer day in NYC.\nVerify that the original, rolled back run was deleted\nPython\nJavascript\nCopy\ntry: await client.runs.get(thread[\"thread_id\"], rolled_back_run[\"run_id\"])except httpx.HTTPStatusError as _: print(\"Original run was correctly deleted\")", "tokens": 404, "node_type": "child"}
{"id": 175, "chunk_id": "c16acf8a553ac24f5d77e7d3a1c6e493", "parent_id": 156, "url": "", "namespace": "langchain", "title": "langsmith-rules", "headers": ["langsmith-rules"], "section_index": 0, "chunk_index": 0, "text": "# langsmith-rules\n\n> Source: https://docs.langchain.com/langsmith/rules\n\nWhile you can manually sift through and process production logs from your LLM application, it often becomes difficult as your application scales to more users.\nLangSmith provides a powerful feature called Automations that allow you to trigger certain actions on your trace data.\nAt a high level, automations are defined by a filter, sampling rate, and action.Automation rules can trigger actions such as: adding traces to a dataset, adding to an annotation queue, triggering a webhook (e.g. for remote evaluations) or extending data retention. Some examples of automations you can set up:\nSend all traces with negative feedback to an annotation queue for human review\nSend 10% of all traces to an annotation queue for human review to spot check for issues\nUpgrade all traces with errors for extended data retention\nHead to the Tracing Projects tab and select a tracing project. To view existing automation rules for that tracing project, click on the Automations tab.\nHead to the Tracing Projects tab and select a tracing project. Click on + New in the top right corner of the tracing project page, then click on New Automation.\nConfigure a sampling rate to control the percentage of filtered runs that trigger the automation action.You can specify a sampling rate between 0 and 1 for automations. This will control the percent of the filtered runs that are sent to an automation action. For example, if you set the sampling rate to 0.5, then 50% of the traces that pass the filter will be sent to the action.\nApply rule to past runs by toggling the Apply to past runs and entering a \u201cBackfill from\u201d date. This is only possible upon rule creation. Note: the backfill is processed as a background job, so you will not see the results immediately. In order to track progress of the backfill, you can view logs for your automations\nTrigger webhook: Trigger a webhook with the trace data. For more information on webhooks, you can refer to this guide.\nExtend data retention: Extends the data retention period on matching traces that use base retention (see data retention docs for more details).\nNote that all other rules will also extend data retention on matching traces through the\nauto-upgrade mechanism described in the aforementioned data retention docs,\nbut this rule takes no additional action.\nLogs allow you to gain confidence that your rules are working as expected. You can view logs for your automations by heading to the Automations tab within a tracing project and clicking the Logs button for the rule you created.The logs tab allows you to:\nView all runs processed by a given rule for the time period selected\nIf a particular rule execution has triggered an error, you can view the error message by hovering over the error icon\nYou can monitor the progress of a backfill job by filtering to the rule\u2019s creation timestamp. This is because the backfill starts from when the rule was created.\nInspect the run that the automation rule applied to using the View run button. For rules that add runs as examples to datasets, you can view the example produced.", "tokens": 525, "node_type": "child"}
{"id": 176, "chunk_id": "c16acf8a553ac24f5d77e7d3a1c6e493", "parent_id": 157, "url": "", "namespace": "langchain", "title": "langsmith-run-backtests-new-agent", "headers": ["langsmith-run-backtests-new-agent"], "section_index": 0, "chunk_index": 0, "text": "# langsmith-run-backtests-new-agent\n\n> Source: https://docs.langchain.com/langsmith/run-backtests-new-agent\n\n- Select sample runs from your production tracing project to test against.\n- Transform the run inputs into a dataset and record the run outputs as an initial experiment against that dataset.\n- Execute your new system on the new dataset and compare the results of the experiments.\nOften, you won\u2019t have definitive \u201cground truth\u201d answers available. In such cases, you can manually label the outputs or use evaluators that don\u2019t rely on reference data. If your application allows for capturing ground-truth labels, for example by allowing users to leave feedback, we strongly recommend doing so.\nSetup\nConfigure the environment\nInstall and set environment variables. This guide requireslangsmith>=0.2.4\n.\nFor convenience we\u2019ll use the LangChain OSS framework in this tutorial, but the LangSmith functionality shown is framework-agnostic.\nDefine the application\nFor this example lets create a simple Tweet-writing application that has access to some internet search tools:Simulate production data\nNow lets simulate some production data:Convert Production Traces to Experiment\nThe first step is to generate a dataset based on the production inputs. Then copy over all the traces to serve as a baseline experiment.Select runs to backtest on\nYou can select the runs to backtest on using thefilter\nargument of list_runs\n. The filter\nargument uses the LangSmith trace query syntax to select runs.\nConvert runs to experiment\nconvert_runs_to_test\nis a function which takes some runs and does the following:\n- The inputs, and optionally the outputs, are saved to a dataset as Examples.\n- The inputs and outputs are stored as an experiment, as if you had run the\nevaluate\nfunction and received those outputs.\nBenchmark against new system\nNow we can start the process of benchmarking our production runs against a new system.Define evaluators\nFirst let\u2019s define the evaluators we will use to compare the two systems. Note that we have no reference outputs, so we\u2019ll need to come up with evaluation metrics that only require the actual outputs.Evaluate baseline\nNow, let\u2019s run our evaluators against the baseline experiment.Define and evaluate new system\nNow, let\u2019s define and evaluate our new system. In this example our new system will be the same as the old system, but will use GPT-4o instead of GPT-3.5. Since we\u2019ve made our model configurable we can just update the default config passed to our agent:Comparing the results\nAfter running both experiments, you can view them in your dataset: The results reveal an interesting tradeoff between the two models:- GPT-4o shows improved performance in following formatting rules, consistently including the requested number of emojis\n- However, GPT-4o is less reliable at staying grounded in the provided search results\n- Refine our prompts to more strongly emphasize using only provided information\n- Or modify our system architecture to better constrain the model\u2019s outputs", "tokens": 462, "node_type": "child"}
{"id": 177, "chunk_id": "c16acf8a553ac24f5d77e7d3a1c6e493", "parent_id": 158, "url": "", "namespace": "langchain", "title": "langsmith-run-data-format", "headers": ["langsmith-run-data-format"], "section_index": 0, "chunk_index": 0, "text": "# langsmith-run-data-format\n\n> Source: https://docs.langchain.com/langsmith/run-data-format\n\nLangSmith stores and processes trace data in a simple format that is easy to export and import.\nMany of these fields are optional or not important to know about but are included for completeness. The bolded fields are the most important ones to know about.\n| Field Name | Type | Description |\n|---|---|---|\n| id | UUID | Unique identifier for the span. |\n| name | string | The name associated with the run. |\n| inputs | object | A map or set of inputs provided to the run. |\n| run_type | string | Type of run, e.g., \u201cllm\u201d, \u201cchain\u201d, \u201ctool\u201d. |\n| start_time | datetime | Start time of the run. |\n| end_time | datetime | End time of the run. |\n| extra | object | Any extra information run. |\n| error | string | Error message if the run encountered an error. |\n| outputs | object | A map or set of outputs generated by the run. |\n| events | array of objects | A list of event objects associated with the run. This is relevant for runs executed with streaming. |\n| tags | array of strings | Tags or labels associated with the run. |\n| trace_id | UUID | Unique identifier for the trace the run is a part of. This is also the id field of the root run of the trace |\n| dotted_order | string | Ordering string, hierarchical. Format: run_start_time Zrun_uuid .child_run_start_time Zchild_run_uuid \u2026 |\n| status | string | Current status of the run execution, e.g., \u201cerror\u201d, \u201cpending\u201d, \u201csuccess\u201d |\n| child_run_ids | array of UUIDs | List of IDs for all child runs. |\n| direct_child_run_ids | array of UUIDs | List of IDs for direct children of this run. |\n| parent_run_ids | array of UUIDs | List of IDs for all parent runs. |\n| feedback_stats | object | Aggregations of feedback statistics for this run |\n| reference_example_id | UUID | ID of a reference example associated with the run. This is usually only present for evaluation runs. |\n| total_tokens | integer | Total number of tokens processed by the run. |\n| prompt_tokens | integer | Number of tokens in the prompt of the run. |\n| completion_tokens | integer | Number of tokens in the completion of the run. |\n| total_cost | string | Total cost associated with processing the run. |\n| prompt_cost | string | Cost associated with the prompt part of the run. |\n| completion_cost | string | Cost associated with the completion of the run. |\n| first_token_time | datetime | Time when the first token of a model output was generated. Only applies for runs with run_type=\"llm\" and streaming enabled. |\n| session_id | string | Session identifier for the run, also known as the tracing project ID. |\n| in_dataset | boolean | Indicates whether the run is included in a dataset. |\n| parent_run_id | UUID | Unique identifier of the parent run. |\n| execution_order (deprecated) | integer | The order in which this run was executed within the trace. |\n| serialized | object | Serialized state of the object executing the run if applicable. |\n| manifest_id (deprecated) | UUID | Identifier for a manifest associated with the span. |\n| manifest_s3_id | UUID | S3 identifier for the manifest. |\n| inputs_s3_urls | object | S3 URLs for the inputs. |\n| outputs_s3_urls | object | S3 URLs for the outputs. |\n| price_model_id | UUID | Identifier for the pricing model applied to the run. |\n| app_path | string | Application (UI) path for this run. |\n| last_queued_at | datetime | Last time the span was queued. |\n| share_token | string | Token for sharing access to the run\u2019s data. |\nWhat is dotted_order\n?\nA run\u2019s dotted order is a sortable key that fully specifies its location within the tracing hierarchy.\nTake the following example:\n- The \u201cid\u201d is equal to the last 36 characters of the dotted order (the suffix after the final \u201cZ\u201d). See\n0ec6b845-18b9-4aa1-8f1b-6ba3f9fdefd6\nfor example in the grandchild. - The \u201ctrace_id\u201d is equal to the first UUID in the dotted order (i.e.,\ndotted_order.split('.')[0].split('Z')[1]\n) - If \u201cparent_run_id\u201d exists, it is the penultimate UUID in the dotted order. See\na8024e23-5b82-47fd-970e-f6a5ba3f5097\nin the grandchild, for an example. - If you split the dotted_order on the dots, each segment is formatted as (\n<run_start_time>Z<run_id>\n)", "tokens": 755, "node_type": "child"}
{"id": 178, "chunk_id": "c16acf8a553ac24f5d77e7d3a1c6e493", "parent_id": 159, "url": "", "namespace": "langchain", "title": "langsmith-run-evals-api-only", "headers": ["langsmith-run-evals-api-only"], "section_index": 0, "chunk_index": 0, "text": "# langsmith-run-evals-api-only\n\n> Source: https://docs.langchain.com/langsmith/run-evals-api-only\n\nrequests\nlibrary in Python as an example. However, the same principles apply to any language.\nBefore diving into this content, it might be helpful to read the following:\nCreate a dataset\nHere, we are using the python SDK for convenience. You can also use the API directly use the UI, see this guide for more information.Copy\nimport os\nimport requests\nfrom datetime import datetime\nfrom langsmith import Client\nfrom openai import OpenAI\nfrom uuid import uuid4\nclient = Client()\noa_client = OpenAI()\n# Create a dataset\nexamples = [\n{\n\"inputs\": {\"text\": \"Shut up, idiot\"},\n\"outputs\": {\"label\": \"Toxic\"},\n},\n{\n\"inputs\": {\"text\": \"You're a wonderful person\"},\n\"outputs\": {\"label\": \"Not toxic\"},\n},\n{\n\"inputs\": {\"text\": \"This is the worst thing ever\"},\n\"outputs\": {\"label\": \"Toxic\"},\n},\n{\n\"inputs\": {\"text\": \"I had a great day today\"},\n\"outputs\": {\"label\": \"Not toxic\"},\n},\n{\n\"inputs\": {\"text\": \"Nobody likes you\"},\n\"outputs\": {\"label\": \"Toxic\"},\n},\n{\n\"inputs\": {\"text\": \"This is unacceptable. I want to speak to the manager.\"},\n\"outputs\": {\"label\": \"Not toxic\"},\n},\n]\ndataset_name = \"Toxic Queries - API Example\"\ndataset = client.create_dataset(dataset_name=dataset_name)\nclient.create_examples(dataset_id=dataset.id, examples=examples)\nRun a single experiment\nFirst, pull all of the examples you\u2019d want to use in your experiment.Copy\n# Pick a dataset id. In this case, we are using the dataset we created above.\n# Spec: https://api.smith.langchain.com/redoc#tag/examples/operation/delete_example_api_v1_examples__example_id__delete\ndataset_id = dataset.id\nparams = { \"dataset\": dataset_id }\nresp = requests.get(\n\"https://api.smith.langchain.com/api/v1/examples\",\nparams=params,\nheaders={\"x-api-key\": os.environ[\"LANGSMITH_API_KEY\"]}\n)\nexamples = resp.json()\nCopy\nos.environ[\"OPENAI_API_KEY\"] = \"sk-...\"\ndef run_completion_on_example(example, model_name, experiment_id):\n\"\"\"Run completions on a list of examples.\"\"\"\n# We are using the OpenAI API here, but you can use any model you like\ndef _post_run(run_id, name, run_type, inputs, parent_id=None):\n\"\"\"Function to post a new run to the API.\"\"\"\ndata = {\n\"id\": run_id.hex,\n\"name\": name,\n\"run_type\": run_type,\n\"inputs\": inputs,\n\"start_time\": datetime.utcnow().isoformat(),\n\"reference_example_id\": example[\"id\"],\n\"session_id\": experiment_id,\n}\nif parent_id:\ndata[\"parent_run_id\"] = parent_id.hex\nresp = requests.post(\n\"https://api.smith.langchain.com/api/v1/runs\", # Update appropriately for self-hosted installations or the EU region\njson=data,\nheaders=headers\n)\nresp.raise_for_status()\ndef _patch_run(run_id, outputs):\n\"\"\"Function to patch a run with outputs.\"\"\"\nresp = requests.patch(\nf\"https://api.smith.langchain.com/api/v1/runs/{run_id}\",\njson={\n\"outputs\": outputs,\n\"end_time\": datetime.utcnow().isoformat(),\n},\nheaders=headers,\n)\nresp.raise_for_status()\n# Send your API Key in the request headers\nheaders = {\"x-api-key\": os.environ[\"LANGSMITH_API_KEY\"]}\ntext = example[\"inputs\"][\"text\"]\nmessages = [\n{\n\"role\": \"system\",\n\"content\": \"Please review the user query below and determine if it contains any form of toxic behavior, such as insults, threats, or highly negative comments. Respond with 'Toxic' if it does, and 'Not toxic' if it doesn't.\",\n},\n{\"role\": \"user\", \"content\": text},\n]\n# Create parent run\nparent_run_id = uuid4()\n_post_run(parent_run_id, \"LLM Pipeline\", \"chain\", {\"text\": text})\n# Create child run\nchild_run_id = uuid4()\n_post_run(child_run_id, \"OpenAI Call\", \"llm\", {\"messages\": messages}, parent_run_id)\n# Generate completion\nchat_completion = oa_client.chat.completions.create(model=model_name, messages=messages)\noutput_text = chat_completion.choices[0].message.content\n# End run\n_patch_run(child_run_id, {\n\"messages\": messages,\n\"output\": output_text,\n\"model\": model_name\n})\n_patch_run(parent_run_id, {\"label\": output_text})\nCopy\n# Create a new experiment using the /sessions endpoint\n# An experiment is a collection of runs with a reference to the dataset used\n# Spec: https://api.smith.langchain.com/redoc#tag/tracer-sessions/operation/create_tracer_session_api_v1_sessions_post\nmodel_names = (\"gpt-3.5-turbo\", \"gpt-4o-mini\")\nexperiment_ids = []\nfor model_name in model_names:\nresp = requests.post(\n\"https://api.smith.langchain.com/api/v1/sessions\",\njson={\n\"start_time\": datetime.utcnow().isoformat(),\n\"reference_dataset_id\": str(dataset_id),\n\"description\": \"An optional description for the experiment\",\n\"name\": f\"Toxicity detection - API Example - {model_name} - {str(uuid4())[0:8]}\", # A name for the experiment\n\"extra\": {\n\"metadata\": {\"foo\": \"bar\"}, # Optional metadata\n},\n},\nheaders={\"x-api-key\": os.environ[\"LANGSMITH_API_KEY\"]}\n)\nexperiment = resp.json()\nexperiment_ids.append(experiment[\"id\"])\n# Run completions on all examples\nfor example in examples:\nrun_completion_on_example(example, model_name, experiment[\"id\"])\n# Issue a patch request to \"end\" the experiment by updating the end_time\nrequests.patch(\nf\"https://api.smith.langchain.com/api/v1/sessions/{experiment['id']}\",\njson={\"end_time\": datetime.utcnow().isoformat()},\nheaders={\"x-api-key\": os.environ[\"LANGSMITH_API_KEY\"]}\n)\nRun a pairwise experiment\nNext, we\u2019ll demonstrate how to run a pairwise experiment. In a pairwise experiment, you compare two examples against each other. For more information, check out this guide.Copy\n# A comparative experiment allows you to provide a preferential ranking on the outputs of two or more experiments\n# Spec: https://api.smith.langchain.com/redoc#tag/datasets/operation/create_comparative_experiment_api_v1_datasets_comparative_post\nresp = requests.post(\n\"https://api.smith.langchain.com/api/v1/datasets/comparative\",\njson={\n\"experiment_ids\": experiment_ids,\n\"name\": \"Toxicity detection - API Example - Comparative - \" + str(uuid4())[0:8],\n\"description\": \"An optional description for the comparative experiment\",\n\"extra\": {\n\"metadata\": {\"foo\": \"bar\"}, # Optional metadata\n},\n\"reference_dataset_id\": str(dataset_id),\n},\nheaders={\"x-api-key\": os.environ[\"LANGSMITH_API_KEY\"]}\n)\ncomparative_experiment = resp.json()\ncomparative_experiment_id = comparative_experiment[\"id\"]\n# You can iterate over the runs in the experiments belonging to the comparative experiment and preferentially rank the outputs\n# Fetch the comparative experiment\nresp = requests.get(\nf\"https://api.smith.langchain.com/api/v1/datasets/{str(dataset_id)}/comparative\",\nparams={\"id\": comparative_experiment_id},\nheaders={\"x-api-key\": os.environ[\"LANGSMITH_API_KEY\"]}\n)\ncomparative_experiment = resp.json()[0]\nexperiment_ids = [info[\"id\"] for info in comparative_experiment[\"experiments_info\"]]\nfrom collections import defaultdict\nexample_id_to_runs_map = defaultdict(list)\n# Spec: https://api.smith.langchain.com/redoc#tag/run/operation/query_runs_api_v1_runs_query_post\nruns = requests.post(\nf\"https://api.smith.langchain.com/api/v1/runs/query\",\nheaders={\"x-api-key\": os.environ[\"LANGSMITH_API_KEY\"]},\njson={\n\"session\": experiment_ids,\n\"is_root\": True, # Only fetch root runs (spans) which contain the end outputs\n\"select\": [\"id\", \"reference_example_id\", \"outputs\"],\n}\n).json()\nruns = runs[\"runs\"]\nfor run in runs:\nexample_id = run[\"reference_example_id\"]\nexample_id_to_runs_map[example_id].append(run)\nfor example_id, runs in example_id_to_runs_map.items():\nprint(f\"Example ID: {example_id}\")\n# Preferentially rank the outputs, in this case we will always prefer the first output\n# In reality, you can use an LLM to rank the outputs\nfeedback_group_id = uuid4()\n# Post a feedback score for each run, with the first run being the preferred one\n# Spec: https://api.smith.langchain.com/redoc#tag/feedback/operation/create_feedback_api_v1_feedback_post\n# We'll use the feedback group ID to associate the feedback scores with the same group\nfor i, run in enumerate(runs):\nprint(f\"Run ID: {run['id']}\")\nfeedback = {\n\"score\": 1 if i == 0 else 0,\n\"run_id\": str(run[\"id\"]),\n\"key\": \"ranked_preference\",\n\"feedback_group_id\": str(feedback_group_id),\n\"comparative_experiment_id\": comparative_experiment_id,\n}\nresp = requests.post(\n\"https://api.smith.langchain.com/api/v1/feedback\",\njson=feedback,\nheaders={\"x-api-key\": os.environ[\"LANGSMITH_API_KEY\"]}\n)\nresp.raise_for_status()", "tokens": 892, "node_type": "child"}
{"id": 179, "chunk_id": "c16acf8a553ac24f5d77e7d3a1c6e493", "parent_id": 160, "url": "", "namespace": "langchain", "title": "langsmith-run-evaluation-from-prompt-playground", "headers": ["langsmith-run-evaluation-from-prompt-playground"], "section_index": 0, "chunk_index": 0, "text": "# langsmith-run-evaluation-from-prompt-playground\n\n> Source: https://docs.langchain.com/langsmith/run-evaluation-from-prompt-playground\n\nLangSmith allows you to run evaluations directly in the UI. The Prompt Playground allows you to test your prompt or model configuration over a series of inputs to see how well it scores across different contexts or scenarios, without having to write any code.Before you run an evaluation, you need to have an existing dataset. Learn how to create a dataset from the UI.If you prefer to run experiments in code, visit run an evaluation using the SDK.\nNavigate to the playground by clicking Playground in the sidebar.\nAdd a prompt by selecting an existing saved a prompt or creating a new one.\nSelect a dataset from the Test over dataset dropdown\nNote that the keys in the dataset input must match the input variables of the prompt. For example, in the above video the selected dataset has inputs with the key \u201cblog\u201d, which correctly match the input variable of the prompt.\nThere is a maximum of 15 input variables allowed in the prompt playground.\nStart the experiment by clicking on the Start or CMD+Enter. This will run the prompt over all the examples in the dataset and create an entry for the experiment in the dataset details page. We recommend committing the prompt to the prompt hub before starting the experiment so that it can be easily referenced later when reviewing your experiment.\nView the full results by clicking View full experiment. This will take you to the experiment details page where you can see the results of the experiment.\nEvaluate your experiment over specific critera by adding evaluators. Add LLM-as-a-judge or custom code evaluators in the playground using the +Evaluator button.To learn more about adding evaluators in via UI, visit how to define an LLM-as-a-judge evaluator.", "tokens": 293, "node_type": "child"}
{"id": 180, "chunk_id": "c16acf8a553ac24f5d77e7d3a1c6e493", "parent_id": 161, "url": "", "namespace": "langchain", "title": "langsmith-same-thread", "headers": ["langsmith-same-thread"], "section_index": 0, "chunk_index": 0, "text": "# langsmith-same-thread\n\n> Source: https://docs.langchain.com/langsmith/same-thread\n\nIn LangSmith Deployment, a thread is not explicitly associated with a particular agent.\nThis means that you can run multiple agents on the same thread, which allows a different agent to continue from an initial agent\u2019s progress.In this example, we will create two agents and then call them both on the same thread.\nYou\u2019ll see that the second agent will respond using information from the checkpoint generated in the thread by the first agent as context.\nfrom langgraph_sdk import get_clientclient = get_client(url=<DEPLOYMENT_URL>)openai_assistant = await client.assistants.create( graph_id=\"agent\", config={\"configurable\": {\"model_name\": \"openai\"}})# There should always be a default assistant with no configurationassistants = await client.assistants.search()default_assistant = [a for a in assistants if not a[\"config\"]][0]\nNow, we can run it on the default assistant and see that this second assistant is aware of the initial question, and can answer the question, \u201cand you?\u201d:\nPython\nJavascript\nCURL\nCopy\ninput = {\"messages\": [{\"role\": \"user\", \"content\": \"and you?\"}]}async for event in client.runs.stream( thread[\"thread_id\"], default_assistant[\"assistant_id\"], input=input, stream_mode=\"updates\",): print(f\"Receiving event of type: {event.event}\") print(event.data) print(\"\\n\\n\")\nOutput:\nCopy\nReceiving event of type: metadata{'run_id': '1ef6722d-80b3-6fbb-9324-253796b1cd13'}Receiving event of type: updates{'agent': {'messages': [{'content': [{'text': 'I am an artificial intelligence created by Anthropic, not by OpenAI. I should not have stated that OpenAI created me, as that is incorrect. Anthropic is the company that developed and trained me using advanced language models and AI technology. I will be more careful about providing accurate information regarding my origins in the future.', 'type': 'text', 'index': 0}], 'additional_kwargs': {}, 'response_metadata': {'stop_reason': 'end_turn', 'stop_sequence': None}, 'type': 'ai', 'name': None, 'id': 'run-ebaacf62-9dd9-4165-9535-db432e4793ec', 'example': False, 'tool_calls': [], 'invalid_tool_calls': [], 'usage_metadata': {'input_tokens': 302, 'output_tokens': 72, 'total_tokens': 374}}]}}", "tokens": 271, "node_type": "child"}
{"id": 181, "chunk_id": "c16acf8a553ac24f5d77e7d3a1c6e493", "parent_id": 163, "url": "", "namespace": "langchain", "title": "langsmith-scalability-and-resilience", "headers": ["langsmith-scalability-and-resilience"], "section_index": 0, "chunk_index": 0, "text": "# langsmith-scalability-and-resilience\n\n> Source: https://docs.langchain.com/langsmith/scalability-and-resilience\n\nLangSmith is designed to scale horizontally with your workload. Each instance of the service is stateless, and keeps no resources in memory. The service is designed to gracefully handle new instances being added or removed, including hard shutdown cases.\nAs you add more instances to a service, they will share the HTTP load as long as an appropriate load balancer mechanism is placed in front of them. In most deployment modalities we configure a load balancer for the service automatically. In the \u201cself-hosted without control plane\u201d modality it\u2019s your responsibility to add a load balancer. Since the instances are stateless any load balancing strategy will work, no session stickiness is needed, or recommended. Any instance of the server can communicate with any queue instance (through Redis PubSub), meaning that requests to cancel or stream an in-progress run can be handled by any arbitrary instance.\nAs you add more instances to a service, they will increase run throughput linearly, as each instance is configured to handle a set number of concurrent runs (by default 10). Each attempt for each run will be handled by a single instance, with exactly-once semantics enforced through Postgres\u2019s MVCC model (refer to section below for crash resilience details). Attempts that fail due to transient database errors are retried up to 3 times. We do not make use of long-lived transactions or locks, this enables us to make more efficient use of Postgres resources.\nWhile a run is being handled by a queue instance, a periodic heartbeat timestamp will be recorded in Redis by that queue worker.When a graceful shutdown request is received (SIGINT) an instance enters shutdown mode, which\nstops accepting new HTTP requests\ngives any in-progress runs a limited number of seconds to finish (if not finished it will be put back in the queue)\nstops the instance from picking up more runs from the queue\nIf a hard shutdown occurs due to a server crash or an infrastructure failure, any runs that were in progress will be picked up by an internal sweeper task that looks for in-progress runs that have breached their heartbeat window. The sweeper runs every 2 minutes and will put the runs back in the queue for another instance to pick them up.\nFor deployment modalities where we manage the Postgres database, we have periodic backups and continuously replicated standby replicas for automatic failover. This Postgres configuration is available in the Cloud deployment option for Production deployment types only.All communication with Postgres implements retries for retry-able errors. If Postgres is momentarily unavailable, such as during a database restart, most/all traffic should continue to succeed. Prolonged failure of Postgres will render the LangGraph Server unavailable.\nAll data that requires durable storage is stored in Postgres, not Redis. Redis is used only for ephemeral metadata, and communication between instances. Therefore we place no durability requirements on Redis.All communication with Redis implements retries for retry-able errors. If Redis is momentarily unavailable, such as during a database restart, most/all traffic should continue to succeed. Prolonged failure of Redis will render the LangGraph Server unavailable.", "tokens": 515, "node_type": "child"}
{"id": 182, "chunk_id": "c16acf8a553ac24f5d77e7d3a1c6e493", "parent_id": 164, "url": "", "namespace": "langchain", "title": "langsmith-script-delete-a-workspace", "headers": ["langsmith-script-delete-a-workspace"], "section_index": 0, "chunk_index": 0, "text": "# langsmith-script-delete-a-workspace\n\n> Source: https://docs.langchain.com/langsmith/script-delete-a-workspace\n\nThe LangSmith UI does not currently support the deletion of an individual workspace from an organization. This, however, can be accomplished by directly removing all traces from all materialized views in ClickHouse (except the runs_history views) and the runs and feedbacks tables and then removing the Workspace from the Postgres tenants table.\nThis command using the Workspace ID as an argument.\nPrerequisites\nEnsure you have the following tools/items ready.- kubectl\n- PostgreSQL client\n-\nPostgreSQL database connection:\n- Host\n- Port\n- Username\n- If using the bundled version, this is\npostgres\n- If using the bundled version, this is\n- Password\n- If using the bundled version, this is\npostgres\n- If using the bundled version, this is\n- Database name\n- If using the bundled version, this is\npostgres\n- If using the bundled version, this is\n-\nClickhouse database credentials\n- Host\n- Port\n- Username\n- If using the bundled version, this is\ndefault\n- If using the bundled version, this is\n- Password\n- If using the bundled version, this is\npassword\n- If using the bundled version, this is\n- Database name\n- If using the bundled version, this is\ndefault\n- If using the bundled version, this is\n-\nConnectivity to the PostgreSQL database from the machine you will be running the migration script on.\n- If you are using the bundled version, you may need to port forward the postgresql service to your local machine.\n- Run\nkubectl port-forward svc/langsmith-postgres 5432:5432\nto port forward the postgresql service to your local machine.\n-\nConnectivity to the Clickhouse database from the machine you will be running the migration script on.\n- If you are using the bundled version, you may need to port forward the clickhouse service to your local machine.\n- Run\nkubectl port-forward svc/langsmith-clickhouse 8123:8123\nto port forward the clickhouse service to your local machine.\n- Run\n- If you are using Clickhouse Cloud you will want to specify the \u2014ssl flag and use port\n8443\n- If you are using the bundled version, you may need to port forward the clickhouse service to your local machine.\n-\nThe script to delete a workspace\n- You can download the script from here", "tokens": 375, "node_type": "child"}
{"id": 183, "chunk_id": "c16acf8a553ac24f5d77e7d3a1c6e493", "parent_id": 165, "url": "", "namespace": "langchain", "title": "langsmith-script-delete-an-organization", "headers": ["langsmith-script-delete-an-organization"], "section_index": 0, "chunk_index": 0, "text": "# langsmith-script-delete-an-organization\n\n> Source: https://docs.langchain.com/langsmith/script-delete-an-organization\n\nSkip to main content\nThe LangSmith UI does not currently support the deletion of an individual organization from a self-hosted instance of LangSmith. This, however, can be accomplished by directly removing all traces from all materialized views in ClickHouse (except the runs_history views) and the runs and feedbacks tables and then removing the Organization from the Postgres tenants table.\nThis command using the Organization ID as an argument.\nPrerequisites\nEnsure you have the following tools/items ready.\nkubectl\nPostgreSQL client\nPostgreSQL database connection:\nHost\nPort\nUsername\nIf using the bundled version, this is postgres\nPassword\nIf using the bundled version, this is postgres\nDatabase name\nIf using the bundled version, this is postgres\nClickhouse database credentials\nHost\nPort\nUsername\nIf using the bundled version, this is default\nPassword\nIf using the bundled version, this is password\nDatabase name\nIf using the bundled version, this is default\nConnectivity to the PostgreSQL database from the machine you will be running the migration script on.\nIf you are using the bundled version, you may need to port forward the postgresql service to your local machine.\nRun kubectl port-forward svc/langsmith-postgres 5432:5432\nto port forward the postgresql service to your local machine.\nConnectivity to the Clickhouse database from the machine you will be running the migration script on.\nIf you are using the bundled version, you may need to port forward the clickhouse service to your local machine.\nRun kubectl port-forward svc/langsmith-clickhouse 8123:8123\nto port forward the clickhouse service to your local machine.\nIf you are using Clickhouse Cloud you will want to specify the \u2014ssl flag and use port 8443\nThe script to delete an organization\nYou can download the script from here\nRunning the deletion script for a single organization\nRun the following command to run the organization removal script:\nFor example, if you are using the bundled version with port-forwarding, the command would look like:\nIf you visit the LangSmith UI, you should now see organization is no longer present.", "tokens": 332, "node_type": "child"}
{"id": 184, "chunk_id": "c16acf8a553ac24f5d77e7d3a1c6e493", "parent_id": 166, "url": "", "namespace": "langchain", "title": "langsmith-script-delete-traces", "headers": ["langsmith-script-delete-traces"], "section_index": 0, "chunk_index": 0, "text": "# langsmith-script-delete-traces\n\n> Source: https://docs.langchain.com/langsmith/script-delete-traces\n\nSkip to main content\nThe LangSmith UI does not currently support the deletion of an individual trace. This, however, can be accomplished by directly removing the trace from all materialized views in ClickHouse (except the runs_history views) and the runs and feedback table themselves.\nThis command can either be run using a trace ID as an argument or using a file that is a list of trace IDs.\nPrerequisites\nEnsure you have the following tools/items ready.\nkubectl\nClickhouse database credentials\nHost\nPort\nUsername\nIf using the bundled version, this is default\nPassword\nIf using the bundled version, this is password\nDatabase name\nIf using the bundled version, this is default\nConnectivity to the Clickhouse database from the machine you will be running the delete_trace_by_id\nscript on.\nIf you are using the bundled version, you may need to port forward the clickhouse service to your local machine.\nRun kubectl port-forward svc/langsmith-clickhouse 8123:8123\nto port forward the clickhouse service to your local machine.\nThe script to delete a trace\nYou can download the script from here\nRunning the deletion script for a single trace\nRun the following command to run the trace deletion script using a single trace ID:\nFor example, if you are using the bundled version with port-forwarding, the command would look like:\nIf you visit the LangSmith UI, you should now see specified trace ID is no longer present nor reflected in stats.\nRunning the deletion script for a multiple traces from a file with one trace ID per line\nRun the following command to run the trace deletion script using a list of trace IDs:\nFor example, if you are using the bundled version with port-forwarding, the command would look like:\nIf you visit the LangSmith UI, you should now see all the specified traces have been removed.", "tokens": 305, "node_type": "child"}
{"id": 185, "chunk_id": "c16acf8a553ac24f5d77e7d3a1c6e493", "parent_id": 167, "url": "", "namespace": "langchain", "title": "langsmith-script-generate-clickhouse-stats", "headers": ["langsmith-script-generate-clickhouse-stats"], "section_index": 0, "chunk_index": 0, "text": "# langsmith-script-generate-clickhouse-stats\n\n> Source: https://docs.langchain.com/langsmith/script-generate-clickhouse-stats\n\nSkip to main content\nAs part of troubleshooting your self-hosted instance of LangSmith, the LangChain team may ask you to generate Clickhouse statistics that will help us understand memory and CPU consumption and connection concurrency.\nThis command will generate a CSV that can be shared with the LangChain team.\nPrerequisites\nEnsure you have the following tools/items ready.\nkubectl\nClickhouse database credentials\nHost\nPort\nUsername\nIf using the bundled version, this is default\nPassword\nIf using the bundled version, this is password\nDatabase name\nIf using the bundled version, this is default\nConnectivity to the Clickhouse database from the machine you will be running the get_clickhouse_stats\nscript on.\nIf you are using the bundled version, you may need to port forward the clickhouse service to your local machine.\nRun kubectl port-forward svc/langsmith-clickhouse 8123:8123\nto port forward the clickhouse service to your local machine.\nThe script to generate ClickHouse stats\nYou can download the script from here\nRunning the clickhouse stats generation script\nRun the following command to run the stats generation script:\nFor example, if you are using the bundled version with port-forwarding, the command would look like:\nand after running this command you should see a file, clickhouse_stats.csv, has been created with Clickhouse statistics.", "tokens": 209, "node_type": "child"}
{"id": 186, "chunk_id": "c16acf8a553ac24f5d77e7d3a1c6e493", "parent_id": 168, "url": "", "namespace": "langchain", "title": "langsmith-script-generate-query-stats", "headers": ["langsmith-script-generate-query-stats"], "section_index": 0, "chunk_index": 0, "text": "# langsmith-script-generate-query-stats\n\n> Source: https://docs.langchain.com/langsmith/script-generate-query-stats\n\nSkip to main content\nAs part of troubleshooting your self-hosted instance of LangSmith, the LangChain team may ask you to generate LangSmith query statistics that will help us understand the performance of various queries that drive the LangSmith product experience.\nThis command will generate a CSV that can be shared with the LangChain team.\nPrerequisites\nEnsure you have the following tools/items ready.\nkubectl\nClickhouse database credentials\nHost\nPort\nUsername\nIf using the bundled version, this is default\nPassword\nIf using the bundled version, this is password\nDatabase name\nIf using the bundled version, this is default\nConnectivity to the Clickhouse database from the machine you will be running the get_query_stats\nscript on.\nIf you are using the bundled version, you may need to port forward the clickhouse service to your local machine.\nRun kubectl port-forward svc/langsmith-clickhouse 8123:8123\nto port forward the clickhouse service to your local machine.\nThe script to generate query stats\nYou can download the script from here\nRunning the query stats generation script\nRun the following command to run the stats generation script:\nFor example, if you are using the bundled version with port-forwarding, the command would look like:\nand after running this command you should see a file, query_stats.csv, has been created with LangSmith query statistics.", "tokens": 215, "node_type": "child"}
{"id": 187, "chunk_id": "c16acf8a553ac24f5d77e7d3a1c6e493", "parent_id": 169, "url": "", "namespace": "langchain", "title": "langsmith-script-running-ch-support-queries", "headers": ["langsmith-script-running-ch-support-queries"], "section_index": 0, "chunk_index": 0, "text": "# langsmith-script-running-ch-support-queries\n\n> Source: https://docs.langchain.com/langsmith/script-running-ch-support-queries\n\nThis Helm repository contains queries to produce output that the LangSmith UI does not currently support directly (e.g. obtaining query exception logs from Clickhouse).This command takes a clickhouse connection string that contains an embedded name and password (which can be passed in from a call to a secrets manager) and executes a query from an input file. In the example below, we are using the ch_get_query_exceptions.sql input file in the support_queries/clickhouse directory.\nRun the following command to run the desired query:\nCopy\nsh run_support_query_ch.sh <clickhouse_url> --input path/to/query.sql\nFor example, if you are using the bundled version with port-forwarding, the command might look like:\nCopy\nsh run_support_query_ch.sh \"clickhouse://default:password@localhost:8123/default\" --input support_queries/clickhouse/ch_get_query_exceptions.sql\nwhich will output query logs for all queries that have thrown exceptions in Clickhouse in the last 7 days. To extract this to a file add the flag --output path/to/file.csv", "tokens": 144, "node_type": "child"}
{"id": 188, "chunk_id": "c16acf8a553ac24f5d77e7d3a1c6e493", "parent_id": 170, "url": "", "namespace": "langchain", "title": "langsmith-script-running-pg-support-queries", "headers": ["langsmith-script-running-pg-support-queries"], "section_index": 0, "chunk_index": 0, "text": "# langsmith-script-running-pg-support-queries\n\n> Source: https://docs.langchain.com/langsmith/script-running-pg-support-queries\n\npg_get_trace_counts_daily.sql\ninput file in the support_queries/postgres\ndirectory.\nPrerequisites\nEnsure you have the following tools/items ready.- kubectl\n- PostgreSQL client\n-\nPostgreSQL database connection:\n- Host\n- Port\n- Username\n- If using the bundled version, this is\npostgres\n- If using the bundled version, this is\n- Password\n- If using the bundled version, this is\npostgres\n- If using the bundled version, this is\n- Database name\n- If using the bundled version, this is\npostgres\n- If using the bundled version, this is\n-\nConnectivity to the PostgreSQL database from the machine you will be running the migration script on.\n- If you are using the bundled version, you may need to port forward the postgresql service to your local machine.\n- Run\nkubectl port-forward svc/langsmith-postgres 5432:5432\nto port forward the postgresql service to your local machine.\n-\nThe script to run a support query\n- You can download the script from here\nRunning the query script\nRun the following command to run the desired query:--output path/to/file.csv\nExport usage data\nExporting usage data requires running Helm chart version 0.11.4 or later.\nGet customer information\nYou need to retrieve your customer information from the LangSmith API before running the export scripts. This information is required as input for the export scripts.customer_id\nand customer_name\nfrom this response to use as input for the export scripts.\nProcess the API response with jq\nYou can use jq to parse the JSON response and set bash variables for use in your scripts:jq\n, run these commands to set the environment variables based on the curl output:\nInitial export\nThese scripts export usage data to a CSV for reporting to LangChain. They additionally track the export by assigning a backfill ID and timestamp. To export LangSmith trace usage:Status update\nThese scripts update the status of usage events in your installation to reflect that the events have been successfully processed by LangChain. The scripts require passing in the correspondingbackfill_id\n, which will be confirmed by your LangChain rep.\nTo update LangSmith trace usage:", "tokens": 343, "node_type": "child"}
{"id": 189, "chunk_id": "c16acf8a553ac24f5d77e7d3a1c6e493", "parent_id": 171, "url": "", "namespace": "langchain", "title": "langsmith-self-host-basic-auth", "headers": ["langsmith-self-host-basic-auth"], "section_index": 0, "chunk_index": 0, "text": "# langsmith-self-host-basic-auth\n\n> Source: https://docs.langchain.com/langsmith/self-host-basic-auth\n\n- You cannot change an existing installation from basic auth mode to OAuth with PKCE (deprecated) or vice versa - installations must be either one or the other. A basic auth installation requires a completely fresh installation including a separate PostgreSQL database/schema, unless migrating from an existing\nNone\ntype installation (see below). - Users must be given their initial auto-generated password once they are invited. This password may be changed later by any Organization Admin.\n- You cannot use both basic auth and OAuth with client secret at the same time.\nRequirements and features\n- There is a single\nDefault\norganization that is provisioned during initial installation, and creating additional organizations is not supported - Your initial password (configured below) must be least 12 characters long and have at least one lowercase, uppercase, and symbol\n- There are no strict requirements for the secret used for signing JWTs, but we recommend securely generating a string of at least 32 characters. For example:\nopenssl rand -base64 32\nMigrating from None auth\nOnly supported in versions 0.7 and above. Migrating an installation from None auth mode replaces the single \u201cdefault\u201d user with a user with the configured credentials and keeps all existing resources. The single pre-existing workspace ID post-migration remains00000000-0000-0000-0000-000000000000\n, but everything else about the migrated installation is standard for a basic auth installation.\nTo migrate, simply update your configuration as shown below and run helm upgrade\n(or docker-compose up\n) as usual.\nConfiguration\nChanging the JWT secret will log out your users\ninitialOrgAdminEmail\nand initialOrgAdminPassword\nvalues, and your user will be auto-provisioned with role Organization Admin\n. See the admin guide for more details on organization roles.", "tokens": 282, "node_type": "child"}
{"id": 190, "chunk_id": "c16acf8a553ac24f5d77e7d3a1c6e493", "parent_id": 172, "url": "", "namespace": "langchain", "title": "langsmith-self-host-blob-storage", "headers": ["langsmith-self-host-blob-storage"], "section_index": 0, "chunk_index": 0, "text": "# langsmith-self-host-blob-storage > Source: https://docs.langchain.com/langsmith/self-host-blob-storage - In high trace environments, inputs, outputs, errors, manifests, extras, and events may balloon the size of your databases. - If using LangSmith Managed ClickHouse, you may want sensitive information in blob storage that resides in your environment. To alleviate this, LangSmith supports storing run inputs, outputs, errors, manifests, extras, events, and attachments in an external blob storage system. Requirements - Access to a valid blob storage service - A bucket/directory in your blob storage to store the data. We highly recommend creating a separate bucket/directory for LangSmith data. - If you are using TTLs, you will need to set up a lifecycle policy to delete old data. You can find more information on configuring TTLs here. These policies should mirror the TTLs you have set in your LangSmith configuration, or you may experience data loss. See here on how to setup the lifecycle rules for TTLs for blob storage. - Credentials to permit LangSmith Services to access the bucket/directory - You will need to provide your LangSmith instance with the necessary credentials to access the bucket/directory. Read the authentication section below for more information. - If using S3 or GCS, an API url for your blob storage service - This will be the URL that LangSmith uses to access your blob storage system - For Amazon S3, this will be the URL of the S3 endpoint. Something like: https://s3.amazonaws.com orhttps://s3.us-west-1.amazonaws.com if using a regional endpoint. - For Google Cloud Storage, this will be the URL of the GCS endpoint. Something like: https://storage.googleapis.com Authentication Amazon S3 To authenticate to Amazon S3, you will need to create an IAM policy granting the following permissions on your bucket.- (Recommended) IAM Role for Service Account: You can create an IAM role for your LangSmith instance and attach the policy to that role. You can then provide the role to LangSmith. This is the recommended way to authenticate with Amazon S3 in production. - You will need to create an IAM role with the policy attached. - You will need to allow LangSmith service accounts to assume the role. The langsmith-queue ,langsmith-backend , andlangsmith-platform-backend service accounts will need to be able to assume the role.The service account names will be different if you are using a custom release name. You can find the service account names by runningkubectl get serviceaccounts in your cluster. - You will need to provide the role ARN to LangSmith. You can do this by adding the eks.amazonaws.com/role-arn: \"<role_arn>\" annotation to thequeue ,backend , andplatform-backend services in your Helm Chart installation. - Access Key and Secret Key: You can provide LangSmith with an access key and secret key. This is the simplest way to authenticate with Amazon S3. However, it is not recommended for production use as it is less secure. - You will need to create a user with the policy attached. Then you can provision an access key and secret key for that user. - VPC Endpoint Access: You can enable access to your S3 bucket via a VPC endpoint, which allows traffic to flow securely from your VPC to your S3 bucket. - You\u2019ll need to provision a VPC endpoint and configure it to allow access to your S3 bucket. - You can refer to our public Terraform modules for guidance and an example of configuring this. KMS encryption header support for S3 Starting with LangSmith Helm chart version 0.11.24, you can pass a KMS encryption key header and enforce a specific KMS key for writes by providing its ARN. To enable this, set the following values in your Helm chart:Helm Google Cloud Storage To authenticate with Google Cloud Storage, you will need to create aservice account with the necessary permissions to access your bucket. Your service account will need the Storage Admin role or a custom role with equivalent permissions. This can be scoped to the bucket that LangSmith will be using. Once you have a provisioned service account, you will need to generate a HMAC key for that service account. This key and secret will be used to authenticate with Google Cloud Storage. Azure Blob Storage To authenticate with Azure Blob Storage, you will need to use one of the following methods to grant LangSmith workloads permission to access your container (listed in order of precedence):- Storage account and access key - Connection string - Workload identity (recommended), managed identity, or environment variables supported by DefaultAzureCredential . This is the default authentication method when configuration for either option above is not present.- To use workload identity, add the label azure.workload.identity/use: true to thequeue ,backend , andplatform-backend deployments. Additionally, add theazure.workload.identity/client-id annotation to the corresponding service accounts, which should be an existing Azure AD Application\u2019s client ID or user-assigned managed identity\u2019s client ID. See Azure\u2019s documentation for additional details. - To use workload identity, add the label Some deployments may need further customization of the connection configuration using a Service URL Override instead of the default service URL ( https://<storage_account_name>.blob.core.windows.net/ ). For example, this override is necessary in order to use a different blob storage domain (e.g. government or china).CH Search By default, LangSmith will still store tokens for search in ClickHouse. If you are using LangSmith Managed Clickhouse, you may want to disable this feature to avoid sending potentially sensitive information to ClickHouse. You can do this in your blob storage configuration.Configuration After creating your bucket and obtaining the necessary credentials, you can configure LangSmith to use your blob storage system.If using an access key and secret, you can also provide an existing Kubernetes secret that contains the authentication information. This is recommended over providing the access key and secret key directly in your config. See the generated secret template for the expected secret keys. TTL Configuration If using the TTL feature with LangSmith, you\u2019ll also have to configure TTL rules for your blob storage. Trace information stored on blob storage is stored on a particular prefix path, which determines the TTL for the data. When a trace\u2019s retention is", "tokens": 1000, "node_type": "child"}
{"id": 191, "chunk_id": "9b0fd3b850698317bef57fa6e71a38ba", "parent_id": 172, "url": "", "namespace": "langchain", "title": "langsmith-self-host-blob-storage", "headers": ["langsmith-self-host-blob-storage"], "section_index": 0, "chunk_index": 1, "text": "By default, LangSmith will still store tokens for search in ClickHouse. If you are using LangSmith Managed Clickhouse, you may want to disable this feature to avoid sending potentially sensitive information to ClickHouse. You can do this in your blob storage configuration.Configuration After creating your bucket and obtaining the necessary credentials, you can configure LangSmith to use your blob storage system.If using an access key and secret, you can also provide an existing Kubernetes secret that contains the authentication information. This is recommended over providing the access key and secret key directly in your config. See the generated secret template for the expected secret keys. TTL Configuration If using the TTL feature with LangSmith, you\u2019ll also have to configure TTL rules for your blob storage. Trace information stored on blob storage is stored on a particular prefix path, which determines the TTL for the data. When a trace\u2019s retention is extended, its corresponding blob storage path changes to ensure that it matches the new extended retention. The following TTL prefix are used:ttl_s/ : Short term TTL, configured for 14 days.ttl_l/ : Long term TTL, configured for 400 days.", "tokens": 188, "node_type": "child"}
{"id": 192, "chunk_id": "c16acf8a553ac24f5d77e7d3a1c6e493", "parent_id": 173, "url": "", "namespace": "langchain", "title": "langsmith-self-host-custom-tls-certificates", "headers": ["langsmith-self-host-custom-tls-certificates"], "section_index": 0, "chunk_index": 0, "text": "# langsmith-self-host-custom-tls-certificates\n\n> Source: https://docs.langchain.com/langsmith/self-host-custom-tls-certificates\n\n- Mounting internal certificate authorities (CAs) system-wide to enable TLS for database connections and Playground model calls\n- Using Playground-specific TLS settings to provide client certs/keys for mTLS with supported model providers\nMount internal CAs for TLS\nYou must use Helm chart version 0.11.9 or later to mount internal CAs using the configuration below.\n- Create a file containing all CAs required for TLS with databases and external services. If your deployment is communicating directly to\nbeacon.langchain.com\nwithout a proxy, make sure to include a public trusted CA. All certs should be concatenated in this file with an empty line in between. - Create a Kubernetes secret with a key containing the contents of this file.\n- If using custom CA for TLS with your databases and other external services, provide the following values to your LangSmith helm chart:\nHelm\n- Make sure to use TLS supported connection strings:\n- Postgres: Add\n?sslmode=verify-full&sslrootcert=system\nto the end. - Redis: Use\nrediss://\ninstead ofredis://\nas the prefix.\n- Postgres: Add\nUse custom TLS certificates for model providers\nThis feature is currently only available for the following model providers:\n- Azure OpenAI\n- OpenAI\n- Custom (our custom model server). Refer to the custom model server documentation for more information.\nLANGSMITH_PLAYGROUND_TLS_MODEL_PROVIDERS\n: A comma-separated list of model providers that require custom TLS certificates. Note thatazure_openai\n,openai\n, andcustom\nare currently the only supported model providers, but more providers will be supported in the future.- [Optional]\nLANGSMITH_PLAYGROUND_TLS_KEY\n: The private key in PEM format. This must be a file path (for a mounted volume). This is usually only necessary for mutual TLS authentication. - [Optional]\nLANGSMITH_PLAYGROUND_TLS_CERT\n: The certificate in PEM format. This must be a file path (for a mounted volume). This is usually only necessary for mutual TLS authentication. - [Optional]\nLANGSMITH_PLAYGROUND_TLS_CA\n: The custom certificate authority (CA) certificate in PEM format. This must be a file path (for a mounted volume). Use this to mount CAs only if you\u2019re using a helm version below0.11.9\n; otherwise, use the Mount internal CAs for TLS section above.", "tokens": 349, "node_type": "child"}
{"id": 193, "chunk_id": "c16acf8a553ac24f5d77e7d3a1c6e493", "parent_id": 174, "url": "", "namespace": "langchain", "title": "langsmith-self-host-egress", "headers": ["langsmith-self-host-egress"], "section_index": 0, "chunk_index": 0, "text": "# langsmith-self-host-egress\n\n> Source: https://docs.langchain.com/langsmith/self-host-egress\n\nThis section only applies to customers who are not running in offline mode and assumes you are using a self-hosted LangSmith instance serving version 0.9.0 or later. Previous versions of LangSmith did not have this feature.\nhttps://beacon.langchain.com\n.\nIn the future, we will be introducing support diagnostics to help us ensure that LangSmith is running at an optimal level within your environment.\nGenerally, data that we send to Beacon can be categorized as follows:\n-\nSubscription Metrics\n-\nSubscription metrics are used to determine level of access and utilization of LangSmith. This includes, but are not limited to:\n- Number of traces\n- Seats allocated per contract\n- Seats in currently use\n-\nSubscription metrics are used to determine level of access and utilization of LangSmith. This includes, but are not limited to:\n-\nOperational Metadata\n- This metadata will contain and collect the above subscription metrics to assist with remote support, allowing the LangChain team to diagnose and troubleshoot performance issues more effectively and proactively.\nLangSmith Telemetry\nAs of version 0.11, LangSmith deployments will by default send telemetry data back to our backend. All telemetry data is associated with an organization and deployment, but never identified with individual users. We do not collect PII (personally identifiable information) in any form.What we use it for\n- To provide more proactive support and faster troubleshooting of self-hosted instances.\n- Assisting with performance tuning.\n- Understanding real-world usage to prioritize improvements.\nWhat we collect\n- Request metadata: anonymized request counts, sizes, and durations.\n- Database metrics: query durations, error rates, and performance counters.\n- Distributed traces: end-to-end traces with timing and error information for high-latency or failed requests.\nWe do not collect actual payload contents, database records, or any data that can identify your end users or customers.\nHow to disable\nSet the following values in yourlangsmith_config.yaml\nfile:\nExample payloads\nIn an effort to maximize transparency, we provide sample payloads here:License Verification\nEndpoint:POST beacon.langchain.com/v1/beacon/verify\nRequest:\nUsage Reporting\nEndpoint:POST beacon.langchain.com/v1/beacon/ingest-traces\nRequest:\nTelemetry: Metrics\nEndpoint:POST beacon.langchain.com/v1/beacon/v1/metrics\nRequest:\nTelemetry: Traces\nEndpoint:POST beacon.langchain.com/v1/beacon/v1/traces\nRequest:", "tokens": 343, "node_type": "child"}
{"id": 194, "chunk_id": "c16acf8a553ac24f5d77e7d3a1c6e493", "parent_id": 175, "url": "", "namespace": "langchain", "title": "langsmith-self-host-external-clickhouse", "headers": ["langsmith-self-host-external-clickhouse"], "section_index": 0, "chunk_index": 0, "text": "# langsmith-self-host-external-clickhouse\n\n> Source: https://docs.langchain.com/langsmith/self-host-external-clickhouse\n\n- LangSmith-managed ClickHouse\n- Provision a ClickHouse Cloud either directly or through a cloud provider marketplace:\n- On a VM in your cloud provider\nUsing the first two options (LangSmith-managed ClickHouse or ClickHouse Cloud) will provision a Clickhouse service OUTSIDE of your VPC. However, both options support private endpoints, meaning that you can direct traffic to the ClickHouse service without exposing it to the public internet (eg via AWS PrivateLink, or GCP Private Service Connect).Additionally, sensitive information can be configured to be not stored in Clickhouse. Please reach out to support@langchain.dev for more information.\nRequirements\n- A provisioned ClickHouse instance that your LangSmith application will have network access to (see above for options).\n- A user with admin access to the ClickHouse database. This user will be used to create the necessary tables, indexes, and views.\n- We support both standalone ClickHouse and externally managed clustered deployments. For clustered deployments, ensure all nodes are running the same version. Note that clustered setups are not supported with bundled ClickHouse installations.\n- We only support ClickHouse versions >= 23.9. Use of ClickHouse versions >= 24.2 requires LangSmith v0.6 or later.\n- We rely on a few configuration parameters to be set on your ClickHouse instance. These are detailed below:\nOur system has been tuned to work with the above configuration parameters. Changing these parameters may result in unexpected behavior.\nHA Replicated Clickhouse Cluster\nBy default, the setup process above will only work with a single node Clickhouse cluster.\n- You need to have a Clickhouse cluster that is setup with Keeper or Zookeeper for data replication and the appropriate settings. See Clickhouse Replication Setup Docs.\n- You need to set the cluster setting in the LangSmith Configuration section, specifically the\ncluster\nsettings to match your Clickhouse Cluster name. This will use theReplicated\ntable engines when running the Clickhouse migrations. - If in addition to HA, you would like to load balance among the Clickhouse nodes (to distribute reads or writes), we suggest using a load balancer or DNS load balancing to round robin among your Clickhouse servers.\n- Note: You will need to enable your\ncluster\nsetting before launching LangSmith for the first time and running the Clickhouse migrations. This is a requirement since the table engine will need to be created as aReplicated\ntable engine vs the non replicated engine type.\ncluster\nenabled, the migration will create the Replicated\ntable engine flavor. This means that data will be replicated among the servers in the cluster. This is a master-master setup where any server can process reads, writes, or merges.\nLangSmith-managed ClickHouse\n- If using LangSmith-managed ClickHouse, you will need to set up a VPC peering connection between the LangSmith VPC and the ClickHouse VPC. Please reach out to support@langchain.dev for more information.\n- You will also need to set up Blob Storage. You can read more about Blob Storage in the Blob Storage documentation.\nClickHouse installations managed by LangSmith use a SharedMerge engine, which automatically clusters them and separates compute from storage.\nParameters\nYou will need to provide several parameters to your LangSmith installation to configure an external ClickHouse database. These parameters include:- Host: The hostname or IP address of the ClickHouse database\n- HTTP Port: The port that the ClickHouse database listens on for HTTP connections\n- Native Port: The port that the ClickHouse database listens on for native connections\n- Database: The name of the ClickHouse database that LangSmith should use\n- Username: The username to use to connect to the ClickHouse database\n- Password: The password to use to connect to the ClickHouse database\n- Cluster (Optional): The name of the ClickHouse cluster if using an external Clickhouse cluster. When set, LangSmith will run migrations on the cluster and replicate data across instances.\nImportant considerations for clustered deployments:\n- Clustered setups must be configured on a fresh schema - existing standalone ClickHouse instances cannot be converted to clustered mode.\n- Clustering is only supported with externally managed ClickHouse deployments. It is not compatible with bundled ClickHouse installations as these do not include required ZooKeeper configurations.\n- When using a clustered deployment, LangSmith will automatically:\n- Run database migrations across all nodes in the cluster\n- Configure tables for data replication across the cluster\nConfiguration\nWith these parameters in hand, you can configure your LangSmith instance to use the provisioned ClickHouse database. You can do this by modifying theconfig.yaml\nfile for your LangSmith Helm Chart installation or the .env\nfile for your Docker installation.", "tokens": 750, "node_type": "child"}
{"id": 195, "chunk_id": "c16acf8a553ac24f5d77e7d3a1c6e493", "parent_id": 176, "url": "", "namespace": "langchain", "title": "langsmith-self-host-external-postgres", "headers": ["langsmith-self-host-external-postgres"], "section_index": 0, "chunk_index": 0, "text": "# langsmith-self-host-external-postgres\n\n> Source: https://docs.langchain.com/langsmith/self-host-external-postgres\n\nLangSmith uses a PostgreSQL database as the primary data store for transactional workloads and operational data (almost everything besides runs). By default, LangSmith Self-Hosted will use an internal PostgreSQL database. However, you can configure LangSmith to use an external PostgreSQL database (). By configuring an external PostgreSQL database, you can more easily manage backups, scaling, and other operational tasks for your database.\nNote: We only officially support PostgreSQL versions >= 14.\nA user with admin access to the PostgreSQL database. This user will be used to create the necessary tables, indexes, and schemas.\nThis user will also need to have the ability to create extensions in the database. We use/will try to install the btree_gin, btree_gist, pgcrypto, citext, and pg_trgm extensions.\nIf using a schema other than public, ensure that you do not have any other schemas with the extensions enabled, or you must include that in your search path.\nSupport for pgbouncer and other connection poolers is community-based. Community members have reported that pgbouncer has worked with pool_mode = session and a suitable setting for ignore_startup_parameters (as of writing, search_path and lock_timeout need to be ignored). Care is needed to avoid polluting connection pools; some level of PostgreSQL expertise is advisable. LangChain Inc currently does not have roadmap plans for formal test coverage or commercial support of pgbouncer or amazon rds proxy or any other poolers, but the community is welcome to discuss and collaborate on support through GitHub issues.\nBy default, we recommend an instance with at least 2 vCPUs and 8GB of memory. However, the actual requirements will depend on your workload and the number of users you have. We recommend monitoring your PostgreSQL instance and scaling up as needed.\nWith your connection string in hand, you can configure your LangSmith instance to use an external PostgreSQL database. You can do this by modifying the values file for your LangSmith Helm Chart installation or the .env file for your Docker installation.\nOnce configured, you should be able to reinstall your LangSmith instance. If everything is configured correctly, your LangSmith instance should now be using your external PostgreSQL database.", "tokens": 356, "node_type": "child"}
{"id": 196, "chunk_id": "c16acf8a553ac24f5d77e7d3a1c6e493", "parent_id": 177, "url": "", "namespace": "langchain", "title": "langsmith-self-host-external-redis", "headers": ["langsmith-self-host-external-redis"], "section_index": 0, "chunk_index": 0, "text": "# langsmith-self-host-external-redis\n\n> Source: https://docs.langchain.com/langsmith/self-host-external-redis\n\nRequirements\n- A provisioned Redis instance that your LangSmith instance will have network access to. We recommend using a managed Redis service like:\n- Note: We only officially support Redis versions >= 5.\n- We do not support Redis Cluster.\n- By default, we recommend an instance with at least 2 vCPUs and 8GB of memory. However, the actual requirements will depend on your tracing workload. We recommend monitoring your Redis instance and scaling up as needed.\nCertain tiers of managed Redis services may use Redis Cluster under the hood, but you can point to a single node in the cluster. For example on Azure Cache for Redis, the\nPremium\ntier and above use Redis Cluster, so you will need to use a lower tier.Connection String\nWe useredis-py\nto connect to Redis. This library supports a variety of connection strings. You can find more information on the connection string format here.\nYou will need to assemble the connection string for your Redis instance. This connection string should include the following information:\n- Host\n- Database\n- Port\n- URL params\nrediss://\nprefix. An example connection string with SSL might look like:\nConfiguration\nWith your connection string in hand, you can configure your LangSmith instance to use an external Redis instance. You can do this by modifying thevalues\nfile for your LangSmith Helm Chart installation or the .env\nfile for your Docker installation.", "tokens": 237, "node_type": "child"}
{"id": 197, "chunk_id": "c16acf8a553ac24f5d77e7d3a1c6e493", "parent_id": 178, "url": "", "namespace": "langchain", "title": "langsmith-self-host-ingress", "headers": ["langsmith-self-host-ingress"], "section_index": 0, "chunk_index": 0, "text": "# langsmith-self-host-ingress\n\n> Source: https://docs.langchain.com/langsmith/self-host-ingress\n\nlangsmith-frontend\n. Depending on your cloud provider, this may result in a public IP address being assigned to the service. If you would like to use a custom domain or have more control over the routing of traffic to your LangSmith installation, you can configure an Ingress.\nRequirements\n- An existing Kubernetes cluster\n- An existing Ingress Controller installed in your Kubernetes cluster\nParameters\nYou may need to provide certain parameters to your LangSmith installation to configure the Ingress. Additionally, we will want to convert thelangsmith-frontend\nservice to a ClusterIP service.\n-\nHostname (optional): The hostname that you would like to use for your LangSmith installation. E.g\n\"langsmith.example.com\"\n. If you leave this empty, the ingress will serve all traffic to the LangSmith installation. -\nSubdomain (optional): If you would like to serve LangSmith under a URL path, you can specify it here. For example, adding\n\"langsmith\"\nwill serve the application at\"example.hostname.com/langsmith\"\n. This will apply to UI paths as well as API endpoints. - IngressClassName (optional): The name of the Ingress class that you would like to use. If not set, the default Ingress class will be used.\n-\nAnnotations (optional): Additional annotations to add to the Ingress. Certain providers like AWS may use annotations to control things like TLS termination.\nFor example, you can add the following annotations using the AWS ALB Ingress Controller to attach an ACM certificate to the Ingress:\n- Labels (optional): Additional labels to add to the Ingress.\n-\nTLS (optional): If you would like to serve LangSmith over HTTPS, you can add TLS configuration here (many Ingress controllers may have other ways of controlling TLS so this is often not needed). This should be an array of TLS configurations. Each TLS configuration should have the following fields:\n- hosts: An array of hosts that the certificate should be valid for. E.g [\u201clangsmith.example.com\u201d]\n-\nsecretName: The name of the Kubernetes secret that contains the certificate and private key. This secret should have the following keys:\n- tls.crt: The certificate\n- tls.key: The private key\n- You can read more about creating a TLS secret here.\nConfiguration\nWith these parameters in hand, you can configure your LangSmith instance to use an Ingress. You can do this by modifying theconfig.yaml\nfile for your LangSmith Helm Chart installation.\nIf you do not have automated DNS setup, you will need to add the IP address to your DNS provider manually.", "tokens": 407, "node_type": "child"}
{"id": 198, "chunk_id": "c16acf8a553ac24f5d77e7d3a1c6e493", "parent_id": 179, "url": "", "namespace": "langchain", "title": "langsmith-self-host-mirroring-images", "headers": ["langsmith-self-host-mirroring-images"], "section_index": 0, "chunk_index": 0, "text": "# langsmith-self-host-mirroring-images\n\n> Source: https://docs.langchain.com/langsmith/self-host-mirroring-images\n\nBy default, LangSmith will pull images from our public Docker registry. However, if you are running LangSmith in an environment that does not have internet access, or if you would like to use a private Docker registry, you can mirror the images to your own registry and then configure your LangSmith installation to use those images.\nFor your convenience, we have provided a script that will mirror the images for you. You can find the script in the LangSmith Helm Chart repositoryTo use the script, you will need to run the script with the following command specifying your registry and platform:\nWhere <your-registry> is the URL of your Docker registry (e.g. myregistry.com) and <platform> is the platform you are using (e.g. linux/amd64, linux/arm64, etc.). If you do not specify a platform, it will default to linux/amd64.For example, if your registry is myregistry.com, your platform is linux/arm64, and you want to use the latest version of the images, you would run:\nNote that this script will assume that you have Docker installed and that you are authenticated to your registry. It will also push the images to the specified registry with the same repository/tag as the original images.Alternatively, you can pull, mirror, and push the images manually. The images that you will need to mirror are found in the values.yaml file of the LangSmith Helm Chart. These can be found here: LangSmith Helm Chart values.yamlHere is an example of how to mirror the images using Docker:\nCopy\n# Pull the images from the public registrydocker pull langchain/langsmith-backend:latestdocker tag langchain/langsmith-backend:latest <your-registry>/langsmith-backend:latestdocker push <your-registry>/langsmith-backend:latest\nYou will need to repeat this for each image that you want to mirror.\nOnce the images are mirrored, you will need to configure your LangSmith installation to use the mirrored images. You can do this by modifying the values.yaml file for your LangSmith Helm Chart installation or the .env file for your Docker installation. Replace tag with the version you want to use, e.g. 0.10.66 for the latest version at the time of writing.\nCopy\nimages: imagePullSecrets: [] # Add your image pull secrets here if needed registry: \"\" # Set this to your registry URL if you mirrored all images to the same registry using our script. Then you can remove the repository prefix from the images below. aceBackendImage: repository: \"(your-registry)/langchain/langsmith-ace-backend\" pullPolicy: IfNotPresent tag: \"0.10.66\" backendImage: repository: \"(your-registry)/langchain/langsmith-backend\" pullPolicy: IfNotPresent tag: \"0.10.66\" frontendImage: repository: \"(your-registry)/langchain/langsmith-frontend\" pullPolicy: IfNotPresent tag: \"0.10.66\" hostBackendImage: repository: \"(your-registry)/langchain/hosted-langserve-backend\" pullPolicy: IfNotPresent tag: \"0.10.66\" operatorImage: repository: \"(your-registry)/langchain/langgraph-operator\" pullPolicy: IfNotPresent tag: \"6cc83a8\" platformBackendImage: repository: \"(your-registry)/langchain/langsmith-go-backend\" pullPolicy: IfNotPresent tag: \"0.10.66\" playgroundImage: repository: \"(your-registry)/langchain/langsmith-playground\" pullPolicy: IfNotPresent tag: \"0.10.66\" postgresImage: repository: \"(your-registry)/postgres\" pullPolicy: IfNotPresent tag: \"14.7\" redisImage: repository: \"(your-registry)/redis\" pullPolicy: IfNotPresent tag: \"7\" clickhouseImage: repository: \"(your-registry)/clickhouse/clickhouse-server\" pullPolicy: Always tag: \"24.8\"\nOnce configured, you will need to update your LangSmith installation. You can follow our upgrade guide here: Upgrading LangSmith.If your upgrade is successful, your LangSmith instance should now be using the mirrored images from your Docker registry.", "tokens": 493, "node_type": "child"}
{"id": 199, "chunk_id": "c16acf8a553ac24f5d77e7d3a1c6e493", "parent_id": 180, "url": "", "namespace": "langchain", "title": "langsmith-self-host-organization-charts", "headers": ["langsmith-self-host-organization-charts"], "section_index": 0, "chunk_index": 0, "text": "# langsmith-self-host-organization-charts\n\n> Source: https://docs.langchain.com/langsmith/self-host-organization-charts\n\nThis feature is available on Helm chart versions 0.9.5 and later.\nLangSmith automatically generates and syncs organization usage charts for self-hosted installations.\nThese charts are available under Settings > Usage and billing > Usage graph\n:\n- Usage by Workspace: this counts traces (root runs) by workspace\n- Organization Usage: this counts all traces (root runs) for the organization\nThe charts are refreshed to include any new workspaces every 5 minutes. Note that the charts are not editable.\nProgrammatically fetch trace counts\nYou can retrieve trace counts programmatically using two different methods:\nMethod 1: Use the LangSmith REST API\nIf your self-hosted installation uses an online key, you can use the LangSmith REST API to fetch organization usage data.\nMethod 2: Use PostgreSQL support queries\nFor installations using offline keys or when you need more detailed export capabilities, you can run support queries directly against the PostgreSQL database. All available scripts are in the support queries repository.\nFor more detailed information about running support queries, see the Run support queries against PostgreSQL guide.", "tokens": 178, "node_type": "child"}
{"id": 200, "chunk_id": "c16acf8a553ac24f5d77e7d3a1c6e493", "parent_id": 181, "url": "", "namespace": "langchain", "title": "langsmith-self-host-playground-environment-settings", "headers": ["langsmith-self-host-playground-environment-settings"], "section_index": 0, "chunk_index": 0, "text": "# langsmith-self-host-playground-environment-settings\n\n> Source: https://docs.langchain.com/langsmith/self-host-playground-environment-settings\n\nThis feature is only available on Helm chart versions 0.10.27 (application version 0.10.74) and later.\nplayground\nservice, which allows you to configure many of those environment variables directly on the pod itself. This can be useful to avoid having to set credentials in the UI.\nRequirements\n- A self-hosted LangSmith instance with the\nplayground\nservice running. - The provider you want to configure must support environment variables for configuration. Check the provider\u2019s Chat Model documentation for more information.\n- The secrets/roles you may want to attach to the\nplayground\nservice.- Note that for IRSA you may need to grant the\nlangsmith-playground\nservice account the necessary permissions to access the secrets or roles in your cloud provider.\n- Note that for IRSA you may need to grant the\nConfiguration\nWith the parameters from above, you can configure your LangSmith instance to use environment variables for model providers. You can do this by modifying thelangsmith_config.yaml\nfile for your LangSmith Helm Chart installation or the docker-compose.yaml\nfile for your Docker installation.", "tokens": 173, "node_type": "child"}
{"id": 201, "chunk_id": "c16acf8a553ac24f5d77e7d3a1c6e493", "parent_id": 182, "url": "", "namespace": "langchain", "title": "langsmith-self-host-scale", "headers": ["langsmith-self-host-scale"], "section_index": 0, "chunk_index": 0, "text": "# langsmith-self-host-scale > Source: https://docs.langchain.com/langsmith/self-host-scale Summary The table below provides an overview comparing different LangSmith configurations for various load patterns (reads / writes):| Low / low | Low / high | High / low | Medium / medium | High / high | | |---|---|---|---|---|---| | 5 | 5 | 50 | 20 | 50 | | | 10 | 1000 | 10 | 100 | 1000 | | | Frontend replicas | 1 (default) | 4 | 2 | 2 | 4 | | Platform backend replicas | 3 (default) | 20 | 3 (default) | 3 (default) | 20 | | Queue replicas | 3 (default) | 160 | 6 | 10 | 160 | | Backend replicas | 2 (default) | 5 | 40 | 16 | 50 | | Redis resources | 8 Gi (default) | 200 Gi external | 8 Gi (default) | 13Gi external | 200 Gi external | | ClickHouse resources | 4 CPU 16 Gi (default) | 10 CPU 32Gi memory | 8 CPU 16 Gi per replica | 16 CPU 24Gi memory | 14 CPU 24 Gi per replica | | ClickHouse setup | Single instance | Single instance | 3-node | Single instance | 3-node | | 2 CPU 8 GB memory 10GB storage (external) | 2 CPU 8 GB memory 10GB storage (external) | 2 CPU 8 GB memory 10GB storage (external) | 2 CPU 8 GB memory 10GB storage (external) | 2 CPU 8 GB memory 10GB storage (external) | | | Blob storage | Disabled | Enabled | Enabled | Enabled | Enabled | values.yaml snippet for you to start with for your self-hosted LangSmith instance. Trace ingestion (write path) Common usage that put load on the write path:- Ingesting traces via the Python or JavaScript LangSmith SDK - Ingesting traces via the @traceable wrapper - Submitting traces via the /runs/multipart endpoint - Platform backend service: Receives initial request to ingest traces and places traces on a Redis queue - Redis cache: Used to queue traces that need to be persisted - Queue service: Persists traces for querying - ClickHouse: Persistent storage used for traces - Give ClickHouse more resources (CPU and memory) if it is approaching resource limits. - Increase the number of platform-backend pods if ingest requests are taking long to respond. - Increase queue service pod replicas if traces are not being processed from Redis fast enough. - Use a larger Redis cache if you notice that the current Redis instance is reaching resource limits. This could also be a reason why ingest requests take a long time. Trace querying (read path) Common usage that puts load on the read path:- Users on the frontend looking at tracing projects or individual traces - Scripts used to query for trace info - Hitting either the /runs/query or/runs/<run-id> api endpoints - Backend service: Receives the request and submits a query to ClickHouse to then respond to the request - ClickHouse: Persistent storage for traces. This is the main database that is queried when requesting trace info. - Increase the number of backend service pods. This would be most impactful if backend service pods are reaching 1 core CPU usage. - Give ClickHouse more resources (CPU or Memory). ClickHouse can be very resource intensive, but it should lead to better performance. - Move to a replicated ClickHouse cluster. Adding replicas of ClickHouse helps with read performance, but we recommend staying below 5 replicas (start with 3). Example LangSmith configurations for scale Below we provide some example LangSmith configurations based on expected read and write loads. For read load (trace querying):- Low means roughly 5 users looking at traces at a time (about 10 requests per second) - Medium means roughly 20 users looking at traces at a time (about 40 requests per second) - High means roughly 50 users looking at traces at a time (about 100 requests per second) - Low means up to 10 traces submitted per second - Medium means up to 100 traces submitted per second - High means up to 1000 traces submitted per second The exact optimal configuration depends on your usage and trace payloads. Use the examples below in combination with the information above and your specific usage to update your LangSmith configuration as you see fit. If you have any questions, please reach out to the LangChain team. /runs/query or /runs/<run-id> endpoints frequently. For this, we strongly recommend setting up a replicated ClickHouse cluster to enable high read scale at low latency. See our external ClickHouse doc for more guidance on how to setup a replicated ClickHouse cluster. For this load pattern, we recommend using a 3 node replicated setup, where each replica in the cluster should have resource requests of 8+ cores and 16+ GB memory, and resource limit of 12 cores and 32 GB memory. For this, we recommend a configuration like this: You have a very high rate of trace ingestion (approaching 1000 traces submitted per second) and also have many users querying traces on the frontend (over 50 users) and/or scripts that are consistently making requests to /runs/query or /runs/<run-id> endpoints. For this, we very strongly recommend setting up a replicated ClickHouse cluster to prevent degraded read performance at high write scale. See our external ClickHouse doc for more guidance on how to set up a replicated ClickHouse cluster. For this load pattern, we recommend using a 3 node replicated setup, where each replica in the cluster should have resource requests of 14+ cores and 24+ GB memory, and resource limit of 20 cores and 48 GB memory. We also recommend that each node/instance of ClickHouse has 600 Gi of volume storage for each day of TTL that you enable (as per the configuration below). Overall, we recommend a configuration like this: Ensure that the Kubernetes cluster is configured with sufficient resources to scale to the recommended size. After deployment, all of the pods in the Kubernetes cluster should be in a Running state. Pods", "tokens": 1000, "node_type": "child"}
{"id": 202, "chunk_id": "9b0fd3b850698317bef57fa6e71a38ba", "parent_id": 182, "url": "", "namespace": "langchain", "title": "langsmith-self-host-scale", "headers": ["langsmith-self-host-scale"], "section_index": 0, "chunk_index": 1, "text": "requests to /runs/query or /runs/<run-id> endpoints. For this, we very strongly recommend setting up a replicated ClickHouse cluster to prevent degraded read performance at high write scale. See our external ClickHouse doc for more guidance on how to set up a replicated ClickHouse cluster. For this load pattern, we recommend using a 3 node replicated setup, where each replica in the cluster should have resource requests of 14+ cores and 24+ GB memory, and resource limit of 20 cores and 48 GB memory. We also recommend that each node/instance of ClickHouse has 600 Gi of volume storage for each day of TTL that you enable (as per the configuration below). Overall, we recommend a configuration like this: Ensure that the Kubernetes cluster is configured with sufficient resources to scale to the recommended size. After deployment, all of the pods in the Kubernetes cluster should be in a Running state. Pods stuck in Pending may indicate that you are reaching node pool limits or need larger nodes.Also, ensure that any ingress controller deployed on the cluster is able to handle the desired load to prevent bottlenecks.", "tokens": 185, "node_type": "child"}
{"id": 203, "chunk_id": "c16acf8a553ac24f5d77e7d3a1c6e493", "parent_id": 183, "url": "", "namespace": "langchain", "title": "langsmith-self-host-sso", "headers": ["langsmith-self-host-sso"], "section_index": 0, "chunk_index": 0, "text": "# langsmith-self-host-sso > Source: https://docs.langchain.com/langsmith/self-host-sso Overview You may upgrade a basic auth installation to this mode, but not a none auth installation. In order to upgrade, simply remove the basic auth configuration and add the required configuration parameters as shown below. Users may then login via OAuth only. In order to maintain access post-upgrade, you must have access to login via OAuth using an email address that previously logged in via basic auth. LangSmith does not support moving from SSO to basic auth mode in self-hosted at the moment. We also do not support moving from OAuth Mode with client secret to OAuth mode without a client secret and vice versa. Finally, we do not support having both basic auth and OAuth at the same time. Ensure you disable the basic auth configuration when enabling OAuth. With Client Secret (Recommended) By default, LangSmith Self-Hosted supports theAuthorization Code flow with Client Secret . In this version of the flow, your client secret is stored security in LangSmith (not on the frontend) and used for authentication and establishing auth sessions. Prerequisites - You must be self-hosted and on an Enterprise plan. - Your IdP must support the Authorization Code flow withClient Secret . - Your IdP must support using an external discovery/issuer URL. We will use this to fetch the necessary routes and keys for your IdP. - You must provide the OIDC ,email , andprofile scopes to LangSmith. We use these to fetch the necessary user information and email for your users. LangSmith SSO is only supported over https .Configuration - You will need to set the callback URL in your IdP to https://<host>/api/v1/oauth/custom-oidc/callback , wherehost is the domain or IP you have provisioned for your LangSmith instance. This is where your IdP will redirect the user after they have authenticated. - You will need to provide the oauthClientId ,oauthClientSecret ,hostname , andoauthIssuerUrl in yourvalues.yaml file. This is where you will configure your LangSmith instance. - If you have not already configured Oauth with client secret or if you only have personal orgs, you must provide an email address to assign as the initial org admin for the newly provisioned SSO org. If you are upgrading from basic auth, your existing org will be reused instead. Session length controls All of the environment variables in this section are for the platform-backend service and can be added using platformBackend.deployment.extraEnv in Helm.- By default, session length is controlled by the expiration of the identity token returned by the identity provider - Most setups should use refresh tokens to enable session length extension beyond the identity token expiration up to OAUTH_SESSION_MAX_SEC , which may require including theoffline_access scope by adding tooauthScopes (Helm) orOAUTH_SCOPES (Docker) OAUTH_SESSION_MAX_SEC (default 1 day) can be overridden to a maximum of one week (604800 )- For identity provider setups that don\u2019t support refresh tokens, setting OAUTH_OVERRIDE_TOKEN_EXPIRY=\"true\" will takeOAUTH_SESSION_MAX_SEC as the session length, ignoring the identity token expiration Override Sub Claim In some scenarios, it may be necessary to override which claim is used as thesub claim from your identity provider. For example, in SCIM, the resolved sub claim and SCIM externalId must match in order for login to succeed. If there are restrictions on the source attribute of the sub claim and/or the SCIM externalId , set the ISSUER_SUB_CLAIM_OVERRIDES environment variable to select which OIDC JWT claim is used as the sub . If an issuer URL starts with one of the URLs in this configuration, the sub claim is taken from the field name specified. For example, with the following configuration, a token with the issuer https://idp.yourdomain.com/application/uuid would use the customClaim value as the sub : oid claim when Azure Entra ID is used as the identity provider: Google Workspace IdP setup You can use Google Workspace as a single sign-on (SSO) provider using OAuth2.0 and OIDC without PKCE.You must have administrator-level access to your organization\u2019s Google Cloud Platform (GCP) account to create a new project, or permissions to create and configure OAuth 2.0 credentials for an existing project. We recommend that you create a new project for managing access, since each GCP project has a single OAuth consent screen. - Create a new GCP project, see the Google documentation topic creating and managing projects - After you have created the project, open the Credentials page in the Google API Console (making sure the project in the top left corner is correct) - Create new credentials: Create Credentials \u2192 OAuth client ID - Choose Web application as theApplication type and enter a name for the application e.g.LangSmith - In Authorized Javascript origins put the domain of your LangSmith instance e.g.https://langsmith.yourdomain.com - In Authorized redirect URIs put the domain of your LangSmith instance followed by/api/v1/oauth/custom-oidc/callback e.g.https://langsmith.yourdomain.com/api/v1/oauth/custom-oidc/callback - Click Create , then download the JSON or copy and save theClient ID (ends with.apps.googleusercontent.com ) andClient secret somewhere secure. You will be able to access these later if needed. - Select OAuth consent screen from the navigation menu on the left- Choose the Application type as Internal . If you selectPublic , anyone with a Google account can sign in. - Enter a descriptive Application name . This name is shown to users on the consent screen when they sign in. For example, useLangSmith or<organization_name> SSO for LangSmith . - Verify that the Scopes for Google APIs only lists email, profile, and openid scopes. Only these scopes are required for single sign-on. If you grant additional scopes it increases the risk of exposing sensitive data. - Choose the Application type as - (Optional) control who within your organization has access to LangSmith: https://admin.google.com/ac/owl/list?tab=configuredApps. See Google\u2019s documentation for additional details. - Configure LangSmith to use this OAuth application. For examples, here are the config values that would be used for Kubernetes configuration:oauthClientId :Client ID (ends with.apps.googleusercontent.com )oauthClientSecret :Client secret hostname : the domain of your LangSmith instance e.g.https://langsmith.yourdomain.com (no trailing slash)oauthIssuerUrl :https://accounts.google.com oauth.enabled :true authType :mixed Okta IdP setup Supported features - IdP-initiated SSO - SP-initiated SSO Configuration steps For additional information, see", "tokens": 1000, "node_type": "child"}
{"id": 204, "chunk_id": "9b0fd3b850698317bef57fa6e71a38ba", "parent_id": 183, "url": "", "namespace": "langchain", "title": "langsmith-self-host-sso", "headers": ["langsmith-self-host-sso"], "section_index": 0, "chunk_index": 1, "text": "a descriptive Application name . This name is shown to users on the consent screen when they sign in. For example, useLangSmith or<organization_name> SSO for LangSmith . - Verify that the Scopes for Google APIs only lists email, profile, and openid scopes. Only these scopes are required for single sign-on. If you grant additional scopes it increases the risk of exposing sensitive data. - Choose the Application type as - (Optional) control who within your organization has access to LangSmith: https://admin.google.com/ac/owl/list?tab=configuredApps. See Google\u2019s documentation for additional details. - Configure LangSmith to use this OAuth application. For examples, here are the config values that would be used for Kubernetes configuration:oauthClientId :Client ID (ends with.apps.googleusercontent.com )oauthClientSecret :Client secret hostname : the domain of your LangSmith instance e.g.https://langsmith.yourdomain.com (no trailing slash)oauthIssuerUrl :https://accounts.google.com oauth.enabled :true authType :mixed Okta IdP setup Supported features - IdP-initiated SSO - SP-initiated SSO Configuration steps For additional information, see Okta\u2019s documentation. If you have any questions or issues, please reach out to support@langchain.dev.Via Okta Integration Network (recommended) This method of configuration is required in order to use SCIM with Okta. - Sign in to Okta. - In the upper-right corner, select Admin. The button is not visible from the Admin area. - Select Browse App Integration Catalog . - Find and select the LangSmith application. - On the application overview page, select Add Integration. - Fill in ApiUrlBase :- Your LangSmith API URL without the protocol ( https:// ) formatted as<langsmith_domain>/api/v1 , e.g.,langsmith.yourdomain.com/api/v1 . - If your installation is configured with a subdomain / path prefix, include that in the URL, e.g., langsmith.yourdomain.com/prefix/api/v1 . - Your LangSmith API URL without the protocol ( - Leave AuthHost empty. - (Optional, if planning to use SCIM as well) Fill in LangSmithUrl : The<langsmith_url> portion from above, e.g.,langsmith.yourdomain.com . - Under Application Visibility, keep the box unchecked. - Select Next. - Select OpenID Connect . - Fill in Sign-On Options :Application username format :Email .Update application username on :Create and update .Allow users to securely see their password : leave unchecked. - Click Save. - Configure LangSmith to use this OAuth application (see general configuration section for details about initialOrgAdminEmail ): Via Custom App Integration - Log in to Okta as an administrator, and go to the Okta Admin console. - Under Applications > Applications click Create App Integration. - Select OIDC - OpenID Connect as the Sign-in method and Web Application as the Application type, then click Next. - Enter an App integration name (e.g.,LangSmith ). - Recommended: Check Core grants > Refresh Token (see session length controls). - In Sign-in redirect URIs put the domain of your LangSmith instance followed by /api/v1/oauth/custom-oidc/callback , e.g.,https://langsmith.yourdomain.com/api/v1/oauth/custom-oidc/callback . If your installation is configured with a subdomain / path prefix, include that in the URL, e.g.,https://langsmith.yourdomain.com/prefix/api/v1/oauth/custom-oidc/callback . - Remove the default URI under Sign-out redirect URIs. - Under Trusted Origins > Base URIs add your langsmith URL with the protocol, e.g., https://langsmith.yourdomain.com . - Select your desired option under Assignments > Controlled access: - Allow everyone in your organization to access. - Limit access to selected groups. - Skip group assignment for now. - Click Save. - Under Sign On > OpenID Connect ID Token set Issuer to Okta URL. - (Optional) Under General > Login set Login initiated by to Either Okta or App to enable IdP-initiated login. - (Recommended) Under General > Login > Email verification experience fill in the Callback URI with the LangSmith URL, e.g., https://langsmith.yourdomain.com . - Configure LangSmith to use this OAuth application (see general configuration section for details about initialOrgAdminEmail ): SP-initiated SSO Users can sign in using the Login via SSO button on the LangSmith homepage.Without Client Secret (PKCE) (Deprecated) We recommend running with aClient Secret if possible (previously we didn\u2019t support this). However, if your IdP does not support this, you can use the Authorization Code with PKCE flow. This flow does not require a Client Secret . For the alternative workflow, refer to With client secret. Requirements There are a couple of requirements for using OAuth SSO with LangSmith:- Your IdP must support the Authorization Code with PKCE flow (Google does not support this flow for example, but see above for an alternative configuration that Google supports). This is often displayed in your OAuth Provider as configuring a \u201cSingle Page Application (SPA)\u201d - Your IdP must support using an external discovery/issuer URL. We will use this to fetch the necessary routes and keys for your IdP. - You must provide the OIDC ,email , andprofile scopes to LangSmith. We use these to fetch the necessary user information and email for your users. - You will need to set the callback URL in your IdP to http://<host>/oauth-callback , where host is the domain or IP you have provisioned for your LangSmith instance. This is where your IdP will redirect the user after they have authenticated. - You will need to provide the oauthClientId andoauthIssuerUrl in yourvalues.yaml file. This is where you will configure your LangSmith instance.", "tokens": 831, "node_type": "child"}
{"id": 205, "chunk_id": "c16acf8a553ac24f5d77e7d3a1c6e493", "parent_id": 184, "url": "", "namespace": "langchain", "title": "langsmith-self-host-ttl", "headers": ["langsmith-self-host-ttl"], "section_index": 0, "chunk_index": 0, "text": "# langsmith-self-host-ttl\n\n> Source: https://docs.langchain.com/langsmith/self-host-ttl\n\nRequirements\nYou can configure retention through helm or environment variable settings. There are a few options that are configurable:- Enabled: Whether data retention is enabled or disabled. If enabled, via the UI you can your default organization and project TTL tiers to apply to traces (see data retention guide for details).\n- Retention Periods: You can configure system-wide retention periods for shortlived and longlived traces. Once configured, you can manage the retention level at each project as well as set an organization-wide default for new projects.\nClickHouse TTL Cleanup Job\nAs of version 0.11, a cron job runs on weekends to assist in deleting expired data that may not have been cleaned up by ClickHouse\u2019s built-in TTL mechanism.This job uses potentially long running mutations (\nALTER TABLE DELETE\n), which are expensive operations that can impact ClickHouse\u2019s performance. We recommend running these operations only during off-peak hours (nights and weekends). During testing with 1 concurrent active mutation (default), we did not observe significant CPU, memory, or latency increases.Default Schedule\nBy default, the cleanup job runs:- Saturday: 8pm and 10pm UTC\n- Sunday: 12am, 2am, and 4am UTC\nDisabling the Job\nTo disable the cleanup job entirely:Configuring the Schedule\nYou can customize when the cleanup job runs by modifying the cron expressions:To run the job on a single cron schedule, set both\nCLICKHOUSE_TTL_CLEANUP_CRON_WEEKEND_EVENING\nand CLICKHOUSE_TTL_CLEANUP_CRON_WEEKEND_MORNING\nto the same value. Job locking prevents overlapping executions.Configuring Minimum Expired Rows Per Part\nThe job goes table by table, scanning parts and deleting data from parts containing a minimum number of expired rows. This threshold balances efficiency and thoroughness:- Too low: Job scans entire parts to clear minimal data (inefficient)\n- Too high: Job misses parts with significant expired data\nChecking Expired Rows\nUse this query to analyze expired rows in your tables, and tweak your minimum value accordingly:Configuring Maximum Active Mutations\nDelete operations can be time-consuming (~50 minutes for a 100GB part). You can increase concurrent mutations to speed up the process:Increasing concurrent DELETE operations can severely impact system performance. Monitor your system carefully and only increase this value if you can tolerate potentially slower insert and read latencies.\nEmergency: Stopping Running Mutations\nIf you experience latency spikes and need to terminate a running mutation:-\nFind active mutations:\nLook for the\nmutation_id\nwhere thecommand\ncolumn contains aDELETE\nstatement. -\nKill the mutation:\nBackups and Data Retention\nIf disk space does not decrease after running this job, or if it continues to increase, backups may be causing the issue by creating file system hard links. These links prevent ClickHouse from cleaning up the data. To verify, check the following directories inside your ClickHouse pod:/var/lib/clickhouse/backup\n/var/lib/clickhouse/shadow", "tokens": 445, "node_type": "child"}
{"id": 206, "chunk_id": "c16acf8a553ac24f5d77e7d3a1c6e493", "parent_id": 185, "url": "", "namespace": "langchain", "title": "langsmith-self-host-upgrades", "headers": ["langsmith-self-host-upgrades"], "section_index": 0, "chunk_index": 0, "text": "# langsmith-self-host-upgrades\n\n> Source: https://docs.langchain.com/langsmith/self-host-upgrades\n\nKubernetes(Helm)\nIf you don\u2019t have the repo added, run the following command to add it:If you are using a namespace other than the default namespace, you will need to specify the namespace in the\nhelm\nand kubectl\ncommands by using the -n <namespace\nflag.Running\nstate. Verify that clickhouse is running and that both migrations\njobs have completed.\nValidate your deployment:\n-\nRun\nkubectl get services\nOutput should look something like:\n-\nCurl the external ip of the\nlangsmith-frontend\nservice:\n-\nVisit the external ip for the\nlangsmith-frontend\nservice on your browser The LangSmith UI should be visible/operational\nDocker\nUpgrading the Docker version of LangSmith is a bit more involved than the Helm version and may require a small amount of downtime. Please follow the instructions below to upgrade your Docker version of LangSmith.- Update your\ndocker-compose.yml\nfile to the file used in the latest release. You can find this in the LangSmith SDK GitHub repository - Update your\n.env\nfile with any new environment variables that are required in the new version. These will be detailed in the release notes for the new version. - Run the following command to stop your current LangSmith instance:\n- Run the following command to start your new LangSmith instance in the background:\nValidate your deployment:\n-\nCurl the exposed port of the\ncli-langchain-frontend-1\ncontainer: -\nVisit the exposed port of the\ncli-langchain-frontend-1\ncontainer on your browser", "tokens": 238, "node_type": "child"}
{"id": 207, "chunk_id": "c16acf8a553ac24f5d77e7d3a1c6e493", "parent_id": 186, "url": "", "namespace": "langchain", "title": "langsmith-self-host-usage", "headers": ["langsmith-self-host-usage"], "section_index": 0, "chunk_index": 0, "text": "# langsmith-self-host-usage\n\n> Source: https://docs.langchain.com/langsmith/self-host-usage\n\nAfter setting the above, you should be able to run your code and see the results in your self-hosted instance. We recommend running through the quickstart guide to get a feel for how to use LangSmith.\nIf you are using self-signed certificates for your self-hosted LangSmith instance, this can be problematic as Python comes with its own set of trusted certificates, which may not include your self-signed certificate. To resolve this, you may need to use something like truststore to load system certificates into your Python environment.You can do this like so:\npip install truststore (or similar depending on the package manager you are using)\nThen use the following code to load the system certificates:\nCopy\nimport truststoretruststore.inject_into_ssl()# The rest of your codeimport langsmithlangsmith_client = langsmith.Client( api_key='<api_key>', api_url='http(s)://<host>/api/v1',)", "tokens": 133, "node_type": "child"}
{"id": 208, "chunk_id": "c16acf8a553ac24f5d77e7d3a1c6e493", "parent_id": 187, "url": "", "namespace": "langchain", "title": "langsmith-self-host-user-management", "headers": ["langsmith-self-host-user-management"], "section_index": 0, "chunk_index": 0, "text": "# langsmith-self-host-user-management\n\n> Source: https://docs.langchain.com/langsmith/self-host-user-management\n\nFeatures\nWorkspace level invites to an organization\nThe default behavior in LangSmith requires a user to be an Organization Admin in order to invite new users to an organization. For self-hosted customers that would like to delegate this responsibility to workspace Admins, a feature flag may be set that enables workspace Admins to invite new users to the organization as well as their specific workspace at the workspace level. Once this feature is enabled via the configuration option below, workspace Admins may add new users in theWorkspace members\ntab under Settings\n> Workspaces\n. Both of the following cases are supported when inviting at the workspace level, while the organization level invite functions the same as before.\n- Invite users who are NOT already active in the organization: this will add the users as pending to the organization and specific workspace\n- Invite users who ARE already active in the organization: adds the users directly to the workspace as an active member (no pending state).\nConfiguration\nSSO New Member Login Flow\nAs of helm v0.11.10, self-hosted deployments using OAuth SSO will no longer need to manually add members in LangSmith settings for them to join. Deployments will have a default organization, to which new users will automatically be added upon their first login to LangSmith. For your default organization, you can set which workspace(s) and workspace role is assigned to new members. For non-default organizations, the invitation flow remains the same. Once a user joins an organization, any changes to their workspaces or roles beyond the default organization settings must be managed either through LangSmith settings (as before) or via SCIM.By default, all new users are added to the organization\u2019s initially provisioned workspace (Workspace 1 by default) with the Workspace Editor role.\nTo change your default organization, use Set Default Organization in the organization selector dropdown. (Org Admin permissions required in both the source and target organization.)", "tokens": 322, "node_type": "child"}
{"id": 209, "chunk_id": "c16acf8a553ac24f5d77e7d3a1c6e493", "parent_id": 188, "url": "", "namespace": "langchain", "title": "langsmith-self-host-using-an-existing-secret", "headers": ["langsmith-self-host-using-an-existing-secret"], "section_index": 0, "chunk_index": 0, "text": "# langsmith-self-host-using-an-existing-secret\n\n> Source: https://docs.langchain.com/langsmith/self-host-using-an-existing-secret\n\nlangsmith-secrets\n: This secret contains the license key and some other basic configuration parameters. You can see the template for this secret herelangsmith-redis\n: This secret contains the Redis connection string and password. You can see the template for this secret herelangsmith-postgres\n: This secret contains the Postgres connection string and password. You can see the template for this secret herelangsmith-clickhouse\n: This secret contains the ClickHouse connection string and password. You can see the template for this secret here\nRequirements\n- An existing Kubernetes cluster\n- A way to create Kubernetes secrets in your cluster. This can be done using\nkubectl\n, a Helm chart, or a secrets operator like Sealed Secrets\nParameters\nYou will need to create your own Kubernetes secrets that adhere to the structure of the secrets provisioned by the LangSmith Helm Chart.The secrets must have the same structure as the ones provisioned by the LangSmith Helm Chart (refer to the links above to see the specific secrets). If you miss any of the required keys, your LangSmith instance may not work correctly.\nConfiguration\nWith these secrets provisioned, you can configure your LangSmith instance to use the secrets directly to avoid passing in secret values through plaintext. You can do this by modifying thelangsmith_config.yaml\nfile for your LangSmith Helm Chart installation.", "tokens": 220, "node_type": "child"}
{"id": 210, "chunk_id": "c16acf8a553ac24f5d77e7d3a1c6e493", "parent_id": 189, "url": "", "namespace": "langchain", "title": "langsmith-self-hosted", "headers": ["langsmith-self-hosted"], "section_index": 0, "chunk_index": 0, "text": "# langsmith-self-hosted\n\n> Source: https://docs.langchain.com/langsmith/self-hosted\n\nImportant\nSelf-hosted LangSmith is an add-on to the Enterprise plan designed for our largest, most security-conscious customers. For more details, refer to Pricing. Contact our sales team if you want to get a license key to trial LangSmith in your environment.\nSelf-hosted LangSmith is an add-on to the Enterprise plan designed for our largest, most security-conscious customers. For more details, refer to Pricing. Contact our sales team if you want to get a license key to trial LangSmith in your environment.\n- LangSmith: Deploy an instance of the LangSmith application that includes observability, tracing, and evaluations in the UI and API. Best for teams who want self-hosted monitoring and evaluation without deploying agents.\n- LangSmith with deployment: Deploy a graph to LangGraph Server via the control plane. The control plane and data plane form the full LangSmith platform, providing UI and API management for running and monitoring agents. This includes observability, evaluation, and deployment management.\n- Standalone server: Deploy a LangGraph Server directly without the control plane UI. Ideal for lightweight setups running one or a few agents as independent services, with full control over scaling and integration.\n| Model | Includes | Best for | Methods |\n|---|---|---|---|\n| LangSmith |\n|\n|\n|\n| LangSmith with deployment |\n|\n|\n|\n| Standalone server |\n|\n|\n|\nLangSmith\nDeploy an instance of the LangSmith application that includes observability, tracing, and evaluations in the UI and API\u2014but without the ability to deploy agents through the control plane. This includes: Services:- LangSmith frontend UI\n- LangSmith backend API\n- LangSmith Platform backend\n- LangSmith Playground\n- LangSmith queue\n- LangSmith ACE (Arbitrary Code Execution) backend\n- ClickHouse (traces and feedback data)\n- PostgreSQL (operational data)\n- Redis (queuing and caching)\n- Blob storage (optional, but recommended for production)\nServices\nStorage services\nLangSmith will bundle all storage services by default. You can configure it to use external versions of all storage services. In a production setting, we strongly recommend using external storage services.\n| Service | Description |\n|---|---|\n| ClickHouse | ClickHouse is a high-performance, column-oriented SQL database management system (DBMS) for online analytical processing (OLAP). LangSmith uses ClickHouse as the primary data store for traces and feedback (high-volume data). |\n| PostgreSQL | PostgreSQL is a powerful, open source object-relational database system that uses and extends the SQL language combined with many features that safely store and scale the most complicated data workloads. LangSmith uses PostgreSQL as the primary data store for transactional workloads and operational data (almost everything besides traces and feedback). |\n| Redis | Redis is a powerful in-memory key-value database that persists on disk. By holding data in memory, Redis offers high performance for operations like caching. LangSmith uses Redis to back queuing and caching operations. |\n| Blob storage | LangSmith supports several blob storage providers, including AWS S3, Azure Blob Storage, and Google Cloud Storage. LangSmith uses blob storage to store large files, such as trace artifacts, feedback attachments, and other large data objects. Blob storage is optional, but highly recommended for production deployments. |\nSetup methods\n- Docker Compose (development/testing only)\n- Kubernetes + Helm (recommended for production)\nSetup guides\n- Install on Kubernetes (production)\n- Install with Docker (development only)\nLangSmith with Deployment\nLangSmith with deployment builds on top of the LangSmith option. Enabling deployment is ideal for enterprise teams who want a centralized, UI-driven platform to deploy and manage multiple agents and graphs, with all infrastructure, data, and orchestration fully under their control. This includes everything from LangSmith, plus:| Component | Responsibilities | Where it runs | Who manages it |\n|---|---|---|---|\n| Your cloud | You | |\n| Your cloud | You |\nRequirements\n- You must already have a self-hosted LangSmith instance installed in your cloud\n- Kubernetes cluster (required for control plane and data plane)\n- Use\nlanggraph-cli\nor Studio to test your graph locally - Build a Docker image with\nlanggraph build\n- Deploy your LangGraph Server via the LangSmith control plane UI or through your container tooling of choice\n- All agents are deployed as Kubernetes services behind the ingress configured for your LangSmith instance\nSupported compute platforms\n- Kubernetes: LangSmith with deployment supports running control plane and data plane infrastructure on any Kubernetes cluster.\nSetup guide\nStandalone Server\nThe Standalone server option is the most lightweight and flexible way to run LangSmith. Unlike the other models, you only manage a simplified made up of LangGraph Servers and their required backing services (PostgreSQL, Redis, etc.). This includes:| Component | Responsibilities | Where it runs | Who manages it |\n|---|---|---|---|\n| Control plane | n/a | n/a | n/a |\n| Data plane |\n| Your cloud | You |\nDo not run standalone servers in serverless environments. Scale-to-zero may cause task loss and scaling up will not work reliably.\nWorkflow\n- Define and test your graph locally using the\nlanggraph-cli\nor Studio - Package your agent as a Docker image\n- Deploy the LangGraph Server to your compute platform of choice (Kubernetes, Docker, VM)\n- Optionally, configure LangSmith API keys and endpoints so the server reports traces and evaluations back to LangSmith (self-hosted or SaaS)\nSupported compute platforms\n- Kubernetes: Use the LangSmith Helm chart to run LangGraph Servers in a Kubernetes cluster. This is the recommended option for production-grade deployments.\n- Docker: Run in any Docker-supported compute platform (local dev machine, VM, ECS, etc.). This is best suited for development or small-scale workloads.", "tokens": 917, "node_type": "child"}
{"id": 211, "chunk_id": "c16acf8a553ac24f5d77e7d3a1c6e493", "parent_id": 190, "url": "", "namespace": "langchain", "title": "langsmith-semantic-search", "headers": ["langsmith-semantic-search"], "section_index": 0, "chunk_index": 0, "text": "# langsmith-semantic-search\n\n> Source: https://docs.langchain.com/langsmith/semantic-search\n\nHow to add semantic search to your agent deployment\nThis guide explains how to add semantic search to your deployment\u2019s cross-thread store, so that your agent can search for memories and other documents by semantic similarity.\nOnce configured, you can use semantic search in your nodes. The store requires a namespace tuple to organize memories:\nCopy\ndef search_memory(state: State, *, store: BaseStore): # Search the store using semantic similarity # The namespace tuple helps organize different types of memories # e.g., (\"user_facts\", \"preferences\") or (\"conversation\", \"summaries\") results = store.search( namespace=(\"memory\", \"facts\"), # Organize memories by type query=\"your search query\", limit=3 # number of results to return ) return results\nThe deployment will look for the function in the specified path. The function must be async and accept a list of strings:\nCopy\n# path/to/embedding_function.pyfrom openai import AsyncOpenAIclient = AsyncOpenAI()async def aembed_texts(texts: list[str]) -> list[list[float]]: \"\"\"Custom embedding function that must: 1. Be async 2. Accept a list of strings 3. Return a list of float arrays (embeddings) \"\"\" response = await client.embeddings.create( model=\"text-embedding-3-small\", input=texts ) return [e.embedding for e in response.data]", "tokens": 185, "node_type": "child"}
{"id": 212, "chunk_id": "c16acf8a553ac24f5d77e7d3a1c6e493", "parent_id": 191, "url": "", "namespace": "langchain", "title": "langsmith-server-a2a", "headers": ["langsmith-server-a2a"], "section_index": 0, "chunk_index": 0, "text": "# langsmith-server-a2a\n\n> Source: https://docs.langchain.com/langsmith/server-a2a\n\nAgent2Agent (A2A) is Google\u2019s protocol for enabling communication between conversational AI agents. LangSmith implements A2A support, allowing your agents to communicate with other A2A-compatible agents through a standardized protocol.The A2A endpoint is available in LangGraph Server at /a2a/{assistant_id}.\nEach assistant automatically exposes an A2A Agent Card that describes its capabilities and provides the information needed for other agents to connect. You can retrieve the agent card for any assistant using:\nCopy\nGET /.well-known/agent-card.json?assistant_id={assistant_id}\nThe agent card includes the assistant\u2019s name, description, available skills, supported input/output modes, and the A2A endpoint URL for communication.\nThis example creates an A2A-compatible agent that processes incoming messages using OpenAI\u2019s API and maintains conversational state. The agent defines a message-based state structure and handles the A2A protocol\u2019s message format.To be compatible with the A2A \u201ctext\u201d parts, the agent must have a messages key in state. Here\u2019s an example:\nCopy\n\"\"\"LangGraph A2A conversational agent.Supports the A2A protocol with messages input for conversational interactions.\"\"\"from __future__ import annotationsimport osfrom dataclasses import dataclassfrom typing import Any, Dict, List, TypedDictfrom langgraph.graph import StateGraphfrom langgraph.runtime import Runtimefrom openai import AsyncOpenAIclass Context(TypedDict): \"\"\"Context parameters for the agent.\"\"\" my_configurable_param: str@dataclassclass State: \"\"\"Input state for the agent. Defines the initial structure for A2A conversational messages. \"\"\" messages: List[Dict[str, Any]]async def call_model(state: State, runtime: Runtime[Context]) -> Dict[str, Any]: \"\"\"Process conversational messages and returns output using OpenAI.\"\"\" # Initialize OpenAI client client = AsyncOpenAI(api_key=os.getenv(\"OPENAI_API_KEY\")) # Process the incoming messages latest_message = state.messages[-1] if state.messages else {} user_content = latest_message.get(\"content\", \"No message content\") # Create messages for OpenAI API openai_messages = [ { \"role\": \"system\", \"content\": \"You are a helpful conversational agent. Keep responses brief and engaging.\" }, { \"role\": \"user\", \"content\": user_content } ] try: # Make OpenAI API call response = await client.chat.completions.create( model=\"gpt-3.5-turbo\", messages=openai_messages, max_tokens=100, temperature=0.7 ) ai_response = response.choices[0].message.content except Exception as e: ai_response = f\"I received your message but had trouble processing it. Error: {str(e)[:50]}...\" # Create a response message response_message = { \"role\": \"assistant\", \"content\": ai_response } return { \"messages\": state.messages + [response_message] }# Define the graphgraph = ( StateGraph(State, context_schema=Context) .add_node(call_model) .add_edge(\"__start__\", \"call_model\") .compile())\nOnce your agents are running locally via langgraph dev or deployed to production, you can facilitate communication between them using the A2A protocol.This example demonstrates how two agents can communicate by sending JSON-RPC messages to each other\u2019s A2A endpoints. The script simulates a multi-turn conversation where each agent processes the other\u2019s response and continues the dialogue.\nCopy\n#!/usr/bin/env python3\"\"\"Agent-to-Agent conversation simulation using LangGraph A2A protocol.\"\"\"import asyncioimport aiohttpimport osasync def send_message(session, port, assistant_id, text): \"\"\"Send a message to an agent and return the response text.\"\"\" url = f\"http://127.0.0.1:{port}/a2a/{assistant_id}\" payload = { \"jsonrpc\": \"2.0\", \"id\": \"\", \"method\": \"message/send\", \"params\": { \"message\": { \"role\": \"user\", \"parts\": [{\"kind\": \"text\", \"text\": text}] }, \"messageId\": \"\", \"thread\": {\"threadId\": \"\"} } } headers = {\"Accept\": \"application/json\"} async with session.post(url, json=payload, headers=headers) as response: try: result = await response.json() return result[\"result\"][\"artifacts\"][0][\"parts\"][0][\"text\"] except Exception as e: text = await response.text() print(f\"Response error from port {port}: {response.status} - {text}\") return f\"Error from port {port}: {response.status}\"async def simulate_conversation(): \"\"\"Simulate a conversation between two agents.\"\"\" agent_a_id = os.getenv(\"AGENT_A_ID\") agent_b_id = os.getenv(\"AGENT_B_ID\") if not agent_a_id or not agent_b_id: print(\"Set AGENT_A_ID and AGENT_B_ID environment variables\") return message = \"Hello! Let's have a conversation.\" async with aiohttp.ClientSession() as session: for i in range(3): print(f\"--- Round {i + 1} ---\") # Agent A responds message = await send_message(session, 2024, agent_a_id, message) print(f\"\ud83d\udd35 Agent A: {message}\") # Agent B responds message = await send_message(session, 2025, agent_b_id, message) print(f\"\ud83d\udd34 Agent B: {message}\") print()if __name__ == \"__main__\": asyncio.run(simulate_conversation())", "tokens": 588, "node_type": "child"}
{"id": 213, "chunk_id": "c16acf8a553ac24f5d77e7d3a1c6e493", "parent_id": 193, "url": "", "namespace": "langchain", "title": "langsmith-server-mcp", "headers": ["langsmith-server-mcp"], "section_index": 0, "chunk_index": 0, "text": "# langsmith-server-mcp\n\n> Source: https://docs.langchain.com/langsmith/server-mcp\n\n/mcp\non LangGraph Server.\nYou can set up custom authentication middleware to authenticate a user with an MCP server to get access to user-scoped tools within your LangSmith deployment.\nAn example architecture for this flow:\nRequirements\nTo use MCP, ensure you have the following dependencies installed:langgraph-api >= 0.2.3\nlanggraph-sdk >= 0.1.61\nUsage overview\nTo enable MCP:- Upgrade to use langgraph-api>=0.2.3. If you are deploying LangSmith, this will be done for you automatically if you create a new revision.\n- MCP tools (agents) will be automatically exposed.\n- Connect with any MCP-compliant client that supports Streamable HTTP.\nClient\nUse an MCP-compliant client to connect to the LangGraph server. The following examples show how to connect using different programming languages.- JavaScript/TypeScript\n- Python\nNote\nReplace serverUrl\nwith your LangGraph server URL and configure authentication headers as needed.\nExpose an agent as MCP tool\nWhen deployed, your agent will appear as a tool in the MCP endpoint with this configuration:- Tool name: The agent\u2019s name.\n- Tool description: The agent\u2019s description.\n- Tool input schema: The agent\u2019s input schema.\nSetting name and description\nYou can set the name and description of your agent inlanggraph.json\n:\nSchema\nDefine clear, minimal input and output schemas to avoid exposing unnecessary internal complexity to the LLM. The default MessagesState usesAnyMessage\n, which supports many message types but is too general for direct LLM exposure.\nInstead, define custom agents or workflows that use explicitly typed input and output structures.\nFor example, a workflow answering documentation questions might look like this:\nUse user-scoped MCP tools in your deployment\nTo make user-scoped tools available to your LangSmith deployment, start with implementing a snippet like the following:\n- MCP only supports adding headers to requests made to\nstreamable_http\nandsse\ntransport\nservers. - Your MCP server URL.\n- Get available tools from your MCP server.\nSession behavior\nThe current LangGraph MCP implementation does not support sessions. Each/mcp\nrequest is stateless and independent.\nAuthentication\nThe/mcp\nendpoint uses the same authentication as the rest of the LangGraph API. Refer to the authentication guide for setup details.\nDisable MCP\nTo disable the MCP endpoint, setdisable_mcp\nto true\nin your langgraph.json\nconfiguration file:\n/mcp\nendpoint.", "tokens": 364, "node_type": "child"}
{"id": 214, "chunk_id": "c16acf8a553ac24f5d77e7d3a1c6e493", "parent_id": 194, "url": "", "namespace": "langchain", "title": "langsmith-serverless-environments", "headers": ["langsmith-serverless-environments"], "section_index": 0, "chunk_index": 0, "text": "# langsmith-serverless-environments\n\n> Source: https://docs.langchain.com/langsmith/serverless-environments\n\nWhen tracing JavaScript functions, LangSmith will trace runs in the background by default to avoid adding latency. In serverless environments where the execution context may be terminated abruptly, it\u2019s important to ensure that all tracing data is properly flushed before the function completes.\nTo make sure this occurs, you can either:\n- Set an environment variable named\nLANGSMITH_TRACING_BACKGROUND\nto\"false\"\n. This will cause your traced functions to wait for tracing to complete before returning.- Note that this is named differently from the environment variable in LangChain.js because LangSmith can be used without LangChain.\n- Pass a custom client into your traced runs and\nawait\ntheclient.awaitPendingTraceBatches();\nmethod.\nawaitPendingTraceBatches\nalongside the traceable\nmethod:\nmanualFlushMode: true\nin your client like this:\nclient.flush()\nlike this before your serverless function closes:\n.flush()\n.", "tokens": 133, "node_type": "child"}
{"id": 215, "chunk_id": "c16acf8a553ac24f5d77e7d3a1c6e493", "parent_id": 195, "url": "", "namespace": "langchain", "title": "langsmith-set-up-a-workspace", "headers": ["langsmith-set-up-a-workspace"], "section_index": 0, "chunk_index": 0, "text": "# langsmith-set-up-a-workspace\n\n> Source: https://docs.langchain.com/langsmith/set-up-a-workspace\n\n- Set up an organization: Create and manage organizations for team collaboration, including user management and role assignments.\n- Set up a workspace: Set up and configure workspaces to organize your LangSmith resources, manage workspace members, and configure settings for team collaboration.\nSet up an organization\nCreate an organization\nWhen you log in for the first time, LangSmith will create a personal organization for you automatically. If you\u2019d like to collaborate with others, you can create a separate organization and invite your team members to join. To do this, open the Organizations drawer by clicking your profile icon in the bottom left and click + New. Shared organizations require a credit card before they can be used. You will need to set up billing to proceed.Manage and navigate workspaces\nOnce you\u2019ve subscribed to a plan that allows for multiple users per organization, you can set up workspaces to collaborate more effectively and isolate LangSmith resources between different groups of users. To navigate between workspaces and access the resources within each workspace (trace projects, annotation queues, etc.), select the desired workspace from the picker in the top left:Manage users\nManage membership in your shared organization in the Members and roles tabs on the Settings page. Here you can:- Invite new users to your organization, selecting workspace membership and (if RBAC is enabled) workspace role.\n- Edit a user\u2019s organization role.\n- Remove users from your organization.\nOrganization roles\nOrganization-scoped roles are used to determine access to organization settings. The role selected also impacts workspace membership:Organization Admin\ngrants full access to manage all organization configuration, users, billing, and workspaces. AnyOrganization Admin\nhasAdmin\naccess to all workspaces in an organization.\nOrganization User\nmay read organization information, but cannot execute any write actions at the organization level. You can add anOrganization User\nto a subset of workspaces and assigned workspace roles as usual (if RBAC is enabled), which specify permissions at the workspace level.\nThe\nOrganization User\nrole is only available in organizations on plans with multiple workspaces. In organizations limited to a single workspace, all users are Organization Admins\n. Custom organization-scoped roles are not available.Set up a workspace\nWhen you log in for the first time, a default workspace will be created for you in your personal organization. Workspaces are often used to separate resources between different teams or business units to establish clear trust boundaries between them. Within each workspace, Role-Based Access Control (RBAC) manages permissions and access levels, which ensures that users only have access to the resources and settings necessary for their role. Most LangSmith activity happens in the context of a workspace, each of which has its own settings and access controls. To organize resources within a workspace, you can use resource tags.Create a workspace\nTo create a new workspace, navigate to the Settings page Workspaces tab in your shared organization and click Add Workspace. Once you have created your workspace, you can manage its members and other configuration by selecting it on this page.Manage users\nOnly workspace\nAdmins\ncan manage workspace membership and, if RBAC is enabled, change a user\u2019s workspace role.Admin\nmay add them to a workspace in the Workspace members tab under Workspaces settings page. Users may also be invited directly to one or more workspaces when they are invited to an organization.\nConfigure workspace settings\nWorkspace configuration exists in the Workspaces settings page tab. Select the workspace to configure and then the desired configuration sub-tab. The following example shows the API keys, and other configuration options including secrets, models, and shared URLs are available here as well.Delete a workspace\nDeleting a workspace will permanently delete the workspace and all associated data. This action cannot be undone.\nAdmin\nin order to delete a workspace.\nDelete a workspace via the UI\n- Navigate to Settings.\n- Select the workspace you want to delete.\n- Click Delete in the top-right corner of the screen.", "tokens": 652, "node_type": "child"}
{"id": 216, "chunk_id": "c16acf8a553ac24f5d77e7d3a1c6e493", "parent_id": 196, "url": "", "namespace": "langchain", "title": "langsmith-set-up-custom-auth", "headers": ["langsmith-set-up-custom-auth"], "section_index": 0, "chunk_index": 0, "text": "# langsmith-set-up-custom-auth\n\n> Source: https://docs.langchain.com/langsmith/set-up-custom-auth\n\n- Set up custom authentication (you are here) - Control who can access your bot\n- Make conversations private - Let users have private conversations\n- Connect an authentication provider - Add real user accounts and validate using OAuth2 for production\nCustom auth is only available for LangSmith SaaS deployments or Enterprise Self-Hosted deployments.\n1. Create your app\nCreate a new chatbot using the LangGraph starter template:2. Add authentication\nNow that you have a base LangGraph app, add authentication to it.In this tutorial, you will start with a hard-coded token for example purposes. You will get to a \u201cproduction-ready\u201d authentication scheme in the third tutorial.\nAuth\nobject lets you register an authentication function that the LangGraph platform will run on every request. This function receives each request and decides whether to accept or reject.\nCreate a new file src/security/auth.py\n. This is where your code will live to check if users are allowed to access your bot:\nsrc/security/auth.py\n- Checks if a valid token is provided in the request\u2019s Authorization header\n- Returns the user\u2019s identity\nlanggraph.json\nconfiguration:\nlanggraph.json\n3. Test your bot\nStart the server again to test everything out:--no-browser\n, the Studio UI will open in the browser. By default, we also permit access from Studio, even when using custom auth. This makes it easier to develop and test your bot in Studio. You can remove this alternative authentication option by setting disable_studio_auth: \"true\"\nin your auth configuration:\n4. Chat with your bot\nYou should now only be able to access the bot if you provide a valid token in the request header. Users will still, however, be able to access each other\u2019s resources until you add resource authorization handlers in the next section of the tutorial. Run the following code in a file or notebook:- Without a valid token, we can\u2019t access the bot\n- With a valid token, we can create threads and chat\nNext steps\nNow that you can control who accesses your bot, you might want to:- Continue the tutorial by going to Make conversations private to learn about resource authorization.\n- Read more about authentication concepts.\n- Check out the API reference for more authentication details.", "tokens": 368, "node_type": "child"}
{"id": 217, "chunk_id": "c16acf8a553ac24f5d77e7d3a1c6e493", "parent_id": 197, "url": "", "namespace": "langchain", "title": "langsmith-set-up-feedback-criteria", "headers": ["langsmith-set-up-feedback-criteria"], "section_index": 0, "chunk_index": 0, "text": "# langsmith-set-up-feedback-criteria\n\n> Source: https://docs.langchain.com/langsmith/set-up-feedback-criteria\n\nFeedback criteria are represented in the application as feedback tags. For human feedback, you can set up new feedback criteria as continuous feedback or categorical feedback.\nTo set up a new feedback criteria, follow this link to view all existing tags for your workspace, then click New Tag.\nContinuous feedback\nFor continuous feedback, you can enter a feedback tag name, then select a minimum and maximum value. Every value, including floating-point numbers, within this range will be accepted as feedback scores.Categorical feedback\nFor categorical feedback, you can enter a feedback tag name, then add a list of categories, each category mapping to a score. When you provide feedback, you can select one of these categories as the feedback score. Both the category label and the score will be logged as feedback invalue\nand score\nfields, respectively.", "tokens": 141, "node_type": "child"}
{"id": 218, "chunk_id": "c16acf8a553ac24f5d77e7d3a1c6e493", "parent_id": 198, "url": "", "namespace": "langchain", "title": "langsmith-set-up-resource-tags", "headers": ["langsmith-set-up-resource-tags"], "section_index": 0, "chunk_index": 0, "text": "# langsmith-set-up-resource-tags\n\n> Source: https://docs.langchain.com/langsmith/set-up-resource-tags\n\nBefore diving into this content, it might be helpful to read the following:\nResource tags are available for Plus and Enterprise plans.\nWhile workspaces help separate trust boundaries and access control, tags help you organize resources within a workspace. Tags are key-value pairs that you can attach to resources.\nCreate a tag\nTo create a tag, head to the workspace settings and click on the \u201cResource Tags\u201d tab. Here, you\u2019ll be able to see the existing tag values, grouped by key. Two keys Application\nand Environment\nare created by default.\nTo create a new tag, click on the \u201cNew Tag\u201d button. You\u2019ll be prompted to enter a key and a value for the tag. Note that you can use an existing key or create a new one.\nAssign a tag to a resource\nWithin the same side panel for creating a new tag, you can also create assign resources to tags. Search for corresponding resources in the \u201cAssign Resources\u201d section and select the resources you want to tag.\nYou can only tag workspace-scoped resources with resource tags. This includes Tracing Projects, Annotation Queues, Deployments, Experiments, Datasets, and Prompts.\nYou can also assign tags to resources from the resource\u2019s detail page. Click on the Resource tags button to open up the tag panel and assign tags.\nTo un-assign a tag from a resource, click on the Trash icon next to the tag, both in the tag panel and the resource tag panel.\nDelete a tag\nYou can delete either a key or a value of a tag from the workspace settings page. To delete a key, click on the Trash icon next to the key. To delete a value, click on the Trash icon next to the value.\nNote that if you delete a key, all values associated with that key will also be deleted. When you delete a value, you will lose all associations between that value and resources.\nYou can use resource tags to organize your experience navigating resources in the workspace.\nTo filter resources by tags in your workspace, open up the left-hand side panel and click on the tags icon. Here, you can select the tags you want to filter by.\nIn the homepage, you can see updated counts for resources based on the tags you\u2019ve selected.\nAs you navigate through the different product surfaces, you will only see resources that match the tags you\u2019ve selected. At any time, you can clear the tags to see all resources in the workspace or select different tags to filter by.", "tokens": 426, "node_type": "child"}
{"id": 219, "chunk_id": "c16acf8a553ac24f5d77e7d3a1c6e493", "parent_id": 199, "url": "", "namespace": "langchain", "title": "langsmith-setup-app-requirements-txt", "headers": ["langsmith-setup-app-requirements-txt"], "section_index": 0, "chunk_index": 0, "text": "# langsmith-setup-app-requirements-txt\n\n> Source: https://docs.langchain.com/langsmith/setup-app-requirements-txt\n\nrequirements.txt\nto specify project dependencies.\nThis example is based on this repository, which uses the LangGraph framework.\nThe final repository structure will look something like this:\nLangSmith Deployment supports deploying a LangGraph graph. However, the implementation of a node of a graph can contain arbitrary Python code. This means any framework can be implemented within a node and deployed on LangSmith Deployment. This lets you keep your core application logic outside LangGraph while still using LangSmith for deployment, scaling, and observability.\npyproject.toml\n: If you prefer using poetry for dependency management, check out this how-to guide on usingpyproject.toml\nfor LangSmith.- a monorepo: If you are interested in deploying a graph located inside a monorepo, take a look at this repository for an example of how to do so.\nSpecify Dependencies\nDependencies can optionally be specified in one of the following files:pyproject.toml\n, setup.py\n, or requirements.txt\n. If none of these files is created, then dependencies can be specified later in the configuration file.\nThe dependencies below will be included in the image, you can also use them in your code, as long as with a compatible version range:\nrequirements.txt\nfile:\nSpecify Environment Variables\nEnvironment variables can optionally be specified in a file (e.g..env\n). See the Environment Variables reference to configure additional variables for a deployment.\nExample .env\nfile:\nDefine Graphs\nImplement your graphs. Graphs can be defined in a single file or multiple files. Make note of the variable names of each CompiledStateGraph to be included in the application. The variable names will be used later when creating the LangGraph configuration file. Exampleagent.py\nfile, which shows how to import from other modules you define (code for the modules is not shown here, please see this repository to see their implementation):\nCreate the configuration file\nCreate a configuration file calledlanggraph.json\n. See the configuration file reference for detailed explanations of each key in the JSON object of the configuration file.\nExample langgraph.json\nfile:\nCompiledGraph\nappears at the end of the value of each subkey in the top-level graphs\nkey (i.e. :<variable_name>\n).\nConfiguration File Location\nThe configuration file must be placed in a directory that is at the same level or higher than the Python files that contain compiled graphs and associated dependencies.", "tokens": 379, "node_type": "child"}
{"id": 220, "chunk_id": "c16acf8a553ac24f5d77e7d3a1c6e493", "parent_id": 200, "url": "", "namespace": "langchain", "title": "langsmith-setup-javascript", "headers": ["langsmith-setup-javascript"], "section_index": 0, "chunk_index": 0, "text": "# langsmith-setup-javascript\n\n> Source: https://docs.langchain.com/langsmith/setup-javascript\n\npackage.json\nto specify project dependencies.\nThis walkthrough is based on this repository, which you can play around with to learn more about how to set up your application for deployment.\nThe final repository structure will look something like this:\nLangSmith Deployment supports deploying a LangGraph graph. However, the implementation of a node of a graph can contain arbitrary Python code. This means any framework can be implemented within a node and deployed on LangSmith Deployment. This lets you keep your core application logic outside LangGraph while still using LangSmith for deployment, scaling, and observability.\nSpecify Dependencies\nDependencies can be specified in apackage.json\n. If none of these files is created, then dependencies can be specified later in the configuration file.\nExample package.json\nfile:\nSpecify Environment Variables\nEnvironment variables can optionally be specified in a file (e.g..env\n). See the Environment Variables reference to configure additional variables for a deployment.\nExample .env\nfile:\nDefine Graphs\nImplement your graphs. Graphs can be defined in a single file or multiple files. Make note of the variable names of each compiled graph to be included in the application. The variable names will be used later when creating the configuration file. Here is an exampleagent.ts\n:\nCreate the API Config\nCreate a configuration file calledlanggraph.json\n. See the configuration file reference for detailed explanations of each key in the JSON object of the configuration file.\nExample langgraph.json\nfile:\nCompiledGraph\nappears at the end of the value of each subkey in the top-level graphs\nkey (i.e. :<variable_name>\n).\nConfiguration Location\nThe configuration file must be placed in a directory that is at the same level or higher than the TypeScript files that contain compiled graphs and associated dependencies.", "tokens": 287, "node_type": "child"}
{"id": 221, "chunk_id": "c16acf8a553ac24f5d77e7d3a1c6e493", "parent_id": 201, "url": "", "namespace": "langchain", "title": "langsmith-setup-pyproject", "headers": ["langsmith-setup-pyproject"], "section_index": 0, "chunk_index": 0, "text": "# langsmith-setup-pyproject\n\n> Source: https://docs.langchain.com/langsmith/setup-pyproject\n\npyproject.toml\nto define your package\u2019s dependencies.\nThis example is based on this repository, which uses the LangGraph framework.\nThe final repository structure will look something like this:\nLangSmith Deployment supports deploying a LangGraph graph. However, the implementation of a node of a graph can contain arbitrary Python code. This means any framework can be implemented within a node and deployed on LangSmith Deployment. This lets you keep your core application logic outside LangGraph while still using LangSmith for deployment, scaling, and observability.\nrequirements.txt\n: for dependency management, check out this how-to guide on usingrequirements.txt\nfor LangSmith.- a monorepo: To deploy a graph located inside a monorepo, take a look at this repository for an example of how to do so.\nSpecify Dependencies\nDependencies can optionally be specified in one of the following files:pyproject.toml\n, setup.py\n, or requirements.txt\n. If none of these files is created, then dependencies can be specified later in the configuration file.\nThe dependencies below will be included in the image, you can also use them in your code, as long as with a compatible version range:\npyproject.toml\nfile:\nSpecify Environment Variables\nEnvironment variables can optionally be specified in a file (e.g..env\n). See the Environment Variables reference to configure additional variables for a deployment.\nExample .env\nfile:\nDefine Graphs\nImplement your graphs. Graphs can be defined in a single file or multiple files. Make note of the variable names of each CompiledStateGraph to be included in the application. The variable names will be used later when creating the configuration file. Exampleagent.py\nfile, which shows how to import from other modules you define (code for the modules is not shown here, please see this repository to see their implementation):\nCreate the configuration file\nCreate a configuration file calledlanggraph.json\n. See the configuration file reference for detailed explanations of each key in the JSON object of the configuration file.\nExample langgraph.json\nfile:\nCompiledGraph\nappears at the end of the value of each subkey in the top-level graphs\nkey (i.e. :<variable_name>\n).\nConfiguration File Location\nThe configuration file must be placed in a directory that is at the same level or higher than the Python files that contain compiled graphs and associated dependencies.", "tokens": 370, "node_type": "child"}
{"id": 222, "chunk_id": "c16acf8a553ac24f5d77e7d3a1c6e493", "parent_id": 202, "url": "", "namespace": "langchain", "title": "langsmith-share-trace", "headers": ["langsmith-share-trace"], "section_index": 0, "chunk_index": 0, "text": "# langsmith-share-trace\n\n> Source: https://docs.langchain.com/langsmith/share-trace\n\nSharing a trace publicly will make it accessible to anyone with the link. Make sure you\u2019re not sharing sensitive information.If your self-hosted or hybrid LangSmith deployment is within a VPC, then the public link is accessible only to members authenticated within your VPC. For enhanced security, we recommend configuring your instance with a private URL accessible only to users with access to your network.\n- Click on Unshare by clicking on Public in the upper right hand corner of any publicly shared trace, then Unshare in the dialog.\n- Navigate to your organization\u2019s list of publicly shared traces, by clicking on Settings -> Shared URLs, then click on Unshare next to the trace you want to unshare.", "tokens": 122, "node_type": "child"}
{"id": 223, "chunk_id": "c16acf8a553ac24f5d77e7d3a1c6e493", "parent_id": 203, "url": "", "namespace": "langchain", "title": "langsmith-stateless-runs", "headers": ["langsmith-stateless-runs"], "section_index": 0, "chunk_index": 0, "text": "# langsmith-stateless-runs\n\n> Source: https://docs.langchain.com/langsmith/stateless-runs\n\nMost of the time, you provide a thread_id to your client when you run your graph in order to keep track of prior runs through the persistent state implemented in LangSmith Deployment. However, if you don\u2019t need to persist the runs you don\u2019t need to use the built-in persistent state and can create stateless runs.\nfrom langgraph_sdk import get_clientclient = get_client(url=<DEPLOYMENT_URL>)# Using the graph deployed with the name \"agent\"assistant_id = \"agent\"# create threadthread = await client.threads.create()\nWe can stream the results of a stateless run in an almost identical fashion to how we stream from a run with the state attribute, but instead of passing a value to the thread_id parameter, we pass None:\nPython\nJavascript\nCURL\nCopy\ninput = { \"messages\": [ {\"role\": \"user\", \"content\": \"Hello! My name is Bagatur and I am 26 years old.\"} ]}async for chunk in client.runs.stream( # Don't pass in a thread_id and the stream will be stateless None, assistant_id, input=input, stream_mode=\"updates\",): if chunk.data and \"run_id\" not in chunk.data: print(chunk.data)\nOutput:\nCopy\n{'agent': {'messages': [{'content': \"Hello Bagatur! It's nice to meet you. Thank you for introducing yourself and sharing your age. Is there anything specific you'd like to know or discuss? I'm here to help with any questions or topics you're interested in.\", 'additional_kwargs': {}, 'response_metadata': {}, 'type': 'ai', 'name': None, 'id': 'run-489ec573-1645-4ce2-a3b8-91b391d50a71', 'example': False, 'tool_calls': [], 'invalid_tool_calls': [], 'usage_metadata': None}]}}\n{'messages': [{'content': 'Hello! My name is Bagatur and I am 26 years old.','additional_kwargs': {},'response_metadata': {},'type': 'human','name': None,'id': '5e088543-62c2-43de-9d95-6086ad7f8b48','example': False},{'content': \"Hello Bagatur! It's nice to meet you. Thank you for introducing yourself and sharing your age. Is there anything specific you'd like to know or discuss? I'm here to help with any questions or topics you'd like to explore.\",'additional_kwargs': {},'response_metadata': {},'type': 'ai','name': None,'id': 'run-d6361e8d-4d4c-45bd-ba47-39520257f773','example': False,'tool_calls': [],'invalid_tool_calls': [],'usage_metadata': None}]}", "tokens": 299, "node_type": "child"}
{"id": 224, "chunk_id": "c16acf8a553ac24f5d77e7d3a1c6e493", "parent_id": 204, "url": "", "namespace": "langchain", "title": "langsmith-streaming", "headers": ["langsmith-streaming"], "section_index": 0, "chunk_index": 0, "text": "# langsmith-streaming\n\n> Source: https://docs.langchain.com/langsmith/streaming\n\nBasic usage\nBasic usage example:- Python\n- JavaScript\n- cURL\nExtended example: streaming updates\nExtended example: streaming updates\nThis is an example graph you can run in the LangGraph API server.\nSee LangSmith quickstart for more details.Once you have a running LangGraph API server, you can interact with it using\nLangGraph SDK\n- Python\n- JavaScript\n- cURL\n- The\nclient.runs.stream()\nmethod returns an iterator that yields streamed outputs. 2. Setstream_mode=\"updates\"\nto stream only the updates to the graph state after each node. Other stream modes are also available. See supported stream modes for details.\nSupported stream modes\n| Mode | Description | LangGraph Library Method |\n|---|---|---|\nvalues | Stream the full graph state after each super-step. | .stream() / .astream() with stream_mode=\"values\" |\nupdates | Streams the updates to the state after each step of the graph. If multiple updates are made in the same step (e.g., multiple nodes are run), those updates are streamed separately. | .stream() / .astream() with stream_mode=\"updates\" |\nmessages-tuple | Streams LLM tokens and metadata for the graph node where the LLM is invoked (useful for chat apps). | .stream() / .astream() with stream_mode=\"messages\" |\ndebug | Streams as much information as possible throughout the execution of the graph. | .stream() / .astream() with stream_mode=\"debug\" |\ncustom | Streams custom data from inside your graph | .stream() / .astream() with stream_mode=\"custom\" |\nevents | Stream all events (including the state of the graph); mainly useful when migrating large LCEL apps. | .astream_events() |\nStream multiple modes\nYou can pass a list as thestream_mode\nparameter to stream multiple modes at once.\nThe streamed outputs will be tuples of (mode, chunk)\nwhere mode\nis the name of the stream mode and chunk\nis the data streamed by that mode.\n- Python\n- JavaScript\n- cURL\nStream graph state\nUse the stream modesupdates\nand values\nto stream the state of the graph as it executes.\nupdates\nstreams the updates to the state after each step of the graph.values\nstreams the full value of the state after each step of the graph.\nExample graph\nExample graph\nStateful runs\nExamples below assume that you want to persist the outputs of a streaming run in the checkpointer DB and have created a thread. To create a thread:If you don\u2019t need to persist the outputs of a run, you can pass\n- Python\n- JavaScript\n- cURL\nNone\ninstead of thread_id\nwhen streaming.Stream Mode: updates\nUse this to stream only the state updates returned by the nodes after each step. The streamed outputs include the name of the node as well as the update.\n- Python\n- JavaScript\n- cURL\nStream Mode: values\nUse this to stream the full state of the graph after each step.\n- Python\n- JavaScript\n- cURL\nSubgraphs\nTo include outputs from subgraphs in the streamed outputs, you can setsubgraphs=True\nin the .stream()\nmethod of the parent graph. This will stream outputs from both the parent graph and any subgraphs.\n- Set\nstream_subgraphs=True\nto stream outputs from subgraphs.\nExtended example: streaming from subgraphs\nExtended example: streaming from subgraphs\nThis is an example graph you can run in the LangGraph API server.\nSee LangSmith quickstart for more details.Once you have a running LangGraph API server, you can interact with it using\nLangGraph SDKNote that we are receiving not just the node updates, but we also the namespaces which tell us what graph (or subgraph) we are streaming from.\n- Python\n- JavaScript\n- cURL\n- Set\nstream_subgraphs=True\nto stream outputs from subgraphs.\nDebugging\nUse thedebug\nstreaming mode to stream as much information as possible throughout the execution of the graph. The streamed outputs include the name of the node as well as the full state.\n- Python\n- JavaScript\n- cURL\nLLM tokens\nUse themessages-tuple\nstreaming mode to stream Large Language Model (LLM) outputs token by token from any part of your graph, including nodes, tools, subgraphs, or tasks.\nThe streamed output from messages-tuple\nmode is a tuple (message_chunk, metadata)\nwhere:\nmessage_chunk\n: the token or message segment from the LLM.metadata\n: a dictionary containing details about the graph node and LLM invocation.\nExample graph\nExample graph\n- Note that the message events are emitted even when the LLM is run using\ninvoke\nrather thanstream\n.\n- Python\n- JavaScript\n- cURL\n- The \u201cmessages-tuple\u201d stream mode returns an iterator of tuples\n(message_chunk, metadata)\nwheremessage_chunk\nis the token streamed by the LLM andmetadata\nis a dictionary with information about the graph node where the LLM was called and other information.\nFilter LLM tokens\n- To filter the streamed tokens by LLM invocation, you can associate\ntags\nwith LLM invocations. - To stream tokens only from specific nodes, use\nstream_mode=\"messages\"\nand filter the outputs by thelanggraph_node\nfield in the streamed metadata.\nStream custom data\nTo send custom user-defined data:- Python\n- JavaScript\n- cURL\nStream events\nTo stream all events, including the state of the graph:- Python\n- JavaScript\n- cURL\nStateless runs\nIf you don\u2019t want to persist the outputs of a streaming run in the checkpointer DB, you can create a stateless run without creating a thread:- Python\n- JavaScript\n- cURL\n- We are passing\nNone\ninstead of athread_id\nUUID.\nJoin and stream\nLangSmith allows you to join an active background run and stream outputs from it. To do so, you can use LangGraph SDK\u2019sclient.runs.join_stream\nmethod:\n- Python\n- JavaScript\n- cURL\n- This is the\nrun_id\nof an existing run you want to join.\nOutputs not buffered\nWhen you use\n.join_stream\n, output is not buffered, so any output produced before joining will not be received.", "tokens": 938, "node_type": "child"}
{"id": 225, "chunk_id": "c16acf8a553ac24f5d77e7d3a1c6e493", "parent_id": 205, "url": "", "namespace": "langchain", "title": "langsmith-studio", "headers": ["langsmith-studio"], "section_index": 0, "chunk_index": 0, "text": "# langsmith-studio\n\n> Source: https://docs.langchain.com/langsmith/studio\n\nStudio is a specialized agent IDE that enables visualization, interaction, and debugging of agentic systems that implement the LangGraph Server API protocol. Studio also integrates with tracing, evaluation, and prompt engineering.\nGraph mode exposes the full feature-set and is useful when you would like as many details about the execution of your agent, including the nodes traversed, intermediate states, and LangSmith integrations (such as adding to datasets and playground).\nChat mode is a simpler UI for iterating on and testing chat-specific agents. It is useful for business users and those who want to test overall agent behavior. Chat mode is only supported for graph\u2019s whose state includes or extends MessagesState.", "tokens": 115, "node_type": "child"}
{"id": 226, "chunk_id": "c16acf8a553ac24f5d77e7d3a1c6e493", "parent_id": 206, "url": "", "namespace": "langchain", "title": "langsmith-summary", "headers": ["langsmith-summary"], "section_index": 0, "chunk_index": 0, "text": "# langsmith-summary\n\n> Source: https://docs.langchain.com/langsmith/summary\n\nSome metrics can only be defined on the entire experiment level as opposed to the individual runs of the experiment. For example, you may want to compute the overall pass rate or f1 score of your evaluation target across all examples in the dataset. These are called summary evaluators.\nHere, we\u2019ll compute the f1-score, which is a combination of precision and recall.This sort of metric can only be computed over all of the examples in our experiment, so our evaluator takes in a list of outputs, and a list of reference_outputs.\nSummary evaluator functions must have specific argument names. They can take any subset of the following arguments:\ninputs: list[dict]: A list of the inputs corresponding to a single example in a dataset.\noutputs: list[dict]: A list of the dict outputs produced by each experiment on the given inputs.\nreference_outputs/referenceOutputs: list[dict]: A list of the reference outputs associated with the example, if available.\nruns: list[Run]: A list of the full Run objects generated by the two experiments on the given example. Use this if you need access to intermediate steps or metadata about each run.\nexamples: list[Example]: All of the dataset Example objects, including the example inputs, outputs (if available), and metdata (if available).\nSummary evaluators are expected to return one of the following types:Python and JS/TS\ndict: dicts of the form {\"score\": ..., \"name\": ...} allow you to pass a numeric or boolean score and metric name.\nCurrently Python only\nint | float | bool: this is interepreted as an continuous metric that can be averaged, sorted, etc. The function name is used as the name of the metric.", "tokens": 275, "node_type": "child"}
{"id": 227, "chunk_id": "c16acf8a553ac24f5d77e7d3a1c6e493", "parent_id": 207, "url": "", "namespace": "langchain", "title": "langsmith-test-react-agent-pytest", "headers": ["langsmith-test-react-agent-pytest"], "section_index": 0, "chunk_index": 0, "text": "# langsmith-test-react-agent-pytest\n\n> Source: https://docs.langchain.com/langsmith/test-react-agent-pytest\n\nSetup\nThis tutorial uses LangGraph for agent orchestration, OpenAI\u2019s GPT-4o, Tavily for search, E2B\u2019s code interpreter, and Polygon to retrieve stock data but it can be adapted for other frameworks, models and tools with minor modifications. Tavily, E2B and Polygon are free to sign up for.Installation\nFirst, install the packages required for making the agent:Environment Variables\nSet the following environment variables:Create your app\nTo define our React agent, we will use LangGraph/LangGraph.js for the orchestation and LangChain for the LLM and tools.Define tools\nFirst we are going to define the tools we are going to use in our agent. There are going to be 3 tools:- A search tool using Tavily\n- A code interpreter tool using E2B\n- A stock information tool using Polygon\nDefine agent\nNow that we have defined all of our tools, we can usecreate_agent\nto create our agent.\nWrite tests\nNow that we have defined our agent, let\u2019s write a few tests to ensure basic functionality. In this tutorial we are going to test whether the agent\u2019s tool calling abilities are working, whether the agent knows to ignore irrelevant questions, and whether it is able to answer complex questions that involve using all of the tools. We need to first set up a test file and add the imports needed at the top of the file.Test 1: Handle off-topic questions\nThe first test will be a simple check that the agent does not use tools on irrelevant queries.Test 2: Simple tool calling\nFor tool calling, we are going to verify that the agent calls the correct tool with the correct parameters.Test 3: Complex tool calling\nSome tool calls are easier to test than others. With the ticker lookup, we can assert that the correct ticker is searched. With the coding tool, the inputs and outputs of the tool are much less constrained, and there are lots of ways to get to the right answer. In this case, it\u2019s simpler to test that the tool is used correctly by running the full agent and asserting that it both calls the coding tool and that it ends up with the right answer.Test 4: LLM-as-a-judge\nWe are going to ensure that the agent\u2019s answer is grounded in the search results by running an LLM-as-a-judge evaluation. In order to trace the LLM as a judge call separately from our agent, we will use the LangSmith providedtrace_feedback\ncontext manager in Python and wrapEvaluator\nfunction in JS/TS.\nRun tests\nOnce you have setup your config files (if you are using Vitest or Jest), you can run your tests using the following commands:Config files for Vitest/Jest\nConfig files for Vitest/Jest\nReference code\nRemember to also add the config files for Vitest and Jest to your project.Agent\nAgent code\nAgent code\nTests\nTest code\nTest code", "tokens": 468, "node_type": "child"}
{"id": 228, "chunk_id": "c16acf8a553ac24f5d77e7d3a1c6e493", "parent_id": 208, "url": "", "namespace": "langchain", "title": "langsmith-threads", "headers": ["langsmith-threads"], "section_index": 0, "chunk_index": 0, "text": "# langsmith-threads\n\n> Source: https://docs.langchain.com/langsmith/threads\n\nMany LLM applications have a chatbot-like interface in which the user and the LLM application engage in a multi-turn conversation. In order to track these conversations, you can use the\nThreads\nfeature in LangSmith.\nGroup traces into threads\nAThread\nis a sequence of traces representing a single conversation. Each response is represented as its own trace, but these traces are linked together by being part of the same thread.\nTo associate traces together, you need to pass in a special metadata\nkey where the value is the unique identifier for that thread.\nThe key value is the unique identifier for that conversation.\nThe key name should be one of:\nsession_id\nthread_id\nconversation_id\n.\nf47ac10b-58cc-4372-a567-0e02b2c3d479\n.\nCode example\nThis example demonstrates how to log and retrieve conversation history from LangSmith to maintain long-running chats. You can add metadata to your traces in LangSmith in a variety of ways, this code will show how to do so dynamically, but read the previously linked guide to learn about all the ways you can add thread identifier metadata to your traces.getChatHistory: true\n,\nyou can continue the conversation from where it left off. This means that the LLM will receive the entire message history and respond to it,\ninstead of just responding to the latest message.\nView threads\nYou can view threads by clicking on theThreads\ntab in any project details page. You will then see a list of all threads, sorted by the most recent activity.\nYou can then click into a particular thread. This will open the history for a particular thread. If your threads are formatted as chat messages, you will a chatbot-like UI where you can see a history of inputs and outputs.\nYou can open up the trace or annotate the trace in a side panel by clicking on Annotate\nand Open trace\n, respectively.", "tokens": 311, "node_type": "child"}
{"id": 229, "chunk_id": "c16acf8a553ac24f5d77e7d3a1c6e493", "parent_id": 209, "url": "", "namespace": "langchain", "title": "langsmith-trace-anthropic", "headers": ["langsmith-trace-anthropic"], "section_index": 0, "chunk_index": 0, "text": "# langsmith-trace-anthropic\n\n> Source: https://docs.langchain.com/langsmith/trace-anthropic\n\nwrap_anthropic\nmethods in Python allows you to wrap your Anthropic client in order to automatically log traces \u2014 no decorator or function wrapping required! Using the wrapper ensures that messages, including tool calls and multimodal content blocks will be rendered nicely in LangSmith. The wrapper works seamlessly with the @traceable\ndecorator or traceable\nfunction and you can use both in the same application.\nThe\nLANGSMITH_TRACING\nenvironment variable must be set to 'true'\nin order for traces to be logged to LangSmith, even when using wrap_anthropic\n. This allows you to toggle tracing on and off without changing your code.Additionally, you will need to set the LANGSMITH_API_KEY\nenvironment variable to your API key (see Setup for more information).If your LangSmith API key is linked to multiple workspaces, set the LANGSMITH_WORKSPACE_ID\nenvironment variable to specify which workspace to use.By default, the traces will be logged to a project named default\n. To log traces to a different project, see this section.", "tokens": 164, "node_type": "child"}
{"id": 230, "chunk_id": "c16acf8a553ac24f5d77e7d3a1c6e493", "parent_id": 213, "url": "", "namespace": "langchain", "title": "langsmith-trace-openai", "headers": ["langsmith-trace-openai"], "section_index": 0, "chunk_index": 0, "text": "# langsmith-trace-openai\n\n> Source: https://docs.langchain.com/langsmith/trace-openai\n\nwrap_openai\n/wrapOpenAI\nmethods in Python/TypeScript allow you to wrap your OpenAI client in order to automatically log traces \u2014 no decorator or function wrapping required! Using the wrapper ensures that messages, including tool calls and multimodal content blocks will be rendered nicely in LangSmith. Also note that the wrapper works seamlessly with the @traceable\ndecorator or traceable\nfunction and you can use both in the same application.\nThe\nLANGSMITH_TRACING\nenvironment variable must be set to 'true'\nin order for traces to be logged to LangSmith, even when using wrap_openai\nor wrapOpenAI\n. This allows you to toggle tracing on and off without changing your code.Additionally, you will need to set the LANGSMITH_API_KEY\nenvironment variable to your API key (see Setup for more information).If your LangSmith API key is linked to multiple workspaces, set the LANGSMITH_WORKSPACE_ID\nenvironment variable to specify which workspace to use.By default, the traces will be logged to a project named default\n. To log traces to a different project, see this section.", "tokens": 170, "node_type": "child"}
{"id": 231, "chunk_id": "c16acf8a553ac24f5d77e7d3a1c6e493", "parent_id": 214, "url": "", "namespace": "langchain", "title": "langsmith-trace-query-syntax", "headers": ["langsmith-trace-query-syntax"], "section_index": 0, "chunk_index": 0, "text": "# langsmith-trace-query-syntax\n\n> Source: https://docs.langchain.com/langsmith/trace-query-syntax\n\nproject_id / project_name | The project(s) to fetch runs from - can be a single project or a list of projects. |\ntrace_id | Fetch runs that are part of a specific trace. |\nrun_type | The type of run to get, such as llm , chain , tool , retriever , etc. |\ndataset_name / dataset_id | Fetch runs that are associated with an example row in the specified dataset. This is useful for comparing prompts or models over a given dataset. |\nreference_example_id | Fetch runs that are associated with a specific example row. This is useful for comparing prompts or models on a given input. |\nparent_run_id | Fetch runs that are children of a given run. This is useful for fetching runs grouped together using the context manager or for fetching an agent trajectory. |\nerror | Fetch runs that errored or did not error. |\nrun_ids | Fetch runs with a given list of run ids. Note: This will ignore all other filtering arguments. |\nfilter | Fetch runs that match a given structured filter statement. See the guide below for more information. |\ntrace_filter | Filter to apply to the ROOT run in the trace tree. This is meant to be used in conjunction with the regular filter parameter to let you filter runs by attributes of the root run within a trace. |\ntree_filter | Filter to apply to OTHER runs in the trace tree, including sibling and child runs. This is meant to be used in conjunction with the regular filter parameter to let you filter runs by attributes of any run within a trace. |\nis_root | Only return root runs. |\nselect | Select the fields to return in the response. By default, all fields are returned. |\nquery (experimental) | Natural language query, which translates your query into a filter statement. |", "tokens": 318, "node_type": "child"}
{"id": 232, "chunk_id": "c16acf8a553ac24f5d77e7d3a1c6e493", "parent_id": 215, "url": "", "namespace": "langchain", "title": "langsmith-trace-with-api", "headers": ["langsmith-trace-with-api"], "section_index": 0, "chunk_index": 0, "text": "# langsmith-trace-with-api\n\n> Source: https://docs.langchain.com/langsmith/trace-with-api\n\nBasic tracing\nThe simplest way to log runs is via the POST and PATCH/runs\nendpoint. These routes expect minimal contextual information about the tree structure to\nWhen using the LangSmith REST API, you will need to provide your API key in the request headers as\n\"x-api-key\"\n.If your API key is linked to multiple workspaces, you will need to specify the workspace being used in the header with \"x-tenant-id\"\n.In the simple example, you do not need to set the dotted_order\nopr trace_id\nfields in the request body. These fields will be automatically generated by the system. Though this is simpler, it is slower and has a lower rate limit in LangSmith.Batch Ingestion\nFor faster ingestion of runs and higher rate limits, you can use the POST/runs/multipart\nlink endpoint. Below is an example. It requires orjson\n(for fast json ) and requests_toolbelt\nto run", "tokens": 149, "node_type": "child"}
{"id": 233, "chunk_id": "c16acf8a553ac24f5d77e7d3a1c6e493", "parent_id": 217, "url": "", "namespace": "langchain", "title": "langsmith-trace-with-crewai", "headers": ["langsmith-trace-with-crewai"], "section_index": 0, "chunk_index": 0, "text": "# langsmith-trace-with-crewai\n\n> Source: https://docs.langchain.com/langsmith/trace-with-crewai\n\npip install langsmith crewai openinference-instrumentation-crewai openinference-instrumentation-openai\nexport LANGSMITH_API_KEY=<your_langsmith_api_key>\nexport LANGSMITH_PROJECT=<your_project_name>\nexport OPENAI_API_KEY=<your_openai_api_key>\nfrom langsmith.integrations.otel import OtelSpanProcessor\nfrom opentelemetry import trace\nfrom opentelemetry.sdk.trace import TracerProvider\nfrom openinference.instrumentation.crewai import CrewAIInstrumentor\nfrom openinference.instrumentation.openai import OpenAIInstrumentor\n# Get or create tracer provider\ntracer_provider = trace.get_tracer_provider()\nif not isinstance(tracer_provider, TracerProvider):\ntracer_provider = TracerProvider()\ntrace.set_tracer_provider(tracer_provider)\n# Add OtelSpanProcessor to the tracer provider\ntracer_provider.add_span_processor(OtelSpanProcessor())\n# Instrument CrewAI and OpenAI\nCrewAIInstrumentor().instrument()\nOpenAIInstrumentor().instrument()\nfrom crewai import Agent, Task, Crew\nfrom langsmith.integrations.otel import OtelSpanProcessor\nfrom opentelemetry import trace\nfrom opentelemetry.sdk.trace import TracerProvider\nfrom openinference.instrumentation.crewai import CrewAIInstrumentor\nfrom openinference.instrumentation.openai import OpenAIInstrumentor\nimport dotenv\n# Load environment variables\ndotenv.load_dotenv(\".env.local\")\n# Configure OpenTelemetry\ntracer_provider = trace.get_tracer_provider()\nif not isinstance(tracer_provider, TracerProvider):\ntracer_provider = TracerProvider()\ntrace.set_tracer_provider(tracer_provider)\n# Add OtelSpanProcessor to the tracer provider\ntracer_provider.add_span_processor(OtelSpanProcessor())\n# Instrument CrewAI and OpenAI\nCrewAIInstrumentor().instrument()\nOpenAIInstrumentor().instrument()\n# Define your agents\nmarket_researcher = Agent(\nrole=\"Senior Market Researcher\",\ngoal=\"Analyze market trends and consumer behavior in the tech industry\",\nbackstory=\"\"\"You are an experienced market researcher with 10+ years of experience\nanalyzing technology markets. You excel at identifying emerging trends and\nunderstanding consumer needs.\"\"\",\nverbose=True,\nallow_delegation=False,\n)\ncontent_strategist = Agent(\nrole=\"Content Marketing Strategist\",\ngoal=\"Create compelling marketing content based on research insights\",\nbackstory=\"\"\"You are a creative content strategist who transforms complex market\nresearch into engaging marketing materials. You understand how to communicate\ntechnical concepts to different audiences.\"\"\",\nverbose=True,\nallow_delegation=False,\n)\ndata_analyst = Agent(\nrole=\"Data Analyst\",\ngoal=\"Provide statistical analysis and data-driven insights\",\nbackstory=\"\"\"You are a skilled data analyst who can interpret complex datasets\nand provide actionable insights. You excel at finding patterns and trends\nin data that others might miss.\"\"\",\nverbose=True,\nallow_delegation=False,\n)\n# Define your tasks\nresearch_task = Task(\ndescription=\"\"\"Conduct comprehensive research on the current state of AI adoption\nin small to medium businesses. Focus on:\n1. Current adoption rates and trends\n2. Main barriers to adoption\n3. Most popular AI tools and use cases\n4. ROI and business impact metrics\nProvide a detailed analysis with supporting data and statistics.\"\"\",\nagent=market_researcher,\nexpected_output=\"A comprehensive market research report on AI adoption in SMBs with data, trends, and insights.\",\n)\nanalysis_task = Task(\ndescription=\"\"\"Analyze the research findings and identify key statistical patterns.\nCreate data visualizations and provide quantitative insights on:\n1. Adoption rate trends over time\n2. Industry-specific adoption patterns\n3. ROI correlation analysis\n4. Barrier impact assessment\nPresent findings in a clear, data-driven format.\"\"\",\nagent=data_analyst,\nexpected_output=\"Statistical analysis report with key metrics, trends, and data-driven insights.\",\ncontext=[research_task],\n)\ncontent_task = Task(\ndescription=\"\"\"Based on the research and analysis, create a compelling marketing\nstrategy document that includes:\n1. Executive summary of key findings\n2. Target audience personas based on adoption patterns\n3. Key messaging framework addressing main barriers\n4. Content recommendations for different business segments\n5. Campaign strategy to drive AI adoption\nMake the content actionable and business-focused.\"\"\",\nagent=content_strategist,\nexpected_output=\"Complete marketing strategy document with personas, messaging, and campaign recommendations.\",\ncontext=[research_task, analysis_task],\n)\n# Create and run the crew\ncrew = Crew(\nagents=[market_researcher, data_analyst, content_strategist],\ntasks=[research_task, analysis_task, content_task],\nverbose=True,\nprocess=\"sequential\" # Tasks will be executed in order\n)\ndef run_market_research_crew():\n\"\"\"Run the market research crew and return results.\"\"\"\nresult = crew.kickoff()\nreturn result\nif __name__ == \"__main__\":\nprint(\"Running CrewAI market research process...\")\noutput = run_market_research_crew()\nprint(\"\\n\" + \"=\"*50)\nprint(\"CrewAI Process Output:\")\nprint(\"=\"*50)\nprint(output)\nfrom opentelemetry import trace\n# Get the current tracer\ntracer = trace.get_tracer(__name__)\ndef run_market_research_crew():\nwith tracer.start_as_current_span(\"crewai_market_research\") as span:\n# Add custom metadata\nspan.set_attribute(\"langsmith.metadata.crew_type\", \"market_research\")\nspan.set_attribute(\"langsmith.metadata.agent_count\", \"3\")\nspan.set_attribute(\"langsmith.metadata.task_complexity\", \"high\")\nspan.set_attribute(\"langsmith.span.tags\", \"crewai,market-research,multi-agent\")\n# Run your crew\nresult = crew.kickoff()\nreturn result\nfrom openinference.instrumentation.crewai import CrewAIInstrumentor\nfrom openinference.instrumentation.openai import OpenAIInstrumentor\nfrom openinference.instrumentation.dspy import DSPyInstrumentor\n# Initialize multiple instrumentors\nCrewAIInstrumentor().instrument()\nOpenAIInstrumentor().instrument()\nDSPyInstrumentor().instrument()\nWas this page helpful?", "tokens": 589, "node_type": "child"}
{"id": 234, "chunk_id": "c16acf8a553ac24f5d77e7d3a1c6e493", "parent_id": 218, "url": "", "namespace": "langchain", "title": "langsmith-trace-with-google-adk", "headers": ["langsmith-trace-with-google-adk"], "section_index": 0, "chunk_index": 0, "text": "# langsmith-trace-with-google-adk\n\n> Source: https://docs.langchain.com/langsmith/trace-with-google-adk\n\nInstallation\nInstall the required packages using your preferred package manager:Requires LangSmith Python SDK version\nlangsmith>=0.4.26\nfor optimal OpenTelemetry support.Setup\n1. Configure environment variables\nSet your LangSmith API key and project name:2. Configure OpenTelemetry integration\nIn your Google ADK application, import and configure the LangSmith OpenTelemetry integration. This will automatically instrument Google ADK spans for OpenTelemetry.You do not need to set any OpenTelemetry environment variables or configure exporters manually\u2014\nconfigure()\nhandles everything automatically.3. Create and run your ADK agent\nOnce configured, your Google ADK application will automatically send traces to LangSmith: This example includes a minimal app that sets up an agent, session, and runner, then sends a message and streams events.View traces in LangSmith\n- Agent conversations: Complete conversation flows between users and your ADK agents.\n- Tool calls: Individual function calls made by your agents.\n- Model interactions: LLM requests and responses using Gemini models.\n- Session information: User and session context for organizing related traces.\n- Model interactions: LLM requests and responses using Gemini models", "tokens": 172, "node_type": "child"}
{"id": 235, "chunk_id": "c16acf8a553ac24f5d77e7d3a1c6e493", "parent_id": 219, "url": "", "namespace": "langchain", "title": "langsmith-trace-with-instructor", "headers": ["langsmith-trace-with-instructor"], "section_index": 0, "chunk_index": 0, "text": "# langsmith-trace-with-instructor\n\n> Source: https://docs.langchain.com/langsmith/trace-with-instructor\n\nLangSmith provides a convenient integration with Instructor, a popular open-source library for generating structured outputs with LLMs.In order to use, you first need to set your LangSmith API key.\nCopy\nexport LANGSMITH_API_KEY=<your-api-key># For LangSmith API keys linked to multiple workspaces, set the LANGSMITH_WORKSPACE_ID environment variable to specify which workspace to use.export LANGSMITH_WORKSPACE_ID=<your-workspace-id>\nNext, you will need to install the LangSmith SDK:\nCopy\npip install -U langsmith\nWrap your OpenAI client with langsmith.wrappers.wrap_openai\nCopy\nfrom openai import OpenAIfrom langsmith import wrappersclient = wrappers.wrap_openai(OpenAI())\nAfter this, you can patch the wrapped OpenAI client using instructor:\nNow, you can use instructor as you normally would, but now everything is logged to LangSmith!\nCopy\nfrom pydantic import BaseModelclass UserDetail(BaseModel): name: str age: intuser = client.chat.completions.create( model=\"gpt-4o-mini\", response_model=UserDetail, messages=[ {\"role\": \"user\", \"content\": \"Extract Jason is 25 years old\"}, ])\nOftentimes, you use instructor inside of other functions.\nYou can get nested traces by using this wrapped client and decorating those functions with @traceable.\nPlease see this guide for more information on how to annotate your code for tracing with the @traceable decorator.\nCopy\n# You can customize the run name with the `name` keyword argument@traceable(name=\"Extract User Details\")def my_function(text: str) -> UserDetail: return client.chat.completions.create( model=\"gpt-4o-mini\", response_model=UserDetail, messages=[ {\"role\": \"user\", \"content\": f\"Extract {text}\"}, ] )my_function(\"Jason is 25 years old\")", "tokens": 217, "node_type": "child"}
{"id": 236, "chunk_id": "c16acf8a553ac24f5d77e7d3a1c6e493", "parent_id": 220, "url": "", "namespace": "langchain", "title": "langsmith-trace-with-langchain", "headers": ["langsmith-trace-with-langchain"], "section_index": 0, "chunk_index": 0, "text": "# langsmith-trace-with-langchain > Source: https://docs.langchain.com/langsmith/trace-with-langchain Installation Install the core library and the OpenAI integration for Python and JS (we use the OpenAI integration for the code snippets below). For a full list of packages available, see the LangChain Python docs and LangChain JS docs.Quick start 1. Configure your environment If you are using LangChain.js with LangSmith and are not in a serverless environment, we also recommend setting the following explicitly to reduce latency: export LANGCHAIN_CALLBACKS_BACKGROUND=true If you are in a serverless environment, we recommend setting the reverse to allow tracing to finish before your function ends:export LANGCHAIN_CALLBACKS_BACKGROUND=false See this LangChain.js guide for more information.2. Log a trace No extra code is needed to log a trace to LangSmith. Just run your LangChain code as you normally would.3. View your trace By default, the trace will be logged to the project with the namedefault . An example of a trace logged using the above code is made public and can be viewed here. Trace selectively The previous section showed how to trace all invocations of a LangChain runnables within your applications by setting a single environment variable. While this is a convenient way to get started, you may want to trace only specific invocations or parts of your application. There are two ways to do this in Python: by manually passing in aLangChainTracer (reference docs) instance as a callback, or by using the tracing_context context manager (reference docs). In JS/TS, you can pass a LangChainTracer (reference docs) instance as a callback. Log to a specific project Statically As mentioned in the tracing conceptual guide LangSmith uses the concept of a Project to group traces. If left unspecified, the tracer project is set to default. You can set theLANGSMITH_PROJECT environment variable to configure a custom project name for an entire application run. This should be done before executing your application. The LANGSMITH_PROJECT flag is only supported in JS SDK versions >= 0.2.16, use LANGCHAIN_PROJECT instead if you are using an older version.Dynamically This largely builds off of the previous section and allows you to set the project name for a specificLangChainTracer instance or as parameters to the tracing_context context manager in Python. Add metadata and tags to traces You can annotate your traces with arbitrary metadata and tags by providing them in theRunnableConfig . This is useful for associating additional information with a trace, such as the environment in which it was executed, or the user who initiated it. For information on how to query traces and runs by metadata and tags, see this guide Customize run name You can customize the name of a given run when invoking or streaming your LangChain code by providing it in the Config. This name is used to identify the run in LangSmith and can be used to filter and group runs. The name is also used as the title of the run in the LangSmith UI. This can be done by setting arun_name in the RunnableConfig object at construction or by passing a run_name in the invocation parameters in JS/TS. The run_name parameter only changes the name of the runnable you invoke (e.g., a chain, function). It does not rename the nested run automatically created when you invoke an LLM object like ChatOpenAI (gpt-4o-mini ). In the example, the enclosing run will appear in LangSmith as MyCustomChain , while the nested LLM run still shows the model\u2019s default name.To give the LLM run a more meaningful name, you can either:- Wrap the model in another runnable and assign a run_name to that step. - Use a tracing decorator or helper (e.g., @traceable in Python, ortraceable fromlangsmith in JS/TS) to create a custom run around the model call. Customize run ID You can customize the ID of a given run when invoking or streaming your LangChain code by providing it in the Config. This ID is used to uniquely identify the run in LangSmith and can be used to query specific runs. The ID can be useful for linking runs across different systems or for implementing custom tracking logic. This can be done by setting arun_id in the RunnableConfig object at construction or by passing a run_id in the invocation parameters. This feature is not currently supported directly for LLM objects. trace_id ). Access run (span) ID for LangChain invocations When you invoke a LangChain object, you can manually specify the run ID of the invocation. This run ID can be used to query the run in LangSmith. In JS/TS, you can use aRunCollectorCallbackHandler instance to access the run ID. Ensure all traces are submitted before exiting In LangChain Python, LangSmith\u2019s tracing is done in a background thread to avoid obstructing your production application. This means that your process may end before all traces are successfully posted to LangSmith. This is especially prevalent in a serverless environment, where your VM may be terminated immediately once your chain or agent completes. You can make callbacks synchronous by setting theLANGCHAIN_CALLBACKS_BACKGROUND environment variable to \"false\" . For both languages, LangChain exposes methods to wait for traces to be submitted before exiting your application. Below is an example: Trace without setting environment variables As mentioned in other guides, the following environment variables allow you to configure tracing enabled, the api endpoint, the api key, and the tracing project:LANGSMITH_TRACING LANGSMITH_API_KEY LANGSMITH_ENDPOINT LANGSMITH_PROJECT Distributed tracing with LangChain (Python) LangSmith supports distributed tracing with LangChain Python. This allows you to link runs (spans) across different services and applications. The principles are similar to the distributed tracing guide for the LangSmith SDK.Interoperability between LangChain (Python) and LangSmith SDK If you are using LangChain for part of your application and the LangSmith SDK (see this guide) for other parts, you can still trace the entire application seamlessly. LangChain objects will be traced when invoked within atraceable function and be bound as a child run of the traceable function. Interoperability between LangChain.JS and LangSmith SDK Tracing LangChain objects inside traceable (JS only) Starting with langchain@0.2.x , LangChain objects are traced automatically when used inside @traceable", "tokens": 1000, "node_type": "child"}
{"id": 237, "chunk_id": "9b0fd3b850698317bef57fa6e71a38ba", "parent_id": 220, "url": "", "namespace": "langchain", "title": "langsmith-trace-with-langchain", "headers": ["langsmith-trace-with-langchain"], "section_index": 0, "chunk_index": 1, "text": "without setting environment variables As mentioned in other guides, the following environment variables allow you to configure tracing enabled, the api endpoint, the api key, and the tracing project:LANGSMITH_TRACING LANGSMITH_API_KEY LANGSMITH_ENDPOINT LANGSMITH_PROJECT Distributed tracing with LangChain (Python) LangSmith supports distributed tracing with LangChain Python. This allows you to link runs (spans) across different services and applications. The principles are similar to the distributed tracing guide for the LangSmith SDK.Interoperability between LangChain (Python) and LangSmith SDK If you are using LangChain for part of your application and the LangSmith SDK (see this guide) for other parts, you can still trace the entire application seamlessly. LangChain objects will be traced when invoked within atraceable function and be bound as a child run of the traceable function. Interoperability between LangChain.JS and LangSmith SDK Tracing LangChain objects inside traceable (JS only) Starting with langchain@0.2.x , LangChain objects are traced automatically when used inside @traceable functions, inheriting the client, tags, metadata and project name of the traceable function. For older versions of LangChain below 0.2.x , you will need to manually pass an instance LangChainTracer created from the tracing context found in @traceable . Tracing LangChain child runs via traceable / RunTree API (JS only) We\u2019re working on improving the interoperability between traceable and LangChain. The following limitations are present when using combining LangChain with traceable :- Mutating RunTree obtained from getCurrentRunTree() of the RunnableLambda context will result in a no-op. - It\u2019s discouraged to traverse the RunTree obtained from RunnableLambda via getCurrentRunTree() as it may not contain all the RunTree nodes. - Different child runs may have the same execution_order andchild_execution_order value. Thus in extreme circumstances, some runs may end up in a different order, depending on thestart_time . traceable functions as part of the RunnableSequence or trace child runs of LangChain run imperatively via the RunTree API. Starting with LangSmith 0.1.39 and @langchain/core 0.2.18, you can directly invoke traceable -wrapped functions within RunnableLambda. RunnableConfig to a equivalent RunTree object by using RunTree.fromRunnableConfig or pass the RunnableConfig as the first argument of traceable -wrapped function.", "tokens": 341, "node_type": "child"}
{"id": 238, "chunk_id": "c16acf8a553ac24f5d77e7d3a1c6e493", "parent_id": 221, "url": "", "namespace": "langchain", "title": "langsmith-trace-with-langgraph", "headers": ["langsmith-trace-with-langgraph"], "section_index": 0, "chunk_index": 0, "text": "# langsmith-trace-with-langgraph\n\n> Source: https://docs.langchain.com/langsmith/trace-with-langgraph\n\nWith LangChain\nIf you are using LangChain modules within LangGraph, you only need to set a few environment variables to enable tracing. This guide will walk through a basic example. For more detailed information on configuration, see the Trace With LangChain guide.1. Installation\nInstall the LangGraph library and the OpenAI integration for Python and JS (we use the OpenAI integration for the code snippets below). For a full list of packages available, see the LangChain Python docs and LangChain JS docs.2. Configure your environment\nIf you are using LangChain.js with LangSmith and are not in a serverless environment, we also recommend setting the following explicitly to reduce latency:\nexport LANGCHAIN_CALLBACKS_BACKGROUND=true\nIf you are in a serverless environment, we recommend setting the reverse to allow tracing to finish before your function ends:export LANGCHAIN_CALLBACKS_BACKGROUND=false\nSee this LangChain.js guide for more information.3. Log a trace\nOnce you\u2019ve set up your environment, you can call LangChain runnables as normal. LangSmith will infer the proper tracing config:Without LangChain\nIf you are using other SDKs or custom functions within LangGraph, you will need to wrap or decorate them appropriately (with the@traceable\ndecorator in Python or the traceable\nfunction in JS, or something like e.g. wrap_openai\nfor SDKs). If you do so, LangSmith will automatically nest traces from those wrapped methods.\nHere\u2019s an example. You can also see this page for more information.\n1. Installation\nInstall the LangGraph library and the OpenAI SDK for Python and JS (we use the OpenAI integration for the code snippets below).2. Configure your environment\nIf you are using LangChain.js with LangSmith and are not in a serverless environment, we also recommend setting the following explicitly to reduce latency:\nexport LANGCHAIN_CALLBACKS_BACKGROUND=true\nIf you are in a serverless environment, we recommend setting the reverse to allow tracing to finish before your function ends:export LANGCHAIN_CALLBACKS_BACKGROUND=false\nSee this LangChain.js guide for more information.", "tokens": 313, "node_type": "child"}
{"id": 239, "chunk_id": "c16acf8a553ac24f5d77e7d3a1c6e493", "parent_id": 223, "url": "", "namespace": "langchain", "title": "langsmith-trace-with-opentelemetry", "headers": ["langsmith-trace-with-opentelemetry"], "section_index": 0, "chunk_index": 0, "text": "# langsmith-trace-with-opentelemetry > Source: https://docs.langchain.com/langsmith/trace-with-opentelemetry LangSmith supports OpenTelemetry-based tracing, allowing you to send traces from any OpenTelemetry-compatible application. This guide covers both automatic instrumentation for LangChain applications and manual instrumentation for other frameworks. Learn how to trace your LLM applications using OpenTelemetry with LangSmith. Update the LangSmith URL appropriately for self-hosted installations or organizations in the EU region in the requests below. For the EU region, use eu.api.smith.langchain.com . Trace a LangChain application If you\u2019re using LangChain or LangGraph, use the built-in integration to trace your application: - Install the LangSmith package with OpenTelemetry support: Requires Python SDK version langsmith>=0.3.18 . We recommend langsmith>=0.4.25 to benefit from important OpenTelemetry fixes. - In your LangChain/LangGraph App, enable the OpenTelemetry integration by setting the LANGSMITH_OTEL_ENABLED environment variable: - Create a LangChain application with tracing. For example: - View the traces in your LangSmith dashboard (example) once your application runs. Trace a non-LangChain application For non-LangChain applications or custom instrumentation, you can trace your application in LangSmith with a standard OpenTelemetry client. (We recommend langsmith \u2265 0.4.25.) - Install the OpenTelemetry SDK, OpenTelemetry exporter packages, as well as the OpenAI package: - Setup environment variables for the endpoint, substitute your specific values: Depending on how your otel exporter is configured, you may need to append /v1/traces to the endpoint if you are only sending traces. If you\u2019re self-hosting LangSmith, replace the base endpoint with your LangSmith api endpoint and append /api/v1 . For example: OTEL_EXPORTER_OTLP_ENDPOINT=https://ai-company.com/api/v1/otel Optional: Specify a custom project name other than \u201cdefault\u201d: - Log a trace. This code sets up an OTEL tracer and exporter that will send traces to LangSmith. It then calls OpenAI and sends the required OpenTelemetry attributes. - View the trace in your LangSmith dashboard (example). Send traces to an alternate provider While LangSmith is the default destination for OpenTelemetry traces, you can also configure OpenTelemetry to send traces to other observability platforms. Available in LangSmith Python SDK \u2265 0.4.1. We recommend \u2265 0.4.25 for fixes that improve OTEL export and hybrid fan-out stability. Use environment variables for global configuration By default, the LangSmith OpenTelemetry exporter will send data to the LangSmith API OTEL endpoint, but this can be customized by setting standard OTEL environment variables: LangSmith uses the HTTP trace exporter by default. If you\u2019d like to use your own tracing provider, you can either: - Set the OTEL environment variables as shown above, or - Set a global trace provider before initializing LangChain components, which LangSmith will detect and use instead of creating its own. To send traces to a different provider, configure the OTLP exporter with your provider\u2019s endpoint: Hybrid tracing is available in version \u2265 0.4.1. To send traces only to your OTEL endpoint, set:LANGSMITH_OTEL_ONLY=\"true\" (Recommendation: use langsmith \u2265 0.4.25.) Supported OpenTelemetry attribute and event mapping When sending traces to LangSmith via OpenTelemetry, the following attributes are mapped to LangSmith fields: Core LangSmith attributes | OpenTelemetry attribute | LangSmith field | Notes | |---| langsmith.trace.name | Run name | Overrides the span name for the run | langsmith.span.kind | Run type | Values: llm , chain , tool , retriever , embedding , prompt , parser | langsmith.trace.session_id | Session ID | Session identifier for related traces | langsmith.trace.session_name | Session name | Name of the session | langsmith.span.tags | Tags | Custom tags attached to the span (comma-separated) | langsmith.metadata.{key} | metadata.{key} | Custom metadata with langsmith prefix | GenAI standard attributes | OpenTelemetry attribute | LangSmith field | Notes | |---| gen_ai.system | metadata.ls_provider | The GenAI system (e.g., \u201copenai\u201d, \u201canthropic\u201d) | gen_ai.operation.name | Run type | Maps \u201cchat\u201d/\u201ccompletion\u201d to \u201cllm\u201d, \u201cembedding\u201d to \u201cembedding\u201d | gen_ai.prompt | inputs | The input prompt sent to the model | gen_ai.completion | outputs | The output generated by the model | gen_ai.prompt.{n}.role | inputs.messages[n].role | Role for the nth input message | gen_ai.prompt.{n}.content | inputs.messages[n].content | Content for the nth input message | gen_ai.prompt.{n}.message.role | inputs.messages[n].role | Alternative format for role | gen_ai.prompt.{n}.message.content | inputs.messages[n].content | Alternative format for content | gen_ai.completion.{n}.role | outputs.messages[n].role | Role for the nth output message | gen_ai.completion.{n}.content | outputs.messages[n].content | Content for the nth output message | gen_ai.completion.{n}.message.role | outputs.messages[n].role | Alternative format for role | gen_ai.completion.{n}.message.content | outputs.messages[n].content | Alternative format for content | gen_ai.input.messages | inputs.messages | Array of input messages | gen_ai.output.messages | outputs.messages | Array of output messages | gen_ai.tool.name | invocation_params.tool_name | Tool name, also sets run type to \u201ctool\u201d | GenAI request parameters | OpenTelemetry attribute | LangSmith field | Notes | |---| gen_ai.request.model | invocation_params.model | The model name used for the request | gen_ai.response.model | invocation_params.model | The model name returned in the response | gen_ai.request.temperature | invocation_params.temperature | Temperature setting | gen_ai.request.top_p | invocation_params.top_p | Top-p sampling setting | gen_ai.request.max_tokens | invocation_params.max_tokens | Maximum tokens setting | gen_ai.request.frequency_penalty | invocation_params.frequency_penalty | Frequency penalty setting | gen_ai.request.presence_penalty | invocation_params.presence_penalty | Presence penalty setting | gen_ai.request.seed | invocation_params.seed | Random seed used for generation | gen_ai.request.stop_sequences | invocation_params.stop | Sequences that stop generation | gen_ai.request.top_k | invocation_params.top_k | Top-k sampling parameter | gen_ai.request.encoding_formats | invocation_params.encoding_formats | Output encoding formats | GenAI usage metrics | OpenTelemetry attribute | LangSmith field | Notes | |---| gen_ai.usage.input_tokens | usage_metadata.input_tokens | Number of input tokens used | gen_ai.usage.output_tokens | usage_metadata.output_tokens | Number of output tokens used | gen_ai.usage.total_tokens | usage_metadata.total_tokens | Total number of tokens used | gen_ai.usage.prompt_tokens | usage_metadata.input_tokens | Number of input tokens used (deprecated) | gen_ai.usage.completion_tokens | usage_metadata.output_tokens | Number of output tokens used (deprecated) | gen_ai.usage.details.reasoning_tokens | usage_metadata.reasoning_tokens | Number of reasoning tokens used | TraceLoop attributes | OpenTelemetry attribute | LangSmith field | Notes | |---| traceloop.entity.input | inputs | Full input value from TraceLoop | traceloop.entity.output | outputs | Full output value from TraceLoop | traceloop.entity.name | Run name | Entity name from TraceLoop | traceloop.span.kind | Run type | Maps to LangSmith run types | traceloop.llm.request.type | Run type | \u201dembedding\u201d maps to \u201cembedding\u201d, others to \u201cllm\u201d | traceloop.association.properties.{key} | metadata.{key} | Custom metadata with traceloop prefix |", "tokens": 1000, "node_type": "child"}
{"id": 240, "chunk_id": "9b0fd3b850698317bef57fa6e71a38ba", "parent_id": 223, "url": "", "namespace": "langchain", "title": "langsmith-trace-with-opentelemetry", "headers": ["langsmith-trace-with-opentelemetry"], "section_index": 0, "chunk_index": 1, "text": "usage metrics | OpenTelemetry attribute | LangSmith field | Notes | |---| gen_ai.usage.input_tokens | usage_metadata.input_tokens | Number of input tokens used | gen_ai.usage.output_tokens | usage_metadata.output_tokens | Number of output tokens used | gen_ai.usage.total_tokens | usage_metadata.total_tokens | Total number of tokens used | gen_ai.usage.prompt_tokens | usage_metadata.input_tokens | Number of input tokens used (deprecated) | gen_ai.usage.completion_tokens | usage_metadata.output_tokens | Number of output tokens used (deprecated) | gen_ai.usage.details.reasoning_tokens | usage_metadata.reasoning_tokens | Number of reasoning tokens used | TraceLoop attributes | OpenTelemetry attribute | LangSmith field | Notes | |---| traceloop.entity.input | inputs | Full input value from TraceLoop | traceloop.entity.output | outputs | Full output value from TraceLoop | traceloop.entity.name | Run name | Entity name from TraceLoop | traceloop.span.kind | Run type | Maps to LangSmith run types | traceloop.llm.request.type | Run type | \u201dembedding\u201d maps to \u201cembedding\u201d, others to \u201cllm\u201d | traceloop.association.properties.{key} | metadata.{key} | Custom metadata with traceloop prefix | OpenInference attributes | OpenTelemetry attribute | LangSmith field | Notes | |---| input.value | inputs | Full input value, can be string or JSON | output.value | outputs | Full output value, can be string or JSON | openinference.span.kind | Run type | Maps various kinds to LangSmith run types | llm.system | metadata.ls_provider | LLM system provider | llm.model_name | metadata.ls_model_name | Model name from OpenInference | tool.name | Run name | Tool name when span kind is \u201cTOOL\u201d | metadata | metadata.* | JSON string of metadata to be merged | LLM attributes | OpenTelemetry attribute | LangSmith field | Notes | |---| llm.input_messages | inputs.messages | Input messages | llm.output_messages | outputs.messages | Output messages | llm.token_count.prompt | usage_metadata.input_tokens | Prompt token count | llm.token_count.completion | usage_metadata.output_tokens | Completion token count | llm.token_count.total | usage_metadata.total_tokens | Total token count | llm.usage.total_tokens | usage_metadata.total_tokens | Alternative total token count | llm.invocation_parameters | invocation_params.* | JSON string of invocation parameters | llm.presence_penalty | invocation_params.presence_penalty | Presence penalty | llm.frequency_penalty | invocation_params.frequency_penalty | Frequency penalty | llm.request.functions | invocation_params.functions | Function definitions | Prompt template attributes | OpenTelemetry attribute | LangSmith field | Notes | |---| llm.prompt_template.variables | Run type | Sets run type to \u201cprompt\u201d, used with input.value | Retriever attributes | OpenTelemetry attribute | LangSmith field | Notes | |---| retrieval.documents.{n}.document.content | outputs.documents[n].page_content | Content of the nth retrieved document | retrieval.documents.{n}.document.metadata | outputs.documents[n].metadata | Metadata of the nth retrieved document (JSON) | | OpenTelemetry attribute | LangSmith field | Notes | |---| tools | invocation_params.tools | Array of tool definitions | tool_arguments | invocation_params.tool_arguments | Tool arguments as JSON or key-value pairs | Logfire attributes | OpenTelemetry attribute | LangSmith field | Notes | |---| prompt | inputs | Logfire prompt input | all_messages_events | outputs | Logfire message events output | events | inputs /outputs | Logfire events array, splits input/choice events | OpenTelemetry event mapping | Event name | LangSmith field | Notes | |---| gen_ai.content.prompt | inputs | Extracts prompt content from event attributes | gen_ai.content.completion | outputs | Extracts completion content from event attributes | gen_ai.system.message | inputs.messages[] | System message in conversation | gen_ai.user.message | inputs.messages[] | User message in conversation | gen_ai.assistant.message | outputs.messages[] | Assistant message in conversation | gen_ai.tool.message | outputs.messages[] | Tool response message | gen_ai.choice | outputs | Model choice/response with finish reason | exception | status , error | Sets status to \u201cerror\u201d and extracts exception message/stacktrace | For message events, the following attributes are extracted: content \u2192 message content role \u2192 message role id \u2192 tool_call_id (for tool messages) gen_ai.event.content \u2192 full message JSON For choice events: finish_reason \u2192 choice finish reason message.content \u2192 choice message content message.role \u2192 choice message role tool_calls.{n}.id \u2192 tool call ID tool_calls.{n}.function.name \u2192 tool function name tool_calls.{n}.function.arguments \u2192 tool function arguments tool_calls.{n}.type \u2192 tool call type For exception events: exception.message \u2192 error message exception.stacktrace \u2192 error stacktrace (appended to message) Implementation examples Trace using the LangSmith SDK Use the LangSmith SDK\u2019s OpenTelemetry helper to configure export: You do not need to set OTEL environment variables or exporters. configure() wires them for LangSmith automatically; instrumentors (like GoogleADKInstrumentor ) create the spans. - View the trace in your LangSmith dashboard (example). Advanced configuration Use OpenTelemetry Collector for fan-out For more advanced scenarios, you can use the OpenTelemetry Collector to fan out your telemetry data to multiple destinations. This is a more scalable approach than configuring multiple exporters in your application code. - Install the OpenTelemetry Collector for your environment. - Create a configuration file (e.g., otel-collector-config.yaml ) that exports to multiple destinations: - Configure your application to send to the collector: This approach offers several advantages: - Centralized configuration for all your telemetry destinations - Reduced overhead in your application code - Better scalability and resilience - Ability to add or remove destinations without changing application code Distributed tracing with LangChain and OpenTelemetry Distributed tracing is essential when your LLM application spans multiple services or processes. OpenTelemetry\u2019s context propagation capabilities ensure that traces remain connected across service boundaries. Context propagation in distributed tracing In distributed systems, context propagation passes trace metadata between services so that related spans are linked to the same trace: - Trace ID: A unique identifier for the entire trace - Span ID: A unique identifier for the current span - Sampling Decision: Indicates whether this trace should be sampled Set up distributed tracing with LangChain To enable distributed tracing across multiple services:", "tokens": 895, "node_type": "child"}
{"id": 241, "chunk_id": "c16acf8a553ac24f5d77e7d3a1c6e493", "parent_id": 225, "url": "", "namespace": "langchain", "title": "langsmith-trace-with-vercel-ai-sdk", "headers": ["langsmith-trace-with-vercel-ai-sdk"], "section_index": 0, "chunk_index": 0, "text": "# langsmith-trace-with-vercel-ai-sdk\n\n> Source: https://docs.langchain.com/langsmith/trace-with-vercel-ai-sdk\n\nInstallation\nInstall the Vercel AI SDK. This guide uses Vercel\u2019s OpenAI integration for the code snippets below, but you can use any of their other options as well.\nEnvironment configuration\nBasic setup\nImport and wrap AI SDK methods, then use them as you normally would:With traceable\nYou can wrap traceable\ncalls around AI SDK calls or within AI SDK tool calls. This is useful if you\nwant to group runs together in LangSmith:\nTracing in serverless environments\nWhen tracing in serverless environments, you must wait for all runs to flush before your environment shuts down. To do this, you can pass a LangSmithClient\ninstance when wrapping the AI SDK method,\nthen call await client.awaitPendingTraceBatches()\n.\nMake sure to also pass it into any traceable\nwrappers you create as well:\nNext.js\n, there is a convenient after\nhook\nwhere you can put this logic:\nPassing LangSmith config\nYou can pass LangSmith-specific config to your wrapper both when initially wrapping your AI SDK methods and while running them viaproviderOptions.langsmith\n.\nThis includes metadata (which you can later use to filter runs in LangSmith), top-level run name,\ntags, custom client instances, and more.\nConfig passed while wrapping will apply to all future calls you make with the wrapped method:\nproviderOptions.langsmith\nwill apply only to that run.\nWe suggest importing and wrapping your config in createLangSmithProviderOptions\nto ensure\nproper typing:\nRedacting data\nYou can customize what inputs and outputs the AI SDK sends to LangSmith by specifying custom input/output processing functions. This is useful if you are dealing with sensitive data that you would like to avoid sending to LangSmith. Because output formats vary depending on which AI SDK method you are using, we suggest defining and passing config individually into wrapped methods. You will also need to provide separate functions for child LLM runs within AI SDK calls, since callinggenerateText\nat top level calls the LLM internally and can do so multiple times.\nWe also suggest passing a generic parameter into createLangSmithProviderOptions\nto get proper types for inputs and outputs.\nHere\u2019s an example for generateText\n:\nexecute\nmethod in a traceable\nlike this:\ntraceable\nreturn type is complex, which makes the cast necessary. You may also omit the AI SDK tool\nwrapper function\nif you wish to avoid the cast.", "tokens": 382, "node_type": "child"}
{"id": 242, "chunk_id": "c16acf8a553ac24f5d77e7d3a1c6e493", "parent_id": 226, "url": "", "namespace": "langchain", "title": "langsmith-trace-without-env-vars", "headers": ["langsmith-trace-without-env-vars"], "section_index": 0, "chunk_index": 0, "text": "# langsmith-trace-without-env-vars\n\n> Source: https://docs.langchain.com/langsmith/trace-without-env-vars\n\nLANGSMITH_TRACING\nLANGSMITH_API_KEY\nLANGSMITH_ENDPOINT\nLANGSMITH_PROJECT\nDue to a number of asks for finer-grained control of tracing using the\ntrace\ncontext manager, we changed the behavior of with trace\nto honor the LANGSMITH_TRACING\nenvironment variable in version 0.1.95 of the Python SDK. You can find more details in the release notes. The recommended way to disable/enable tracing without setting environment variables is to use the with tracing_context\ncontext manager, as shown in the example below.- Python: The recommended way to do this in Python is to use the\ntracing_context\ncontext manager. This works for both code annotated withtraceable\nand code within thetrace\ncontext manager. - TypeScript: You can pass in both the client and the\ntracingEnabled\nflag to thetraceable\ndecorator.", "tokens": 123, "node_type": "child"}
{"id": 243, "chunk_id": "c16acf8a553ac24f5d77e7d3a1c6e493", "parent_id": 227, "url": "", "namespace": "langchain", "title": "langsmith-troubleshooting-studio", "headers": ["langsmith-troubleshooting-studio"], "section_index": 0, "chunk_index": 0, "text": "# langsmith-troubleshooting-studio\n\n> Source: https://docs.langchain.com/langsmith/troubleshooting-studio\n\nBrave blocks plain-HTTP traffic on localhost when Brave Shields are enabled. When running Studio with langgraph dev, you may see \u201cFailed to load assistants\u201d errors.\nUndefined conditional edges may show unexpected connections in your graph. This is\nbecause without proper definition, Studio assumes the conditional edge could access all other nodes. To address this, explicitly define the routing paths using one of these methods:\nDeployed application: If your application is deployed on LangSmith, you may need to create a new revision to enable this feature.\nLocal development server: If you are running your application locally, make sure you have upgraded to the latest version of the langgraph-cli (pip install -U langgraph-cli). Additionally, ensure you have tracing enabled by setting the LANGSMITH_API_KEY in your project\u2019s .env file.\nWhen you run an experiment, any attached evaluators are scheduled for execution in a queue. If you don\u2019t see results immediately, it likely means they are still pending.", "tokens": 159, "node_type": "child"}
{"id": 244, "chunk_id": "c16acf8a553ac24f5d77e7d3a1c6e493", "parent_id": 228, "url": "", "namespace": "langchain", "title": "langsmith-troubleshooting-variable-caching", "headers": ["langsmith-troubleshooting-variable-caching"], "section_index": 0, "chunk_index": 0, "text": "# langsmith-troubleshooting-variable-caching\n\n> Source: https://docs.langchain.com/langsmith/troubleshooting-variable-caching\n\nIf you\u2019re not seeing traces in your tracing project or notice traces logged to the wrong project/workspace, the issue might be due to LangSmith\u2019s default environment variable caching. This is especially common when running LangSmith within a Jupyter notebook. Follow these steps to diagnose and resolve the issue:\nReload your environment variables from the .env file by executing:\nCopy\nfrom dotenv import load_dotenvimport osload_dotenv(<path to .env file>, override=True)\nAfter reloading, your environment variables should be set correctly.If you continue to experience issues, please reach out to us via a shared Slack channel or email support (available for Plus and Enterprise plans), or in the LangChain Forum.", "tokens": 111, "node_type": "child"}
{"id": 245, "chunk_id": "c16acf8a553ac24f5d77e7d3a1c6e493", "parent_id": 229, "url": "", "namespace": "langchain", "title": "langsmith-troubleshooting", "headers": ["langsmith-troubleshooting"], "section_index": 0, "chunk_index": 0, "text": "# langsmith-troubleshooting\n\n> Source: https://docs.langchain.com/langsmith/troubleshooting\n\nThis guide will walk you through common issues you may encounter when running a self-hosted instance of LangSmith.While running LangSmith, you may encounter unexpected 500 errors, slow performance, or other issues. This guide will help you diagnose and resolve these issues.\nTo diagnose and resolve an issue, you will first need to retrieve some relevant information. Below, we explain how to do this for a kubernetes setup, a docker setup, as well as how to pull helpful browser info.Generally, the main services you will want to analyze are:\nlangsmith-backend: The main backend service.\nlangsmith-platform-backend: Another important backend service.\nThe first step in troubleshooting is to gather important debugging information about your LangSmith deployment. Service logs, kubernetes events, and resource utilization of containers can help identify the root cause of an issue.You can run our k8s troubleshooting script which will pull all of the relevant kubernetes information and output it to a folder for investigation. The script also compresses this folder into a zip file for sharing. Here is an example of how to run this script, assuming your langsmith deployment was brought up in a langsmith namespace:\nYou can then inspect the contents of the produced folder for any relevant errors or information. If you would like the LangSmith team to assist in debugging, please share this zip file with the team.\nIf you are experiencing an issue that surfaces as a browser error, it may also be helpful to inspect a HAR file which may include key information. To get the HAR file, you can follow this guide which explains the short process for various browsers.You can then use Google\u2019s HAR analyzer to investigate. You can also send your HAR file to the LangSmith team to help with debugging.\nIn Kubernetes, you will need to increase the size of the ClickHouse PVC. To achieve this, you can perform the following steps:\nGet the storage class of the PVC: kubectl get pvc data-langsmith-clickhouse-0 -n <namespace> -o jsonpath='{.spec.storageClassName}'\nEnsure the storage class has AllowVolumeExpansion: true: kubectl get sc <storage-class-name> -o jsonpath='{.allowVolumeExpansion}'\nIf it is false, some storage classes can be updated to allow volume expansion.\nTo update the storage class, you can run kubectl patch sc <storage-class-name> -p '{\"allowVolumeExpansion\": true}'\nIf this fails, you may need to create a new storage class with the correct settings.\nEdit your pvc to have the new size: kubectl edit pvc data-langsmith-clickhouse-0 -n <namespace> or kubectl patch pvc data-langsmith-clickhouse-0 '{\"spec\":{\"resources\":{\"requests\":{\"storage\":\"100Gi\"}}}}' -n <namespace>\nUpdate your helm chart langsmith_config.yaml to new size(e.g 100 Gi)\nerror: Dirty database version \u2018version\u2019. Fix and force version\nThis error occurs when the ClickHouse database is in an inconsistent state with our migrations. You will need to reset to an earlier database version and then rerun your upgrade/migrations.\nThis error occurs when the request size exceeds the maximum allowed size. You will need to increase the maximum request size in your Nginx configuration.\nDetails: code: 497, message: default: Not enough privileges. To execute this query, it\u2019s necessary to have the grant CREATE ROW POLICY ON default.feedbacks_rmt\nThis error occurs when your user does not have the necessary permissions to create row policies in Clickhouse. When deploying the Docker deployment, you need to copy the users.xml file from the github repo as well. This adds the <access_management> tag to the users.xml file, which allows the user to create row policies. Below is the default users.xml file that we expect to be used.\nIn some environments, your mount point may not be writable by the container. In these cases we suggest building a custom image with the users.xml file included.Example Dockerfile:\nCopy\nFROM clickhouse/clickhouse-server:24.8COPY ./users.xml /etc/clickhouse-server/users.d/users.xml\nThen take the following steps:\nBuild your custom image.\nCopy\ndocker build -t <image-name> .\nUpdate your docker-compose.yaml to use the custom image. Make sure to remove the users.xml mount point.\nClickHouse fails to start up when running a cluster with AquaSec\nIn some environments, AquaSec may prevent ClickHouse from starting up correctly. This may manifest as the ClickHouse pod not emitting any logs and failing to get marked as ready.\nGenerally this is due to LD_PRELOAD being set by AquaSec, which interferes with ClickHouse. To resolve this, you can add the following environment variable to your ClickHouse deployment:", "tokens": 706, "node_type": "child"}
{"id": 246, "chunk_id": "c16acf8a553ac24f5d77e7d3a1c6e493", "parent_id": 230, "url": "", "namespace": "langchain", "title": "langsmith-upload-existing-experiments", "headers": ["langsmith-upload-existing-experiments"], "section_index": 0, "chunk_index": 0, "text": "# langsmith-upload-existing-experiments > Source: https://docs.langchain.com/langsmith/upload-existing-experiments How to upload experiments run outside of LangSmith with the REST API Some users prefer to manage their datasets and run their experiments outside of LangSmith, but want to use the LangSmith UI to view the results. This is supported via our endpoint.This guide will show you how to upload evals using the REST API, using the requests library in Python as an example. However, the same principles apply to any language. Uploading an experiment requires specifying the relevant high-level information for your experiment and dataset, along with the individual data for your examples and runs within the experiment. Each object in the results represents a \u201crow\u201d in the experiment - a single dataset example, along with an associated run. Note that dataset_id and dataset_name refer to your dataset identifier in your external system and will be used to group external experiments together in a single dataset. They should not refer to an existing dataset in LangSmith (unless that dataset was created via this endpoint).You may use the following schema to upload experiments to the /datasets/upload-experiment endpoint: Copy { \"experiment_name\": \"string (required)\", \"experiment_description\": \"string (optional)\", \"experiment_start_time\": \"datetime (required)\", \"experiment_end_time\": \"datetime (required)\", \"dataset_id\": \"uuid (optional - an external dataset id, used to group experiments together)\", \"dataset_name\": \"string (optional - must provide either dataset_id or dataset_name)\", \"dataset_description\": \"string (optional)\", \"experiment_metadata\": { // Object (any shape - optional) \"key\": \"value\" }, \"summary_experiment_scores\": [ // List of summary feedback objects (optional) { \"key\": \"string (required)\", \"score\": \"number (optional)\", \"value\": \"string (optional)\", \"comment\": \"string (optional)\", \"feedback_source\": { // Object (optional) \"type\": \"string (required)\" }, \"feedback_config\": { // Object (optional) \"type\": \"string enum: continuous, categorical, or freeform\", \"min\": \"number (optional)\", \"max\": \"number (optional)\", \"categories\": [ // List of feedback category objects (optional) { \"value\": \"number (required)\", \"label\": \"string (optional)\" } ] }, \"created_at\": \"datetime (optional - defaults to now)\", \"modified_at\": \"datetime (optional - defaults to now)\", \"correction\": \"Object or string (optional)\" } ], \"results\": [ // List of experiment row objects (required) { \"row_id\": \"uuid (required)\", \"inputs\": { // Object (required - any shape). This will \"key\": \"val\" // be the input to both the run and the dataset example. }, \"expected_outputs\": { // Object (optional - any shape). \"key\": \"val\" // These will be the outputs of the dataset examples. }, \"actual_outputs\": { // Object (optional - any shape). \"key\": \"val\" // These will be the outputs of the runs. }, \"evaluation_scores\": [ // List of feedback objects for the run (optional) { \"key\": \"string (required)\", \"score\": \"number (optional)\", \"value\": \"string (optional)\", \"comment\": \"string (optional)\", \"feedback_source\": { // Object (optional) \"type\": \"string (required)\" }, \"feedback_config\": { // Object (optional) \"type\": \"string enum: continuous, categorical, or freeform\", \"min\": \"number (optional)\", \"max\": \"number (optional)\", \"categories\": [ // List of feedback category objects (optional) { \"value\": \"number (required)\", \"label\": \"string (optional)\" } ] }, \"created_at\": \"datetime (optional - defaults to now)\", \"modified_at\": \"datetime (optional - defaults to now)\", \"correction\": \"Object or string (optional)\" } ], \"start_time\": \"datetime (required)\", // The start/end times for the runs will be used to \"end_time\": \"datetime (required)\", // calculate latency. They must all fall between the \"run_name\": \"string (optional)\", // start and end times for the experiment. \"error\": \"string (optional)\", \"run_metadata\": { // Object (any shape - optional) \"key\": \"value\" } } ]} The response JSON will be a dict with keys experiment and dataset, each of which is an object that contains relevant information about the experiment and dataset that was created. You may upload multiple experiments to the same dataset by providing the same dataset_id or dataset_name between multiple calls. Your experiments will be grouped together under a single dataset, and you will be able to use the comparison view to compare results between experiments.Ensure that the start and end times of your individual rows are all between the start and end time of your experiment.You must provide either a dataset_id or a dataset_name. If you only provide an ID and the dataset does not yet exist, we will generate a name for you, and vice versa if you only provide a name.You may not upload experiments to a dataset that was not created via this endpoint. Uploading experiments is only supported for externally-managed datasets. Below is an example of a simple call to the /datasets/upload-experiment. This is a basic example that just uses the most important fields as an illustration. Copy import osimport requestsbody = { \"experiment_name\": \"My external experiment\", \"experiment_description\": \"An experiment uploaded to LangSmith\", \"dataset_name\": \"my-external-dataset\", \"summary_experiment_scores\": [ { \"key\": \"summary_accuracy\", \"score\": 0.9, \"comment\": \"Great job!\" } ], \"results\": [ { \"row_id\": \"<<uuid>>\", \"inputs\": { \"input\": \"Hello, what is the weather in San Francisco today?\" }, \"expected_outputs\": { \"output\": \"Sorry, I am unable to provide information about the current weather.\" }, \"actual_outputs\": { \"output\": \"The weather is partly cloudy with a high of 65.\" }, \"evaluation_scores\": [ { \"key\": \"hallucination\", \"score\": 1, \"comment\": \"The chatbot made up the weather instead of identifying that \" \"they don't have enough info to answer the question. This is \" \"a hallucination.\" } ], \"start_time\": \"2024-08-03T00:12:39\", \"end_time\": \"2024-08-03T00:12:41\", \"run_name\": \"Chatbot\" }, { \"row_id\": \"<<uuid>>\", \"inputs\": { \"input\": \"Hello, what is the square root of 49?\" }, \"expected_outputs\": { \"output\": \"The square root of 49 is 7.\" }, \"actual_outputs\": { \"output\": \"7.\" }, \"evaluation_scores\": [ { \"key\": \"hallucination\", \"score\": 0, \"comment\": \"The chatbot correctly identified the answer. This is not a \" \"hallucination.\" } ], \"start_time\": \"2024-08-03T00:12:40\", \"end_time\": \"2024-08-03T00:12:42\", \"run_name\": \"Chatbot\" } ], \"experiment_start_time\": \"2024-08-03T00:12:38\", \"experiment_end_time\": \"2024-08-03T00:12:43\"}resp = requests.post( \"https://api.smith.langchain.com/api/v1/datasets/upload-experiment\", # Update appropriately for self-hosted installations or the EU region json=body, headers={\"x-api-key\": os.environ[\"LANGSMITH_API_KEY\"]})print(resp.json()) Note that the latency and feedback stats in the experiment results are null because the runs haven\u2019t had a chance to be persisted yet, which may take a few seconds. If you save the experiment id and query again in a few seconds, you will see all the stats (although tokens/cost will still be null, because we don\u2019t ask for this information in the request body). Now, login to the UI and click on your", "tokens": 1000, "node_type": "child"}
{"id": 247, "chunk_id": "9b0fd3b850698317bef57fa6e71a38ba", "parent_id": 230, "url": "", "namespace": "langchain", "title": "langsmith-upload-existing-experiments", "headers": ["langsmith-upload-existing-experiments"], "section_index": 0, "chunk_index": 1, "text": "\"Hello, what is the square root of 49?\" }, \"expected_outputs\": { \"output\": \"The square root of 49 is 7.\" }, \"actual_outputs\": { \"output\": \"7.\" }, \"evaluation_scores\": [ { \"key\": \"hallucination\", \"score\": 0, \"comment\": \"The chatbot correctly identified the answer. This is not a \" \"hallucination.\" } ], \"start_time\": \"2024-08-03T00:12:40\", \"end_time\": \"2024-08-03T00:12:42\", \"run_name\": \"Chatbot\" } ], \"experiment_start_time\": \"2024-08-03T00:12:38\", \"experiment_end_time\": \"2024-08-03T00:12:43\"}resp = requests.post( \"https://api.smith.langchain.com/api/v1/datasets/upload-experiment\", # Update appropriately for self-hosted installations or the EU region json=body, headers={\"x-api-key\": os.environ[\"LANGSMITH_API_KEY\"]})print(resp.json()) Note that the latency and feedback stats in the experiment results are null because the runs haven\u2019t had a chance to be persisted yet, which may take a few seconds. If you save the experiment id and query again in a few seconds, you will see all the stats (although tokens/cost will still be null, because we don\u2019t ask for this information in the request body). Now, login to the UI and click on your newly-created dataset! You should see a single experiment: Your examples will have been uploaded: Clicking on your experiment will bring you to the comparison view: As you upload more experiments to your dataset, you will be able to compare the results and easily identify regressions in the comparison view.", "tokens": 199, "node_type": "child"}
{"id": 248, "chunk_id": "c16acf8a553ac24f5d77e7d3a1c6e493", "parent_id": 231, "url": "", "namespace": "langchain", "title": "langsmith-upload-files-with-traces", "headers": ["langsmith-upload-files-with-traces"], "section_index": 0, "chunk_index": 0, "text": "# langsmith-upload-files-with-traces\n\n> Source: https://docs.langchain.com/langsmith/upload-files-with-traces\n\nThe following features are available in the following SDK versions:\n- Python SDK: >=0.1.141\n- JS/TS SDK: >=0.2.5\nAttachment\ntype in Python and Uint8Array\n/ ArrayBuffer\nin TypeScript.\nPython\nIn the Python SDK, you can use theAttachment\ntype to add files to your traces. Each Attachment\nrequires:\nmime_type\n(str): The MIME type of the file (e.g.,\"image/png\"\n).data\n(bytes | Path): The binary content of the file, or the file path.\n(mime_type, data)\nfor convenience.\nSimply decorate a function with @traceable\nand include your Attachment\ninstances as arguments. Note that to use the file path instead of the raw bytes, you need to set the dangerously_allow_filesystem\nflag to True\nin your traceable decorator.\nPython\nTypeScript\nIn the TypeScript SDK, you can add attachments to traces by usingUint8Array\nor ArrayBuffer\nas data types. Each attachment\u2019s MIME type is specified within extractAttachments\n:\nUint8Array\n: Useful for handling binary data directly.ArrayBuffer\n: Represents fixed-length binary data, which can be converted toUint8Array\nas needed.\ntraceable\nand include your attachments within the extractAttachments\noption.\nIn the TypeScript SDK, the extractAttachments\nfunction is an optional parameter in the traceable\nconfiguration. When the traceable-wrapped function is invoked, it extracts binary data (e.g., images, audio files) from your inputs and logs them alongside other trace data, specifying their MIME types.\nNote that you cannot directly pass in a file path in the TypeScript SDK, as accessing local files is not supported in all runtime environments.\nTypeScript\nTypeScript", "tokens": 244, "node_type": "child"}
{"id": 249, "chunk_id": "c16acf8a553ac24f5d77e7d3a1c6e493", "parent_id": 232, "url": "", "namespace": "langchain", "title": "langsmith-use-remote-graph", "headers": ["langsmith-use-remote-graph"], "section_index": 0, "chunk_index": 0, "text": "# langsmith-use-remote-graph\n\n> Source: https://docs.langchain.com/langsmith/use-remote-graph\n\nRemoteGraph\nis a client-side interface that allows you to interact with your deployment as if it were a local graph. It provides API parity with CompiledGraph\n, which means that you can use the same methods (invoke()\n, stream()\n, get_state()\n, etc.) in your development and production environments. This page describes how to initialize a RemoteGraph\nand interact with it.\nRemoteGraph\nis useful for the following:\n- Separation of development and deployment: Build and test a graph locally with\nCompiledGraph\n, deploy it to LangSmith, and then useRemoteGraph\nto call it in production while working with the same API interface. - Thread-level persistence: Persist and fetch the state of a conversation across calls with a thread ID.\n- Subgraph embedding: Compose modular graphs for a multi-agent workflow by embedding a\nRemoteGraph\nas a subgraph within another graph. - Reusable workflows: Use deployed graphs as nodes or tools, so that you can reuse and expose complex logic.\nImportant: Avoid calling the same deployment\nRemoteGraph\nis designed to call graphs on other deployments. Do not use RemoteGraph\nto call itself or another graph on the same deployment, as this can lead to deadlocks and resource exhaustion. Instead, use local graph composition or subgraphs for graphs within the same deployment.Prerequisites\nBefore getting started withRemoteGraph\n, make sure you have:\n- Access to LangSmith, where your graphs are developed and managed.\n- A running LangGraph Server, which hosts your deployed graphs for remote interaction.\nInitialize the graph\nWhen initializing aRemoteGraph\n, you must always specify:\nname\n: The name of the graph you want to interact with or an assistant ID. If you specify a graph name, the default assistant will be used. If you specify an assistant ID, that specific assistant will be used. The graph name is the same name you use in thelanggraph.json\nconfiguration file for your deployment.api_key\n: A valid LangSmith API key. You can set as an environment variable (LANGSMITH_API_KEY\n) or pass directly in theapi_key\nargument. You can also provide the API key in theclient\n/sync_client\narguments, ifLangGraphClient\n/SyncLangGraphClient\nwas initialized with theapi_key\nargument.\nurl\n: The URL of the deployment you want to interact with. If you pass theurl\nargument, both sync and async clients will be created using the provided URL, headers (if provided), and default configuration values (e.g., timeout).client\n: ALangGraphClient\ninstance for interacting with the deployment asynchronously (e.g., using.astream()\n,.ainvoke()\n,.aget_state()\n,.aupdate_state()\n).sync_client\n: ASyncLangGraphClient\ninstance for interacting with the deployment synchronously (e.g., using.stream()\n,.invoke()\n,.get_state()\n,.update_state()\n).\nIf you pass both\nclient\nor sync_client\nas well as the url\nargument, they will take precedence over the url\nargument. If none of the client\n/ sync_client\n/ url\narguments are provided, RemoteGraph\nwill raise a ValueError\nat runtime.Use a URL\nUse a client\nInvoke the graph\nRemoteGraph\nimplements the same Runnable interface as CompiledGraph\n, so you can use it in the same way as a compiled graph. It supports the full set of standard methods, including .invoke()\n, .stream()\n, .get_state()\n, and .update_state()\n, as well as their asynchronous variants.\nAsynchronously\nTo use the graph asynchronously, you must provide either the\nurl\nor client\nwhen initializing the RemoteGraph\n.Synchronously\nTo use the graph synchronously, you must provide either the\nurl\nor sync_client\nwhen initializing the RemoteGraph\n.Persist state at the thread level\nBy default, graph runs (for example, calls made with.invoke()\nor .stream()\n) are stateless, which means that intermediate checkpoints and the final state are not persisted after a run.\nIf you want to preserve the outputs of a run\u2014for example, to support human-in-the-loop workflows\u2014you can create a thread and pass its ID through the config\nargument. This works the same way as with a regular compiled graph:\nUse as a subgraph\nIf you need to use a\ncheckpointer\nwith a graph that has a RemoteGraph\nsubgraph node, make sure to use UUIDs as thread IDs.RemoteGraph\ninstances as subgraph nodes. This allows for modular, scalable workflows where different responsibilities are split across separate graphs.\nRemoteGraph\nexposes the same interface as a regular CompiledGraph\n, so you can use it directly as a subgraph inside another graph. For example:", "tokens": 693, "node_type": "child"}
{"id": 250, "chunk_id": "c16acf8a553ac24f5d77e7d3a1c6e493", "parent_id": 233, "url": "", "namespace": "langchain", "title": "langsmith-use-stream-react", "headers": ["langsmith-use-stream-react"], "section_index": 0, "chunk_index": 0, "text": "# langsmith-use-stream-react > Source: https://docs.langchain.com/langsmith/use-stream-react The useStream() React hook provides a seamless way to integrate LangGraph into your React applications. It handles all the complexities of streaming, state management, and branching logic, letting you focus on building great chat experiences.Key features: Messages streaming: Handle a stream of message chunks to form a complete message Automatic state management for messages, interrupts, loading states, and errors Conversation branching: Create alternate conversation paths from any point in the chat history UI-agnostic design: bring your own components and styling Let\u2019s explore how to use useStream() in your React application.The useStream() provides a solid foundation for creating bespoke chat experiences. For pre-built chat components and interfaces, we also recommend checking out CopilotKit and assistant-ui. The useStream() hook takes care of all the complex state management behind the scenes, providing you with simple interfaces to build your UI. Here\u2019s what you get out of the box: Thread state management Loading and error states Interrupts Message handling and updates Branching support Here are some examples on how to use these features effectively: The useStream() hook can automatically resume an ongoing run upon mounting by setting reconnectOnMount: true. This is useful for continuing a stream after a page refresh, ensuring no messages and events generated during the downtime are lost. By default the ID of the created run is stored in window.sessionStorage, which can be swapped by passing a custom storage in reconnectOnMount instead. The storage is used to persist the in-flight run ID for a thread (under lg:stream:${threadId} key). You can also manually manage the resuming process by using the run callbacks to persist the run metadata and the joinStream function to resume the stream. Make sure to pass streamResumable: true when creating the run; otherwise some events might be lost. Copy import type { Message } from \"@langchain/langgraph-sdk\";import { useStream } from \"@langchain/langgraph-sdk/react\";import { useCallback, useState, useEffect, useRef } from \"react\";export default function App() { const [threadId, onThreadId] = useSearchParam(\"threadId\"); const thread = useStream<{ messages: Message[] }>({ apiUrl: \"http://localhost:2024\", assistantId: \"agent\", threadId, onThreadId, onCreated: (run) => { window.sessionStorage.setItem(`resume:${run.thread_id}`, run.run_id); }, onFinish: (_, run) => { window.sessionStorage.removeItem(`resume:${run?.thread_id}`); }, }); // Ensure that we only join the stream once per thread. const joinedThreadId = useRef<string | null>(null); useEffect(() => { if (!threadId) return; const resume = window.sessionStorage.getItem(`resume:${threadId}`); if (resume && joinedThreadId.current !== threadId) { thread.joinStream(resume); joinedThreadId.current = threadId; } }, [threadId]); return ( <form onSubmit={(e) => { e.preventDefault(); const form = e.target as HTMLFormElement; const message = new FormData(form).get(\"message\") as string; thread.submit( { messages: [{ type: \"human\", content: message }] }, { streamResumable: true } ); }} > <div> {thread.messages.map((message) => ( <div key={message.id}>{message.content as string}</div> ))} </div> <input type=\"text\" name=\"message\" /> <button type=\"submit\">Send</button> </form> );}// Utility method to retrieve and persist data in URL as search paramfunction useSearchParam(key: string) { const [value, setValue] = useState<string | null>(() => { const params = new URLSearchParams(window.location.search); return params.get(key) ?? null; }); const update = useCallback( (value: string | null) => { setValue(value); const url = new URL(window.location.href); if (value == null) { url.searchParams.delete(key); } else { url.searchParams.set(key, value); } window.history.pushState({}, \"\", url.toString()); }, [key] ); return [value, update] as const;} The useStream() hook will keep track of the message chunks received from the server and concatenate them together to form a complete message. The completed message chunks can be retrieved via the messages property.By default, the messagesKey is set to messages, where it will append the new messages chunks to values[\"messages\"]. If you store messages in a different key, you can change the value of messagesKey. Copy import type { Message } from \"@langchain/langgraph-sdk\";import { useStream } from \"@langchain/langgraph-sdk/react\";export default function HomePage() { const thread = useStream<{ messages: Message[] }>({ apiUrl: \"http://localhost:2024\", assistantId: \"agent\", messagesKey: \"messages\", }); return ( <div> {thread.messages.map((message) => ( <div key={message.id}>{message.content as string}</div> ))} </div> );} Under the hood, the useStream() hook will use the streamMode: \"messages-tuple\" to receive a stream of messages (i.e. individual LLM tokens) from any LangChain chat model invocations inside your graph nodes. Learn more about messages streaming in the streaming guide. For each message, you can use getMessagesMetadata() to get the first checkpoint from which the message has been first seen. You can then create a new run from the checkpoint preceding the first seen checkpoint to create a new branch in a thread.A branch can be created in following ways: Edit a previous user message. Request a regeneration of a previous assistant message. For advanced use cases you can use the experimental_branchTree property to get the tree representation of the thread, which can be used to render branching controls for non-message based graphs. You can optimistically update the client state before performing a network request to the agent, allowing you to provide immediate feedback to the user, such as showing the user message immediately before the agent has seen the request. Use the initialValues option to display cached thread data immediately while the history is being loaded from the server. This improves user experience by showing cached data instantly when navigating to existing threads. Copy import { useStream } from \"@langchain/langgraph-sdk/react\";const CachedThreadExample = ({ threadId, cachedThreadData }) => { const stream = useStream({ apiUrl: \"http://localhost:2024\", assistantId: \"agent\", threadId, // Show cached data immediately while history loads initialValues: cachedThreadData?.values, messagesKey: \"messages\", }); return ( <div> {stream.messages.map((message) => ( <div key={message.id}>{message.content as string}</div> ))} </div> );}; Use the threadId option in submit function to enable optimistic UI patterns where you need to know the thread ID before the thread is actually created. Copy import { useState } from \"react\";import { useStream } from \"@langchain/langgraph-sdk/react\";const OptimisticThreadExample = () => { const [threadId, setThreadId] = useState<string | null>(null); const [optimisticThreadId] = useState(() => crypto.randomUUID()); const stream = useStream({ apiUrl: \"http://localhost:2024\", assistantId: \"agent\", threadId, onThreadId: setThreadId, // (3) Updated after thread has been created. messagesKey: \"messages\", }); const handleSubmit = (text: string) => { // (1) Perform a soft navigation to /threads/${optimisticThreadId} // without waiting for thread creation. window.history.pushState({}, \"\", `/threads/${optimisticThreadId}`); // (2) Submit message to create thread with the predetermined ID. stream.submit( {", "tokens": 1000, "node_type": "child"}
{"id": 251, "chunk_id": "9b0fd3b850698317bef57fa6e71a38ba", "parent_id": 233, "url": "", "namespace": "langchain", "title": "langsmith-use-stream-react", "headers": ["langsmith-use-stream-react"], "section_index": 0, "chunk_index": 1, "text": "const stream = useStream({ apiUrl: \"http://localhost:2024\", assistantId: \"agent\", threadId, // Show cached data immediately while history loads initialValues: cachedThreadData?.values, messagesKey: \"messages\", }); return ( <div> {stream.messages.map((message) => ( <div key={message.id}>{message.content as string}</div> ))} </div> );}; Use the threadId option in submit function to enable optimistic UI patterns where you need to know the thread ID before the thread is actually created. Copy import { useState } from \"react\";import { useStream } from \"@langchain/langgraph-sdk/react\";const OptimisticThreadExample = () => { const [threadId, setThreadId] = useState<string | null>(null); const [optimisticThreadId] = useState(() => crypto.randomUUID()); const stream = useStream({ apiUrl: \"http://localhost:2024\", assistantId: \"agent\", threadId, onThreadId: setThreadId, // (3) Updated after thread has been created. messagesKey: \"messages\", }); const handleSubmit = (text: string) => { // (1) Perform a soft navigation to /threads/${optimisticThreadId} // without waiting for thread creation. window.history.pushState({}, \"\", `/threads/${optimisticThreadId}`); // (2) Submit message to create thread with the predetermined ID. stream.submit( { messages: [{ type: \"human\", content: text }] }, { threadId: optimisticThreadId } ); }; return ( <div> <p>Thread ID: {threadId ?? optimisticThreadId}</p> {/* Rest of component */} </div> );}; The useStream() hook is friendly for apps written in TypeScript and you can specify types for the state to get better type safety and IDE support. Copy // Define your typestype State = { messages: Message[]; context?: Record<string, unknown>;};// Use them with the hookconst thread = useStream<State>({ apiUrl: \"http://localhost:2024\", assistantId: \"agent\", messagesKey: \"messages\",}); You can also optionally specify types for different scenarios, such as: ConfigurableType: Type for the config.configurable property (default: Record<string, unknown>) InterruptType: Type for the interrupt value - i.e. contents of interrupt(...) function (default: unknown) CustomEventType: Type for the custom events (default: unknown) UpdateType: Type for the submit function (default: Partial<State>) If you\u2019re using LangGraph.js, you can also reuse your graph\u2019s annotation types. However, make sure to only import the types of the annotation schema in order to avoid importing the entire LangGraph.js runtime (i.e. via import type { ... } directive).", "tokens": 323, "node_type": "child"}
{"id": 252, "chunk_id": "c16acf8a553ac24f5d77e7d3a1c6e493", "parent_id": 234, "url": "", "namespace": "langchain", "title": "langsmith-use-studio", "headers": ["langsmith-use-studio"], "section_index": 0, "chunk_index": 0, "text": "# langsmith-use-studio\n\n> Source: https://docs.langchain.com/langsmith/use-studio\n\n- Run application: Execute your application or agent and observe its behavior.\n- Manage assistants: Create, edit, and select the assistant configuration used by your application.\n- Manage threads: View and organize the threads, including forking or editing past runs for debugging.\nRun application\n- Graph\n- Chat\nSpecify input\n- Define the input to your graph in the Input section on the left side of the page, below the graph interface. Studio will attempt to render a form for your input based on the graph\u2019s defined state schema. To disable this, click the View Raw button, which will present you with a JSON editor.\n- Click the up or down arrows at the top of the Input section to toggle through and use previously submitted inputs.\nRun settings\nAssistant\nTo specify the assistant that is used for the run:- Click the Settings button in the bottom left corner. If an assistant is currently selected the button will also list the assistant name. If no assistant is selected it will say Manage Assistants.\n- Select the assistant to run.\n- Click the Active toggle at the top of the modal to activate it.\nStreaming\nClick the dropdown next to Submit and click the toggle to enable or disable streaming.Breakpoints\nTo run your graph with breakpoints:- Click Interrupt.\n- Select a node and whether to pause before or after that node has executed.\n- Click Continue in the thread log to resume execution.\nSubmit run\nTo submit the run with the specified input and run settings:Manage assistants\nStudio lets you view, edit, and update your assistants, and allows you to run your graph using these assistant configurations. For more conceptual details, refer to the Assistants overview.- Graph\n- Chat\nTo view your assistants:\n- Click Manage Assistants in the bottom left corner. This opens a modal for you to view all the assistants for the selected graph.\n- Specify the assistant and its version you would like to mark as Active. LangSmith will use this assistant when runs are submitted.\nManage threads\nStudio provides tools to view all threads saved on the server and edit their state. You can create new threads, switch between threads, and modify past states both in graph mode and chat mode.- Graph\n- Chat\nView threads\n- In the top of the right-hand pane, select the dropdown menu to view existing threads.\n- Select the desired thread, and the thread history will populate in the right-hand side of the page.\n- To create a new thread, click + New Thread and submit a run.\n- To view more granular information in the thread, drag the slider at the top of the page to the right. To view less information, drag the slider to the left. Additionally, collapse or expand individual turns, nodes, and keys of the state.\n- Switch between\nPretty\nandJSON\nmode for different rendering formats.\nEdit thread history\nTo edit the state of the thread:- Select Edit node state next to the desired node.\n- Edit the node\u2019s output as desired and click Fork to confirm. This will create a new forked run from the checkpoint of the selected node.", "tokens": 531, "node_type": "child"}
{"id": 253, "chunk_id": "c16acf8a553ac24f5d77e7d3a1c6e493", "parent_id": 235, "url": "", "namespace": "langchain", "title": "langsmith-use-threads", "headers": ["langsmith-use-threads"], "section_index": 0, "chunk_index": 0, "text": "# langsmith-use-threads\n\n> Source: https://docs.langchain.com/langsmith/use-threads\n\nAlternatively, if you already have a thread in your application whose state you wish to copy, you can use the copy method. This will create an independent thread whose history is identical to the original thread at the time of the operation. See the Python and JS SDK reference docs for more information.\nFinally, you can create a thread with an arbitrary pre-defined state by providing a list of supersteps into the create method. The supersteps describe a list of a sequence of state updates. For example:\nTo list threads, use the LangGraph SDKsearch method. This will list the threads in the application that match the provided filters. See the Python and JS SDK reference docs for more information.\nUse the status field to filter threads based on their status. Supported values are idle, busy, interrupted, and error. See here for information on each status. For example, to view idle threads:\nYou can also view threads in a deployment via the LangSmith UI.Inside your deployment, select the \u201cThreads\u201d tab. This will load a table of all of the threads in your deployment.To filter by thread status, select a status in the top bar. To sort by a supported property, click on the arrow icon for the desired column.\nTo view a thread\u2019s history, use the get_history method. This returns a list of every state the thread experienced. For more information see the Python and JS reference docs.\nYou can also view threads in a deployment via the LangSmith UI.Inside your deployment, select the \u201cThreads\u201d tab. This will load a table of all of the threads in your deployment.Select a thread to inspect its current state. To view its full history and for further debugging, open the thread in Studio.", "tokens": 294, "node_type": "child"}
{"id": 254, "chunk_id": "c16acf8a553ac24f5d77e7d3a1c6e493", "parent_id": 236, "url": "", "namespace": "langchain", "title": "langsmith-use-tools", "headers": ["langsmith-use-tools"], "section_index": 0, "chunk_index": 0, "text": "# langsmith-use-tools\n\n> Source: https://docs.langchain.com/langsmith/use-tools\n\nTools allow language models to interact with external systems and perform actions beyond just generating text. In the LangSmith playground, you can use two types of tools:\nBuilt-in tools: Pre-configured tools provided by model providers (like OpenAI and Anthropic) that are ready to use. These include capabilities like web search, code interpretation, and more.\nCustom tools: Functions you define to perform specific tasks. These are useful when you need to integrate with your own systems or create specialized functionality. When you define custom tools within the LangSmith Playground, you can verify that the model correctly identifies and calls these tools with the correct arguments. Soon we plan to support executing these custom tool calls directly.\nThe LangSmith Playground has native support for a variety of tools from OpenAI and Anthropic. If you want to use a tool that isn\u2019t explicitly listed in the Playground, you can still add it by manually specifying its type and any required arguments.\nIn the tool section, select the built-in tool you want to use. You\u2019ll only see the tools that are compatible with the provider and model you\u2019ve chosen.\nWhen the model calls the tool, the playground will display the response\nDescription: Clear explanation of what the tool does\nArguments: The inputs your tool requires\nNote: When running a custom tool in the playground, the model will respond with a JSON object containing the tool name and the tool call. Currently, there\u2019s no way to connect this to a hosted tool via MCP.\nSome models provide control over which tools are called. To configure this:\nGo to prompt settings\nNavigate to tool settings\nSelect tool choice\nTo understand the available tool choice options, check the documentation for your specific provider. For example, OpenAI\u2019s documentation on tool choice.", "tokens": 298, "node_type": "child"}
{"id": 255, "chunk_id": "c16acf8a553ac24f5d77e7d3a1c6e493", "parent_id": 237, "url": "", "namespace": "langchain", "title": "langsmith-use-webhooks", "headers": ["langsmith-use-webhooks"], "section_index": 0, "chunk_index": 0, "text": "# langsmith-use-webhooks\n\n> Source: https://docs.langchain.com/langsmith/use-webhooks\n\nwebhook\nparameter. If this parameter is specified by an endpoint that can accept POST requests, LangSmith will send a request at the completion of a run.\nWhen working with LangSmith, you may want to use webhooks to receive updates after an API call completes. Webhooks are useful for triggering actions in your service once a run has finished processing. To implement this, you need to expose an endpoint that can accept POST\nrequests and pass this endpoint as a webhook\nparameter in your API request.\nCurrently, the SDK does not provide built-in support for defining webhook endpoints, but you can specify them manually using API requests.\nSupported endpoints\nThe following API endpoints accept awebhook\nparameter:\n| Operation | HTTP Method | Endpoint |\n|---|---|---|\n| Create Run | POST | /thread/{thread_id}/runs |\n| Create Thread Cron | POST | /thread/{thread_id}/runs/crons |\n| Stream Run | POST | /thread/{thread_id}/runs/stream |\n| Wait Run | POST | /thread/{thread_id}/runs/wait |\n| Create Cron | POST | /runs/crons |\n| Stream Run Stateless | POST | /runs/stream |\n| Wait Run Stateless | POST | /runs/wait |\nSet up your assistant and thread\nBefore making API calls, set up your assistant and thread.- Python\n- JavaScript\n- CURL\nUse a webhook with a graph run\nTo use a webhook, specify thewebhook\nparameter in your API request. When the run completes, LangSmith sends a POST\nrequest to the specified webhook URL.\nFor example, if your server listens for webhook events at https://my-server.app/my-webhook-endpoint\n, include this in your request:\n- Python\n- JavaScript\n- CURL\nWebhook payload\nLangSmith sends webhook notifications in the format of a Run. See the API Reference for details. The request payload includes run input, configuration, and other metadata in thekwargs\nfield.\nSecure webhooks\nTo ensure only authorized requests hit your webhook endpoint, consider adding a security token as a query parameter:Disable webhooks\nAs oflanggraph-api>=0.2.78\n, developers can disable webhooks in the langgraph.json\nfile:\nTest webhooks\nYou can test your webhook using online services like:- Beeceptor \u2013 Quickly create a test endpoint and inspect incoming webhook payloads.\n- Webhook.site \u2013 View, debug, and log incoming webhook requests in real time.", "tokens": 363, "node_type": "child"}
{"id": 256, "chunk_id": "c16acf8a553ac24f5d77e7d3a1c6e493", "parent_id": 238, "url": "", "namespace": "langchain", "title": "langsmith-user-management", "headers": ["langsmith-user-management"], "section_index": 0, "chunk_index": 0, "text": "# langsmith-user-management > Source: https://docs.langchain.com/langsmith/user-management - Set up access control: Configure role-based access control (RBAC) to manage user permissions within workspaces, including creating custom roles and assigning them to users. - SAML SSO (Enterprise plan): Set up Single Sign-On authentication for Enterprise customers using SAML 2.0, including configuration for popular identity providers. - SCIM User Provisioning (Enterprise plan): Automate user provisioning and deprovisioning between your identity provider and LangSmith using SCIM. Set up access control RBAC (Role-Based Access Control) is a feature that is only available to Enterprise customers. If you are interested in this feature, contact our sales team. Other plans default to using the Admin role for all users.workspace:manage permission can manage access control settings for a workspace. Create a role By default, LangSmith comes with a set of system roles:Admin : has full access to all resources within the workspace.Viewer : has read-only access to all resources within the workspace.Editor : has full permissions except for workspace management (adding/removing users, changing roles, configuring service keys). Organization Admins can create custom roles to suit your needs. To create a role, navigate to the Roles tab in the Members and roles section of the Organization settings page. Note that new roles that you create will be usable across all workspaces within your organization. Click on the Create Role button to create a new role. A Create role form will open. Assign permissions for the different LangSmith resources that you want to control access to. Assign a role to a user Once you have your roles set up, you can assign them to users. To assign a role to a user, navigate to theWorkspace members tab in the Workspaces section of the Organization settings page Each user will have a Role dropdown that you can use to assign a role to them. You can also invite new users with a given role. Set up SAML SSO for your organization Single Sign-On (SSO) functionality is available for Enterprise Cloud customers to access LangSmith through a single authentication source. This allows administrators to centrally manage team access and keeps information more secure. LangSmith\u2019s SSO configuration is built using the SAML (Security Assertion Markup Language) 2.0 standard. SAML 2.0 enables connecting an Identity Provider (IdP) to your organization for an easier, more secure login experience. SSO services permit a user to use one set of credentials (for example, a name or email address and password) to access multiple applications. The service authenticates the end user only once for all the applications the user has been given rights to and eliminates further prompts when the user switches applications during the same session. The benefits of SSO include:- Streamlines user management across systems for organization owners. - Enables organizations to enforce their own security policies (e.g., MFA). - Removes the need for end users to remember and manage multiple passwords. Simplifies the end-user experience, by allowing sign in at one single access point across multiple applications. Just-in-time (JIT) provisioning User invites are not supported in organizations with SAML SSO enabled. Initial workspace membership and role is determined by JIT provisioning, and changes afterwards can be managed in the UI. For additional flexibility in automated user management, LangSmith supports SCIM. Login methods and access Once you have completed your configuration of SAML SSO for your organization, users will be able to log in via SAML SSO in addition to other login methods, such as username/password or Google Authentication\u201d:- When logged in via SAML SSO, users can only access the corresponding organization with SAML SSO configured. - Users with SAML SSO as their only login method do not have personal organizations. - When logged in via any other method, users can access the organization with SAML SSO configured along with any other organizations they are a part of. Enforce SAML SSO only To ensure users can only access the organization when logged in using SAML SSO and no other method, check the Login via SSO only checkbox and click Save. Once this happens, users accessing the organization that are logged-in via a non-SSO login method are required to log back in using SAML SSO. This setting can be switched back to allow all login methods by unselecting the checkbox and clicking Save.You must be logged in via SAML SSO in order to update this setting to Only SAML SSO . This is to ensure the SAML settings are valid and avoid locking users out of your organization.Prerequisites - Your organization must be on an Enterprise plan. - Your Identity Provider (IdP) must support the SAML 2.0 standard. - Only Organization Admins can configure SAML SSO. Initial configuration - In your IdP: Configure a SAML application with the following details, then copy the metadata URL or XML for step 3. The following URLs are different for the US and EU regions. Ensure you select the correct link. - Single sign-on URL (or ACS URL): - Audience URI (or SP Entity ID): - Name ID format: email address. - Application username: email address. - Required claims: sub andemail . - In LangSmith: Go to Settings -> Members and roles -> SSO Configuration. Fill in the required information and submit to activate SSO login: - Fill in either the SAML metadata URL orSAML metadata XML . - Select the Default workspace role andDefault workspaces . New users logging in via SSO will be added to the specified workspaces with the selected role. - Fill in either the Default workspace role andDefault workspaces are editable. The updated settings will apply to new users only, not existing users.- (Coming soon) SAML metadata URL andSAML metadata XML are editable. This is usually only necessary when cryptographic keys are rotated/expired or the metadata URL has changed but the same IdP is still used. Entra ID (Azure) For additional information, see Microsoft\u2019s documentation. Step 1: Create a new Entra ID application integration- Log in to the Azure portal with a privileged role (e.g., Global Administrator ). On the left navigation pane, select theEntra ID service.", "tokens": 1000, "node_type": "child"}
{"id": 257, "chunk_id": "9b0fd3b850698317bef57fa6e71a38ba", "parent_id": 238, "url": "", "namespace": "langchain", "title": "langsmith-user-management", "headers": ["langsmith-user-management"], "section_index": 0, "chunk_index": 1, "text": "-> SSO Configuration. Fill in the required information and submit to activate SSO login: - Fill in either the SAML metadata URL orSAML metadata XML . - Select the Default workspace role andDefault workspaces . New users logging in via SSO will be added to the specified workspaces with the selected role. - Fill in either the Default workspace role andDefault workspaces are editable. The updated settings will apply to new users only, not existing users.- (Coming soon) SAML metadata URL andSAML metadata XML are editable. This is usually only necessary when cryptographic keys are rotated/expired or the metadata URL has changed but the same IdP is still used. Entra ID (Azure) For additional information, see Microsoft\u2019s documentation. Step 1: Create a new Entra ID application integration- Log in to the Azure portal with a privileged role (e.g., Global Administrator ). On the left navigation pane, select theEntra ID service. - Navigate to Enterprise Applications and then select All Applications. - Click Create your own application. - In the Create your own application window: - Enter a name for your application (e.g., LangSmith ). - Select *Integrate any other application you don\u2019t find in the gallery (Non-gallery)**. - Enter a name for your application (e.g., - Click Create. - Open the enterprise application that you created. - In the left-side navigation, select Manage > Single sign-on. - On the Single sign-on page, click SAML. - Update the Basic SAML Configuration: Identifier (Entity ID) :Reply URL (Assertion Consumer Service URL) :- Leave Relay State ,Logout Url , andSign on URL empty. - Click Save. - Ensure required claims are present with Namespace: http://schemas.xmlsoap.org/ws/2005/05/identity/claims :sub :user.objectid .emailaddress :user.userprincipalname oruser.mail (if using the latter, ensure all users have theEmail field filled in underContact Information ).- (Optional) For SCIM, see the setup documentation for specific instructions about Unique User Identifier (Name ID) . - On the SAML-based Sign-on page, under SAML Certificates, copy the App Federation Metadata URL. Fill in required information step, using the metadata URL from the previous step. Step 4: Verify the SSO setup - Assign the application to users/groups in Entra ID: - Select Manage > Users and groups. - Click Add user/group. - In the Add Assignment window: - Under Users, click None Selected. - Search for the user you want to assign to the enterprise application, and then click Select. - Verify that the user is selected, and click Assign. - Have the user sign in via the unique login URL from the SSO Configuration page, or go to Manage > Single sign-on and select Test single sign-on with (application name). - Make sure you\u2019re signed into an administrator account with the appropriate permissions. - In the Admin console, go to Menu -> Apps -> Web and mobile apps. - Click Add App and then Add custom SAML app. - Enter the app name and, optionally, upload an icon. Click Continue. - On the Google Identity Provider details page, download the IDP metadata and save it for Step 2. Click Continue. - In the Service Provider Details window, enter:ACS URL :Entity ID :- Leave Start URL and theSigned response box empty. - Set Name ID format toEMAIL and leaveName ID as the default (Basic Information > Primary email ). - Click Continue . - Use Add mapping to ensure required claims are present:Basic Information > Primary email ->email Fill in required information step, using the IDP metadata from the previous step as the metadata XML. Step 3: Turn on the SAML app in Google - Select the SAML app under Menu -> Apps -> Web and mobile apps - Click User access . - Turn on the service: - To turn the service on for everyone in your organization, click On for everyone , and then clickSave . - To turn the service on for an organizational unit: - At the left, select the organizational unit then On . - If the Service status is set to Inherited and you want to keep the updated setting, even if the parent setting changes, clickOverride . - If the Service status is set to Overridden , either clickInherit to revert to the same setting as its parent, or clickSave to keep the new setting, even if the parent setting changes. - At the left, select the organizational unit then - To turn on a service for a set of users across or within organizational units, select an access group. For details, go to Use groups to customize service access. - To turn the service on for everyone in your organization, click - Ensure that the email addresses your users use to sign in to LangSmith match the email addresses they use to sign in to your Google domain. Okta Supported features - IdP-initiated SSO (Single Sign-On) - SP-initiated SSO - Just-In-Time provisioning - Enforce SSO only Configuration steps For additional information, see Okta\u2019s documentation. Step 1: Create and configure the Okta SAML applicationVia Okta Integration Network (recommended) - Sign in to Okta. - In the upper-right corner, select Admin. The button is not visible from the Admin area. - Select Browse App Integration Catalog . - Find and select the LangSmith application. - On the application overview page, select Add Integration. - Leave ApiUrlBase empty. - Fill in AuthHost :- US: auth.langchain.com - EU: eu.auth.langchain.com - US: - (Optional, if planning to use SCIM as well) Fill in LangSmithUrl :- US: api.smith.langchain.com - EU: eu.api.smith.langchain.com - US: - Under Application Visibility, keep the box unchecked. - Select Next. - Select SAML 2.0 . - Fill in Sign-On Options :Application username format :Email Update application username on :Create and update Allow users to securely see their password : leave unchecked. - Copy the Metadata URL from the Sign On Options page to use in the next step. - Log in to Okta as an administrator, and go to the Okta Admin console. - Under Applications > Applications click Create App Integration. - Select SAML 2.0. - Enter an App name (e.g.,LangSmith", "tokens": 1000, "node_type": "child"}
{"id": 258, "chunk_id": "5345104ec252a41359fa4a3d2dfbec23", "parent_id": 238, "url": "", "namespace": "langchain", "title": "langsmith-user-management", "headers": ["langsmith-user-management"], "section_index": 0, "chunk_index": 2, "text": "Browse App Integration Catalog . - Find and select the LangSmith application. - On the application overview page, select Add Integration. - Leave ApiUrlBase empty. - Fill in AuthHost :- US: auth.langchain.com - EU: eu.auth.langchain.com - US: - (Optional, if planning to use SCIM as well) Fill in LangSmithUrl :- US: api.smith.langchain.com - EU: eu.api.smith.langchain.com - US: - Under Application Visibility, keep the box unchecked. - Select Next. - Select SAML 2.0 . - Fill in Sign-On Options :Application username format :Email Update application username on :Create and update Allow users to securely see their password : leave unchecked. - Copy the Metadata URL from the Sign On Options page to use in the next step. - Log in to Okta as an administrator, and go to the Okta Admin console. - Under Applications > Applications click Create App Integration. - Select SAML 2.0. - Enter an App name (e.g.,LangSmith ) and optionally an App logo, then click Next. - Enter the following information in the Configure SAML page: Single sign-on URL (ACS URL ). KeepUse this for Recipient URL and Destination URL checked:Audience URI (SP Entity ID) :Name ID format : Persistent.Application username :email .- Leave the rest of the fields empty or set to their default. - Click Next. - Click Finish. - Copy the Metadata URL from the Sign On page to use in the next step. - Under Applications > Applications, select the SAML application created in Step 1. - Under the Assignments tab, click Assign then either Assign to People or Assign to Groups. - Make the desired selection(s), then Assign and Done. SSO Configuration page, or have a user select the application from their Okta dashboard. SP-initiated SSO Once service-provider\u2013initiated SSO is configured, users can sign in using a unique login URL. You can find this in the LangSmith UI under Organization members and roles then SSO configuration.Set up SCIM for your organization System for Cross-domain Identity Management (SCIM) is an open standard that allows for the automation of user provisioning. Using SCIM, you can automatically provision and de-provision users in your LangSmith organization and workspaces, keeping user access synchronized with your organization\u2019s identity provider.SCIM is available for organizations on the Enterprise plan. Contact sales to learn more.SCIM is available on Helm chart versions 0.10.41 (application version 0.10.108) and later.SCIM support is API-only (see instructions below). - Automated user management: Users are automatically added, updated, and removed from LangSmith based on their status in your IdP. - Reduced administrative overhead: No need to manage user access manually across multiple systems. - Improved security: Users who leave your organization are automatically deprovisioned from LangSmith. - Consistent access control: User attributes and group memberships are synchronized between systems. - Scaling team access control: Efficiently manage large teams with many workspaces and custom roles. - Role assignment: Select specific Organization Roles and Workspace Roles for groups of users. Requirements Prerequisites - Your organization must be on an Enterprise plan. - Your Identity Provider (IdP) must support SCIM 2.0. - Only Organization Admins can configure SCIM. - For cloud customers: SAML SSO must be configurable for your organization. - For self-hosted customers: OAuth with Client Secret authentication mode must be enabled. - For self-hosted customers, network traffic must be allowed from the identity provider to LangSmith: Role Precedence When a user belongs to multiple groups for the same workspace, the following precedence applies:- Organization Admin groups take highest precedence. Users in these groups will be Admin in all workspaces. - Most recently created workspace-specific group takes precedence over other workspace groups. When a group is deleted or a user is removed from a group, their access is updated according to their remaining group membership, following the precedence rules.SCIM group membership will override manually assigned roles or roles assigned via Just-in-time (JIT) provisioning. We recommend disabling JIT provisioning to avoid conflicts. Email verification In cloud only, creating a new user with SCIM triggers an email to the user. They must verify their email address by clicking the link in this email. The link expires in 24 hours, and can be resent if needed by removing and re-adding the user via SCIM.Attributes and Mapping Group Naming Convention Group membership maps to LangSmith workspace membership and workspace roles with a specific naming convention: Organization Admin Groups Format:<optional_prefix>Organization Admin or <optional_prefix>Organization Admins Examples: LS:Organization Admins Groups-Organization Admins Organization Admin <optional_prefix><org_role_name>:<workspace_name>:<workspace_role_name> Examples: LS:Organization User:Production:Annotators Groups-Organization User:Engineering:Developers Organization User:Marketing:Viewers Mapping While specific instructions depending on the identity provider may vary, these mappings show what is supported by the LangSmith SCIM integration:User Attributes | LangSmith App Attribute | Identity Provider Attribute | Matching Precedence | |---|---|---| userName 1 | email address | | active | !deactivated | | emails[type eq \"work\"].value | email address2 | | name.formatted | displayName OR givenName + familyName 3 | | givenName | givenName | | familyName | familyName | | externalId | sub 4 | 1 | userName is not required by LangSmith- Email address is required - Use the computed expression if your displayName does not match the format ofFirstname Lastname - To avoid inconsistency, this should match the SAML NameID assertion for cloud customers, or thesub OAuth2.0 claim for self-hosted. Group Attributes | LangSmith App Attribute | Identity Provider Attribute | Matching Precedence | |---|---|---| displayName | displayName 1 | 1 | externalId | objectId | | members | members | - Groups must follow the naming convention described in the Group Naming Convention section. If your company has a group naming policy, you should instead map from the description identity provider attribute and set the description based on the Group Naming Convention section. Step 1 - Configure SAML SSO (Cloud only) There are two scenarios for SAML SSO configuration:- If SAML SSO is already configured for your organization, you should skip the steps to initially add the application (Add application from Okta Integration Network or Create a new Entra ID application integration), as you already have an application configured and just need to enable provisioning. -", "tokens": 1000, "node_type": "child"}
{"id": 259, "chunk_id": "0f7e5a6eec52bd2b622d96bc793bb7d8", "parent_id": 238, "url": "", "namespace": "langchain", "title": "langsmith-user-management", "headers": ["langsmith-user-management"], "section_index": 0, "chunk_index": 3, "text": "the SAML NameID assertion for cloud customers, or thesub OAuth2.0 claim for self-hosted. Group Attributes | LangSmith App Attribute | Identity Provider Attribute | Matching Precedence | |---|---|---| displayName | displayName 1 | 1 | externalId | objectId | | members | members | - Groups must follow the naming convention described in the Group Naming Convention section. If your company has a group naming policy, you should instead map from the description identity provider attribute and set the description based on the Group Naming Convention section. Step 1 - Configure SAML SSO (Cloud only) There are two scenarios for SAML SSO configuration:- If SAML SSO is already configured for your organization, you should skip the steps to initially add the application (Add application from Okta Integration Network or Create a new Entra ID application integration), as you already have an application configured and just need to enable provisioning. - If you are configuring SAML SSO for the first time alongside SCIM, first follow the instructions to set up SAML SSO, then follow the instructions here to enable SCIM. NameID Format LangSmith uses the SAML NameID to identify users. The NameID is a required field in the SAML response and is case-insensitive. The NameID must:- Be unique to each user. - Be a persistent value that never changes, such as a randomly generated unique user ID. - Match exactly on each sign-in attempt. It should not rely on user input. Persistent , unless you are using a field, like email, that requires a different format. Step 2 - Disable JIT provisioning Before enabling SCIM, disable Just-in-time (JIT) provisioning to prevent conflicts between automatic and manual user provisioning.Disabling JIT for Cloud Use thePATCH /orgs/current/info endpoint: Disabling JIT for Self-Hosted As of LangSmith chart version 0.11.14, you can disable JIT provisioning for your self-hosted organization using SSO. To disable, set the following values:Step 3 - Generate SCIM bearer token In self-hosted environments, the full URL below may look like https://langsmith.yourdomain.com/api/v1/platform/orgs/current/scim/tokens (without a subdomain, note the /api/v1 path prefix) or https://langsmith.yourdomain.com/subdomain/api/v1/platform/orgs/current/scim/tokens (with a subdomain) - see the ingress docs for more details.GET /v1/platform/orgs/current/scim/tokens GET /v1/platform/orgs/current/scim/tokens/{scim_token_id} PATCH /v1/platform/orgs/current/scim/tokens/{scim_token_id} (only thedescription field is supported)DELETE /v1/platform/orgs/current/scim/tokens/{scim_token_id} Step 4 - Configure your Identity Provider If you use Azure Entra ID (formerly Azure AD) or Okta, there are specific instructions for identity provider setup (refer to Azure Entra ID, Okta). The requirements and steps above are applicable for all identity providers. Azure Entra ID configuration steps For additional information, see Microsoft\u2019s documentation. Step 1: Configure SCIM in your Enterprise Application - Log in to the Azure portal with a privileged role (e.g., Global Administrator ). - Navigate to your existing LangSmith Enterprise Application. - In the left-side navigation, select Manage > Provisioning. - Click Get started. - Under Admin Credentials: - Tenant URL: - US: https://api.smith.langchain.com/scim/v2 - EU: https://eu.api.smith.langchain.com/scim/v2 - Self-hosted: <langsmith_url>/scim/v2 - US: - Secret Token: Enter the SCIM Bearer Token generated in Step 3. - Tenant URL: - Click Test Connection to verify the configuration. - Click Save. Mappings : User Attributes Set Target Object Actions to Create and Update (start with Delete disabled for safety): | LangSmith App Attribute | Microsoft Entra ID Attribute | Matching Precedence | |---|---|---| userName | userPrincipalName | | active | Not([IsSoftDeleted]) | | emails[type eq \"work\"].value | mail 1 | | name.formatted | displayName OR Join(\" \", [givenName], [surname]) 2 | | externalId | objectId 3 | 1 | - User\u2019s email address must be present in Entra ID. - Use the Join expression if yourdisplayName does not match the format ofFirstname Lastname . - To avoid inconsistency, this should match the SAML NameID assertion and the sub OAuth2.0 claim. For SAML SSO in cloud, theUnique User Identifier (Name ID) required claim should beuser.objectID and theName identifier format should bepersistent . Create and Update only (start with Delete disabled for safety): | LangSmith App Attribute | Microsoft Entra ID Attribute | Matching Precedence | |---|---|---| displayName | displayName 1 | 1 | externalId | objectId | | members | members | - Groups must follow the naming convention described in the Group Naming Convention section. If your company has a group naming policy, you should instead map from the description Microsoft Entra ID Attribute and set the description based on the Group Naming Convention section. - Under Applications > Applications, select your LangSmith Enterprise Application. - Under the Assignments tab, click Assign then either Assign to People or Assign to Groups. - Make the desired selection(s), then Assign and Done. - Set Provisioning Status to On under Provisioning. - Monitor the initial sync to ensure users and groups are provisioned correctly. - Once verified, enable Delete actions for both User and Group mappings. Okta configuration steps Supported features - Create users - Update user attributes - Deactivate users - Group push - Import users - Import groups Step 1: Add application from Okta Integration Network If you have already configured SSO login via SAML (cloud) or OAuth2.0 with OIDC (self-hosted), skip this step. - In the General tab, ensure the LangSmithUrl is filled in according to the instructions from Step 1 - In the Provisioning tab, select Integration . - Select Edit thenEnable API integration . - For API Token, paste the SCIM token you generated above. - Keep Import Groups checked. - To verify the configuration, select Test API Credentials. - Select Save. - After saving the API integration details, new settings tabs appear on the left. Select To App . - Select Edit. - Select the Enable checkbox for Create Users, Update Users, and Deactivate Users. - Select Save. - Assign users and/or groups in the Assignments tab. Assigned users are created and managed in your LangSmith group. - Configure provisioning: under Provisioning > To App > Provisioning to App , clickEdit , then checkCreate Users ,Update User Attributes , andDeactivate Users . - Under <application_name> Attribute Mappings , set the user attribute mappings as shown below, and delete the rest: Follow Okta\u2019s Enable Group Push instructions", "tokens": 1000, "node_type": "child"}
{"id": 260, "chunk_id": "ce808d45926e624bd501771111d7be0d", "parent_id": 238, "url": "", "namespace": "langchain", "title": "langsmith-user-management", "headers": ["langsmith-user-management"], "section_index": 0, "chunk_index": 4, "text": "Step 1 - In the Provisioning tab, select Integration . - Select Edit thenEnable API integration . - For API Token, paste the SCIM token you generated above. - Keep Import Groups checked. - To verify the configuration, select Test API Credentials. - Select Save. - After saving the API integration details, new settings tabs appear on the left. Select To App . - Select Edit. - Select the Enable checkbox for Create Users, Update Users, and Deactivate Users. - Select Save. - Assign users and/or groups in the Assignments tab. Assigned users are created and managed in your LangSmith group. - Configure provisioning: under Provisioning > To App > Provisioning to App , clickEdit , then checkCreate Users ,Update User Attributes , andDeactivate Users . - Under <application_name> Attribute Mappings , set the user attribute mappings as shown below, and delete the rest: Follow Okta\u2019s Enable Group Push instructions to configure groups to push by name or by rule.", "tokens": 160, "node_type": "child"}
{"id": 261, "chunk_id": "c16acf8a553ac24f5d77e7d3a1c6e493", "parent_id": 239, "url": "", "namespace": "langchain", "title": "langsmith-vitest-jest", "headers": ["langsmith-vitest-jest"], "section_index": 0, "chunk_index": 0, "text": "# langsmith-vitest-jest\n\n> Source: https://docs.langchain.com/langsmith/vitest-jest\n\nevaluate()\nevaluation flow, this is useful when:\n- Each example requires different evaluation logic\n- You want to assert binary expectations, and both track these assertions in LangSmith and raise assertion errors locally (e.g. in CI pipelines)\n- You want to take advantage of mocks, watch mode, local results, or other features of the Vitest/Jest ecosystems\nRequires JS/TS SDK version\nlangsmith>=0.3.1\n.The Vitest/Jest integrations are in beta and are subject to change in upcoming releases.\nSetup\nSet up the integrations as follows. Note that while you can add LangSmith evals alongside your other unit tests (as standard*.test.ts\nfiles) using your existing test config files, the below examples will also set up a separate test config file and command to run your evals. It will assume you end your test files with .eval.ts\n.\nThis ensures that the custom test reporter and other LangSmith touchpoints do not modify your existing test outputs.\nVitest\nInstall the required development dependencies if you have not already:openai\n(and of course langsmith\n!) as a dependency:\nls.vitest.config.ts\nfile with the following base config:\ninclude\nensures that only files ending with some variation ofeval.ts\nin your project are runreporters\nis responsible for nicely formatting your output as shown abovesetupFiles\nrunsdotenv\nto load environment variables before running your evals\nJSDom environments are not supported at this time. You should either omit the\n\"environment\"\nfield from your config or set it to \"node\"\n.scripts\nfield in your package.json\nto run Vitest with the config you just created:\nJest\nInstall the required development dependencies if you have not already:openai\n(and of course langsmith\n!) as a dependency:\nThen create a separate config file named\nls.jest.config.cjs\n:\ntestMatch\nensures that only files ending with some variation ofeval.js\nin your project are runreporters\nis responsible for nicely formatting your output as shown abovesetupFiles\nrunsdotenv\nto load environment variables before running your evals\nJSDom environments are not supported at this time. You should either omit the\n\"testEnvironment\"\nfield from your config or set it to \"node\"\n.scripts\nfield in your package.json\nto run Jest with the config you just created:\nDefine and run evals\nYou can now define evals as tests using familiar Vitest/Jest syntax, with a few caveats:- You should import\ndescribe\nandtest\nfrom thelangsmith/jest\norlangsmith/vitest\nentrypoint - You must wrap your test cases in a\ndescribe\nblock - When declaring tests, the signature is slightly different - there is an extra argument containing example inputs and expected outputs\nsql.eval.ts\n(or sql.eval.js\nif you are using Jest without TypeScript) and pasting the below contents into it:\nls.test()\ncase as corresponding to a dataset example, and ls.describe()\nas defining a LangSmith dataset. If you have LangSmith tracing environment variables set when you run the test suite, the SDK does the following:\n- creates a dataset with the same name as the name passed to\nls.describe()\nin LangSmith if it does not exist - creates an example in the dataset for each input and expected output passed into a test case if a matching one does not already exist\n- creates a new experiment with one result for each test case\n- collects the pass/fail rate under the\npass\nfeedback key for each test case\npass\nboolean feedback key based on the test case passing / failing. It will also track any outputs that you log with the ls.logOutputs()\nor return from the test function as \u201cactual\u201d result values from your app for the experiment.\nCreate a .env\nfile with your OPENAI_API_KEY\nand LangSmith credentials if you don\u2019t already have one:\neval\nscript we set up in the previous step to run the test:\nTrace feedback\nBy default LangSmith collects the pass/fail rate under thepass\nfeedback key for each test case. You can add additional feedback with either ls.logFeedback()\nor wrapEvaluator()\n. To do so, try the following as your sql.eval.ts\nfile (or sql.eval.js\nif you are using Jest without TypeScript):\nls.wrapEvaluator()\naround the myEvaluator\nfunction. This makes it so that the LLM-as-judge call is traced separately from the rest of the test case to avoid clutter, and conveniently creates feedback if the return value from the wrapped function matches { key: string; score: number | boolean }\n. In this case, instead of showing up in the main test case run, the evaluator trace will instead show up in a trace associated with the correctness\nfeedback key.\nYou can see the evaluator runs in LangSmith by clicking their corresponding feedback chips in the UI.\nRunning multiple examples against a test case\nYou can run the same test case over multiple examples and parameterize your tests usingls.test.each()\n. This is useful when you want to evaluate your app the same way against different inputs:\nLog outputs\nEvery time we run a test we\u2019re syncing it to a dataset example and tracing it as a run. To trace final outputs for the run, you can usels.logOutputs()\nlike this:\nTrace intermediate calls\nLangSmith will automatically trace any traceable intermediate calls that happen in the course of test case execution.Focusing or skipping tests\nYou can chain the Vitest/Jest.skip\nand .only\nmethods on ls.test()\nand ls.describe()\n:\nConfiguring test suites\nYou can configure test suites with values like metadata or a custom client by passing an extra argument tols.describe()\nfor the full suite or by passing a config\nfield into ls.test()\nfor individual tests:\nprocess.env.ENVIRONMENT\n, process.env.NODE_ENV\nand process.env.LANGSMITH_ENVIRONMENT\nand set them as metadata on created experiments. You can then filter experiments by metadata in LangSmith\u2019s UI.\nSee the API refs for a full list of configuration options.\nDry-run mode\nIf you want to run the tests without syncing the results to LangSmith, you can set omit your LangSmith tracing environment variables or setLANGSMITH_TEST_TRACKING=false\nin your environment.\nThe tests will run as normal, but the experiment logs will not be sent to LangSmith.", "tokens": 970, "node_type": "child"}
{"id": 262, "chunk_id": "c16acf8a553ac24f5d77e7d3a1c6e493", "parent_id": 240, "url": "", "namespace": "langchain", "title": "langsmith-webhooks", "headers": ["langsmith-webhooks"], "section_index": 0, "chunk_index": 0, "text": "# langsmith-webhooks\n\n> Source: https://docs.langchain.com/langsmith/webhooks\n\nWebhook payload\nThe payload we send to your webhook endpoint contains:\"rule_id\"\n: this is the ID of the automation that sent this payload\"start_time\"\nand\"end_time\"\n: these are the time boundaries where we found matching runs\"runs\"\n: this is an array of runs, where each run is a dictionary. If you need more information about each run we suggest using our SDK in your endpoint to fetch it from our API.\"feedback_stats\"\n: this is a dictionary with the feedback statistics for the runs. An example payload for this field is shown below.\nfetching from S3 URLsDepending on how recent your runs are, the\ninputs_s3_urls\nand outputs_s3_urls\nfields may contain S3 URLs to the actual data instead of the data itself.The inputs\nand outputs\ncan be fetched by the ROOT.presigned_url\nprovided in inputs_s3_urls\nand outputs_s3_urls\nrespectively.Security\nWe strongly recommend you add a secret query string parameter to the webhook URL, and verify it on any incoming request. This ensures that if someone discovers your webhook URL you can distinguish those calls from authentic webhook notifications. An example would beWebhook custom HTTP headers\nIf you\u2019d like to send any specific headers with your webhook, this can be configured per URL. To set this up, click on theHeaders\noption next to the URL field and add your headers.\nHeaders are stored in encrypted format.\nWebhook Delivery\nWhen delivering events to your webhook endpoint we follow these guidelines- If we fail to connect to your endpoint, we retry the transport connection up to 2 times, before declaring the delivery failed.\n- If your endpoint takes longer than 5 seconds to reply we declare the delivery failed and do not .\n- If your endpoint returns a 5xx status code in less than 5 seconds we retry up to 2 times with exponential backoff.\n- If your endpoint returns a 4xx status code, we declare the delivery failed and do not retry.\n- Anything your endpoint returns in the body will be ignored\nExample with Modal\nSetup\nFor an example of how to set this up, we will use Modal. Modal provides autoscaling GPUs for inference and fine-tuning, secure containerization for code agents, and serverless Python web endpoints. We\u2019ll focus on the web endpoints here. First, create a Modal account. Then, locally install the Modal SDK:Secrets\nNext, you will need to set up some secrets in Modal. First, LangSmith will need to authenticate to Modal by passing in a secret. The easiest way to do this is to pass in a secret in the query parameters. To validate this secret, we will need to add a secret in Modal to validate it. We will do that by creating a Modal secret. You can see instructions for secrets here. For this purpose, let\u2019s call our secretls-webhook\nand have it set an environment variable with the name LS_WEBHOOK\n.\nWe can also set up a LangSmith secret - luckily there is already an integration template for this!\nService\nAfter that, you can create a Python file that will serve as your endpoint. An example is below, with comments explaining what is going on:modal deploy ...\n(see docs here).\nYou should now get something like:\nhttps://hwchase17--auth-example-f.modal.run\n- the function we created to run.\nNOTE: this is NOT the final deployment URL, make sure not to accidentally use that.\nHooking it up\nWe can now take the function URL we create above and add it as a webhook. We have to remember to also pass in the secret key as a query parameter. Putting it all together, it should look something like:{SECRET}\nwith the secret key you created to access the Modal service.", "tokens": 608, "node_type": "child"}
{"id": 263, "chunk_id": "c16acf8a553ac24f5d77e7d3a1c6e493", "parent_id": 242, "url": "", "namespace": "langchain", "title": "oss-javascript-concepts-context", "headers": ["oss-javascript-concepts-context"], "section_index": 0, "chunk_index": 0, "text": "# oss-javascript-concepts-context\n\n> Source: https://docs.langchain.com/oss/javascript/concepts/context\n\nLangGraph v1.0Welcome to the new LangGraph documentation! If you encounter any issues or have feedback, please open an issue so we can improve. Archived v0 documentation can be found here.See the release notes and migration guide for a complete list of changes and instructions on how to upgrade your code.\n- By mutability:\n- Static context: Immutable data that doesn\u2019t change during execution (e.g., user metadata, database connections, tools)\n- Dynamic context: Mutable data that evolves as the application runs (e.g., conversation history, intermediate results, tool call observations)\n- By lifetime:\n- Runtime context: Data scoped to a single run or invocation\n- Cross-conversation context: Data that persists across multiple conversations or sessions\nRuntime context refers to local context: data and dependencies your code needs to run. It does not refer to:\n- The LLM context, which is the data passed into the LLM\u2019s prompt.\n- The \u201ccontext window\u201d, which is the maximum number of tokens that can be passed to the LLM.\n| Context type | Description | Mutability | Lifetime |\n|---|---|---|---|\n| Config | data passed at the start of a run | Static | Single run |\n| Dynamic runtime context (state) | Mutable data that evolves during a single run | Dynamic | Single run |\n| Dynamic cross-conversation context (store) | Persistent data shared across conversations | Dynamic | Cross-conversation |\nConfig\nConfig is for immutable data like user metadata or API keys. Use this when you have values that don\u2019t change mid-run. Specify configuration using a key called \u201cconfigurable\u201d which is reserved for this purpose.Dynamic runtime context\nDynamic runtime context represents mutable data that can evolve during a single run and is managed through the LangGraph state object. This includes conversation history, intermediate results, and values derived from tools or LLM outputs. In LangGraph, the state object acts as short-term memory during a run.- In an agent\n- In a workflow\nTurning on memory\nPlease see the memory guide for more details on how to enable memory. This is a powerful feature that allows you to persist the agent\u2019s state across multiple invocations. Otherwise, the state is scoped only to a single run.\nDynamic cross-conversation context\nDynamic cross-conversation context represents persistent, mutable data that spans across multiple conversations or sessions and is managed through the LangGraph store. This includes user profiles, preferences, and historical interactions. The LangGraph store acts as long-term memory across multiple runs. This can be used to read or update persistent facts (e.g., user profiles, preferences, prior interactions).See also\n- Memory conceptual overview\n- Short-term memory in LangChain\n- Long-term memory in LangChain\n- Memory in LangGraph", "tokens": 442, "node_type": "child"}
{"id": 264, "chunk_id": "c16acf8a553ac24f5d77e7d3a1c6e493", "parent_id": 243, "url": "", "namespace": "langchain", "title": "oss-javascript-concepts-memory", "headers": ["oss-javascript-concepts-memory"], "section_index": 0, "chunk_index": 0, "text": "# oss-javascript-concepts-memory > Source: https://docs.langchain.com/oss/javascript/concepts/memory LangGraph v1.0Welcome to the new LangGraph documentation! If you encounter any issues or have feedback, please open an issue so we can improve. Archived v0 documentation can be found here.See the release notes and migration guide for a complete list of changes and instructions on how to upgrade your code. - Short-term memory, or thread-scoped memory, tracks the ongoing conversation by maintaining message history within a session. LangGraph manages short-term memory as a part of your agent\u2019s state. State is persisted to a database using a checkpointer so the thread can be resumed at any time. Short-term memory updates when the graph is invoked or a step is completed, and the State is read at the start of each step. - Long-term memory stores user-specific or application-level data across sessions and is shared across conversational threads. It can be recalled at any time and in any thread. Memories are scoped to any custom namespace, not just within a single thread ID. LangGraph provides stores (reference doc) to let you save and recall long-term memories. Short-term memory Short-term memory lets your application remember previous interactions within a single thread or conversation. A thread organizes multiple interactions in a session, similar to the way email groups messages in a single conversation. LangGraph manages short-term memory as part of the agent\u2019s state, persisted via thread-scoped checkpoints. This state can normally include the conversation history along with other stateful data, such as uploaded files, retrieved documents, or generated artifacts. By storing these in the graph\u2019s state, the bot can access the full context for a given conversation while maintaining separation between different threads.Manage short-term memory Conversation history is the most common form of short-term memory, and long conversations pose a challenge to today\u2019s LLMs. A full history may not fit inside an LLM\u2019s context window, resulting in an irrecoverable error. Even if your LLM supports the full context length, most LLMs still perform poorly over long contexts. They get \u201cdistracted\u201d by stale or off-topic content, all while suffering from slower response times and higher costs. Chat models accept context using messages, which include developer provided instructions (a system message) and user inputs (human messages). In chat applications, messages alternate between human inputs and model responses, resulting in a list of messages that grows longer over time. Because context windows are limited and token-rich message lists can be costly, many applications can benefit from using techniques to manually remove or forget stale information. For more information on common techniques for managing messages, see the Add and manage memory guide.Long-term memory Long-term memory in LangGraph allows systems to retain information across different conversations or sessions. Unlike short-term memory, which is thread-scoped, long-term memory is saved within custom \u201cnamespaces.\u201d Long-term memory is a complex challenge without a one-size-fits-all solution. However, the following questions provide a framework to help you navigate the different techniques:- What is the type of memory? Humans use memories to remember facts (semantic memory), experiences (episodic memory), and rules (procedural memory). AI agents can use memory in the same ways. For example, AI agents can use memory to remember specific facts about a user to accomplish a task. - When do you want to update memories? Memory can be updated as part of an agent\u2019s application logic (e.g., \u201con the hot path\u201d). In this case, the agent typically decides to remember facts before responding to a user. Alternatively, memory can be updated as a background task (logic that runs in the background / asynchronously and generates memories). We explain the tradeoffs between these approaches in the section below. Memory types Different applications require various types of memory. Although the analogy isn\u2019t perfect, examining human memory types can be insightful. Some research (e.g., the CoALA paper) have even mapped these human memory types to those used in AI agents.| Memory Type | What is Stored | Human Example | Agent Example | |---|---|---|---| | Semantic | Facts | Things I learned in school | Facts about a user | | Episodic | Experiences | Things I did | Past agent actions | | Procedural | Instructions | Instincts or motor skills | Agent system prompt | Semantic memory Semantic memory, both in humans and AI agents, involves the retention of specific facts and concepts. In humans, it can include information learned in school and the understanding of concepts and their relationships. For AI agents, semantic memory is often used to personalize applications by remembering facts or concepts from past interactions.Semantic memory is different from \u201csemantic search,\u201d which is a technique for finding similar content using \u201cmeaning\u201d (usually as embeddings). Semantic memory is a term from psychology, referring to storing facts and knowledge, while semantic search is a method for retrieving information based on meaning rather than exact matches. Profile Semantic memories can be managed in different ways. For example, memories can be a single, continuously updated \u201cprofile\u201d of well-scoped and specific information about a user, organization, or other entity (including the agent itself). A profile is generally just a JSON document with various key-value pairs you\u2019ve selected to represent your domain. When remembering a profile, you will want to make sure that you are updating the profile each time. As a result, you will want to pass in the previous profile and ask the model to generate a new profile (or some JSON patch to apply to the old profile). This can be become error-prone as the profile gets larger, and may benefit from splitting a profile into multiple documents or strict decoding when generating documents to ensure the memory schemas remains valid.Collection Alternatively, memories can be a collection of documents that are continuously updated and extended over time. Each individual memory can be more narrowly scoped and easier to generate, which means that you\u2019re less likely to lose information over time. It\u2019s easier for an LLM to generate new objects for new information than reconcile new information with an existing profile. As a result, a document collection tends to", "tokens": 1000, "node_type": "child"}
{"id": 265, "chunk_id": "9b0fd3b850698317bef57fa6e71a38ba", "parent_id": 243, "url": "", "namespace": "langchain", "title": "oss-javascript-concepts-memory", "headers": ["oss-javascript-concepts-memory"], "section_index": 0, "chunk_index": 1, "text": "domain. When remembering a profile, you will want to make sure that you are updating the profile each time. As a result, you will want to pass in the previous profile and ask the model to generate a new profile (or some JSON patch to apply to the old profile). This can be become error-prone as the profile gets larger, and may benefit from splitting a profile into multiple documents or strict decoding when generating documents to ensure the memory schemas remains valid.Collection Alternatively, memories can be a collection of documents that are continuously updated and extended over time. Each individual memory can be more narrowly scoped and easier to generate, which means that you\u2019re less likely to lose information over time. It\u2019s easier for an LLM to generate new objects for new information than reconcile new information with an existing profile. As a result, a document collection tends to lead to higher recall downstream. However, this shifts some complexity memory updating. The model must now delete or update existing items in the list, which can be tricky. In addition, some models may default to over-inserting and others may default to over-updating. See the Trustcall package for one way to manage this and consider evaluation (e.g., with a tool like LangSmith) to help you tune the behavior. Working with document collections also shifts complexity to memory search over the list. TheStore currently supports both semantic search and filtering by content. Finally, using a collection of memories can make it challenging to provide comprehensive context to the model. While individual memories may follow a specific schema, this structure might not capture the full context or relationships between memories. As a result, when using these memories to generate responses, the model may lack important contextual information that would be more readily available in a unified profile approach. Regardless of memory management approach, the central point is that the agent will use the semantic memories to ground its responses, which often leads to more personalized and relevant interactions. Episodic memory Episodic memory, in both humans and AI agents, involves recalling past events or actions. The CoALA paper frames this well: facts can be written to semantic memory, whereas experiences can be written to episodic memory. For AI agents, episodic memory is often used to help an agent remember how to accomplish a task. In practice, episodic memories are often implemented through few-shot example prompting, where agents learn from past sequences to perform tasks correctly. Sometimes it\u2019s easier to \u201cshow\u201d than \u201ctell\u201d and LLMs learn well from examples. Few-shot learning lets you \u201cprogram\u201d your LLM by updating the prompt with input-output examples to illustrate the intended behavior. While various best-practices can be used to generate few-shot examples, often the challenge lies in selecting the most relevant examples based on user input. Note that the memory store is just one way to store data as few-shot examples. If you want to have more developer involvement, or tie few-shots more closely to your evaluation harness, you can also use a LangSmith Dataset to store your data. Then dynamic few-shot example selectors can be used out-of-the box to achieve this same goal. LangSmith will index the dataset for you and enable retrieval of few shot examples that are most relevant to the user input based upon keyword similarity. See this how-to video for example usage of dynamic few-shot example selection in LangSmith. Also, see this blog post showcasing few-shot prompting to improve tool calling performance and this blog post using few-shot example to align an LLMs to human preferences.Procedural memory Procedural memory, in both humans and AI agents, involves remembering the rules used to perform tasks. In humans, procedural memory is like the internalized knowledge of how to perform tasks, such as riding a bike via basic motor skills and balance. Episodic memory, on the other hand, involves recalling specific experiences, such as the first time you successfully rode a bike without training wheels or a memorable bike ride through a scenic route. For AI agents, procedural memory is a combination of model weights, agent code, and agent\u2019s prompt that collectively determine the agent\u2019s functionality. In practice, it is fairly uncommon for agents to modify their model weights or rewrite their code. However, it is more common for agents to modify their own prompts. One effective approach to refining an agent\u2019s instructions is through \u201cReflection\u201d or meta-prompting. This involves prompting the agent with its current instructions (e.g., the system prompt) along with recent conversations or explicit user feedback. The agent then refines its own instructions based on this input. This method is particularly useful for tasks where instructions are challenging to specify upfront, as it allows the agent to learn and adapt from its interactions. For example, we built a Tweet generator using external feedback and prompt re-writing to produce high-quality paper summaries for Twitter. In this case, the specific summarization prompt was difficult to specify a priori, but it was fairly easy for a user to critique the generated Tweets and provide feedback on how to improve the summarization process. The below pseudo-code shows how you might implement this with the LangGraph memory store, using the store to save a prompt, theupdate_instructions node to get the current prompt (as well as feedback from the conversation with the user captured in state[\"messages\"] ), update the prompt, and save the new prompt back to the store. Then, the call_model get the updated prompt from the store and uses it to generate a response. Writing memories There are two primary methods for agents to write memories: \u201cin the hot path\u201d and \u201cin the background\u201d.In the hot path Creating memories during runtime offers both advantages and challenges. On the positive side, this approach allows for real-time updates, making new memories immediately available for use in subsequent interactions. It also enables transparency, as users can be notified when memories are created and stored. However, this method also presents challenges. It may increase complexity if the agent requires a new tool to", "tokens": 1000, "node_type": "child"}
{"id": 266, "chunk_id": "5345104ec252a41359fa4a3d2dfbec23", "parent_id": 243, "url": "", "namespace": "langchain", "title": "oss-javascript-concepts-memory", "headers": ["oss-javascript-concepts-memory"], "section_index": 0, "chunk_index": 2, "text": "might implement this with the LangGraph memory store, using the store to save a prompt, theupdate_instructions node to get the current prompt (as well as feedback from the conversation with the user captured in state[\"messages\"] ), update the prompt, and save the new prompt back to the store. Then, the call_model get the updated prompt from the store and uses it to generate a response. Writing memories There are two primary methods for agents to write memories: \u201cin the hot path\u201d and \u201cin the background\u201d.In the hot path Creating memories during runtime offers both advantages and challenges. On the positive side, this approach allows for real-time updates, making new memories immediately available for use in subsequent interactions. It also enables transparency, as users can be notified when memories are created and stored. However, this method also presents challenges. It may increase complexity if the agent requires a new tool to decide what to commit to memory. In addition, the process of reasoning about what to save to memory can impact agent latency. Finally, the agent must multitask between memory creation and its other responsibilities, potentially affecting the quantity and quality of memories created. As an example, ChatGPT uses a save_memories tool to upsert memories as content strings, deciding whether and how to use this tool with each user message. See our memory-agent template as an reference implementation.In the background Creating memories as a separate background task offers several advantages. It eliminates latency in the primary application, separates application logic from memory management, and allows for more focused task completion by the agent. This approach also provides flexibility in timing memory creation to avoid redundant work. However, this method has its own challenges. Determining the frequency of memory writing becomes crucial, as infrequent updates may leave other threads without new context. Deciding when to trigger memory formation is also important. Common strategies include scheduling after a set time period (with rescheduling if new events occur), using a cron schedule, or allowing manual triggers by users or the application logic. See our memory-service template as an reference implementation.Memory storage LangGraph stores long-term memories as JSON documents in a store. Each memory is organized under a customnamespace (similar to a folder) and a distinct key (like a file name). Namespaces often include user or org IDs or other labels that makes it easier to organize information. This structure enables hierarchical organization of memories. Cross-namespace searching is then supported through content filters.", "tokens": 408, "node_type": "child"}
{"id": 267, "chunk_id": "c16acf8a553ac24f5d77e7d3a1c6e493", "parent_id": 244, "url": "", "namespace": "langchain", "title": "oss-javascript-contributing-code", "headers": ["oss-javascript-contributing-code"], "section_index": 0, "chunk_index": 0, "text": "# oss-javascript-contributing-code\n\n> Source: https://docs.langchain.com/oss/javascript/contributing/code\n\nPhilosophy\nAim to follow these core principles for all code contributions:Backwards compatibility\nTesting first\nCode quality\nSecurity focused\nGetting started\nQuick fix: submit a bugfix\nFor simple bugfixes, you can get started immediately:Clone and setup\nCreate a branch\nRun build\nRun tests\nFull development setup\nFor ongoing development or larger contributions:Development environment\n- LangChain\n- LangGraph\nRepository structure\n- LangChain\n- LangGraph\nCore packages\nCore packages\nPartner packages\nPartner packages\nlibs/providers/\n, these are independently versioned packages for specific integrations. For example:@langchain/openai\n: OpenAI integrations@langchain/anthropic\n: Anthropic integrations@langchain/google-genai\n: Google Generative AI integrations\nDevelopment workflow\nTesting requirements\nUnit tests\nsrc/tests/FILENAME_BEING_TESTED.test.ts\nRequirements:- No network calls allowed\n- Test all code paths including edge cases\n- Use mocks for external dependencies\nIntegration tests\nsrc/tests/FILENAME_BEING_TESTED.int.test.ts\nRequirements:- Test real integrations with external services\n- Use environment variables for API keys\n- Skip gracefully if credentials unavailable\nTest quality checklist\n- Tests fail when your code is broken\n- Edge cases and error conditions are tested\n- Proper use of fixtures and mocks\nCode quality standards\nQuality requirements:- Type hints\n- Documentation\n- Code style\nContribution guidelines\nBackwards compatibility\nStable interfaces\nStable interfaces\n- Function signatures and parameter names\n- Class interfaces and method names\n- Return value structure and types\n- Import paths for public APIs\nSafe changes\nSafe changes\n- Adding new optional parameters/type parameters\n- Adding new methods to classes\n- Improving performance without changing behavior\n- Adding new modules or functions\nBefore making changes\nBefore making changes\n- Would this break existing user code?\n- Check if your target is public\n- Are there existing usage patterns in tests?\nBugfixes\nFor bugfix contributions:Reproduce the issue\nWrite failing tests\nImplement the fix\nVerify the fix\nDocument the change\nNew features\nWe aim to keep the bar high for new features. We generally don\u2019t accept new core abstractions, changes to infra, changes to dependencies, or new agents/chains from outside contributors without an existing issue that demonstrates an acute need for them. In general, feature contribution requirements include:Design discussion\n- The problem you\u2019re solving\n- Proposed API design\n- Expected usage patterns\nImplementation\n- Follow existing code patterns\n- Include comprehensive tests and documentation\n- Consider security implications\nIntegration considerations\n- How does this interact with existing features?\n- Are there performance implications?\n- Does this introduce new dependencies?\nSecurity guidelines\nInput validation\nInput validation\n- Validate and sanitize all user inputs\n- Properly escape data in templates and queries\n-\nNever use\neval()\n, as this can lead to arbitrary code execution vulnerabilities\nError handling\nError handling\n- Use specific exception types\n- Don\u2019t expose sensitive information in error messages\n- Implement proper resource cleanup\nDependencies\nDependencies\n- Avoid adding hard dependencies\n- Keep optional dependencies minimal\n- Review third-party packages for security issues\nTesting and validation\nRunning tests locally\nBefore submitting your PR, ensure you have completed the following steps. Note that the requirements differ slightly between LangChain and LangGraph.- LangChain\n- LangGraph\nUnit tests\nIntegration tests\nFormatting\nPR submission\nTest writing guidelines\nIn order to write effective tests, there\u2019s a few good practices to follow:- Encapsulate the test in a\ndescribe\nblock that describes the component being tested - Use natural language to describe the test name\n- Be exhaustive with assertions\n- Only use snapshots for reasonably sized data objects\n- Unit tests\n- Integration tests\n- Mock usage", "tokens": 564, "node_type": "child"}
{"id": 268, "chunk_id": "c16acf8a553ac24f5d77e7d3a1c6e493", "parent_id": 245, "url": "", "namespace": "langchain", "title": "oss-javascript-contributing-comarketing", "headers": ["oss-javascript-contributing-comarketing"], "section_index": 0, "chunk_index": 0, "text": "# oss-javascript-contributing-comarketing\n\n> Source: https://docs.langchain.com/oss/javascript/contributing/comarketing\n\nContent we\u2019re excited to promote\nEducational content\nEducational content\nBlogs, YouTube videos and other media showcasing educational content. Note that we prefer content that is NOT framed as \u201chere\u2019s how to use integration XYZ\u201d, but rather \u201chere\u2019s how to do ABC\u201d, as we find that is more educational and helpful for developers.\nEnd-to-end applications\nEnd-to-end applications\nEnd-to-end applications are great resources for developers looking to build. We prefer to highlight applications that are more complex/agentic in nature, and that use LangGraph as the orchestration framework. We get particularly excited about anything involving:\n- Long-term memory systems\n- Human-in-the-loop interaction patterns\n- Multi-agent architectures\nResearch\nResearch\nWe love highlighting novel research! Whether it is research built on top of LangChain or that integrates with it.", "tokens": 129, "node_type": "child"}
{"id": 269, "chunk_id": "c16acf8a553ac24f5d77e7d3a1c6e493", "parent_id": 246, "url": "", "namespace": "langchain", "title": "oss-javascript-contributing-documentation", "headers": ["oss-javascript-contributing-documentation"], "section_index": 0, "chunk_index": 0, "text": "# oss-javascript-contributing-documentation\n\n> Source: https://docs.langchain.com/oss/javascript/contributing/documentation\n\nConceptual guides\nReferences\nTutorials (Learn)\nHow-to guides\nGetting started\nQuick edit: fix a typo\nFor simple changes like fixing typos, you can edit directly on GitHub without setting up a local development environment:Find the page\nFork the repository\nMake your changes\nCreate pull request\nFull development IDE setup\nFor larger changes or ongoing contributions, it\u2019s important to set up a local development environment on your machine. Our documentation build pipeline offers local preview and live reload as you edit, important for ensuring your changes appear as intended before submitting. Please review the steps to set up your environment outlined in the docs repoREADME.md\n.\nDocumentation types\nConceptual guides\nConceptual guide cover core concepts abstractly, providing deep understanding.Characteristics\nCharacteristics\n- Understanding-focused: Explain why things work as they do\n- Broad perspective: Higher and wider view than other types\n- Design-oriented: Explain decisions and trade-offs\n- Context-rich: Use analogies and comparisons\nTips\nTips\n- Explain design decisions - \u201cwhy does concept X exist?\u201d\n- Use analogies and reference alternatives\n- Avoid blending in too much reference content\n- Link to related tutorials and how-to guides\n- Focus on the \u201cwhy\u201d rather than the \u201chow\u201d\nReferences\nReference documentation contains detailed, low-level information describing exactly what functionality exists and how to use it.JavaScript/TypeScript reference\n- Describe what exists (all parameters, options, return values)\n- Be comprehensive and structured for easy lookup\n- Serve as the authoritative source for technical details\nLangChain reference best practices\nLangChain reference best practices\n- Be consistent; follow existing patterns for provider-specific documentation\n- Include both basic usage (code snippets) and common edge cases/failure modes\n- Note when features require specific versions\nWhen to create new reference documentation\nWhen to create new reference documentation\n- New integrations or providers need dedicated reference pages\n- Complex configuration options require detailed explanation\n- API changes introduce new parameters or behavior\n- Community frequently asks questions about specific functionality\nWriting standard\nMintlify components\nUse appropriate Mintlify components to enhance readability:- Callouts\n- Structure\n- Code\n<Note>\nfor helpful supplementary information<Warning>\nfor important cautions and breaking changes<Tip>\nfor best practices and advice<Info>\nfor neutral contextual information<Check>\nfor success confirmations\nPage structure\nEvery documentation page must begin with YAML frontmatter:Localization\nAll documentation must be localized in both Python and JavaScript/TypeScript when possible. To do so, we use a custom in-line syntax to differentiate between sections that should appear in one or both languages:Quality standards\nGeneral guidelines\nAvoid duplication\nAvoid duplication\nLink frequently\nLink frequently\nBe concise\nBe concise\nAccessibility requirements\nEnsure documentation is accessible to all users:- Structure content for easy scanning with headers and lists\n- Use specific, actionable link text instead of \u201cclick here\u201d\n- Include descriptive alt text for all images and diagrams\nTesting and validation\nBefore submitting documentation:Test all code\nCheck formatting\nBuild locally\nReview links\nIn-code documentation\nLanguage and style\nFollow these standards for all documentation:- Voice: Use second person (\u201cyou\u201d) for instructions\n- Tense: Use active voice and present tense\n- Clarity: Write clear, direct language for technical audiences\n- Consistency: Use consistent terminology throughout\n- Conciseness: Keep sentences concise while providing necessary context\nCode examples\nCompleteness\nRealism\nError handling\nDocumentation", "tokens": 527, "node_type": "child"}
{"id": 270, "chunk_id": "c16acf8a553ac24f5d77e7d3a1c6e493", "parent_id": 249, "url": "", "namespace": "langchain", "title": "oss-javascript-contributing-integrations-langchain", "headers": ["oss-javascript-contributing-integrations-langchain"], "section_index": 0, "chunk_index": 0, "text": "# oss-javascript-contributing-integrations-langchain\n\n> Source: https://docs.langchain.com/oss/javascript/contributing/integrations-langchain\n\nWhy contribute an integration to LangChain?\nDiscoverability\nLangChain is the most used framework for building LLM applications, with over 20 million monthly downloads.\nInteroperability\nLangChain components expose a standard interface, allowing developers to easily swap them for each other. If you implement a LangChain integration, any developer using a different component will easily be able to swap yours in.\nBest Practices\nThrough their standard interface, LangChain components encourage and facilitate best practices (streaming, async, etc.) that improve developer experience and application performance.\nComponents to integrate\nWhile any component can be integrated into LangChain, there are specific types of integrations we encourage more: Integrate these \u2705:- Chat Models: Most actively used component type\n- Tools/Toolkits: Enable agent capabilities\n- Retrievers: Core to RAG applications\n- Embedding Models: Foundation for vector operations\n- Vector Stores: Essential for semantic search\n- LLMs (Text-Completion Models): Deprecated in favor of Chat Models\n- Document Loaders: High maintenance burden\n- Key-Value Stores: Limited usage\n- Document Transformers: Niche use cases\n- Model Caches: Infrastructure concerns\n- Graphs: Complex abstractions\n- Message Histories: Storage abstractions\n- Callbacks: System-level components\n- Chat Loaders: Limited demand\n- Adapters: Edge case utilities\nHow to contribute an integration\n1\n3\n5\nAdd documentation\nOpen a PR to add documentation for your integration to the official LangChain docs.\nIntegration documentation guide\nIntegration documentation guide\nAn integration is only as useful as its documentation. To ensure a consistent experience for users, docs are required for all new integrations. We have a standard starting-point template for each type of integration for you to copy and modify.In a new PR to the LangChain docs repo, create a new file in the relevant directory under\nsrc/oss/python/integrations/<component_type>/integration_name.mdx\nusing the appropriate template file:- Chat models\n- Tools and toolkits\n- Retrievers\n- Text splitters - Coming soon\n- Embedding models - Coming soon\n- Vector stores\n- Document loaders - Coming soon\n- Key-value stores - Coming soon", "tokens": 325, "node_type": "child"}
{"id": 271, "chunk_id": "c16acf8a553ac24f5d77e7d3a1c6e493", "parent_id": 251, "url": "", "namespace": "langchain", "title": "oss-javascript-contributing-overview", "headers": ["oss-javascript-contributing-overview"], "section_index": 0, "chunk_index": 0, "text": "# oss-javascript-contributing-overview\n\n> Source: https://docs.langchain.com/oss/javascript/contributing/overview\n\nLangGraph v1.0Welcome to the new LangGraph documentation! If you encounter any issues or have feedback, please open an issue so we can improve. Archived v0 documentation can be found here.See the release notes and migration guide for a complete list of changes and instructions on how to upgrade your code.\nWays to Contribute\nReport bugs\nReport bugs\nFound a bug? Please help us fix it by following these steps:If you are adding an issue, please try to keep it focused on a single topic. If two issues are related, or blocking, please link them rather than combining them. For example,\n1\n2\nCreate issue\nIf no issue exists, create a new one. When writing, be sure to follow the template provided and to include a minimal, reproducible, example. Attach any relevant labels to the final issue once created. If a project maintainer is unable to reproduce the issue, it is unlikely to be addressed in a timely manner.\n3\nWait\nA project maintainer will triage the issue and may ask for additional information. Please be patient as we manage a high volume of issues. Do not bump the issue unless you have new information to provide.\nSuggest features\nSuggest features\nHave an idea for a new feature or enhancement?\n1\n2\n3\nDescribe\nBe sure to describe the use case and why it would be valuable to others. If possible, provide examples or mockups where applicable. Outline test cases that should pass.\nImprove documentation\nImprove documentation\nContribute code\nContribute code\nWith a large userbase, it can be hard for our small team to keep up with all the feature requests and bug fixes. If you have the skills and time, we would love your help!If you start working on an issue, please assign it to yourself or ask a maintainer to do so. This helps avoid duplicate work.If you are looking for something to work on, check out the issues labeled \u201cgood first issue\u201d or \u201chelp wanted\u201d in our repos:\nHow to make your first Pull Request\nGuide", "tokens": 343, "node_type": "child"}
{"id": 272, "chunk_id": "c16acf8a553ac24f5d77e7d3a1c6e493", "parent_id": 254, "url": "", "namespace": "langchain", "title": "oss-javascript-contributing-standard-tests-langchain", "headers": ["oss-javascript-contributing-standard-tests-langchain"], "section_index": 0, "chunk_index": 0, "text": "# oss-javascript-contributing-standard-tests-langchain\n\n> Source: https://docs.langchain.com/oss/javascript/contributing/standard-tests-langchain\n\nSetup\nFirst, install the required dependencies:langchain-core\nDefines the interfaces we want to import to define our custom components\nlangchain-tests\nProvides the standard tests and plugins necessary to run them\nlangchain-tests\npackage:\nUnit tests\nUnit tests\nLocation:\nsrc.unit_tests\nDesigned to test the component in isolation and without access to external servicesIntegration tests\nIntegration tests\nLocation:\nsrc.integration_tests\nDesigned to test the component with access to external services (in particular, the external service that the component is designed to interact with)Implementing standard tests\nDepending on your integration type, you will need to implement either or both unit and integration tests. By subclassing the standard test suite for your integration type, you get the full collection of standard tests for that type. For a test run to be successful, the a given test should pass only if the model supports the capability being tested. Otherwise, the test should be skipped. Because different integrations offer unique sets of features, most standard tests provided by LangChain are opt-in by default to prevent false positives. Consequently, you will need to override properties to indicate which features your integration supports - see the below example for an illustration.tests/chat_models.standard.int.test.ts\nYou should typically organize tests in these subdirectories relative to the root of your package:\ntests/unit_tests\nfor unit teststests/integration_tests\nfor integration tests\n- Unit tests\n- Integration tests", "tokens": 223, "node_type": "child"}
{"id": 273, "chunk_id": "c16acf8a553ac24f5d77e7d3a1c6e493", "parent_id": 256, "url": "", "namespace": "langchain", "title": "oss-javascript-integrations-chat-anthropic", "headers": ["oss-javascript-integrations-chat-anthropic"], "section_index": 0, "chunk_index": 0, "text": "# oss-javascript-integrations-chat-anthropic\n\n> Source: https://docs.langchain.com/oss/javascript/integrations/chat/anthropic\n\nAnthropic is an AI safety and research company. They are the creator of Claude.\nThis will help you getting started with Anthropic chat models. For detailed documentation of all ChatAnthropic\nfeatures and configurations head to the API reference.\nOverview\nIntegration details\nModel features\nSee the links in the table headers below for guides on how to use specific features.\nSetup\nYou\u2019ll need to sign up and obtain an Anthropic API key, and install the @langchain/anthropic\nintegration package.\nCredentials\nHead to Anthropic\u2019s website to sign up to Anthropic and generate an API key. Once you\u2019ve done this set the ANTHROPIC_API_KEY\nenvironment variable:\nIf you want to get automated tracing of your model calls you can also set your LangSmith API key by uncommenting below:\nInstallation\nThe LangChain ChatAnthropic\nintegration lives in the @langchain/anthropic\npackage:\nInstantiation\nNow we can instantiate our model object and generate chat completions:\nInvocation\nContent blocks\nOne key difference to note between Anthropic models and most others is that the contents of a single Anthropic AIMessage\ncan either be a single string or a list of content blocks. For example when an Anthropic model calls a tool, the tool invocation is part of the message content (as well as being exposed in the standardized AIMessage.tool_calls\nfield):\nYou can pass custom headers in your requests like this:\nPrompt caching\nAnthropic supports caching parts of your prompt in order to reduce costs for use-cases that require long context. You can cache tools and both entire messages and individual blocks.\nThe initial request containing one or more blocks or tool definitions with a \"cache_control\": { \"type\": \"ephemeral\" }\nfield will automatically cache that part of the prompt. This initial caching step will cost extra, but subsequent requests will be billed at a reduced rate. The cache has a lifetime of 5 minutes, but this is refereshed each time the cache is hit.\nThere is also currently a minimum cacheable prompt length, which varies according to model. You can see this information here.\nThis currently requires you to initialize your model with a beta header. Here\u2019s an example of caching part of a system message that contains the LangChain conceptual docs:\nWe can see that there\u2019s a new field called cache_creation_input_tokens\nin the raw usage field returned from Anthropic.\nIf we use the same messages again, we can see that the long text\u2019s input tokens are read from the cache:\nYou can also cache tools by setting the same \"cache_control\": { \"type\": \"ephemeral\" }\nwithin a tool definition. This currently requires you to bind a tool in Anthropic\u2019s raw tool format Here\u2019s an example:\nFor more on how prompt caching works, see Anthropic\u2019s docs.\nCustom clients\nAnthropic models may be hosted on cloud services such as Google Vertex that rely on a different underlying client with the same interface as the primary Anthropic client. You can access these services by providing a createClient\nmethod that returns an initialized instance of an Anthropic client. Here\u2019s an example:\nCitations\nAnthropic supports a citations feature that lets Claude attach context to its answers based on source material supplied by the user. This source material can be provided either as document content blocks, which describe full documents, or as search results, which describe relevant passages or snippets returned from a retrieval system. When \"citations\": { \"enabled\": true }\nis included in a query, Claude may generate direct citations to the provided material in its response.\nDocument example\nIn this example we pass a plain text document. In the background, Claude automatically chunks the input text into sentences, which are used when generating citations.\nSearch results example\nIn this example, we pass in search results as part of our message content. This allows Claude to cite specific passages or snippets from your own retrieval system in its response.\nThis approach is helpful when you want Claude to cite information from a specific set of knowledge, but you want to bring your own pre-fetched/cached content directly rather than having the model search or retrieve them automatically.\nYou can also use a tool to provide search results that the model can cite in its responses. This is well suited for RAG (or Retrieval-Augmented Generation) workflows where Claude can decide when and where to retrieve information from. When returning this information as search results, it gives Claude the ability to create citations from the material returned from the tool.\nHere\u2019s how you can create a tool that returns search results in the format expected by Anthropic\u2019s citations API:\nLearn more about how RAG works in LangChain here\nLearn more about tool calling here\nUsing with text splitters\nAnthropic also lets you specify your own splits using custom document types. LangChain text splitters can be used to generate meaningful splits for this purpose. See the below example, where we split the LangChain.js README (a markdown document) and pass it to Claude as context:\nContext management\nAnthropic supports a context editing feature that will automatically manage the model\u2019s context window (e.g., by clearing tool results).\nSee Anthropic documentation for details and configuration options.\nContext management is supported since @langchain/anthropic@0.3.29\nAPI reference\nFor detailed documentation of all ChatAnthropic features and configurations head to the API reference.", "tokens": 868, "node_type": "child"}
{"id": 274, "chunk_id": "c16acf8a553ac24f5d77e7d3a1c6e493", "parent_id": 257, "url": "", "namespace": "langchain", "title": "oss-javascript-integrations-chat-azure", "headers": ["oss-javascript-integrations-chat-azure"], "section_index": 0, "chunk_index": 0, "text": "# oss-javascript-integrations-chat-azure\n\n> Source: https://docs.langchain.com/oss/javascript/integrations/chat/azure\n\nAzure OpenAI is a Microsoft Azure service that provides powerful language models from OpenAI.This will help you getting started with AzureChatOpenAI chat models. For detailed documentation of all AzureChatOpenAI features and configurations head to the API reference.\nAzure OpenAI is a cloud service to help you quickly develop generative AI experiences with a diverse set of prebuilt and curated models from OpenAI, Meta and beyond.LangChain.js supports integration with Azure OpenAI using the new Azure integration in the OpenAI SDK.You can learn more about Azure OpenAI and its difference with the OpenAI API on this page.\nIf you don\u2019t have an Azure account, you can create a free account to get started.You\u2019ll also need to have an Azure OpenAI instance deployed. You can deploy a version on Azure Portal following this guide.Once you have your instance running, make sure you have the name of your instance and key. You can find the key in the Azure Portal, under the \u201cKeys and Endpoint\u201d section of your instance. Then, if using Node.js, you can set your credentials as environment variables:\nNow we can instantiate our model object and generate chat completions:\nCopy\nimport { AzureChatOpenAI } from \"@langchain/openai\"const llm = new AzureChatOpenAI({ model: \"gpt-4o\", temperature: 0, maxTokens: undefined, maxRetries: 2, azureOpenAIApiKey: process.env.AZURE_OPENAI_API_KEY, // In Node.js defaults to process.env.AZURE_OPENAI_API_KEY azureOpenAIApiInstanceName: process.env.AZURE_OPENAI_API_INSTANCE_NAME, // In Node.js defaults to process.env.AZURE_OPENAI_API_INSTANCE_NAME azureOpenAIApiDeploymentName: process.env.AZURE_OPENAI_API_DEPLOYMENT_NAME, // In Node.js defaults to process.env.AZURE_OPENAI_API_DEPLOYMENT_NAME azureOpenAIApiVersion: process.env.AZURE_OPENAI_API_VERSION, // In Node.js defaults to process.env.AZURE_OPENAI_API_VERSION})\nconst aiMsg = await llm.invoke([ [ \"system\", \"You are a helpful assistant that translates English to French. Translate the user sentence.\", ], [\"human\", \"I love programming.\"],])aiMsg\nIf your instance is hosted under a domain other than the default openai.azure.com, you\u2019ll need to use the alternate AZURE_OPENAI_BASE_PATH environment variable.\nFor example, here\u2019s how you would connect to the domain https://westeurope.api.microsoft.com/openai/deployments/{DEPLOYMENT_NAME}:\nCopy\nimport { AzureChatOpenAI } from \"@langchain/openai\";const llmWithDifferentDomain = new AzureChatOpenAI({ temperature: 0.9, azureOpenAIApiKey: \"<your_key>\", // In Node.js defaults to process.env.AZURE_OPENAI_API_KEY azureOpenAIApiDeploymentName: \"<your_deployment_name>\", // In Node.js defaults to process.env.AZURE_OPENAI_API_DEPLOYMENT_NAME azureOpenAIApiVersion: \"<api_version>\", // In Node.js defaults to process.env.AZURE_OPENAI_API_VERSION azureOpenAIBasePath: \"https://westeurope.api.microsoft.com/openai/deployments\", // In Node.js defaults to process.env.AZURE_OPENAI_BASE_PATH});\nYou can specify custom headers by passing in a configuration field:\nCopy\nimport { AzureChatOpenAI } from \"@langchain/openai\";const llmWithCustomHeaders = new AzureChatOpenAI({ azureOpenAIApiKey: process.env.AZURE_OPENAI_API_KEY, // In Node.js defaults to process.env.AZURE_OPENAI_API_KEY azureOpenAIApiInstanceName: process.env.AZURE_OPENAI_API_INSTANCE_NAME, // In Node.js defaults to process.env.AZURE_OPENAI_API_INSTANCE_NAME azureOpenAIApiDeploymentName: process.env.AZURE_OPENAI_API_DEPLOYMENT_NAME, // In Node.js defaults to process.env.AZURE_OPENAI_API_DEPLOYMENT_NAME azureOpenAIApiVersion: process.env.AZURE_OPENAI_API_VERSION, // In Node.js defaults to process.env.AZURE_OPENAI_API_VERSION configuration: { defaultHeaders: { \"x-custom-header\": `SOME_VALUE`, }, },});await llmWithCustomHeaders.invoke(\"Hi there!\");\nThe configuration field also accepts other ClientOptions parameters accepted by the official SDK.Note: The specific header api-key currently cannot be overridden in this manner and will pass through the value from azureOpenAIApiKey.\nIf you are using the deprecated Azure OpenAI SDK with the @langchain/azure-openai package, you can update your code to use the new Azure integration following these steps:\nInstall the new @langchain/openai package and remove the previous @langchain/azure-openai package:\nCopy\n<Npm2Yarn> @langchain/openai</Npm2Yarn>\nCopy\nnpm uninstall @langchain/azure-openai\nUpdate your imports to use the new @[AzureChatOpenAI] class from the @langchain/openai package:\nCopy\nimport { AzureChatOpenAI } from \"@langchain/openai\";\nUpdate your code to use the new @[AzureChatOpenAI] class and pass the required parameters:\nCopy\nconst model = new AzureChatOpenAI({ azureOpenAIApiKey: \"<your_key>\", azureOpenAIApiInstanceName: \"<your_instance_name>\", azureOpenAIApiDeploymentName: \"<your_deployment_name>\", azureOpenAIApiVersion: \"<api_version>\",});\nNotice that the constructor now requires the azureOpenAIApiInstanceName parameter instead of the azureOpenAIEndpoint parameter, and adds the azureOpenAIApiVersion parameter to specify the API version.\nIf you were using Azure Managed Identity, you now need to use the azureADTokenProvider parameter to the constructor instead of credentials, see the Azure Managed Identity section for more details.\nIf you were using environment variables, you now have to set the AZURE_OPENAI_API_INSTANCE_NAME environment variable instead of AZURE_OPENAI_API_ENDPOINT, and add the AZURE_OPENAI_API_VERSION environment variable to specify the API version.", "tokens": 620, "node_type": "child"}
{"id": 275, "chunk_id": "c16acf8a553ac24f5d77e7d3a1c6e493", "parent_id": 258, "url": "", "namespace": "langchain", "title": "oss-javascript-integrations-chat-bedrock", "headers": ["oss-javascript-integrations-chat-bedrock"], "section_index": 0, "chunk_index": 0, "text": "# oss-javascript-integrations-chat-bedrock\n\n> Source: https://docs.langchain.com/oss/javascript/integrations/chat/bedrock\n\nAmazon Bedrock is a fully managed service that offers a choice of high-performing foundation models (FMs) from leading AI companies like AI21 Labs, Anthropic, Cohere, Meta, Stability AI, and Amazon via a single API, along with a broad set of capabilities you need to build generative AI applications with security, privacy, and responsible AI.This will help you getting started with Amazon Bedrock chat models. For detailed documentation of all BedrockChat features and configurations head to the API reference.\nCopy\n<Tip>The newer [`ChatBedrockConverse` chat model is now available via the dedicated `@langchain/aws`](/oss/javascript/integrations/chat/bedrock_converse) integration package. Use [tool calling](/oss/javascript/langchain/tools) with more models with this package.</Tip>\nTo access Bedrock models you\u2019ll need to create an AWS account, set up the Bedrock API service, get an access key ID and secret key, and install the @langchain/community integration package.\nHead to the AWS docs to sign up for AWS and setup your credentials. You\u2019ll also need to turn on model access for your account, which you can do by following these instructions.If you want to get automated tracing of your model calls you can also set your LangSmith API key by uncommenting below:\nThe LangChain BedrockChat integration lives in the @langchain/community package. You\u2019ll also need to install several official AWS packages as peer dependencies:\nYou can also use BedrockChat in web environments such as Edge functions or Cloudflare Workers by omitting the @aws-sdk/credential-provider-node dependency and using the web entrypoint:\nCurrently, only Anthropic, Cohere, and Mistral models are supported with the chat model integration. For foundation models from AI21 or Amazon, see the text generation Bedrock variant.There are a few different ways to authenticate with AWS - the below examples rely on an access key, secret access key and region set in your environment variables:\nconst aiMsg = await llm.invoke([ [ \"system\", \"You are a helpful assistant that translates English to French. Translate the user sentence.\", ], [\"human\", \"I love programming.\"],])aiMsg\nTool calling with Bedrock models works in a similar way to other models, but note that not all Bedrock models support tool calling. Please refer to the AWS model documentation for more information.", "tokens": 352, "node_type": "child"}
{"id": 276, "chunk_id": "c16acf8a553ac24f5d77e7d3a1c6e493", "parent_id": 259, "url": "", "namespace": "langchain", "title": "oss-javascript-integrations-chat-google-generative-ai", "headers": ["oss-javascript-integrations-chat-google-generative-ai"], "section_index": 0, "chunk_index": 0, "text": "# oss-javascript-integrations-chat-google-generative-ai > Source: https://docs.langchain.com/oss/javascript/integrations/chat/google_generative_ai Google AI offers a number of different chat models, including the powerful Gemini series. For information on the latest models, their features, context windows, etc. head to the Google AI docs.This will help you getting started with ChatGoogleGenerativeAIchat models. For detailed documentation of all ChatGoogleGenerativeAI features and configurations head to the API reference. You can access Google\u2019s gemini and gemini-vision models, as well as other generative models in LangChain through ChatGoogleGenerativeAI class in the @langchain/google-genai integration package. Copy <Tip>**You can also access Google's `gemini` family of models via the LangChain VertexAI and VertexAI-web integrations.**Click [here](/oss/javascript/integrations/chat/google_vertex_ai) to read the docs.</Tip> const aiMsg = await llm.invoke([ [ \"system\", \"You are a helpful assistant that translates English to French. Translate the user sentence.\", ], [\"human\", \"I love programming.\"],])aiMsg Gemini models have default safety settings that can be overridden. If you are receiving lots of \u201cSafety Warnings\u201d from your models, you can try tweaking the safety_settings attribute of the model. For example, to turn off safety blocking for dangerous content, you can import enums from the @google/generative-ai package, then construct your LLM as follows: Copy import { ChatGoogleGenerativeAI } from \"@langchain/google-genai\";import { HarmBlockThreshold, HarmCategory } from \"@google/generative-ai\";const llmWithSafetySettings = new ChatGoogleGenerativeAI({ model: \"gemini-1.5-pro\", temperature: 0, safetySettings: [ { category: HarmCategory.HARM_CATEGORY_HARASSMENT, threshold: HarmBlockThreshold.BLOCK_LOW_AND_ABOVE, }, ], // other params...}); Tool calling with Google AI is mostly the same as tool calling with other models, but has a few restrictions on schema.The Google AI API does not allow tool schemas to contain an object with unknown properties. For example, the following Zod schemas will throw an error:const invalidSchema = z.object({ properties: z.record(z.unknown()) });andconst invalidSchema2 = z.record(z.unknown());Instead, you should explicitly define the properties of the object field. Here\u2019s an example: Copy import { tool } from \"@langchain/core/tools\";import { ChatGoogleGenerativeAI } from \"@langchain/google-genai\";import * as z from \"zod\";// Define your toolconst fakeBrowserTool = tool((_) => { return \"The search result is xyz...\"}, { name: \"browser_tool\", description: \"Useful for when you need to find something on the web or summarize a webpage.\", schema: z.object({ url: z.string().describe(\"The URL of the webpage to search.\"), query: z.string().optional().describe(\"An optional search query to use.\"), }),})const llmWithTool = new ChatGoogleGenerativeAI({ model: \"gemini-pro\",}).bindTools([fakeBrowserTool]) // Bind your tools to the modelconst toolRes = await llmWithTool.invoke([ [ \"human\", \"Search the web and tell me what the weather will be like tonight in new york. use a popular weather website\", ],]);console.log(toolRes.tool_calls); Copy [ { name: 'browser_tool', args: { url: 'https://www.weather.com', query: 'weather tonight in new york' }, type: 'tool_call' }] Google also offers a built in search tool which you can use to ground content generation in real-world information. Here\u2019s an example of how to use it: Copy import { DynamicRetrievalMode, GoogleSearchRetrievalTool } from \"@google/generative-ai\";import { ChatGoogleGenerativeAI } from \"@langchain/google-genai\";const searchRetrievalTool: GoogleSearchRetrievalTool = { googleSearchRetrieval: { dynamicRetrievalConfig: { mode: DynamicRetrievalMode.MODE_DYNAMIC, dynamicThreshold: 0.7, // default is 0.7 } }};const searchRetrievalModel = new ChatGoogleGenerativeAI({ model: \"gemini-1.5-pro\", temperature: 0, maxRetries: 0,}).bindTools([searchRetrievalTool]);const searchRetrievalResult = await searchRetrievalModel.invoke(\"Who won the 2024 MLB World Series?\");console.log(searchRetrievalResult.content); Copy The Los Angeles Dodgers won the 2024 World Series, defeating the New York Yankees in Game 5 on October 30, 2024, by a score of 7-6. This victory marks the Dodgers' eighth World Series title and their first in a full season since 1988. They achieved this win by overcoming a 5-0 deficit, making them the first team in World Series history to win a clinching game after being behind by such a margin. The Dodgers also became the first team in MLB postseason history to overcome a five-run deficit, fall behind again, and still win. Walker Buehler earned the save in the final game, securing the championship for the Dodgers. The response also includes metadata about the search result: Google Generative AI also supports code execution. Using the built in CodeExecutionTool, you can make the model generate code, execute it, and use the results in a final completion: Copy import { CodeExecutionTool } from \"@google/generative-ai\";import { ChatGoogleGenerativeAI } from \"@langchain/google-genai\";const codeExecutionTool: CodeExecutionTool = { codeExecution: {}, // Simply pass an empty object to enable it.};const codeExecutionModel = new ChatGoogleGenerativeAI({ model: \"gemini-1.5-pro\", temperature: 0, maxRetries: 0,}).bindTools([codeExecutionTool]);const codeExecutionResult = await codeExecutionModel.invoke(\"Use code execution to find the sum of the first and last 3 numbers in the following list: [1, 2, 3, 72638, 8, 727, 4, 5, 6]\");console.dir(codeExecutionResult.content, { depth: null }); Copy [ { type: 'text', text: \"Here's how to find the sum of the first and last three numbers in the given list using Python:\\n\" + '\\n' }, { type: 'executableCode', executableCode: { language: 'PYTHON', code: '\\n' + 'my_list = [1, 2, 3, 72638, 8, 727, 4, 5, 6]\\n' + '\\n' + 'first_three_sum = sum(my_list[:3])\\n' + 'last_three_sum = sum(my_list[-3:])\\n' + 'total_sum = first_three_sum + last_three_sum\\n' + '\\n' + 'print(f\"{first_three_sum=}\")\\n' + 'print(f\"{last_three_sum=}\")\\n' + 'print(f\"{total_sum=}\")\\n' + '\\n' } }, { type: 'codeExecutionResult', codeExecutionResult: { outcome: 'OUTCOME_OK', output: 'first_three_sum=6\\nlast_three_sum=15\\ntotal_sum=21\\n' } }, { type: 'text', text: 'Therefore, the sum of the first three numbers (1, 2, 3) is 6, the sum of the last three numbers (4, 5, 6) is 15, and their total sum is 21.\\n' }] You can also pass this generation back to the model as chat history: Copy const codeExecutionExplanation = await codeExecutionModel.invoke([ codeExecutionResult, { role: \"user\", content: \"Please explain the question I asked, the code you wrote, and the answer you got.\", }])console.log(codeExecutionExplanation.content); Copy You asked for the sum of the first three and the last three numbers in the list `[1, 2, 3, 72638, 8, 727, 4, 5, 6]`.Here's a breakdown of the code:1. **`my_list = [1, 2, 3, 72638, 8, 727, 4, 5, 6]`**: This line defines the list of numbers you provided.2. **`first_three_sum = sum(my_list[:3])`**: This calculates the sum of the first three numbers. `my_list[:3]` is a slice of the list that takes elements from the beginning up to (but not including) the index 3. So, it takes elements at indices 0, 1, and 2, which are 1, 2, and 3. The `sum()` function then adds these numbers together.3. **`last_three_sum = sum(my_list[-3:])`**: This calculates the sum of the last three numbers. `my_list[-3:]`", "tokens": 1000, "node_type": "child"}
{"id": 277, "chunk_id": "9b0fd3b850698317bef57fa6e71a38ba", "parent_id": 259, "url": "", "namespace": "langchain", "title": "oss-javascript-integrations-chat-google-generative-ai", "headers": ["oss-javascript-integrations-chat-google-generative-ai"], "section_index": 0, "chunk_index": 1, "text": "history: Copy const codeExecutionExplanation = await codeExecutionModel.invoke([ codeExecutionResult, { role: \"user\", content: \"Please explain the question I asked, the code you wrote, and the answer you got.\", }])console.log(codeExecutionExplanation.content); Copy You asked for the sum of the first three and the last three numbers in the list `[1, 2, 3, 72638, 8, 727, 4, 5, 6]`.Here's a breakdown of the code:1. **`my_list = [1, 2, 3, 72638, 8, 727, 4, 5, 6]`**: This line defines the list of numbers you provided.2. **`first_three_sum = sum(my_list[:3])`**: This calculates the sum of the first three numbers. `my_list[:3]` is a slice of the list that takes elements from the beginning up to (but not including) the index 3. So, it takes elements at indices 0, 1, and 2, which are 1, 2, and 3. The `sum()` function then adds these numbers together.3. **`last_three_sum = sum(my_list[-3:])`**: This calculates the sum of the last three numbers. `my_list[-3:]` is a slice that takes elements starting from the third element from the end and goes to the end of the list. So it takes elements at indices -3, -2, and -1 which correspond to 4, 5, and 6. The `sum()` function adds these numbers.4. **`total_sum = first_three_sum + last_three_sum`**: This adds the sum of the first three numbers and the sum of the last three numbers to get the final result.5. **`print(f\"{first_three_sum=}\")`**, **`print(f\"{last_three_sum=}\")`**, and **`print(f\"{total_sum=}\")`**: These lines print the calculated sums in a clear and readable format.The output of the code was:* `first_three_sum=6`* `last_three_sum=15`* `total_sum=21`Therefore, the answer to your question is 21. Context caching allows you to pass some content to the model once, cache the input tokens, and then refer to the cached tokens for subsequent requests to reduce cost. You can create a CachedContent object using GoogleAICacheManager class and then pass the CachedContent object to your ChatGoogleGenerativeAIModel with enableCachedContent() method. Copy import { ChatGoogleGenerativeAI } from \"@langchain/google-genai\";import { GoogleAICacheManager, GoogleAIFileManager,} from \"@google/generative-ai/server\";const fileManager = new GoogleAIFileManager(process.env.GOOGLE_API_KEY);const cacheManager = new GoogleAICacheManager(process.env.GOOGLE_API_KEY);// uploads file for cachingconst pathToVideoFile = \"/path/to/video/file\";const displayName = \"example-video\";const fileResult = await fileManager.uploadFile(pathToVideoFile, { displayName, mimeType: \"video/mp4\",});// creates cached content AFTER uploading is finishedconst cachedContent = await cacheManager.create({ model: \"models/gemini-1.5-flash-001\", displayName: displayName, systemInstruction: \"You are an expert video analyzer, and your job is to answer \" + \"the user's query based on the video file you have access to.\", contents: [ { role: \"user\", parts: [ { fileData: { mimeType: fileResult.file.mimeType, fileUri: fileResult.file.uri, }, }, ], }, ], ttlSeconds: 300,});// passes cached video to modelconst model = new ChatGoogleGenerativeAI({});model.useCachedContent(cachedContent);// invokes model with cached videoawait model.invoke(\"Summarize the video\"); Note Context caching supports both Gemini 1.5 Pro and Gemini 1.5 Flash. Context caching is only available for stable models with fixed versions (for example, gemini-1.5-pro-001). You must include the version postfix (for example, the -001 in gemini-1.5-pro-001). The minimum input token count for context caching is 32,768, and the maximum is the same as the maximum for the given model. As of the time this doc was written (2023/12/12), Gemini has some restrictions on the types and structure of prompts it accepts. Specifically: When providing multimodal (image) inputs, you are restricted to at most 1 message of \u201chuman\u201d (user) type. You cannot pass multiple messages (though the single human message may have multiple content entries) System messages are not natively supported, and will be merged with the first human message if present. For regular chat conversations, messages must follow the human/ai/human/ai alternating pattern. You may not provide 2 AI or human messages in sequence. Message may be blocked if they violate the safety checks of the LLM. In this case, the model will return an empty response.", "tokens": 599, "node_type": "child"}
{"id": 278, "chunk_id": "c16acf8a553ac24f5d77e7d3a1c6e493", "parent_id": 260, "url": "", "namespace": "langchain", "title": "oss-javascript-integrations-chat-index", "headers": ["oss-javascript-integrations-chat-index"], "section_index": 0, "chunk_index": 0, "text": "# oss-javascript-integrations-chat-index\n\n> Source: https://docs.langchain.com/oss/javascript/integrations/chat/index\n\nInstall and use\nGroq\nGroq\nInstall:Add environment variables:Instantiate the model:\nOpenAI\nOpenAI\nInstall:Add environment variables:Instantiate the model:\nAnthropic\nAnthropic\nInstall:Add environment variables:Instantiate the model:\nGoogle Gemini\nGoogle Gemini\nInstall:Add environment variables:Instantiate the model:\nFireworksAI\nFireworksAI\nInstall:Add environment variables:Instantiate the model:\nMistralAI\nMistralAI\nInstall:Add environment variables:Instantiate the model:\nVertexAI\nVertexAI\nInstall:Add environment variables:Instantiate the model:\nFeatured providers\n| Model | Stream | JSON mode | Tool Calling | withStructuredOutput() | Multimodal |\n|---|---|---|---|---|---|\n| BedrockChat | \u2705 | \u274c | \ud83d\udfe1 (Bedrock Anthropic only) | \ud83d\udfe1 (Bedrock Anthropic only) | \ud83d\udfe1 (Bedrock Anthropic only) |\n| ChatBedrockConverse | \u2705 | \u274c | \u2705 | \u2705 | \u2705 |\n| ChatAnthropic | \u2705 | \u274c | \u2705 | \u2705 | \u2705 |\n| ChatCloudflareWorkersAI | \u2705 | \u274c | \u274c | \u274c | \u274c |\n| ChatCohere | \u2705 | \u274c | \u2705 | \u2705 | \u2705 |\n| ChatFireworks | \u2705 | \u2705 | \u2705 | \u2705 | \u2705 |\n| ChatGoogleGenerativeAI | \u2705 | \u274c | \u2705 | \u2705 | \u2705 |\n| ChatVertexAI | \u2705 | \u274c | \u2705 | \u2705 | \u2705 |\n| ChatGroq | \u2705 | \u2705 | \u2705 | \u2705 | \u2705 |\n| ChatMistralAI | \u2705 | \u2705 | \u2705 | \u2705 | \u2705 |\n| ChatOllama | \u2705 | \u2705 | \u2705 | \u2705 | \u2705 |\n| ChatOpenAI | \u2705 | \u2705 | \u2705 | \u2705 | \u2705 |\n| ChatTogetherAI | \u2705 | \u2705 | \u2705 | \u2705 | \u2705 |\n| ChatXAI | \u2705 | \u2705 | \u2705 | \u2705 | \u274c |", "tokens": 268, "node_type": "child"}
{"id": 279, "chunk_id": "c16acf8a553ac24f5d77e7d3a1c6e493", "parent_id": 261, "url": "", "namespace": "langchain", "title": "oss-javascript-integrations-chat-openai", "headers": ["oss-javascript-integrations-chat-openai"], "section_index": 0, "chunk_index": 0, "text": "# oss-javascript-integrations-chat-openai > Source: https://docs.langchain.com/oss/javascript/integrations/chat/openai Overview Integration details | Class | Package | Local | Serializable | PY support | Downloads | Version | |---|---|---|---|---|---|---| | ChatOpenAI | @langchain/openai | \u274c | \u2705 | \u2705 | Model features See the links in the table headers below for guides on how to use specific features.| Tool calling | Structured output | JSON mode | Image input | Audio input | Video input | Token-level streaming | Token usage | Logprobs | |---|---|---|---|---|---|---|---|---| | \u2705 | \u2705 | \u2705 | \u2705 | \u274c | \u274c | \u2705 | \u2705 | \u2705 | Setup To access OpenAI chat models you\u2019ll need to create an OpenAI account, get an API key, and install the@langchain/openai integration package. Credentials Head to OpenAI\u2019s website to sign up for OpenAI and generate an API key. Once you\u2019ve done this set theOPENAI_API_KEY environment variable: Installation The LangChain @[ChatOpenAI ] integration lives in the @langchain/openai package: Instantiation Now we can instantiate our model object and generate chat completions:Invocation Custom URLs You can customize the base URL the SDK sends requests to by passing aconfiguration parameter like this: configuration field also accepts other ClientOptions parameters accepted by the official SDK. If you are hosting on Azure OpenAI, see the dedicated page instead. Custom headers You can specify custom headers in the sameconfiguration field: Disabling streaming usage metadata Some proxies or third-party providers present largely the same API interface as OpenAI, but don\u2019t support the more recently addedstream_options parameter to return streaming usage. You can use @[ChatOpenAI ] to access these providers by disabling streaming usage like this: Calling fine-tuned models You can call fine-tuned OpenAI models by passing in your correspondingmodelName parameter. This generally takes the form of ft:{OPENAI_MODEL_NAME}:{ORG_NAME}::{MODEL_ID} . For example: Generation metadata If you need additional information like logprobs or token usage, these will be returned directly in theinvoke response within the response_metadata field on the message. Requires @langchain/core version >=0.1.48.Custom Tools Custom tools support tools with arbitrary string inputs. They can be particularly useful when you expect your string arguments to be long or complex. If you use a model that supports custom tools, you can use the @[ChatOpenAI ] class and the customTool function to create a custom tool. strict: true As of Aug 6, 2024, OpenAI supports a strict argument when calling tools that will enforce that the tool argument schema is respected by the model. See more. Requires @langchain/openai >= 0.2.6 If strict: true the tool definition will also be validated, and a subset of JSON schema are accepted. Crucially, schema cannot have optional args (those with default values). Read the full docs on what types of schema are supported.strict: true argument to .bindTools will pass the param through to all tool definitions: Structured output We can also passstrict: true to the .withStructuredOutput() . Here\u2019s an example: Responses API CompatibilityThe below points apply to @langchain/openai>=0.4.5-rc.0 .ChatOpenAI will route to the Responses API if one of these features is used. You can also specify useResponsesApi: true when instantiating ChatOpenAI . Built-in tools Equipping @[ChatOpenAI ] with built-in tools will ground its responses with outside information, such as via context in files or the web. The AIMessage generated from the model will include information about the built-in tool invocation. Web search To trigger a web search, pass{\"type\": \"web_search_preview\"} to the model as you would another tool. You can also pass built-in tools as invocation params: File search To trigger a file search, pass a file search tool to the model as you would another tool. You will need to populate an OpenAI-managed vector store and include the vector store ID in the tool definition. See OpenAI documentation for more details.Computer Use ChatOpenAI supports thecomputer-use-preview model, which is a specialized model for the built-in computer use tool. To enable, pass a computer use tool as you would pass another tool. Currently tool outputs for computer use are present in AIMessage.additional_kwargs.tool_outputs . To reply to the computer use tool call, you need to set additional_kwargs.type: \"computer_call_output\" while creating a corresponding ToolMessage . See OpenAI documentation for more details. Code interpreter ChatOpenAI allows you to use the built-in code interpreter tool to support the sandboxed generation and execution of code.Remote MCP ChatOpenAI supports the built-in remote MCP tool that allows for model-generated calls to MCP servers to happen on OpenAI servers.MCP ApprovalsWhen instructed, OpenAI will request approval before making calls to a remote MCP server.In the above command, we instructed the model to never require approval. We can also configure the model to always request approval, or to always request approval for specific tools:With this configuration, responses can contain tool outputs typed as mcp_approval_request . To submit approvals for an approval request, you can structure it into a content block in a followup message:Image Generation ChatOpenAI allows you to bring the built-in image generation tool to create images as apart of multi-turn conversations through the responses API.Reasoning models o1 , the default method for withStructuredOutput is OpenAI\u2019s built-in method for structured output (equivalent to passing method: \"jsonSchema\" as an option into withStructuredOutput ). JSON schema mostly works the same as other models, but with one important caveat: when defining schema, z.optional() is not respected, and you should instead use z.nullable() . Here\u2019s an example: z.nullable() : Prompt caching Newer OpenAI models will automatically cache parts of your prompt if your inputs are above a certain size (1024 tokens at the time of writing) in order to reduce costs for use-cases that require long context. Note: The number of tokens cached for a given query is not yet standardized inAIMessage.usage_metadata , and is instead contained in the AIMessage.response_metadata field. Here\u2019s an example Predicted output Some OpenAI models (such as theirgpt-4o and gpt-4o-mini series) support Predicted Outputs, which allow you to pass in a known portion of the LLM\u2019s expected output ahead of time to reduce latency. This is useful for cases such as editing text or code, where only a small part of the model\u2019s output will change. Here\u2019s an example:", "tokens": 1000, "node_type": "child"}
{"id": 280, "chunk_id": "9b0fd3b850698317bef57fa6e71a38ba", "parent_id": 261, "url": "", "namespace": "langchain", "title": "oss-javascript-integrations-chat-openai", "headers": ["oss-javascript-integrations-chat-openai"], "section_index": 0, "chunk_index": 1, "text": "other models, but with one important caveat: when defining schema, z.optional() is not respected, and you should instead use z.nullable() . Here\u2019s an example: z.nullable() : Prompt caching Newer OpenAI models will automatically cache parts of your prompt if your inputs are above a certain size (1024 tokens at the time of writing) in order to reduce costs for use-cases that require long context. Note: The number of tokens cached for a given query is not yet standardized inAIMessage.usage_metadata , and is instead contained in the AIMessage.response_metadata field. Here\u2019s an example Predicted output Some OpenAI models (such as theirgpt-4o and gpt-4o-mini series) support Predicted Outputs, which allow you to pass in a known portion of the LLM\u2019s expected output ahead of time to reduce latency. This is useful for cases such as editing text or code, where only a small part of the model\u2019s output will change. Here\u2019s an example: Audio output Some OpenAI models (such asgpt-4o-audio-preview ) support generating audio output. This example shows how to use that feature: data field. We are also provided an expires_at date field. This field represents the date the audio response will no longer be accessible on the server for use in multi-turn conversations. Streaming Audio Output OpenAI also supports streaming audio output. Here\u2019s an example:Audio input These models also support passing audio as input. For this, you must specifyinput_audio fields as seen below:", "tokens": 231, "node_type": "child"}
{"id": 281, "chunk_id": "c16acf8a553ac24f5d77e7d3a1c6e493", "parent_id": 262, "url": "", "namespace": "langchain", "title": "oss-javascript-integrations-document-loaders-index", "headers": ["oss-javascript-integrations-document-loaders-index"], "section_index": 0, "chunk_index": 0, "text": "# oss-javascript-integrations-document-loaders-index\n\n> Source: https://docs.langchain.com/oss/javascript/integrations/document_loaders/index\n\nInterface\nEach document loader may define its own parameters, but they share a common API:.load()\n: Loads all documents at once..loadAndSplit()\n: Loads all documents at once and splits them into smaller documents.\nBy category\nLangChain.js categorizes document loaders in two different ways:- File loaders, which load data into LangChain formats from your local filesystem.\n- Web loaders, which load data from remote sources.\nFile loaders\nPDFs\n| Document Loader | Description | Package/API |\n|---|---|---|\n| PDFLoader | Load and parse PDF files using pdf-parse | Package |\nCommon File Types\n| Document Loader | Description | Package/API |\n|---|---|---|\n| CSV | Load data from CSV files with configurable column extraction | Package |\n| JSON | Load JSON files using JSON pointer to target specific keys | Package |\n| JSONLines | Load data from JSONLines/JSONL files | Package |\n| Text | Load plain text files | Package |\n| DOCX | Load Microsoft Word documents (.docx and .doc formats) | Package |\n| EPUB | Load EPUB files with optional chapter splitting | Package |\n| PPTX | Load PowerPoint presentations | Package |\n| Subtitles | Load subtitle files (.srt format) | Package |\nSpecialized File Loaders\n| Document Loader | Description | Package/API |\n|---|---|---|\n| DirectoryLoader | Load all files from a directory with custom loader mappings | Package |\n| UnstructuredLoader | Load multiple file types using Unstructured API | API |\n| MultiFileLoader | Load data from multiple individual file paths | Package |\n| ChatGPT | Load ChatGPT conversation exports | Package |\n| Notion Markdown | Load Notion pages exported as Markdown | Package |\n| OpenAI Whisper Audio | Transcribe audio files using OpenAI Whisper API | API |\nWeb loaders\nWebpages\n| Document Loader | Description | Web Support | Package/API |\n|---|---|---|---|\n| Cheerio | Load webpages using Cheerio (lightweight, no JavaScript execution) | \u2705 | Package |\n| Playwright | Load dynamic webpages using Playwright (supports JavaScript rendering) | \u274c | Package |\n| Puppeteer | Load dynamic webpages using Puppeteer (headless Chrome) | \u274c | Package |\n| FireCrawl | Crawl and convert websites into LLM-ready markdown | \u2705 | API |\n| Spider | Fast crawler that converts websites into HTML, markdown, or text | \u2705 | API |\n| RecursiveUrlLoader | Recursively load webpages following links | \u274c | Package |\n| Sitemap | Load all pages from a sitemap.xml | \u2705 | Package |\n| Browserbase | Load webpages using managed headless browsers with stealth mode | \u2705 | API |\n| WebPDFLoader | Load PDF files in web environments | \u2705 | Package |\nCloud Providers\n| Document Loader | Description | Web Support | Package/API |\n|---|---|---|---|\n| S3 | Load files from AWS S3 buckets | \u274c | Package |\n| Azure Blob Storage Container | Load all files from Azure Blob Storage container | \u274c | Package |\n| Azure Blob Storage File | Load individual files from Azure Blob Storage | \u274c | Package |\n| Google Cloud Storage | Load files from Google Cloud Storage buckets | \u274c | Package |\n| Google Cloud SQL for PostgreSQL | Load documents from Cloud SQL PostgreSQL databases | \u2705 | Package |\nProductivity Tools\n| Document Loader | Description | Web Support | Package/API |\n|---|---|---|---|\n| Notion API | Load Notion pages and databases via API | \u2705 | API |\n| Figma | Load Figma file data | \u2705 | API |\n| Confluence | Load pages from Confluence spaces | \u274c | API |\n| GitHub | Load files from GitHub repositories | \u2705 | API |\n| GitBook | Load GitBook documentation pages | \u2705 | Package |\n| Jira | Load issues from Jira projects | \u274c | API |\n| Airtable | Load records from Airtable bases | \u2705 | API |\n| Taskade | Load Taskade project data | \u2705 | API |\nSearch & Data APIs\n| Document Loader | Description | Web Support | Package/API |\n|---|---|---|---|\n| SearchAPI | Load web search results from SearchAPI (Google, YouTube, etc.) | \u2705 | API |\n| SerpAPI | Load web search results from SerpAPI | \u2705 | API |\n| Apify Dataset | Load scraped data from Apify platform | \u2705 | API |\nAudio & Video\n| Document Loader | Description | Web Support | Package/API |\n|---|---|---|---|\n| YouTube | Load YouTube video transcripts | \u2705 | Package |\n| AssemblyAI | Transcribe audio and video files using AssemblyAI API | \u2705 | API |\n| Sonix | Transcribe audio files using Sonix API | \u274c | API |\nOther\n| Document Loader | Description | Web Support | Package/API |\n|---|---|---|---|\n| Couchbase | Load documents from Couchbase database using SQL++ queries | \u2705 | Package |\n| LangSmith | Load datasets and traces from LangSmith | \u2705 | API |\n| Hacker News | Load Hacker News threads and comments | \u2705 | Package |\n| IMSDB | Load movie scripts from Internet Movie Script Database | \u2705 | Package |\n| College Confidential | Load college information from College Confidential | \u2705 | Package |\n| Blockchain Data | Load blockchain data (NFTs, transactions) via Sort.xyz API | \u2705 | API |", "tokens": 894, "node_type": "child"}
{"id": 282, "chunk_id": "c16acf8a553ac24f5d77e7d3a1c6e493", "parent_id": 265, "url": "", "namespace": "langchain", "title": "oss-javascript-integrations-providers-all-providers", "headers": ["oss-javascript-integrations-providers-all-providers"], "section_index": 0, "chunk_index": 0, "text": "# oss-javascript-integrations-providers-all-providers > Source: https://docs.langchain.com/oss/javascript/integrations/providers/all_providers Top providers Anthropic Integrate with Anthropic\u2019s Claude models for advanced reasoning and conversation. AWS Access AWS services and foundation models through comprehensive integrations. Integrate with Google\u2019s AI services including Gemini and Vertex AI. Microsoft Connect to Microsoft Azure services and AI platforms. OpenAI Build with GPT models and OpenAI\u2019s comprehensive AI platform. Chat Models Alibaba Tongyi Alibaba\u2019s Tongyi language model for Chinese and multilingual applications. Anthropic Claude models for advanced conversational AI and reasoning. Arcjet Security-focused AI chat integration with built-in protections. Azure OpenAI OpenAI models through Microsoft Azure\u2019s enterprise platform. Baidu Qianfan Baidu\u2019s Qianfan platform for Chinese language AI models. Baidu Wenxin Baidu\u2019s Wenxin (ERNIE) models for natural language processing. Amazon Bedrock Access foundation models through Amazon Bedrock\u2019s managed service. Bedrock Converse Unified Bedrock Converse API for multiple foundation models. Cerebras Ultra-fast inference with Cerebras Systems\u2019 AI processors. Cloudflare Workers AI Run AI models on Cloudflare\u2019s edge computing platform. Cohere Cohere\u2019s language models for text generation and understanding. Deep Infra Access open-source models through Deep Infra\u2019s cloud platform. DeepSeek DeepSeek\u2019s advanced reasoning and coding models. Fake LLM Mock chat model for testing and development purposes. Fireworks High-performance inference for open-source models. Friendli Optimized inference engine for efficient model serving. Google Generative AI Google\u2019s Gemini models and generative AI capabilities. Google Vertex AI Enterprise AI platform with Google Cloud\u2019s Vertex AI. Groq Ultra-fast inference with Groq\u2019s specialized hardware. IBM IBM Watson AI models and enterprise solutions. Llama.cpp Run local Llama models with llama.cpp backend. Minimax Minimax\u2019s conversational AI models and services. Mistral Mistral\u2019s efficient and powerful language models. Moonshot Moonshot\u2019s AI models for various language tasks. Neural Internet Bittensor Decentralized AI network through Bittensor protocol. Novita Novita\u2019s AI models and cloud computing platform. Ollama Run local models with Ollama\u2019s lightweight inference engine. Ollama Functions Function calling capabilities with Ollama models. OpenAI GPT models and OpenAI\u2019s comprehensive chat capabilities. Perplexity Perplexity\u2019s search-augmented language models. PremAI PremAI\u2019s platform for AI model deployment and management. PromptLayer OpenAI OpenAI integration with PromptLayer\u2019s observability features. Tencent Hunyuan Tencent\u2019s Hunyuan models for Chinese language processing. Together AI Open-source models through Together AI\u2019s cloud platform. WebLLM Run language models directly in web browsers. xAI xAI\u2019s Grok models for conversational AI. Yandex Yandex\u2019s AI models and language processing services. ZhipuAI ZhipuAI\u2019s ChatGLM and other Chinese language models. LLMs AI21 AI21 Labs\u2019 Jurassic models for text generation. Aleph Alpha European AI company\u2019s multilingual language models. Arcjet Security-focused LLM integration with built-in protections. AWS SageMaker Deploy models on Amazon SageMaker\u2019s ML platform. Azure OpenAI OpenAI models through Microsoft Azure\u2019s enterprise platform. Amazon Bedrock Foundation models through Amazon Bedrock service. Chrome AI Browser-based AI using Chrome\u2019s built-in capabilities. Cloudflare Workers AI AI models on Cloudflare\u2019s edge computing platform. Cohere Cohere\u2019s language models for various NLP tasks. Deep Infra Open-source models through Deep Infra\u2019s infrastructure. Fireworks Fast inference for open-source language models. Friendli Optimized serving for efficient model inference. Google Vertex AI Google Cloud\u2019s enterprise AI and ML platform. Gradient AI Private AI model training and deployment platform. Hugging Face Access thousands of models via Hugging Face Inference API. IBM IBM Watson AI and language model services. JigsawStack JigsawStack\u2019s AI infrastructure and model services. LayerUp Security Security-enhanced LLM integration with monitoring. Llama.cpp Run Llama models locally with C++ implementation. Mistral Mistral\u2019s open-source and commercial language models. Neural Internet Bittensor Decentralized AI through Bittensor\u2019s peer-to-peer network. Ollama Local model serving with Ollama\u2019s simple interface. OpenAI GPT models and OpenAI\u2019s language model APIs. PromptLayer OpenAI OpenAI with PromptLayer\u2019s logging and observability. Raycast AI integration for Raycast productivity tool. Replicate Run open-source models through Replicate\u2019s cloud platform. Together AI Fast inference for open-source models on Together\u2019s platform. Writer Writer\u2019s enterprise AI platform for content generation. Yandex Yandex\u2019s language models and AI services. Text Embedding Models Alibaba Tongyi Alibaba\u2019s embedding models for multilingual text representation. Azure OpenAI OpenAI embeddings through Microsoft Azure platform. Baidu Qianfan Baidu\u2019s text embedding models for Chinese content. Amazon Bedrock Foundation model embeddings through Amazon Bedrock. ByteDance Doubao ByteDance\u2019s embedding models for content understanding. Cloudflare AI Text embeddings on Cloudflare\u2019s edge AI platform. Cohere Cohere\u2019s multilingual embedding models. DeepInfra Open-source embedding models via DeepInfra. Fireworks Fast embedding inference through Fireworks platform. Google Generative AI Google\u2019s embedding models for text representation. Google Vertex AI Enterprise embedding models through Vertex AI. Gradient AI Private embedding models with Gradient AI platform. Hugging Face Thousands of embedding models via Hugging Face. IBM IBM Watson embedding models and AI services. Jina Jina\u2019s neural search and embedding models. Llama.cpp Local embedding generation with llama.cpp. Minimax Minimax\u2019s text embedding and representation models. Mistral Mistral\u2019s efficient embedding models. MixedBread AI High-quality multilingual embedding models. Nomic Nomic\u2019s open-source embedding models. Ollama Local embedding models through Ollama. OpenAI OpenAI\u2019s text-embedding models for semantic search. Pinecone Pinecone\u2019s embedding models and vector database. PremAI PremAI\u2019s embedding models and AI platform. Tencent Hunyuan Tencent\u2019s embedding models for Chinese text. TensorFlow TensorFlow-based embedding models and inference. Together AI Open-source embedding models on Together platform. Transformers Local transformer-based embedding models. Voyage AI Voyage AI\u2019s domain-specific embedding models. ZhipuAI ZhipuAI\u2019s Chinese language embedding models. Vector Stores AnalyticDB Alibaba Cloud\u2019s AnalyticDB for vector storage and search. AstraDB DataStax Astra DB vector database for scalable storage. Azion EdgeSQL Edge-based vector storage with Azion\u2019s EdgeSQL. Azure AI Search Microsoft Azure\u2019s AI-powered search and vector storage. Azure Cosmos DB MongoDB Vector search in Azure Cosmos DB with MongoDB API. Azure Cosmos DB NoSQL Vector storage in Azure Cosmos DB NoSQL API. Cassandra Apache Cassandra vector search capabilities. Chroma Open-source embedding database for AI applications. ClickHouse Fast columnar database with vector search support. CloseVector High-performance vector database for similarity search. Cloudflare Vectorize Serverless vector database on Cloudflare\u2019s edge. Convex Full-stack platform with integrated vector storage. Couchbase NoSQL database with vector search capabilities. Elasticsearch Distributed search engine with vector search support. Faiss Facebook\u2019s library for efficient similarity search. Google Cloud SQL PostgreSQL PostgreSQL with vector extensions on Google Cloud. Google Vertex AI Vector search through Google Vertex AI platform. SAP HANA Vector Enterprise vector database with SAP HANA. Hnswlib", "tokens": 1000, "node_type": "child"}
{"id": 283, "chunk_id": "9b0fd3b850698317bef57fa6e71a38ba", "parent_id": 265, "url": "", "namespace": "langchain", "title": "oss-javascript-integrations-providers-all-providers", "headers": ["oss-javascript-integrations-providers-all-providers"], "section_index": 0, "chunk_index": 1, "text": "and search. AstraDB DataStax Astra DB vector database for scalable storage. Azion EdgeSQL Edge-based vector storage with Azion\u2019s EdgeSQL. Azure AI Search Microsoft Azure\u2019s AI-powered search and vector storage. Azure Cosmos DB MongoDB Vector search in Azure Cosmos DB with MongoDB API. Azure Cosmos DB NoSQL Vector storage in Azure Cosmos DB NoSQL API. Cassandra Apache Cassandra vector search capabilities. Chroma Open-source embedding database for AI applications. ClickHouse Fast columnar database with vector search support. CloseVector High-performance vector database for similarity search. Cloudflare Vectorize Serverless vector database on Cloudflare\u2019s edge. Convex Full-stack platform with integrated vector storage. Couchbase NoSQL database with vector search capabilities. Elasticsearch Distributed search engine with vector search support. Faiss Facebook\u2019s library for efficient similarity search. Google Cloud SQL PostgreSQL PostgreSQL with vector extensions on Google Cloud. Google Vertex AI Vector search through Google Vertex AI platform. SAP HANA Vector Enterprise vector database with SAP HANA. Hnswlib Fast approximate nearest neighbor search library. LanceDB Developer-friendly embedded vector database. LibSQL SQLite-compatible database with vector extensions. MariaDB Open-source database with vector search capabilities. Memory Vector Store In-memory vector storage for development and testing. Milvus Open-source vector database for AI applications. Momento Vector Index Serverless vector indexing with Momento\u2019s platform. MongoDB Atlas Vector search in MongoDB Atlas cloud database. MyScale SQL-compatible vector database for analytics. Neo4j Vector Graph database with integrated vector search. Neon Serverless PostgreSQL with vector extensions. OpenSearch Open-source search engine with vector capabilities. PGVector PostgreSQL extension for vector similarity search. Pinecone Managed vector database for machine learning applications. Prisma Type-safe database client with vector support. Qdrant Open-source vector similarity search engine. Redis In-memory database with vector search capabilities. Rockset Real-time analytics database with vector search. SingleStore Distributed database with built-in vector functions. Supabase Open-source Firebase alternative with vector support. Tigris Developer-focused database with vector search. Turbopuffer High-performance vector database for embeddings. TypeORM TypeScript ORM with vector database support. Typesense Open-source search engine with vector capabilities. Upstash Vector Serverless vector database with Redis compatibility. USearch Smaller and faster single-file vector search engine. Vectara Neural search platform with built-in understanding. Vercel Postgres PostgreSQL database with vector extensions on Vercel. Voy WebAssembly-based vector database for browsers. Weaviate Open-source vector database with GraphQL API. Xata Serverless database with built-in vector search. Zep Cloud Long-term memory for AI assistants in the cloud. Zep Long-term memory for AI assistants and agents. Document loaders File Loaders ChatGPT Load and parse ChatGPT conversation exports. CSV Load data from CSV files with customizable parsing. Directory Recursively load documents from filesystem directories. DOCX Extract text and metadata from Microsoft Word documents. EPUB Load and parse EPUB e-book files. JSON Load and parse JSON files with flexible structure handling. JSON Lines Load newline-delimited JSON files. Multi-File Load multiple files of different types simultaneously. Notion Markdown Load Notion pages exported as Markdown. OpenAI Whisper Audio Transcribe audio files using OpenAI\u2019s Whisper model. Extract text from PDF documents. PPTX Load Microsoft PowerPoint presentations. Subtitles Load subtitle files (SRT, VTT formats). Text Load plain text files with encoding detection. Unstructured Load various file formats using Unstructured.io. Web Loaders Airtable Load records from Airtable bases. Apify Dataset Load data from Apify web scraping datasets. AssemblyAI Audio Transcribe audio using AssemblyAI\u2019s API. Azure Blob Storage Container Load files from Azure Blob Storage containers. Azure Blob Storage File Load individual files from Azure Blob Storage. Browserbase Load web content using Browserbase\u2019s cloud browsers. College Confidential Scrape College Confidential forum content. Confluence Load pages from Atlassian Confluence. Couchbase Load documents from Couchbase databases. Figma Load Figma design files and comments. Firecrawl Crawl websites using Firecrawl\u2019s web scraping API. GitBook Load content from GitBook documentation sites. GitHub Load files and repositories from GitHub. Google Cloud Storage Load files from Google Cloud Storage buckets. Google Cloud SQL PostgreSQL Load data from Google Cloud SQL PostgreSQL databases. Hacker News Load posts and comments from Hacker News. IMSDb Load movie scripts from the Internet Movie Script Database. Jira Load issues and projects from Atlassian Jira. LangSmith Load runs and datasets from LangSmith. Notion API Load pages and databases from Notion. PDF (Web) Load PDF files from web URLs. Recursive URL Recursively crawl and load web pages. S3 Load files from Amazon S3 buckets. SearchAPI Load search results using SearchAPI. SerpAPI Load search results using SerpAPI. Sitemap Load URLs from website sitemaps. Sonix Audio Transcribe audio using Sonix\u2019s transcription API. Sort.xyz Blockchain Load blockchain data from Sort.xyz. Spider Fast web crawling using Spider API. Taskade Load projects and tasks from Taskade. Web Cheerio Scrape web pages using Cheerio for server-side parsing. Web Playwright Load dynamic web content using Playwright browser automation. Web Puppeteer Scrape JavaScript-heavy websites using Puppeteer. YouTube Load YouTube video transcripts and metadata. Document Transformers HTML to Text Convert HTML content to clean, readable text. Mozilla Readability Extract main content from web pages using Mozilla\u2019s Readability. OpenAI Metadata Tagger Generate metadata tags for documents using OpenAI. Document Compressors Cohere Rerank Rerank documents using Cohere\u2019s reranking models. IBM Document compression using IBM Watson AI services. MixedBread AI Rerank and compress documents using MixedBread AI. Tools AI Plugin Tool Execute OpenAI ChatGPT plugins as tools. Azure Dynamic Sessions Secure code execution in Azure Dynamic Sessions. Connery Modular AI actions and integrations with Connery. Connery Toolkit Access Connery\u2019s toolkit of pre-built actions. DALL-E Generate images using OpenAI\u2019s DALL-E models. Decodo Code execution and analysis with Decodo. Discord Interact with Discord servers and channels. DuckDuckGo Search Privacy-focused web search with DuckDuckGo. Exa Search AI-powered search engine for better results. Gmail Read and send emails through Gmail API. Goat Simple tool execution framework. Google Calendar Manage events and schedules in Google Calendar. Google Places Search for places using Google Places API. Google Routes Get directions and routing information. Google Scholar Search academic papers and citations. Google Trends Analyze search trends and popularity data. IBM Access IBM Watson AI tools and services. JigsawStack AI infrastructure tools from JigsawStack. JSON Tool Parse and manipulate JSON data structures. Lambda Agent Execute code in AWS Lambda functions. MCP Toolbox Model Context Protocol tools and utilities. OpenAPI Generate", "tokens": 1000, "node_type": "child"}
{"id": 284, "chunk_id": "5345104ec252a41359fa4a3d2dfbec23", "parent_id": 265, "url": "", "namespace": "langchain", "title": "oss-javascript-integrations-providers-all-providers", "headers": ["oss-javascript-integrations-providers-all-providers"], "section_index": 0, "chunk_index": 2, "text": "execution in Azure Dynamic Sessions. Connery Modular AI actions and integrations with Connery. Connery Toolkit Access Connery\u2019s toolkit of pre-built actions. DALL-E Generate images using OpenAI\u2019s DALL-E models. Decodo Code execution and analysis with Decodo. Discord Interact with Discord servers and channels. DuckDuckGo Search Privacy-focused web search with DuckDuckGo. Exa Search AI-powered search engine for better results. Gmail Read and send emails through Gmail API. Goat Simple tool execution framework. Google Calendar Manage events and schedules in Google Calendar. Google Places Search for places using Google Places API. Google Routes Get directions and routing information. Google Scholar Search academic papers and citations. Google Trends Analyze search trends and popularity data. IBM Access IBM Watson AI tools and services. JigsawStack AI infrastructure tools from JigsawStack. JSON Tool Parse and manipulate JSON data structures. Lambda Agent Execute code in AWS Lambda functions. MCP Toolbox Model Context Protocol tools and utilities. OpenAPI Generate tools from OpenAPI specifications. Python Interpreter Execute Python code in a sandboxed environment. SearchAPI Web search capabilities through SearchAPI. SearXNG Privacy-respecting metasearch engine. SerpAPI Google Search results through SerpAPI. Step Functions Agent Execute AWS Step Functions workflows. SQL Query databases using natural language. StackExchange Search Stack Overflow and other SE sites. Stagehand Browser automation for web interactions. Tavily Crawl Web crawling capabilities with Tavily. Tavily Extract Extract structured data from web pages. Tavily Map Map and visualize web crawling results. Tavily Search AI-optimized search for retrieval applications. Tavily Search Community Community-powered search through Tavily. Vector Store Query vector databases as tools. Web Browser Automated web browsing and interaction. Wikipedia Search and retrieve Wikipedia articles. Wolfram Alpha Computational knowledge through Wolfram Alpha. Zapier Agent Automate workflows using Zapier integrations. Retrievers ArXiv Search and retrieve academic papers from ArXiv. Azion EdgeSQL Edge-based document retrieval with Azion. Bedrock Knowledge Bases Retrieve from Amazon Bedrock Knowledge Bases. BM25 BM25 algorithm for keyword-based retrieval. Chaindesk Document retrieval using Chaindesk platform. ChatGPT Retriever Plugin Official ChatGPT retriever plugin integration. Dria Decentralized knowledge retrieval with Dria. Exa AI-powered web search and retrieval. HyDE Hypothetical Document Embeddings for better retrieval. Kendra Enterprise search with Amazon Kendra. Metal Managed vector search with Metal. Supabase Hybrid Hybrid search combining vector and keyword search. Tavily AI-optimized search for RAG applications. Time-Weighted Time-aware document retrieval and ranking. Vespa Big data serving engine for vector search. Zep Cloud Cloud-based long-term memory retrieval. Zep Long-term memory and context retrieval. Stores Cassandra Storage Distributed key-value storage using Cassandra. File System Local file system storage for development. In-Memory Fast in-memory storage for temporary data. IoRedis Storage Redis-based storage using IoRedis client. Upstash Redis Storage Serverless Redis storage with Upstash. Vercel KV Storage Key-value storage on Vercel\u2019s edge network. LLM Caching Callbacks Datadog Tracer Monitor and trace LangChain applications with Datadog. Upstash Rate Limit Rate limiting for AI applications using Upstash.", "tokens": 464, "node_type": "child"}
{"id": 285, "chunk_id": "c16acf8a553ac24f5d77e7d3a1c6e493", "parent_id": 268, "url": "", "namespace": "langchain", "title": "oss-javascript-integrations-providers-google", "headers": ["oss-javascript-integrations-providers-google"], "section_index": 0, "chunk_index": 0, "text": "# oss-javascript-integrations-providers-google\n\n> Source: https://docs.langchain.com/oss/javascript/integrations/providers/google\n\nChat models\nGemini Models\nAccess Gemini models such asgemini-1.5-pro\nand gemini-2.0-flex\nthrough the ChatGoogleGenerativeAI\n,\nor if using VertexAI, via the ChatVertexAI\nclass.\n- GenAI\n- VertexAI\nimage_url\nmust be a base64 encoded image (e.g., data:image/png;base64,abcd124\n).\nGemma\nAccess thegemma-3-27b-it\nmodel through AI Studio using the ChatGoogle\nclass.\n(This class is a superclass of the ChatVertexAI\nclass that works with both Vertex AI and the AI Studio APIs.)\nnpm\nThird Party Models\nSee above for setting up authentication through Vertex AI to use these models. Anthropic Claude models are also available through the Vertex AI platform. See here for more information about enabling access to the models and the model names to use. PaLM models are no longer supported.Vector Store\nVertex AI Vector Search\nVertex AI Vector Search, formerly known as Vertex AI Matching Engine, provides the industry\u2019s leading high-scale low latency vector database. These vector databases are commonly referred to as vector similarity-matching or an approximate nearest neighbor (ANN) service.\nPostgres Vector Store\nThe PostgresVectorStore module from the@langchain/google-cloud-sql-pg\npackage provides a way to use the CloudSQL for PostgresSQL to store\nvector embeddings using the class.\nTools\nGoogle Search\n- Set up a Custom Search Engine, following these instructions\n- Get an API Key and Custom Search Engine ID from the previous step, and set them as environment variables\nGOOGLE_API_KEY\nandGOOGLE_CSE_ID\nrespectively\nGoogleCustomSearch\nutility which wraps this API. To import this utility:", "tokens": 237, "node_type": "child"}
{"id": 286, "chunk_id": "c16acf8a553ac24f5d77e7d3a1c6e493", "parent_id": 269, "url": "", "namespace": "langchain", "title": "oss-javascript-integrations-providers-microsoft", "headers": ["oss-javascript-integrations-providers-microsoft"], "section_index": 0, "chunk_index": 0, "text": "# oss-javascript-integrations-providers-microsoft\n\n> Source: https://docs.langchain.com/oss/javascript/integrations/providers/microsoft\n\nMicrosoft Azure\nand other Microsoft\nproducts.\nChat Models\nAzure OpenAI\nSee a usage exampleLLM\nAzure OpenAI\nMicrosoft Azure, often referred to asAzure\nis a cloud computing platform run byMicrosoft\n, which offers access, management, and development of applications and services through global data centers. It provides a range of capabilities, including software as a service (SaaS), platform as a service (PaaS), and infrastructure as a service (IaaS).Microsoft Azure\nsupports many programming languages, tools, and frameworks, including Microsoft-specific and third-party software and systems.\nAzure OpenAI is a cloud service to help you quickly develop generative AI experiences with a diverse set of prebuilt and curated models from OpenAI, Meta and beyond.LangChain.js supports integration with Azure OpenAI using the new Azure integration in the OpenAI SDK. You can learn more about Azure OpenAI and its difference with the OpenAI API on this page. If you don\u2019t have an Azure account, you can create a free account to get started. You\u2019ll need to have an Azure OpenAI instance deployed. You can deploy a version on Azure Portal following this guide. Once you have your instance running, make sure you have the name of your instance and key. You can find the key in the Azure Portal, under the \u201cKeys and Endpoint\u201d section of your instance. If you\u2019re using Node.js, you can define the following environment variables to use the service:\nnpm\nText Embedding Models\nAzure OpenAI\nSee a usage exampleVector stores\nAzure AI Search\nAzure AI Search (formerly known as Azure Search and Azure Cognitive Search) is a distributed, RESTful search engine optimized for speed and relevance on production-scale workloads on Azure. It supports also vector search using the k-nearest neighbor (kNN) algorithm and also semantic search.\nnpm\nAzure Cosmos DB for NoSQL\nAzure Cosmos DB for NoSQL provides support for querying items with flexible schemas and native support for JSON. It now offers vector indexing and search. This feature is designed to handle high-dimensional vectors, enabling efficient and accurate vector search at any scale. You can now store vectors directly in the documents alongside your data. Each document in your database can contain not only traditional schema-free data, but also high-dimensional vectors as other properties of the documents.\nnpm\nAzure Cosmos DB for MongoDB vCore\nAzure Cosmos DB for MongoDB vCore makes it easy to create a database with full native MongoDB support. You can apply your MongoDB experience and continue to use your favorite MongoDB drivers, SDKs, and tools by pointing your application to the API for MongoDB vCore account\u2019s connection string. Use vector search in Azure Cosmos DB for MongoDB vCore to seamlessly integrate your AI-based applications with your data that\u2019s stored in Azure Cosmos DB.\nnpm\nSemantic Cache\nAzure Cosmos DB NoSQL Semantic Cache\nThe Semantic Cache feature is supported with Azure Cosmos DB for NoSQL integration, enabling users to retrieve cached responses based on semantic similarity between the user input and previously cached results. It leverages AzureCosmosDBNoSQLVectorStore, which stores vector embeddings of cached prompts. These embeddings enable similarity-based searches, allowing the system to retrieve relevant cached results.\nnpm\nDocument loaders\nAzure Blob Storage\nAzure Blob Storage is Microsoft\u2019s object storage solution for the cloud. Blob Storage is optimized for storing massive amounts of unstructured data. Unstructured data is data that doesn\u2019t adhere to a particular data model or definition, such as text or binary data.\nAzure Files offers fully managed file shares in the cloud that are accessible via the industry standard Server Message Block (SMB\n) protocol, Network File System (NFS\n) protocol, andAzure Files REST API\n.Azure Files\nare based on theAzure Blob Storage\n.\nAzure Blob Storage\nis designed for:\n- Serving images or documents directly to a browser.\n- Storing files for distributed access.\n- Streaming video and audio.\n- Writing to log files.\n- Storing data for backup and restore, disaster recovery, and archiving.\n- Storing data for analysis by an on-premises or Azure-hosted service.\nnpm\nTools\nAzure Container Apps Dynamic Sessions\nAzure Container Apps dynamic sessions provide fast access to secure sandboxed environments that are ideal for running code or applications that require strong isolation from other workloads.\nnpm", "tokens": 692, "node_type": "child"}
{"id": 287, "chunk_id": "c16acf8a553ac24f5d77e7d3a1c6e493", "parent_id": 270, "url": "", "namespace": "langchain", "title": "oss-javascript-integrations-providers-openai", "headers": ["oss-javascript-integrations-providers-openai"], "section_index": 0, "chunk_index": 0, "text": "# oss-javascript-integrations-providers-openai\n\n> Source: https://docs.langchain.com/oss/javascript/integrations/providers/openai\n\nOpenAI is American artificial intelligence (AI) research laboratory consisting of the non-profitOpenAI Incorporated\nand its for-profit subsidiary corporationOpenAI Limited Partnership\n. OpenAI conducts AI research with the declared intention of promoting and developing a friendly AI. OpenAI systems run on anAzure\n-based supercomputing platform fromMicrosoft\n.\nThe OpenAI API is powered by a diverse set of models with different capabilities and price points.\nChatGPT is the Artificial Intelligence (AI) chatbot developed by OpenAI\n.\nInstallation and Setup\n- Get an OpenAI api key and set it as an environment variable (\nOPENAI_API_KEY\n)\nChat model\nSee a usage example.LLM\nSee a usage example.npm", "tokens": 108, "node_type": "child"}
{"id": 288, "chunk_id": "c16acf8a553ac24f5d77e7d3a1c6e493", "parent_id": 271, "url": "", "namespace": "langchain", "title": "oss-javascript-integrations-providers-overview", "headers": ["oss-javascript-integrations-providers-overview"], "section_index": 0, "chunk_index": 0, "text": "# oss-javascript-integrations-providers-overview\n\n> Source: https://docs.langchain.com/oss/javascript/integrations/providers/overview\n\n| Anthropic | @langchain/anthropic | | |\n| Azure CosmosDB | @langchain/azure-cosmosdb | | |\n| Cerebras | @langchain/cerebras | | |\n| Cloudflare | @langchain/cloudflare | | |\n| Cohere | @langchain/cohere | | |\n| Exa | langchain-exa | | |\n| Google GenAI | @langchain/google-genai | | |\n| Google VertexAI | @langchain/google-vertexai | | |\n| Google VertexAI (Web Environments) | @langchain/google-vertexai-web | | |\n| Groq | @langchain/groq | | |\n| MistralAI | @langchain/mistralai | | |\n| MongoDB | @langchain/mongodb | | |\n| Nomic | @langchain/nomic | | |\n| OpenAI | @langchain/openai | | |\n| Pinecone | @langchain/pinecone | | |\n| Qdrant | @langchain/qdrant | | |\n| Tavily | @langchain/tavily | | |\n| Weaviate | @langchain/weaviate | | |\n| xAI | @langchain/xai | | |\n| Yandex | @langchain/yandex | | |", "tokens": 151, "node_type": "child"}
{"id": 289, "chunk_id": "c16acf8a553ac24f5d77e7d3a1c6e493", "parent_id": 273, "url": "", "namespace": "langchain", "title": "oss-javascript-integrations-splitters-index", "headers": ["oss-javascript-integrations-splitters-index"], "section_index": 0, "chunk_index": 0, "text": "# oss-javascript-integrations-splitters-index\n\n> Source: https://docs.langchain.com/oss/javascript/integrations/splitters/index\n\nFor most use cases, start with the RecursiveCharacterTextSplitter. It provides a solid balance between keeping context intact and managing chunk size. This default strategy works well out of the box, and you should only consider adjusting it if you need to fine-tune performance for your specific application.\nText structure-based\nText is naturally organized into hierarchical units such as paragraphs, sentences, and words. We can leverage this inherent structure to inform our splitting strategy, creating split that maintain natural language flow, maintain semantic coherence within split, and adapts to varying levels of text granularity. LangChain\u2019sRecursiveCharacterTextSplitter\nimplements this concept:\n- The RecursiveCharacterTextSplitter attempts to keep larger units (e.g., paragraphs) intact.\n- If a unit exceeds the chunk size, it moves to the next level (e.g., sentences).\n- This process continues down to the word level if necessary.\nLength-based\nAn intuitive strategy is to split documents based on their length. This simple yet effective approach ensures that each chunk doesn\u2019t exceed a specified size limit. Key benefits of length-based splitting:- Straightforward implementation\n- Consistent chunk sizes\n- Easily adaptable to different model requirements\n- Token-based: Splits text based on the number of tokens, which is useful when working with language models.\n- Character-based: Splits text based on the number of characters, which can be more consistent across different types of text.\nDocument structure-based\nSome documents have an inherent structure, such as HTML, Markdown, or JSON files. In these cases, it\u2019s beneficial to split the document based on its structure, as it often naturally groups semantically related text. Key benefits of structure-based splitting:- Preserves the logical organization of the document\n- Maintains context within each chunk\n- Can be more effective for downstream tasks like retrieval or summarization", "tokens": 289, "node_type": "child"}
{"id": 290, "chunk_id": "c16acf8a553ac24f5d77e7d3a1c6e493", "parent_id": 274, "url": "", "namespace": "langchain", "title": "oss-javascript-integrations-stores-index", "headers": ["oss-javascript-integrations-stores-index"], "section_index": 0, "chunk_index": 0, "text": "# oss-javascript-integrations-stores-index\n\n> Source: https://docs.langchain.com/oss/javascript/integrations/stores/index\n\nOverview\nLangChain provides a key-value store interface for storing and retrieving data by key. The key-value store interface in LangChain is primarily used for caching embeddings.Interface\nAllBaseStores\nare generic and support the following interface, where K\nrepresents the key type and V\nrepresents the value type:\nmget(keys: K[]): Promise<(V | undefined)[]>\n: get the values for multiple keys, returningundefined\nif a key does not existmset(keyValuePairs: [K, V][]): Promise<void>\n: set the values for multiple keysmdelete(keys: K[]): Promise<void>\n: delete multiple keysyieldKeys(prefix?: string): AsyncGenerator<K | string>\n: asynchronously yield all keys in the store, optionally filtering by a prefix\nBaseStore<string, BaseMessage>\nwould store messages with string keys, while BaseStore<string, number[]>\nwould store arrays of numbers.\nBase stores are designed to work with multiple key-value pairs at once for efficiency. This saves on network round-trips and may allow for more efficient batch operations in the underlying store.\nBuilt-in stores for local development\nCustom stores\nYou can also implement your own custom store by extending theBaseStore\nclass. See the store interface documentation for more details.", "tokens": 177, "node_type": "child"}
{"id": 291, "chunk_id": "c16acf8a553ac24f5d77e7d3a1c6e493", "parent_id": 275, "url": "", "namespace": "langchain", "title": "oss-javascript-integrations-text-embedding-azure-openai", "headers": ["oss-javascript-integrations-text-embedding-azure-openai"], "section_index": 0, "chunk_index": 0, "text": "# oss-javascript-integrations-text-embedding-azure-openai\n\n> Source: https://docs.langchain.com/oss/javascript/integrations/text_embedding/azure_openai\n\nAzure OpenAI is a cloud service to help you quickly develop generative AI experiences with a diverse set of prebuilt and curated models from OpenAI, Meta and beyond.LangChain.js supports integration with Azure OpenAI using the new Azure integration in the OpenAI SDK.You can learn more about Azure OpenAI and its difference with the OpenAI API on this page. If you don\u2019t have an Azure account, you can create a free account to get started.This will help you get started with AzureOpenAIEmbeddings embedding models using LangChain. For detailed documentation on AzureOpenAIEmbeddings features and configuration options, please refer to the API reference.\nCopy\n<Info>**Previously, LangChain.js supported integration with Azure OpenAI using the dedicated [Azure OpenAI SDK](https://github.com/Azure/azure-sdk-for-js/tree/main/sdk/openai/openai). This SDK is now deprecated in favor of the new Azure integration in the OpenAI SDK, which allows to access the latest OpenAI models and features the same day they are released, and allows seamless transition between the OpenAI API and Azure OpenAI.**If you are using Azure OpenAI with the deprecated SDK, see the [migration guide](#migration-from-azure-openai-sdk) to update to the new API.</Info>\nYou\u2019ll need to have an Azure OpenAI instance deployed. You can deploy a version on Azure Portal following this guide.Once you have your instance running, make sure you have the name of your instance and key. You can find the key in the Azure Portal, under the \u201cKeys and Endpoint\u201d section of your instance.If you\u2019re using Node.js, you can define the following environment variables to use the service:\nThe LangChain AzureOpenAIEmbeddings integration lives in the @langchain/openai package:\nCopy\nimport IntegrationInstallTooltip from \"@mdx_components/integration_install_tooltip.mdx\";<IntegrationInstallTooltip></IntegrationInstallTooltip><Npm2Yarn> @langchain/openai @langchain/core</Npm2Yarn><Info>**You can find the list of supported API versions in the [Azure OpenAI documentation](https://learn.microsoft.com/azure/ai-services/openai/reference).**</Info><Tip>**If `AZURE_OPENAI_API_EMBEDDINGS_DEPLOYMENT_NAME` is not defined, it will fall back to the value of `AZURE_OPENAI_API_DEPLOYMENT_NAME` for the deployment name. The same applies to the `azureOpenAIApiEmbeddingsDeploymentName` parameter in the `AzureOpenAIEmbeddings` constructor, which will fall back to the value of `azureOpenAIApiDeploymentName` if not defined.**</Tip>\nNow we can instantiate our model object and embed text:\nCopy\nimport { AzureOpenAIEmbeddings } from \"@langchain/openai\";const embeddings = new AzureOpenAIEmbeddings({ azureOpenAIApiKey: \"<your_key>\", // In Node.js defaults to process.env.AZURE_OPENAI_API_KEY azureOpenAIApiInstanceName: \"<your_instance_name>\", // In Node.js defaults to process.env.AZURE_OPENAI_API_INSTANCE_NAME azureOpenAIApiEmbeddingsDeploymentName: \"<your_embeddings_deployment_name>\", // In Node.js defaults to process.env.AZURE_OPENAI_API_EMBEDDINGS_DEPLOYMENT_NAME azureOpenAIApiVersion: \"<api_version>\", // In Node.js defaults to process.env.AZURE_OPENAI_API_VERSION maxRetries: 1,});\nEmbedding models are often used in retrieval-augmented generation (RAG) flows, both as part of indexing data as well as later retrieving it. For more detailed instructions, please see our RAG tutorials under the Learn tab.Below, see how to index and retrieve data using the embeddings object we initialized above. In this example, we will index and retrieve a sample document using the demo MemoryVectorStore.\nCopy\n// Create a vector store with a sample textimport { MemoryVectorStore } from \"@langchain/classic/vectorstores/memory\";const text = \"LangChain is the framework for building context-aware reasoning applications\";const vectorstore = await MemoryVectorStore.fromDocuments( [{ pageContent: text, metadata: {} }], embeddings,);// Use the vector store as a retriever that returns a single documentconst retriever = vectorstore.asRetriever(1);// Retrieve the most similar textconst retrievedDocuments = await retriever.invoke(\"What is LangChain?\");retrievedDocuments[0].pageContent;\nCopy\nLangChain is the framework for building context-aware reasoning applications\nUnder the hood, the vectorstore and retriever implementations are calling embeddings.embedDocument(...) and embeddings.embedQuery(...) to create embeddings for the text(s) used in fromDocuments and the retriever\u2019s invoke operations, respectively.You can directly call these methods to get embeddings for your own use cases.\nYou can embed multiple texts for indexing with embedDocuments. The internals used for this method may (but do not have to) differ from embedding queries:\nCopy\nconst text2 = \"LangGraph is a library for building stateful, multi-actor applications with LLMs\";const vectors = await embeddings.embedDocuments([text, text2]);console.log(vectors[0].slice(0, 100));console.log(vectors[1].slice(0, 100));\nIf your instance is hosted under a domain other than the default openai.azure.com, you\u2019ll need to use the alternate AZURE_OPENAI_BASE_PATH environment variable.\nFor example, here\u2019s how you would connect to the domain https://westeurope.api.microsoft.com/openai/deployments/{DEPLOYMENT_NAME}:\nCopy\nimport { AzureOpenAIEmbeddings } from \"@langchain/openai\";const embeddingsDifferentDomain = new AzureOpenAIEmbeddings({ azureOpenAIApiKey: \"<your_key>\", // In Node.js defaults to process.env.AZURE_OPENAI_API_KEY azureOpenAIApiEmbeddingsDeploymentName: \"<your_embedding_deployment_name>\", // In Node.js defaults to process.env.AZURE_OPENAI_API_EMBEDDINGS_DEPLOYMENT_NAME azureOpenAIApiVersion: \"<api_version>\", // In Node.js defaults to process.env.AZURE_OPENAI_API_VERSION azureOpenAIBasePath: \"https://westeurope.api.microsoft.com/openai/deployments\", // In Node.js defaults to process.env.AZURE_OPENAI_BASE_PATH});\nThe configuration field also accepts other ClientOptions parameters accepted by the official SDK.Note: The specific header api-key currently cannot be overridden in this manner and will pass through the value from azureOpenAIApiKey.\nIf you are using the deprecated Azure OpenAI SDK with the @langchain/azure-openai package, you can update your code to use the new Azure integration following these steps:\nInstall the new @langchain/openai package and remove the previous @langchain/azure-openai package:\nUpdate your imports to use the new AzureOpenAIEmbeddings classe from the @langchain/openai package:\nCopy\nimport { AzureOpenAIEmbeddings } from \"@langchain/openai\";\nUpdate your code to use the new AzureOpenAIEmbeddings class and pass the required parameters:\nCopy\nconst model = new AzureOpenAIEmbeddings({ azureOpenAIApiKey: \"<your_key>\", azureOpenAIApiInstanceName: \"<your_instance_name>\", azureOpenAIApiEmbeddingsDeploymentName: \"<your_embeddings_deployment_name>\", azureOpenAIApiVersion: \"<api_version>\",});\nNotice that the constructor now requires the azureOpenAIApiInstanceName parameter instead of the azureOpenAIEndpoint parameter, and adds the azureOpenAIApiVersion parameter to specify the API version.\nIf you were using Azure Managed Identity, you now need to use the azureADTokenProvider parameter to the constructor instead of credentials, see the Azure Managed Identity section for more details.\nIf you were using environment variables, you now have to set the AZURE_OPENAI_API_INSTANCE_NAME environment variable instead of AZURE_OPENAI_API_ENDPOINT, and add the AZURE_OPENAI_API_VERSION environment variable to specify the API version.", "tokens": 875, "node_type": "child"}
{"id": 292, "chunk_id": "c16acf8a553ac24f5d77e7d3a1c6e493", "parent_id": 276, "url": "", "namespace": "langchain", "title": "oss-javascript-integrations-text-embedding-bedrock", "headers": ["oss-javascript-integrations-text-embedding-bedrock"], "section_index": 0, "chunk_index": 0, "text": "# oss-javascript-integrations-text-embedding-bedrock\n\n> Source: https://docs.langchain.com/oss/javascript/integrations/text_embedding/bedrock\n\nAmazon Bedrock is a fully managed service that offers a choice of high-performing foundation models (FMs) from leading AI companies like AI21 Labs, Anthropic, Cohere, Meta, Stability AI, and Amazon via a single API, along with a broad set of capabilities you need to build generative AI applications with security, privacy, and responsible AI.This will help you get started with Amazon Bedrock embedding models using LangChain. For detailed documentation on Bedrock features and configuration options, please refer to the API reference.\nTo access Bedrock embedding models you\u2019ll need to create an AWS account, get an API key, and install the @langchain/aws integration package.Head to the AWS docs to sign up for AWS and setup your credentials. You\u2019ll also need to turn on model access for your account, which you can do by following these instructions.\nNow we can instantiate our model object and embed text.There are a few different ways to authenticate with AWS - the below examples rely on an access key, secret access key and region set in your environment variables:\nEmbedding models are often used in retrieval-augmented generation (RAG) flows, both as part of indexing data as well as later retrieving it. For more detailed instructions, please see our RAG tutorials under the Learn tab.Below, see how to index and retrieve data using the embeddings object we initialized above. In this example, we will index and retrieve a sample document using the demo MemoryVectorStore.\nCopy\n// Create a vector store with a sample textimport { MemoryVectorStore } from \"@langchain/classic/vectorstores/memory\";const text = \"LangChain is the framework for building context-aware reasoning applications\";const vectorstore = await MemoryVectorStore.fromDocuments( [{ pageContent: text, metadata: {} }], embeddings,);// Use the vector store as a retriever that returns a single documentconst retriever = vectorstore.asRetriever(1);// Retrieve the most similar textconst retrievedDocuments = await retriever.invoke(\"What is LangChain?\");retrievedDocuments[0].pageContent;\nCopy\nLangChain is the framework for building context-aware reasoning applications\nUnder the hood, the vectorstore and retriever implementations are calling embeddings.embedDocument(...) and embeddings.embedQuery(...) to create embeddings for the text(s) used in fromDocuments and the retriever\u2019s invoke operations, respectively.You can directly call these methods to get embeddings for your own use cases.\nYou can embed multiple texts for indexing with embedDocuments. The internals used for this method may (but do not have to) differ from embedding queries:\nCopy\nconst text2 = \"LangGraph is a library for building stateful, multi-actor applications with LLMs\";const vectors = await embeddings.embedDocuments([text, text2]);console.log(vectors[0].slice(0, 100));console.log(vectors[1].slice(0, 100));", "tokens": 403, "node_type": "child"}
{"id": 293, "chunk_id": "c16acf8a553ac24f5d77e7d3a1c6e493", "parent_id": 277, "url": "", "namespace": "langchain", "title": "oss-javascript-integrations-text-embedding-google-generativeai", "headers": ["oss-javascript-integrations-text-embedding-google-generativeai"], "section_index": 0, "chunk_index": 0, "text": "# oss-javascript-integrations-text-embedding-google-generativeai\n\n> Source: https://docs.langchain.com/oss/javascript/integrations/text_embedding/google_generativeai\n\nGoogleGenerativeAIEmbeddings\nfeatures and configuration options, please refer to the API reference.\nOverview\nIntegration details\n| Class | Package | Local | Py support | Downloads | Version |\n|---|---|---|---|---|---|\nGoogleGenerativeAIEmbeddings | @langchain/google-genai | \u274c | \u2705 |\nSetup\nTo access Google Generative AI embedding models you\u2019ll need to sign up for a Google AI account, get an API key, and install the@langchain/google-genai\nintegration package.\nCredentials\nGet an API key here: ai.google.dev/tutorials/setup. Next, set your key as an environment variable namedGOOGLE_API_KEY\n:\nInstallation\nThe LangChainGoogleGenerativeAIEmbeddings\nintegration lives in the @langchain/google-genai\npackage. You may also wish to install the official SDK:\nInstantiation\nNow we can instantiate our model object and embed text:Indexing and Retrieval\nEmbedding models are often used in retrieval-augmented generation (RAG) flows, both as part of indexing data as well as later retrieving it. For more detailed instructions, please see our RAG tutorials under the Learn tab. Below, see how to index and retrieve data using theembeddings\nobject we initialized above. In this example, we will index and retrieve a sample document using the demo MemoryVectorStore\n.\nDirect Usage\nUnder the hood, the vectorstore and retriever implementations are callingembeddings.embedDocument(...)\nand embeddings.embedQuery(...)\nto create embeddings for the text(s) used in fromDocuments\nand the retriever\u2019s invoke\noperations, respectively.\nYou can directly call these methods to get embeddings for your own use cases.\nEmbed single texts\nYou can embed queries for search withembedQuery\n. This generates a vector representation specific to the query:\nEmbed multiple texts\nYou can embed multiple texts for indexing withembedDocuments\n. The internals used for this method may (but do not have to) differ from embedding queries:\nAPI reference\nFor detailed documentation of allGoogleGenerativeAIEmbeddings\nfeatures and configurations head to the API reference.", "tokens": 288, "node_type": "child"}
{"id": 294, "chunk_id": "c16acf8a553ac24f5d77e7d3a1c6e493", "parent_id": 278, "url": "", "namespace": "langchain", "title": "oss-javascript-integrations-text-embedding-index", "headers": ["oss-javascript-integrations-text-embedding-index"], "section_index": 0, "chunk_index": 0, "text": "# oss-javascript-integrations-text-embedding-index\n\n> Source: https://docs.langchain.com/oss/javascript/integrations/text_embedding/index\n\nOverview\nThis overview covers text-based embedding models. LangChain does not currently support multimodal embeddings.\nHow it works\n- Vectorization \u2014 The model encodes each input string as a high-dimensional vector.\n- Similarity scoring \u2014 Vectors are compared using mathematical metrics to measure how closely related the underlying texts are.\nSimilarity metrics\nSeveral metrics are commonly used to compare embeddings:- Cosine similarity \u2014 measures the angle between two vectors.\n- Euclidean distance \u2014 measures the straight-line distance between points.\n- Dot product \u2014 measures how much one vector projects onto another.\nInterface\nLangChain provides a standard interface for text embedding models (e.g., OpenAI, Cohere, Hugging Face) via the Embeddings interface. Two main methods are available:embedDocuments(documents: string[]) \u2192 number[][]\n: Embeds a list of documents.embedQuery(text: string) \u2192 number[]\n: Embeds a single query.\nThe interface allows queries and documents to be embedded with different strategies, though most providers handle them the same way in practice.\nInstall and use\nOpenAI\nOpenAI\nInstall dependencies:Add environment variables:Instantiate the model:\nAzure\nAzure\nInstall dependenciesAdd environment variables:Instantiate the model:\nAWS\nAWS\nInstall dependencies:Add environment variables:Instantiate the model:\nGoogle Gemini\nGoogle Gemini\nInstall dependencies:Add environment variables:Instantiate the model:\nGoogle Vertex\nGoogle Vertex\nInstall dependencies:Add environment variables:Instantiate the model:\nMistralAI\nMistralAI\nInstall dependencies:Add environment variables:Instantiate the model:\nCohere\nCohere\nInstall dependencies:Add environment variables:Instantiate the model:\nOllama\nOllama\nInstall dependencies:Instantiate the model:\nCaching\nEmbeddings can be stored or temporarily cached to avoid needing to recompute them. Caching embeddings can be done using aCacheBackedEmbeddings\n. This wrapper stores embeddings in a key-value store, where the text is hashed and the hash is used as the key in the cache.\nThe main supported way to initialize a CacheBackedEmbeddings\nis fromBytesStore\n. It takes the following parameters:\n- underlyingEmbeddings: The embedder to use for embedding.\n- documentEmbeddingStore: Any\nBaseStore\nfor caching document embeddings. - options.namespace: (optional, defaults to\n\"\"\n) The namespace to use for the document cache. Helps avoid collisions (e.g., set it to the embedding model name).", "tokens": 331, "node_type": "child"}
{"id": 295, "chunk_id": "c16acf8a553ac24f5d77e7d3a1c6e493", "parent_id": 279, "url": "", "namespace": "langchain", "title": "oss-javascript-integrations-text-embedding-openai", "headers": ["oss-javascript-integrations-text-embedding-openai"], "section_index": 0, "chunk_index": 0, "text": "# oss-javascript-integrations-text-embedding-openai\n\n> Source: https://docs.langchain.com/oss/javascript/integrations/text_embedding/openai\n\nThis will help you get started with OpenAIEmbeddings embedding models using LangChain. For detailed documentation on OpenAIEmbeddings features and configuration options, please refer to the API reference.\nTo access OpenAIEmbeddings embedding models you\u2019ll need to create an OpenAI account, get an API key, and install the @langchain/openai integration package.\nNow we can instantiate our model object and generate chat completions:\nCopy\nimport { OpenAIEmbeddings } from \"@langchain/openai\";const embeddings = new OpenAIEmbeddings({ apiKey: \"YOUR-API-KEY\", // In Node.js defaults to process.env.OPENAI_API_KEY batchSize: 512, // Default value if omitted is 512. Max is 2048 model: \"text-embedding-3-large\",});\nIf you\u2019re part of an organization, you can set process.env.OPENAI_ORGANIZATION to your OpenAI organization id, or pass it in as organization when\ninitializing the model.\nEmbedding models are often used in retrieval-augmented generation (RAG) flows, both as part of indexing data as well as later retrieving it. For more detailed instructions, please see our RAG tutorials under the Learn tab.Below, see how to index and retrieve data using the embeddings object we initialized above. In this example, we will index and retrieve a sample document using the demo MemoryVectorStore.\nCopy\n// Create a vector store with a sample textimport { MemoryVectorStore } from \"@langchain/classic/vectorstores/memory\";const text = \"LangChain is the framework for building context-aware reasoning applications\";const vectorstore = await MemoryVectorStore.fromDocuments( [{ pageContent: text, metadata: {} }], embeddings,);// Use the vector store as a retriever that returns a single documentconst retriever = vectorstore.asRetriever(1);// Retrieve the most similar textconst retrievedDocuments = await retriever.invoke(\"What is LangChain?\");retrievedDocuments[0].pageContent;\nCopy\nLangChain is the framework for building context-aware reasoning applications\nUnder the hood, the vectorstore and retriever implementations are calling embeddings.embedDocument(...) and embeddings.embedQuery(...) to create embeddings for the text(s) used in fromDocuments and the retriever\u2019s invoke operations, respectively.You can directly call these methods to get embeddings for your own use cases.\nYou can embed multiple texts for indexing with embedDocuments. The internals used for this method may (but do not have to) differ from embedding queries:\nCopy\nconst text2 = \"LangGraph is a library for building stateful, multi-actor applications with LLMs\";const vectors = await embeddings.embedDocuments([text, text2]);console.log(vectors[0].slice(0, 100));console.log(vectors[1].slice(0, 100));\nWith the text-embedding-3 class of models, you can specify the size of the embeddings you want returned. For example by default text-embedding-3-large returns embeddings of dimension 3072:\nCopy\nimport { OpenAIEmbeddings } from \"@langchain/openai\";const embeddingsDefaultDimensions = new OpenAIEmbeddings({ model: \"text-embedding-3-large\",});const vectorsDefaultDimensions = await embeddingsDefaultDimensions.embedDocuments([\"some text\"]);console.log(vectorsDefaultDimensions[0].length);\nCopy\n3072\nBut by passing in dimensions: 1024 we can reduce the size of our embeddings to 1024:\nCopy\nimport { OpenAIEmbeddings } from \"@langchain/openai\";const embeddings1024 = new OpenAIEmbeddings({ model: \"text-embedding-3-large\", dimensions: 1024,});const vectors1024 = await embeddings1024.embedDocuments([\"some text\"]);console.log(vectors1024[0].length);\nYou can customize the base URL the SDK sends requests to by passing a configuration parameter like this:\nCopy\nimport { OpenAIEmbeddings } from \"@langchain/openai\";const model = new OpenAIEmbeddings({ configuration: { baseURL: \"https://your_custom_url.com\", },});\nYou can also pass other ClientOptions parameters accepted by the official SDK.If you are hosting on Azure OpenAI, see the dedicated page instead.", "tokens": 488, "node_type": "child"}
{"id": 296, "chunk_id": "c16acf8a553ac24f5d77e7d3a1c6e493", "parent_id": 281, "url": "", "namespace": "langchain", "title": "oss-javascript-integrations-vectorstores-index", "headers": ["oss-javascript-integrations-vectorstores-index"], "section_index": 0, "chunk_index": 0, "text": "# oss-javascript-integrations-vectorstores-index\n\n> Source: https://docs.langchain.com/oss/javascript/integrations/vectorstores/index\n\nOverview\nA vector store stores embedded data and performs similarity search.Interface\nLangChain provides a unified interface for vector stores, allowing you to:addDocuments\n- Add documents to the store.delete\n- Remove stored documents by ID.similaritySearch\n- Query for semantically similar documents.\nInitialization\nMost vectorstores in LangChain accept an embedding model as an argument when initializing the vector store.Adding docuemnts\nYou can add documents to the vector store by using theaddDocuments\nfunction.\nDeleting documents\nYou can delete documents from the vector store by using thedelete\nfunction.\nSimilarity search\nIssue a semantic query usingsimilaritySearch\n, which returns the closest embedded documents:\nk\n\u2014 number of results to returnfilter\n\u2014 conditional filtering based on metadata\nSimilarity metrics & indexing\nEmbedding similarity may be computed using:- Cosine similarity\n- Euclidean distance\n- Dot product\nMetadata filtering\nFiltering by metadata (e.g., source, date) can refine search results:Top integrations\nSelect embedding model:OpenAI\nOpenAI\nInstall dependencies:Add environment variables:Instantiate the model:\nAzure\nAzure\nInstall dependenciesAdd environment variables:Instantiate the model:\nAWS\nAWS\nInstall dependencies:Add environment variables:Instantiate the model:\nGoogle Gemini\nGoogle Gemini\nInstall dependencies:Add environment variables:Instantiate the model:\nGoogle Vertex\nGoogle Vertex\nInstall dependencies:Add environment variables:Instantiate the model:\nMistralAI\nMistralAI\nInstall dependencies:Add environment variables:Instantiate the model:\nCohere\nCohere\nInstall dependencies:Add environment variables:Instantiate the model:\nOllama\nOllama\nInstall dependencies:Instantiate the model:\nMemory\nMemory\nChroma\nChroma\nFAISS\nFAISS\nMongoDB\nMongoDB\nPGVector\nPGVector\nPinecone\nPinecone\nQdrant\nQdrant", "tokens": 229, "node_type": "child"}
{"id": 297, "chunk_id": "c16acf8a553ac24f5d77e7d3a1c6e493", "parent_id": 282, "url": "", "namespace": "langchain", "title": "oss-javascript-langchain-agents", "headers": ["oss-javascript-langchain-agents"], "section_index": 0, "chunk_index": 0, "text": "# oss-javascript-langchain-agents\n\n> Source: https://docs.langchain.com/oss/javascript/langchain/agents\n\nLangChain v1.0Welcome to the new LangChain documentation! If you encounter any issues or have feedback, please open an issue so we can improve. Archived v0 documentation can be found here.See the release notes and migration guide for a complete list of changes and instructions on how to upgrade your code.\ncreateAgent()\nprovides a production-ready agent implementation.\nAn LLM Agent runs tools in a loop to achieve a goal.\nAn agent runs until a stop condition is met - i.e., when the model emits a final output or an iteration limit is reached.\ncreateAgent()\nbuilds a graph-based agent runtime using LangGraph. A graph consists of nodes (steps) and edges (connections) that define how your agent processes information. The agent moves through this graph, executing nodes like the model node (which calls the model), the tools node (which executes tools), or middleware.Learn more about the Graph API.Core components\nModel\nThe model is the reasoning engine of your agent. It can be specified in multiple ways, supporting both static and dynamic model selection.Static model\nStatic models are configured once when creating the agent and remain unchanged throughout execution. This is the most common and straightforward approach. To initialize a static model from a :provider:model\n(e.g. \"openai:gpt-5\"\n). You may want more control over the model configuration, in which case you can initialize a model instance directly using the provider package:\ntemperature\n, max_tokens\n, timeouts\n, or configure API keys, base_url\n, and other provider-specific settings. Refer to the API reference to see available params and methods on your model.\nDynamic model\nDynamic models are selected at based on the current and context. This enables sophisticated routing logic and cost optimization. To use a dynamic model, create middleware withwrapModelCall\nthat modifies the model in the request:\nTools\nTools give agents the ability to take actions. Agents go beyond simple model-only tool binding by facilitating:- Multiple tool calls in sequence (triggered by a single prompt)\n- Parallel tool calls when appropriate\n- Dynamic tool selection based on previous results\n- Tool retry logic and error handling\n- State persistence across tool calls\nDefining tools\nPass a list of tools to the agent.Tool error handling\nTo customize how tool errors are handled, use thewrapToolCall\nhook in a custom middleware:\nToolMessage\n] with the custom error message when a tool fails.\nTool use in the ReAct loop\nAgents follow the ReAct (\u201cReasoning + Acting\u201d) pattern, alternating between brief reasoning steps with targeted tool calls and feeding the resulting observations into subsequent decisions until they can deliver a final answer.Example of ReAct loop\nExample of ReAct loop\nPrompt: Identify the current most popular wireless headphones and verify availability.\n- Reasoning: \u201cPopularity is time-sensitive, I need to use the provided search tool.\u201d\n- Acting: Call\nsearch_products(\"wireless headphones\")\n- Reasoning: \u201cI need to confirm availability for the top-ranked item before answering.\u201d\n- Acting: Call\ncheck_inventory(\"WH-1000XM5\")\n- Reasoning: \u201cI have the most popular model and its stock status. I can now answer the user\u2019s question.\u201d\n- Acting: Produce final answer\nSystem prompt\nYou can shape how your agent approaches tasks by providing a prompt. The @[system_prompt\n] parameter can be provided as a string:\nsystem_prompt\n] is provided, the agent will infer its task from the messages directly.\nDynamic system prompt\nFor more advanced use cases where you need to modify the system prompt based on runtime context or agent state, you can use middleware.Invocation\nYou can invoke an agent by passing an update to its state. All agents include a sequence of messages in their state; to invoke the agent, pass a new message:Advanced concepts\nStructured output\nIn some situations, you may want the agent to return an output in a specific format. LangChain provides a simple, universal way to do this with theresponseFormat\nparameter.\nMemory\nAgents maintain conversation history automatically through the message state. You can also configure the agent to use a custom state schema to remember additional information during the conversation. Information stored in the state can be thought of as the short-term memory of the agent:Streaming\nWe\u2019ve seen how the agent can be called withinvoke\nto get a final response. If the agent executes multiple steps, this may take a while. To show intermediate progress, we can stream back messages as they occur.\nMiddleware\nMiddleware provides powerful extensibility for customizing agent behavior at different stages of execution. You can use middleware to:- Process state before the model is called (e.g., message trimming, context injection)\n- Modify or validate the model\u2019s response (e.g., guardrails, content filtering)\n- Handle tool execution errors with custom logic\n- Implement dynamic model selection based on state or context\n- Add custom logging, monitoring, or analytics", "tokens": 780, "node_type": "child"}
{"id": 298, "chunk_id": "c16acf8a553ac24f5d77e7d3a1c6e493", "parent_id": 283, "url": "", "namespace": "langchain", "title": "oss-javascript-langchain-context-engineering", "headers": ["oss-javascript-langchain-context-engineering"], "section_index": 0, "chunk_index": 0, "text": "# oss-javascript-langchain-context-engineering\n\n> Source: https://docs.langchain.com/oss/javascript/langchain/context-engineering\n\nLangChain v1.0Welcome to the new LangChain documentation! If you encounter any issues or have feedback, please open an issue so we can improve. Archived v0 documentation can be found here.See the release notes and migration guide for a complete list of changes and instructions on how to upgrade your code.\n- The underlying LLM is just not good enough\n- The \u201cright\u201d context was not passed to the LLM\nThe core agent loop\nIt\u2019s important to understand the core agent loop to understand where context should be accessed and/or updated from. The core agent loop is quite simple:- Get user input\n- Call LLM, asking it to either respond or call tools\n- If it decides to call tools - then go and execute those tools\n- Repeat steps 2 and 3 until it decides to finish\nThe model\nThe model (including specific model parameters) that you use is a key part of the agent loop. It drives the whole agent\u2019s reasoning logic. One reason the agent could mess up is the model you are using is just not good enough. In order to build reliable agents, you have to have access to all the possible models. LangChain, with its standard model interfaces, supports this - we have over 50 different provider integrations. Model choice is also related to context engineering, in two ways. First, the way you pass the context to the LLM may depend on what LLM you are using. Some model providers are better at JSON, some at XML. The context engineering you do may be specific to the model choice. Second, the right model to use in the agent loop may depend on the context you want to pass it. As an obvious example - some models have different context windows. If the context in an agent builds up, you may want to use one model provider while the context is small, and then once it gets too large for that model\u2019s context window you may want to switch to another model.Types of context\nThere are a few different types of context that can be used to construct the context that is ultimately passed to the LLM. Instructions: Base instructions from the developer, commonly referred to as the system prompt. This may be static or dynamic. Tools: What tools the agent has access to. The names and descriptions and arguments of these are just as important as the text in the prompt. Structured output: What format the agent should respond in. The name and description and arguments of these are just as important as the text in the prompt. Session context: We also call this \u201cshort term memory\u201d in the docs. In the context of a conversation, this is most easily thought of the list of messages that make up the conversation. But there can often be other, more structured information that you may want the agent to access or update throughout the session. The agent can read and write this context. This context is often put directly into the context that is passed to the LLM. Examples include: messages, files. Long term memory: This is information that should persist across sessions (conversations). Examples include: extracted preferences Runtime configuration context: This is context that is not the \u201cstate\u201d or \u201cmemory\u201d of the agent, but rather configuration for a given agent run. This is not modified by the agent, and typically isn\u2019t passed into the LLM, but is used to guide the agent\u2019s behavior or look up other context. Examples include: user ID, DB connectionsContext engineering with LangChain\nNow we understand the basic agent loop, the importance of the model you use, and the different types of context that exist. Let\u2019s explore the concrete patterns LangChain provides for context engineering.Managing instructions (system prompts)\nStatic instructions\nFor fixed instructions that don\u2019t change, use the @[system_prompt\n] parameter:\nDynamic instructions\nFor instructions that depend on context (user profile, preferences, session data), use the @[@dynamic_prompt\n] middleware:\nWhen to use each:\n- Static prompts: Base instructions that never change\n- Dynamic prompts: Personalization, A/B testing, context-dependent behavior\nManaging conversation context (messages)\nLong conversations can exceed context windows or degrade model performance. Use middleware to manage conversation history:Trimming messages\nSummarizationMiddleware\nwhich automatically summarizes old messages when approaching token limits.\nSee Before model hook for more examples.\nContextual tool execution\nTools can access runtime context, session state, and long-term memory to make context-aware decisions: See Tools for comprehensive examples of accessing state, context, and memory in tools.Dynamic tool selection\nControl which tools the agent can access based on context, state, or user permissions:Dynamic model selection\nSwitch models based on conversation complexity, context window needs, or cost optimization:Best practices\n- Start simple - Begin with static prompts and tools, add dynamics only when needed\n- Test incrementally - Add one context engineering feature at a time\n- Monitor performance - Track model calls, token usage, and latency\n- Use built-in middleware - Leverage\nSummarizationMiddleware\n,LLMToolSelectorMiddleware\n, etc. - Document your context strategy - Make it clear what context is being passed and why\nRelated resources\n- Middleware - Complete middleware guide\n- Tools - Tool creation and context access\n- Memory - Short-term and long-term memory patterns\n- Agents - Core agent concepts", "tokens": 875, "node_type": "child"}
{"id": 299, "chunk_id": "c16acf8a553ac24f5d77e7d3a1c6e493", "parent_id": 284, "url": "", "namespace": "langchain", "title": "oss-javascript-langchain-deploy", "headers": ["oss-javascript-langchain-deploy"], "section_index": 0, "chunk_index": 0, "text": "# oss-javascript-langchain-deploy\n\n> Source: https://docs.langchain.com/oss/javascript/langchain/deploy\n\nLangChain v1.0Welcome to the new LangChain documentation! If you encounter any issues or have feedback, please open an issue so we can improve. Archived v0 documentation can be found here.See the release notes and migration guide for a complete list of changes and instructions on how to upgrade your code.\nPrerequisites\nBefore you begin, ensure you have the following:- A GitHub account\n- A LangSmith account (free to sign up)\nDeploy your agent\n1. Create a repository on GitHub\nYour application\u2019s code must reside in a GitHub repository to be deployed on LangSmith. Both public and private repositories are supported. For this quickstart, first make sure your app is LangGraph-compatible by following the local server setup guide. Then, push your code to the repository.2. Deploy to LangSmith\n2\nCreate new deployment\nClick the + New Deployment button. A pane will open where you can fill in the required fields.\n3\nLink repository\nIf you are a first time user or adding a private repository that has not been previously connected, click the Add new account button and follow the instructions to connect your GitHub account.\n4\nDeploy repository\nSelect your application\u2019s repository. Click Submit to deploy. This may take about 15 minutes to complete. You can check the status in the Deployment details view.\n3. Test your application in Studio\nOnce your application is deployed:- Select the deployment you just created to view more details.\n- Click the Studio button in the top right corner. Studio will open to display your graph.\n4. Get the API URL for your deployment\n- In the Deployment details view in LangGraph, click the API URL to copy it to your clipboard.\n- Click the\nURL\nto copy it to the clipboard.\n5. Test the API\nYou can now test the API:- Python\n- Rest API\n- Install LangGraph Python:\n- Send a message to the agent:", "tokens": 318, "node_type": "child"}
{"id": 300, "chunk_id": "c16acf8a553ac24f5d77e7d3a1c6e493", "parent_id": 285, "url": "", "namespace": "langchain", "title": "oss-javascript-langchain-guardrails", "headers": ["oss-javascript-langchain-guardrails"], "section_index": 0, "chunk_index": 0, "text": "# oss-javascript-langchain-guardrails\n\n> Source: https://docs.langchain.com/oss/javascript/langchain/guardrails\n\nLangChain v1.0Welcome to the new LangChain documentation! If you encounter any issues or have feedback, please open an issue so we can improve. Archived v0 documentation can be found here.See the release notes and migration guide for a complete list of changes and instructions on how to upgrade your code.\n- Preventing PII leakage\n- Detecting and blocking prompt injection attacks\n- Blocking inappropriate or harmful content\n- Enforcing business rules and compliance requirements\n- Validating output quality and accuracy\nGuardrails can be implemented using two complementary approaches:\nDeterministic guardrails\nUse rule-based logic like regex patterns, keyword matching, or explicit checks. Fast, predictable, and cost-effective, but may miss nuanced violations.\nModel-based guardrails\nUse LLMs or classifiers to evaluate content with semantic understanding. Catch subtle issues that rules miss, but are slower and more expensive.\nBuilt-in guardrails\nPII detection\nLangChain provides built-in middleware for detecting and handling Personally Identifiable Information (PII) in conversations. This middleware can detect common PII types like emails, credit cards, IP addresses, and more. PII detection middleware is helpful for cases such as health care and financial applications with compliance requirements, customer service agents that need to sanitize logs, and generally any application handling sensitive user data. The PII middleware supports multiple strategies for handling detected PII:| Strategy | Description | Example |\n|---|---|---|\nredact | Replace with [REDACTED_TYPE] | [REDACTED_EMAIL] |\nmask | Partially obscure (e.g., last 4 digits) | ****-****-****-1234 |\nhash | Replace with deterministic hash | a8f5f167... |\nblock | Raise exception when detected | Error thrown |\nBuilt-in PII types and configuration\nBuilt-in PII types and configuration\nBuilt-in PII types:\nemail\n- Email addressescredit_card\n- Credit card numbers (Luhn validated)ip\n- IP addressesmac_address\n- MAC addressesurl\n- URLs\n| Parameter | Description | Default |\n|---|---|---|\npiiType | Type of PII to detect (built-in or custom) | Required |\nstrategy | How to handle detected PII (\"block\" , \"redact\" , \"mask\" , \"hash\" ) | \"redact\" |\ndetector | Custom detector regex pattern | undefined (uses built-in) |\napplyToInput | Check user messages before model call | true |\napplyToOutput | Check AI messages after model call | false |\napplyToToolResults | Check tool result messages after execution | false |\nHuman-in-the-loop\nLangChain provides built-in middleware for requiring human approval before executing sensitive operations. This is one of the most effective guardrails for high-stakes decisions. Human-in-the-loop middleware is helpful for cases such as financial transactions and transfers, deleting or modifying production data, sending communications to external parties, and any operation with significant business impact.Custom guardrails\nFor more sophisticated guardrails, you can create custom middleware that runs before or after the agent executes. This gives you full control over validation logic, content filtering, and safety checks.Before agent guardrails\nUse \u201cbefore agent\u201d hooks to validate requests once at the start of each invocation. This is useful for session-level checks like authentication, rate limiting, or blocking inappropriate requests before any processing begins.After agent guardrails\nUse \u201cafter agent\u201d hooks to validate final outputs once before returning to the user. This is useful for model-based safety checks, quality validation, or final compliance scans on the complete agent response.Combine multiple guardrails\nYou can stack multiple guardrails by adding them to the middleware array. They execute in order, allowing you to build layered protection:Additional resources\n- Middleware documentation - Complete guide to custom middleware\n- Middleware API reference - Complete guide to custom middleware\n- Human-in-the-loop - Add human review for sensitive operations\n- Testing agents - Strategies for testing safety mechanisms", "tokens": 588, "node_type": "child"}
{"id": 301, "chunk_id": "c16acf8a553ac24f5d77e7d3a1c6e493", "parent_id": 286, "url": "", "namespace": "langchain", "title": "oss-javascript-langchain-human-in-the-loop", "headers": ["oss-javascript-langchain-human-in-the-loop"], "section_index": 0, "chunk_index": 0, "text": "# oss-javascript-langchain-human-in-the-loop\n\n> Source: https://docs.langchain.com/oss/javascript/langchain/human-in-the-loop\n\nLangChain v1.0Welcome to the new LangChain documentation! If you encounter any issues or have feedback, please open an issue so we can improve. Archived v0 documentation can be found here.See the release notes and migration guide for a complete list of changes and instructions on how to upgrade your code.\napprove\n), modified before running (edit\n), or rejected with feedback (reject\n).\nInterrupt decision types\nThe middleware defines three built-in ways a human can respond to an interrupt:| Decision Type | Description | Example Use Case |\n|---|---|---|\n\u2705 approve | The action is approved as-is and executed without changes. | Send an email draft exactly as written |\n\u270f\ufe0f edit | The tool call is executed with modifications. | Change the recipient before sending an email |\n\u274c reject | The tool call is rejected, with an explanation added to the conversation. | Reject an email draft and explain how to rewrite it |\ninterrupt_on\n.\nWhen multiple tool calls are paused at the same time, each action requires a separate decision.\nDecisions must be provided in the same order as the actions appear in the interrupt request.\nWhen editing tool arguments, make changes conservatively. Significant modifications to the original arguments may cause the model to re-evaluate its approach and potentially execute the tool multiple times or take unexpected actions.\nConfiguring interrupts\nTo use HITL, add the middleware to the agent\u2019smiddleware\nlist when creating the agent.\nYou configure it with a mapping of tool actions to the decision types that are allowed for each action. The middleware will interrupt execution when a tool call matches an action in the mapping.\nYou must configure a checkpointer to persist the graph state across interrupts.\nIn production, use a persistent checkpointer like @[\nAsyncPostgresSaver\n]. For testing or prototyping, use @[InMemorySaver\n].When invoking the agent, pass a config\nthat includes the thread ID to associate execution with a conversation thread.\nSee the LangGraph interrupts documentation for details.Responding to interrupts\nWhen you invoke the agent, it runs until it either completes or an interrupt is raised. An interrupt is triggered when a tool call matches the policy you configured ininterrupt_on\n. In that case, the invocation result will include an __interrupt__\nfield with the actions that require review. You can then present those actions to a reviewer and resume execution once decisions are provided.\nDecision types\n- \u2705 approve\n- \u270f\ufe0f edit\n- \u274c reject\nUse\napprove\nto approve the tool call as-is and execute it without changes.Execution lifecycle\nThe middleware defines anafter_model\nhook that runs after the model generates a response but before any tool calls are executed:\n- The agent invokes the model to generate a response.\n- The middleware inspects the response for tool calls.\n- If any calls require human input, the middleware builds a\nHITLRequest\nwithaction_requests\nandreview_configs\nand calls interrupt. - The agent waits for human decisions.\n- Based on the\nHITLResponse\ndecisions, the middleware executes approved or edited calls, synthesizes @[ToolMessage]\u2018s for rejected calls, and resumes execution.", "tokens": 506, "node_type": "child"}
{"id": 302, "chunk_id": "c16acf8a553ac24f5d77e7d3a1c6e493", "parent_id": 288, "url": "", "namespace": "langchain", "title": "oss-javascript-langchain-knowledge-base", "headers": ["oss-javascript-langchain-knowledge-base"], "section_index": 0, "chunk_index": 0, "text": "# oss-javascript-langchain-knowledge-base\n\n> Source: https://docs.langchain.com/oss/javascript/langchain/knowledge-base\n\nLangGraph v1.0Welcome to the new LangGraph documentation! If you encounter any issues or have feedback, please open an issue so we can improve. Archived v0 documentation can be found here.See the release notes and migration guide for a complete list of changes and instructions on how to upgrade your code.\nOverview\nThis tutorial will familiarize you with LangChain\u2019s document loader, embedding, and vector store abstractions. These abstractions are designed to support retrieval of data\u2014 from (vector) databases and other sources \u2014 for integration with LLM workflows. They are important for applications that fetch data to be reasoned over as part of model inference, as in the case of retrieval-augmented generation, or RAG. Here we will build a search engine over a PDF document. This will allow us to retrieve passages in the PDF that are similar to an input query. The guide also includes a minimal RAG implementation on top of the search engine.Concepts\nThis guide focuses on retrieval of text data. We will cover the following concepts:Setup\nInstallation\nThis guide requires@langchain/community\nand pdf-parse\n:\nLangSmith\nMany of the applications you build with LangChain will contain multiple steps with multiple invocations of LLM calls. As these applications get more and more complex, it becomes crucial to be able to inspect what exactly is going on inside your chain or agent. The best way to do this is with LangSmith. After you sign up at the link above, make sure to set your environment variables to start logging traces:1. Documents and Document Loaders\nLangChain implements a Document abstraction, which is intended to represent a unit of text and associated metadata. It has three attributes:pageContent\n: a string representing the content;metadata\n: a dict containing arbitrary metadata;id\n: (optional) a string identifier for the document.\nmetadata\nattribute can capture information about the source of the document, its relationship to other documents, and other information. Note that an individual Document\nobject often represents a chunk of a larger document.\nWe can generate sample documents when desired:\nLoading documents\nLet\u2019s load a PDF into a sequence ofDocument\nobjects. Here is a sample PDF \u2014 a 10-k filing for Nike from 2023. We can consult the LangChain documentation for available PDF document loaders.\nPDFLoader\nloads one Document\nobject per PDF page. For each, we can easily access:\n- The string content of the page;\n- Metadata containing the file name and page number.\nSplitting\nFor both information retrieval and downstream question-answering purposes, a page may be too coarse a representation. Our goal in the end will be to retrieveDocument\nobjects that answer an input query, and further splitting our PDF will help ensure that the meanings of relevant portions of the document are not \u201cwashed out\u201d by surrounding text.\nWe can use text splitters for this purpose. Here we will use a simple text splitter that partitions based on characters. We will split our documents into chunks of 1000 characters\nwith 200 characters of overlap between chunks. The overlap helps\nmitigate the possibility of separating a statement from important\ncontext related to it. We use the\nRecursiveCharacterTextSplitter\n,\nwhich will recursively split the document using common separators like\nnew lines until each chunk is the appropriate size. This is the\nrecommended text splitter for generic text use cases.\n2. Embeddings\nVector search is a common way to store and search over unstructured data (such as unstructured text). The idea is to store numeric vectors that are associated with the text. Given a query, we can embed it as a vector of the same dimension and use vector similarity metrics (such as cosine similarity) to identify related text. LangChain supports embeddings from dozens of providers. These models specify how text should be converted into a numeric vector. Let\u2019s select a model:- OpenAI\n- Azure\n- AWS\n- VertexAI\n- MistralAI\n- Cohere\n3. Vector stores\nLangChain @[VectorStore] objects contain methods for adding text andDocument\nobjects to the store, and querying them using various similarity metrics. They are often initialized with embedding models, which determine how text data is translated to numeric vectors.\nLangChain includes a suite of integrations with different vector store technologies. Some vector stores are hosted by a provider (e.g., various cloud providers) and require specific credentials to use; some (such as Postgres) run in separate infrastructure that can be run locally or via a third-party; others can run in-memory for lightweight workloads. Let\u2019s select a vector store:\n- Memory\n- Chroma\n- FAISS\n- MongoDB\n- PGVector\n- Pinecone\n- Qdrant\nVectorStore\n] that contains documents, we can query it. @[VectorStore] includes methods for querying:\n- Synchronously and asynchronously;\n- By string query and by vector;\n- With and without returning similarity scores;\n- By similarity and @[maximum marginal relevance][VectorStore.max_marginal_relevance_search] (to balance similarity with query to diversity in retrieved results).\n- @[API Reference][VectorStore]\n- Integration-specific docs\n4. Retrievers\nLangChain @[VectorStore\n] objects do not subclass @[Runnable]. LangChain @[Retrievers] are Runnables, so they implement a standard set of methods (e.g., synchronous and asynchronous invoke\nand batch\noperations). Although we can construct retrievers from vector stores, retrievers can interface with non-vector store sources of data, as well (such as external APIs).\nVectorstores implement an as_retriever\nmethod that will generate a Retriever, specifically a VectorStoreRetriever. These retrievers include specific search_type\nand search_kwargs\nattributes that identify what methods of the underlying vector store to call, and how to parameterize them. For instance, we can replicate the above with the following:", "tokens": 908, "node_type": "child"}
{"id": 303, "chunk_id": "c16acf8a553ac24f5d77e7d3a1c6e493", "parent_id": 289, "url": "", "namespace": "langchain", "title": "oss-javascript-langchain-long-term-memory", "headers": ["oss-javascript-langchain-long-term-memory"], "section_index": 0, "chunk_index": 0, "text": "# oss-javascript-langchain-long-term-memory\n\n> Source: https://docs.langchain.com/oss/javascript/langchain/long-term-memory\n\nLangChain v1.0Welcome to the new LangChain documentation! If you encounter any issues or have feedback, please open an issue so we can improve. Archived v0 documentation can be found here.See the release notes and migration guide for a complete list of changes and instructions on how to upgrade your code.\nOverview\nLangChain agents use LangGraph persistence to enable long-term memory. This is a more advanced topic and requires knowledge of LangGraph to use.Memory storage\nLangGraph stores long-term memories as JSON documents in a store. Each memory is organized under a customnamespace\n(similar to a folder) and a distinct key\n(like a file name). Namespaces often include user or org IDs or other labels that makes it easier to organize information.\nThis structure enables hierarchical organization of memories. Cross-namespace searching is then supported through content filters.\nRead long-term memory in tools\nA tool the agent can use to look up user information\nWrite long-term memory from tools\nExample of a tool that updates user information", "tokens": 169, "node_type": "child"}
{"id": 304, "chunk_id": "c16acf8a553ac24f5d77e7d3a1c6e493", "parent_id": 290, "url": "", "namespace": "langchain", "title": "oss-javascript-langchain-mcp", "headers": ["oss-javascript-langchain-mcp"], "section_index": 0, "chunk_index": 0, "text": "# oss-javascript-langchain-mcp\n\n> Source: https://docs.langchain.com/oss/javascript/langchain/mcp\n\nLangChain v1.0Welcome to the new LangChain documentation! If you encounter any issues or have feedback, please open an issue so we can improve. Archived v0 documentation can be found here.See the release notes and migration guide for a complete list of changes and instructions on how to upgrade your code.\nlangchain-mcp-adapters\nlibrary.\nInstall\nInstall the@langchain/mcp-adapters\nlibrary to use MCP tools in LangGraph:\nTransport types\nMCP supports different transport mechanisms for client-server communication:- stdio: Client launches server as a subprocess and communicates via standard input/output. Best for local tools and simple setups.\n- Streamable HTTP: Server runs as an independent process handling HTTP requests. Supports remote connections and multiple clients.\n- Server-Sent Events (SSE): a variant of streamable HTTP optimized for real-time streaming communication.\nUse MCP tools\n@langchain/mcp-adapters\nenables agents to use tools defined across one or more MCP server.\nAccessing multiple MCP servers\nMultiServerMCPClient\nis stateless by default. Each tool invocation creates a fresh MCP ClientSession\n, executes the tool, and then cleans up.Custom MCP servers\nTo create your own MCP servers, you can use the@modelcontextprotocol/sdk\nlibrary. This library provides a simple way to define tools and run them as servers.\nMath server (stdio transport)\nWeather server (SSE transport)\nStateful tool usage\nFor stateful servers that maintain context between tool calls, useclient.session()\nto create a persistent ClientSession\n.\nUsing MCP ClientSession for stateful tool usage", "tokens": 230, "node_type": "child"}
{"id": 305, "chunk_id": "c16acf8a553ac24f5d77e7d3a1c6e493", "parent_id": 291, "url": "", "namespace": "langchain", "title": "oss-javascript-langchain-messages", "headers": ["oss-javascript-langchain-messages"], "section_index": 0, "chunk_index": 0, "text": "# oss-javascript-langchain-messages\n\n> Source: https://docs.langchain.com/oss/javascript/langchain/messages\n\n- Role - Identifies the message type (e.g.\nsystem\n,user\n) - Content - Represents the actual content of the message (like text, images, audio, documents, etc.)\n- Metadata - Optional fields such as response information, message IDs, and token usage\nBasic usage\nThe simplest way to use messages is to create message objects and pass them to a model when invoking.Text prompts\nText prompts are strings - ideal for straightforward generation tasks where you don\u2019t need to retain conversation history.- You have a single, standalone request\n- You don\u2019t need conversation history\n- You want minimal code complexity\nMessage prompts\nAlternatively, you can pass in a list of messages to the model by providing a list of message objects.- Managing multi-turn conversations\n- Working with multimodal content (images, audio, files)\n- Including system instructions\nDictionary format\nYou can also specify messages directly in OpenAI chat completions format.Message types\n- System message - Tells the model how to behave and provide context for interactions\n- Human message - Represents user input and interactions with the model\n- AI message - Responses generated by the model, including text content, tool calls, and metadata\n- Tool message - Represents the outputs of tool calls\nSystem Message\nA @[SystemMessage\n] represent an initial set of instructions that primes the model\u2019s behavior. You can use a system message to set the tone, define the model\u2019s role, and establish guidelines for responses.\nHuman Message\nA @[HumanMessage\n] represents user input and interactions. They can contain text, images, audio, files, and any other amount of multimodal content.\nText content\nMessage metadata\nAI Message\nAnAIMessage\nrepresents the output of a model invocation. They can include multimodal data, tool calls, and provider-specific metadata that you can later access.\nAIMessage\nobjects are returned by the model when calling it, which contains all of the associated metadata in the response. However, that doesn\u2019t mean that\u2019s the only place they can be created/ modified from.\nProviders weight/contextualize types of messages differently, which means it is sometimes helpful to create a new AIMessage\nobject and insert it into the message history as if it came from the model.\nAttributes\nAttributes\nTool calls\nWhen models make tool calls, they\u2019re included in theAIMessage\n:\nToken usage\nAnAIMessage\ncan hold token counts and other usage metadata in its usage_metadata\nfield:\nStreaming and chunks\nDuring streaming, you\u2019ll receiveAIMessageChunk\nobjects that can be combined into a full message:\nTool Message\nFor models that support tool calling, AI messages can contain tool calls. Tool messages are used to pass the results of a single tool execution back to the model. Tools can generate @[ToolMessage\n] objects directly. Below, we show a simple example. Read more in the tools guide.\nAttributes\nAttributes\nartifact\nfield stores supplementary data that won\u2019t be sent to the model but can be accessed programmatically. This is useful for storing raw results, debugging information, or data for downstream processing without cluttering the model\u2019s context.Example: Using artifact for retrieval metadata\nExample: Using artifact for retrieval metadata\ncontent\ncontains text that the model will reference, an artifact\ncan contain document identifiers or other metadata that an application can use (e.g., to render a page). See example below:Content\nYou can think of a message\u2019s content as the payload of data that gets sent to the model. Messages have acontent\nattribute that is loosely-typed, supporting strings and lists of untyped objects (e.g., dictionaries). This allows support for provider-native structures directly in LangChain chat models, such as multimodal content and other data.\nSeparately, LangChain provides dedicated content types for text, reasoning, citations, multi-modal data, server-side tool calls, and other message content. See content blocks below.\nLangChain chat models accept message content in the content\nattribute, and can contain:\n- A string\n- A list of content blocks in a provider-native format\n- A list of LangChain\u2019s standard content blocks\nStandard content blocks\nLangChain provides a standard representation for message content that works across providers. Message objects implement acontentBlocks\nproperty that will lazily parse the content\nattribute into a standard, type-safe representation. For example, messages generated from ChatAnthropic or ChatOpenAI will include thinking\nor reasoning\nblocks in the format of the respective provider, but can be lazily parsed into a consistent ReasoningContentBlock\nrepresentation:\n- Anthropic\n- OpenAI\nLC_OUTPUT_VERSION\nenvironment variable to v1\n. Or,\ninitialize any chat model with outputVersion: \"v1\"\n:Multimodal\nMultimodality refers to the ability to work with data that comes in different forms, such as text, audio, images, and video. LangChain includes standard types for these data that can be used across providers. Chat models can accept multimodal data as input and generate it as output. Below we show short examples of input messages featuring multimodal data:Content block reference\nContent blocks are represented (either when creating a message or accessing thecontentBlocks\nfield) as a list of typed objects. Each item in the list must adhere to one of the following block types:\nCore\nCore\nContentBlock.Text\nContentBlock.Text\nMultimodal\nMultimodal\nContentBlock.Multimodal.Image\nContentBlock.Multimodal.Image\nContentBlock.Multimodal.Audio\nContentBlock.Multimodal.Audio\nContentBlock.Multimodal.Video\nContentBlock.Multimodal.Video\nContentBlock.Multimodal.File\nContentBlock.Multimodal.File\nTool Calling\nTool Calling\nContentBlock.Tools.ToolCall\nContentBlock.Tools.ToolCall\nContentBlock.Tools.ToolCallChunk\nContentBlock.Tools.ToolCallChunk\nContentBlock.Tools.InvalidToolCall\nContentBlock.Tools.InvalidToolCall\nServer-Side Tool Execution\nServer-Side Tool Execution\nContentBlock.Tools.ServerToolCall\nContentBlock.Tools.ServerToolCall\nContentBlock.Tools.ServerToolCallChunk\nContentBlock.Tools.ServerToolCallChunk\nContentBlock.Tools.ServerToolResult\nContentBlock.Tools.ServerToolResult\nContentBlock\n] type.\ncontent\n][BaseMessage(content)] property, but rather a new property that can be used to access the content of a message in a standardized format.Use with chat models\nChat models accept a sequence of message objects as input and return anAIMessage\nas output. Interactions are often stateless, so that a simple conversational loop involves invoking a model with a growing list of messages.\nRefer to the below guides to learn more:\n- Built-in features for persisting and managing conversation histories\n- Strategies for managing context windows, including trimming and summarizing messages", "tokens": 947, "node_type": "child"}
{"id": 306, "chunk_id": "c16acf8a553ac24f5d77e7d3a1c6e493", "parent_id": 292, "url": "", "namespace": "langchain", "title": "oss-javascript-langchain-middleware", "headers": ["oss-javascript-langchain-middleware"], "section_index": 0, "chunk_index": 0, "text": "# oss-javascript-langchain-middleware\n\n> Source: https://docs.langchain.com/oss/javascript/langchain/middleware\n\nWhat can middleware do?\nMonitor\nModify\nControl\nEnforce\ncreate_agent\n]:\nBuilt-in middleware\nLangChain provides prebuilt middleware for common use cases:Summarization\nAutomatically summarize conversation history when approaching token limits.- Long-running conversations that exceed context windows\n- Multi-turn dialogues with extensive history\n- Applications where preserving full conversation context matters\nConfiguration options\nConfiguration options\nHuman-in-the-loop\nPause agent execution for human approval, editing, or rejection of tool calls before they execute.- High-stakes operations requiring human approval (database writes, financial transactions)\n- Compliance workflows where human oversight is mandatory\n- Long running conversations where human feedback is used to guide the agent\nConfiguration options\nConfiguration options\nAnthropic prompt caching\nReduce costs by caching repetitive prompt prefixes with Anthropic models.- Applications with long, repeated system prompts\n- Agents that reuse the same context across invocations\n- Reducing API costs for high-volume deployments\nConfiguration options\nConfiguration options\nModel call limit\nLimit the number of model calls to prevent infinite loops or excessive costs.- Preventing runaway agents from making too many API calls\n- Enforcing cost controls on production deployments\n- Testing agent behavior within specific call budgets\nConfiguration options\nConfiguration options\nTool call limit\nLimit the number of tool calls to specific tools or all tools.- Preventing excessive calls to expensive external APIs\n- Limiting web searches or database queries\n- Enforcing rate limits on specific tool usage\nConfiguration options\nConfiguration options\nModel fallback\nAutomatically fallback to alternative models when the primary model fails.- Building resilient agents that handle model outages\n- Cost optimization by falling back to cheaper models\n- Provider redundancy across OpenAI, Anthropic, etc.\nConfiguration options\nConfiguration options\nPII detection\nDetect and handle Personally Identifiable Information in conversations.- Healthcare and financial applications with compliance requirements\n- Customer service agents that need to sanitize logs\n- Any application handling sensitive user data\nConfiguration options\nConfiguration options\nemail\n, credit_card\n, ip\n, mac_address\n, url\n) or a custom type name.\"block\"\n- Throw error when detected\"redact\"\n- Replace with[REDACTED_TYPE]\n\"mask\"\n- Partially mask (e.g.,****-****-****-1234\n)\"hash\"\n- Replace with deterministic hash\nPlanning\nAdd todo list management capabilities for complex multi-step tasks.write_todos\ntool and system prompts to guide effective task planning.Configuration options\nConfiguration options\nLLM tool selector\nUse an LLM to intelligently select relevant tools before calling the main model.- Agents with many tools (10+) where most aren\u2019t relevant per query\n- Reducing token usage by filtering irrelevant tools\n- Improving model focus and accuracy\nConfiguration options\nConfiguration options\nContext editing\nManage conversation context by trimming, summarizing, or clearing tool uses.- Long conversations that need periodic context cleanup\n- Removing failed tool attempts from context\n- Custom context management strategies\nConfiguration options\nConfiguration options\nCustom middleware\nBuild custom middleware by implementing hooks that run at specific points in the agent execution flow.Class-based middleware\nTwo hook styles\nNode-style hooks\nWrap-style hooks\nNode-style hooks\nRun at specific points in the execution flow:beforeAgent\n- Before agent starts (once per invocation)beforeModel\n- Before each model callafterModel\n- After each model responseafterAgent\n- After agent completes (up to once per invocation)\nWrap-style hooks\nIntercept execution and control when the handler is called:wrapModelCall\n- Around each model callwrapToolCall\n- Around each tool call\nCustom state schema\nMiddleware can extend the agent\u2019s state with custom properties. Define a custom state type and set it as thestate_schema\n:\nContext extension\nContext properties are configuration values passed through the runnable config. Unlike state, context is read-only and typically used for configuration that doesn\u2019t change during execution. Middleware can define context requirements that must be satisfied through the agent\u2019s configuration:Execution order\nWhen using multiple middleware, understanding execution order is important:Execution flow (click to expand)\nExecution flow (click to expand)\nmiddleware1.before_agent()\nmiddleware2.before_agent()\nmiddleware3.before_agent()\nmiddleware1.before_model()\nmiddleware2.before_model()\nmiddleware3.before_model()\nmiddleware1.wrap_model_call()\n\u2192middleware2.wrap_model_call()\n\u2192middleware3.wrap_model_call()\n\u2192 model\nmiddleware3.after_model()\nmiddleware2.after_model()\nmiddleware1.after_model()\nmiddleware3.after_agent()\nmiddleware2.after_agent()\nmiddleware1.after_agent()\nbefore_*\nhooks: First to lastafter_*\nhooks: Last to first (reverse)wrap_*\nhooks: Nested (first middleware wraps all others)\nAgent jumps\nTo exit early from middleware, return a dictionary withjump_to\n:\n\"end\"\n: Jump to the end of the agent execution\"tools\"\n: Jump to the tools node\"model\"\n: Jump to the model node (or the firstbefore_model\nhook)\nbefore_model\nor after_model\n, jumping to \"model\"\nwill cause all before_model\nmiddleware to run again.\nTo enable jumping, decorate your hook with @hook_config(can_jump_to=[...])\n:\nBest practices\n- Keep middleware focused - each should do one thing well\n- Handle errors gracefully - don\u2019t let middleware errors crash the agent\n- Use appropriate hook types:\n- Node-style for sequential logic (logging, validation)\n- Wrap-style for control flow (retry, fallback, caching)\n- Clearly document any custom state properties\n- Unit test middleware independently before integrating\n- Consider execution order - place critical middleware first in the list\n- Use built-in middleware when possible, don\u2019t reinvent the wheel :)\nExamples\nDynamically selecting tools\nSelect relevant tools at runtime to improve performance and accuracy.- Shorter prompts - Reduce complexity by exposing only relevant tools\n- Better accuracy - Models choose correctly from fewer options\n- Permission control - Dynamically filter tools based on user access\nAdditional resources\n- Middleware API reference - Complete guide to custom middleware\n- Human-in-the-loop - Add human review for sensitive operations\n- Testing agents - Strategies for testing safety mechanisms", "tokens": 861, "node_type": "child"}
{"id": 307, "chunk_id": "c16acf8a553ac24f5d77e7d3a1c6e493", "parent_id": 293, "url": "", "namespace": "langchain", "title": "oss-javascript-langchain-models", "headers": ["oss-javascript-langchain-models"], "section_index": 0, "chunk_index": 0, "text": "# oss-javascript-langchain-models > Source: https://docs.langchain.com/oss/javascript/langchain/models - Tool calling - calling external tools (like databases queries or API calls) and use results in their responses. - Structured output - where the model\u2019s response is constrained follow a defined format. - Multimodality - process and return data other than text, such as images, audio, and video. - Reasoning - models perform multi-step reasoning to arrive at a conclusion. Basic usage Models can be utilized in two ways:- With agents - Models can be dynamically specified when creating an agent. See the agents guide for more details on how to do this. - Standalone - Models can be called directly (outside of the agent loop) for tasks like text generation, classification, or extraction without the need for an agent framework. Initialize a model The easiest way to get started with a standalone model in LangChain is to useinitChatModel to initialize one from a provider of your choice (examples below): initChatModel for more detail, including information on how to pass model parameters. Key methods Invoke Stream Batch Parameters A chat model takes parameters that can be used to configure its behavior. The full set of supported parameters varies by model and provider, but standard ones include:initChatModel , pass these parameters as inline parameters: ChatOpenAI ] has use_responses_api to dictate whether to use the OpenAI Responses or Completions API.To find all the parameters supported by a given chat model, head to the chat model integrations page.Invocation A chat model must be invoked to generate an output. There are three primary invocation methods, each suited to different use cases.Invoke The most straightforward way to call a model is to useinvoke() with a single message or a list of messages. Stream Most models can stream their output content while it is being generated. By displaying output progressively, streaming significantly improves user experience, particularly for longer responses. Callingstream() returns an that yields output chunks as they are produced. You can use a loop to process each chunk in real-time: invoke() , which returns a single AIMessage after the model has finished generating its full response, stream() returns multiple AIMessageChunk objects, each containing a portion of the output text. Importantly, each chunk in a stream is designed to be gathered into a full message via summation: invoke() - for example, it can be aggregated into a message history and passed back to the model as conversational context. Advanced streaming topics Advanced streaming topics \"Auto-streaming\" chat models \"Auto-streaming\" chat models model.invoke() within nodes, but LangChain will automatically delegate to streaming if running in a streaming mode.How it works When youinvoke() a chat model, LangChain will automatically switch to an internal streaming mode if it detects that you are trying to stream the overall application. The result of the invocation will be the same as far as the code that was using invoke is concerned; however, while the chat model is being streamed, LangChain will take care of invoking @[on_llm_new_token ] events in LangChain\u2019s callback system.Callback events allow LangGraph stream() and streamEvents() to surface the chat model\u2019s output in real-time.Streaming events Streaming events streamEvents() ][BaseChatModel.streamEvents].This simplifies filtering based on event types and other metadata, and will aggregate the full message in the background. See below for an example.streamEvents() reference for event types and other details.Batch Batching a collection of independent requests to a model can significantly improve performance and reduce costs, as the processing can be done in parallel:batch() , you may want to control the maximum number of parallel calls. This can be done by setting the maxConcurrency attribute in the RunnableConfig dictionary.RunnableConfig reference for a full list of supported attributes.Tool calling Models can request to call tools that perform tasks such as fetching data from a database, searching the web, or running code. Tools are pairings of:- A schema, including the name of the tool, a description, and/or argument definitions (often a JSON schema) - A function or to execute. bindTools() . In subsequent invocations, the model can choose to call any of the bound tools as needed. Some model providers offer built-in tools that can be enabled via model or invocation parameters (e.g. ChatOpenAI , ChatAnthropic ). Check the respective provider reference for details. Tool execution loop Tool execution loop ToolMessage ] returned by the tool includes a tool_call_id that matches the original tool call, helping the model correlate results with requests.Forcing tool calls Forcing tool calls Parallel tool calls Parallel tool calls Streaming tool calls Streaming tool calls ToolCallChunk ]. This allows you to see tool calls as they\u2019re being generated rather than waiting for the complete response.Structured outputs Models can be requested to provide their response in a format matching a given schema. This is useful for ensuring the output can be easily parsed and used in subsequent processing. LangChain supports multiple schema types and methods for enforcing structured outputs.- Zod - JSON Schema - Method parameter: Some providers support different methods ( 'jsonSchema' ,'functionCalling' ,'jsonMode' ) - Include raw: Use @[ includeRaw: true ][BaseChatModel.with_structured_output(include_raw)] to get both the parsed output and the rawAIMessage - Validation: Zod models provide automatic validation, while JSON Schema requires manual validation Example: Message output alongside parsed structure Example: Message output alongside parsed structure AIMessage object alongside the parsed representation to access response metadata such as token counts. To do this, set @[include_raw=True ][BaseChatModel.with_structured_output(include_raw)] when calling @[with_structured_output ][BaseChatModel.with_structured_output]:Example: Nested structures Example: Nested structures Supported models LangChain supports all major model providers, including OpenAI, Anthropic, Google, Azure, AWS Bedrock, and more. Each provider offers a variety of models with different capabilities. For a full list of supported models in LangChain, see the integrations page.Advanced topics Multimodal Certain models can process and return non-textual data such as images, audio, and video. You can pass non-textual data to a model by providing content blocks.- Data in the cross-provider standard format (see our messages guide) - OpenAI chat completions format - Any format that is native to that specific provider (e.g., Anthropic models accept Anthropic native format) AIMessage will have content blocks with multimodal types.", "tokens": 1000, "node_type": "child"}
{"id": 308, "chunk_id": "9b0fd3b850698317bef57fa6e71a38ba", "parent_id": 293, "url": "", "namespace": "langchain", "title": "oss-javascript-langchain-models", "headers": ["oss-javascript-langchain-models"], "section_index": 0, "chunk_index": 1, "text": "validation Example: Message output alongside parsed structure Example: Message output alongside parsed structure AIMessage object alongside the parsed representation to access response metadata such as token counts. To do this, set @[include_raw=True ][BaseChatModel.with_structured_output(include_raw)] when calling @[with_structured_output ][BaseChatModel.with_structured_output]:Example: Nested structures Example: Nested structures Supported models LangChain supports all major model providers, including OpenAI, Anthropic, Google, Azure, AWS Bedrock, and more. Each provider offers a variety of models with different capabilities. For a full list of supported models in LangChain, see the integrations page.Advanced topics Multimodal Certain models can process and return non-textual data such as images, audio, and video. You can pass non-textual data to a model by providing content blocks.- Data in the cross-provider standard format (see our messages guide) - OpenAI chat completions format - Any format that is native to that specific provider (e.g., Anthropic models accept Anthropic native format) AIMessage will have content blocks with multimodal types. Reasoning Newer models are capable of performing multi-step reasoning to arrive at a conclusion. This involves breaking down complex problems into smaller, more manageable steps. If supported by the underlying model, you can surface this reasoning process to better understand how the model arrived at its final answer.'low' or 'high' ) or integer token budgets. For details, see the integrations page or reference for your respective chat model. Local models LangChain supports running models locally on your own hardware. This is useful for scenarios where either data privacy is critical, you want to invoke a custom model, or when you want to avoid the costs incurred when using a cloud-based model. Ollama is one of the easiest ways to run models locally. See the full list of local integrations on the integrations page.Prompt caching Many providers offer prompt caching features to reduce latency and cost on repeat processing of the same tokens. These features can be implicit or explicit:- Implicit prompt caching: providers will automatically pass on cost savings if a request hits a cache. Examples: OpenAI and Gemini (Gemini 2.5 and above). - Explicit caching: providers allow you to manually indicate cache points for greater control or to guarantee cost savings. Examples: @[ ChatOpenAI ] (viaprompt_cache_key ), Anthropic, AWS Bedrock, Gemini. Server-side tool use Some providers support server-side tool-calling loops: models can interact with web search, code interpreters, and other tools and analyze the results in a single conversational turn. If a model invokes a tool server-side, the content of the response message will include content representing the invocation and result of the tool. Accessing the content blocks of the response will return the server-side tool calls and results in a provider-agnostic format:Base URL or proxy For many chat model integrations, you can configure the base URL for API requests, which allows you to use model providers that have OpenAI-compatible APIs or to use a proxy server.Base URL Base URL Log probabilities Certain models can be configured to return token-level log probabilities representing the likelihood of a given token by setting thelogprobs parameter when initializing the model: Token usage A number of model providers return token usage information as part of the invocation response. When available, this information will be included on theAIMessage objects produced by the corresponding model. For more details, see the messages guide. Invocation config When invoking a model, you can pass additional configuration through theconfig parameter using a RunnableConfig object. This provides run-time control over execution behavior, callbacks, and metadata tracking. Common configuration options include: - Debugging with LangSmith tracing - Implementing custom logging or monitoring - Controlling resource usage in production - Tracking invocations across complex pipelines Key configuration attributes Key configuration attributes", "tokens": 599, "node_type": "child"}
{"id": 309, "chunk_id": "c16acf8a553ac24f5d77e7d3a1c6e493", "parent_id": 294, "url": "", "namespace": "langchain", "title": "oss-javascript-langchain-multi-agent", "headers": ["oss-javascript-langchain-multi-agent"], "section_index": 0, "chunk_index": 0, "text": "# oss-javascript-langchain-multi-agent\n\n> Source: https://docs.langchain.com/oss/javascript/langchain/multi-agent\n\nLangChain v1.0Welcome to the new LangChain documentation! If you encounter any issues or have feedback, please open an issue so we can improve. Archived v0 documentation can be found here.See the release notes and migration guide for a complete list of changes and instructions on how to upgrade your code.\n- A single agent has too many tools and makes poor decisions about which to use.\n- Context or memory grows too large for one agent to track effectively.\n- Tasks require specialization (e.g., a planner, researcher, math expert).\nMulti-agent patterns\n| Pattern | How it works | Control flow | Example use case |\n|---|---|---|---|\n| Tool Calling | A supervisor agent calls other agents as tools. The \u201ctool\u201d agents don\u2019t talk to the user directly \u2014 they just run their task and return results. | Centralized: all routing passes through the calling agent. | Task orchestration, structured workflows. |\n| Handoffs | The current agent decides to transfer control to another agent. The active agent changes, and the user may continue interacting directly with the new agent. | Decentralized: agents can change who is active. | Multi-domain conversations, specialist takeover. |\nTutorial: Build a supervisor agent\nLearn how to build a personal assistant using the supervisor pattern, where a central supervisor agent coordinates specialized worker agents.\nThis tutorial demonstrates:\n- Creating specialized sub-agents for different domains (calendar and email)\n- Wrapping sub-agents as tools for centralized orchestration\n- Adding human-in-the-loop review for sensitive actions\nChoosing a pattern\n| Question | Tool Calling | Handoffs |\n|---|---|---|\n| Need centralized control over workflow? | \u2705 Yes | \u274c No |\n| Want agents to interact directly with the user? | \u274c No | \u2705 Yes |\n| Complex, human-like conversation between specialists? | \u274c Limited | \u2705 Strong |\nYou can mix both patterns \u2014 use handoffs for agent switching, and have each agent call subagents as tools for specialized tasks.\nCustomizing agent context\nAt the heart of multi-agent design is context engineering - deciding what information each agent sees. LangChain gives you fine-grained control over:- Which parts of the conversation or state are passed to each agent.\n- Specialized prompts tailored to subagents.\n- Inclusion/exclusion of intermediate reasoning.\n- Customizing input/output formats per agent.\nTool calling\nIn tool calling, one agent (the \u201ccontroller\u201d) treats other agents as tools to be invoked when needed. The controller manages orchestration, while tool agents perform specific tasks and return results. Flow:- The controller receives input and decides which tool (subagent) to call.\n- The tool agent runs its task based on the controller\u2019s instructions.\n- The tool agent returns results to the controller.\n- The controller decides the next step or finishes.\nAgents used as tools are generally not expected to continue conversation with the user.\nTheir role is to perform a task and return results to the controller agent.\nIf you need subagents to be able to converse with the user, use handoffs instead.\nImplementation\nBelow is a minimal example where the main agent is given access to a single subagent via a tool definition:- The main agent invokes\ncall_subagent1\nwhen it decides the task matches the subagent\u2019s description. - The subagent runs independently and returns its result.\n- The main agent receives the result and continues orchestration.\nWhere to customize\nThere are several points where you can control how context is passed between the main agent and its subagents:- Subagent name (\n\"subagent1_name\"\n): This is how the main agent refers to the subagent. Since it influences prompting, choose it carefully. - Subagent description (\n\"subagent1_description\"\n): This is what the main agent \u201cknows\u201d about the subagent. It directly shapes how the main agent decides when to call it. - Input to the subagent: You can customize this input to better shape how the subagent interprets tasks. In the example above, we pass the agent-generated\nquery\ndirectly. - Output from the subagent: This is the response passed back to the main agent. You can adjust what is returned to control how the main agent interprets results. In the example above, we return the final message text, but you could return additional state or metadata.\nControl the input to the subagent\nThere are two main levers to control the input that the main agent passes to a subagent:- Modify the prompt \u2013 Adjust the main agent\u2019s prompt or the tool metadata (i.e., sub-agent\u2019s name and description) to better guide when and how it calls the subagent.\n- Context injection \u2013 Add input that isn\u2019t practical to capture in a static prompt (e.g., full message history, prior results, task metadata) by adjusting the tool call to pull from the agent\u2019s state.\nControl the output from the subagent\nTwo common strategies for shaping what the main agent receives back from a subagent:- Modify the prompt \u2013 Refine the subagent\u2019s prompt to specify exactly what should be returned.\n- Useful when outputs are incomplete, too verbose, or missing key details.\n- A common failure mode is that the subagent performs tool calls or reasoning but does not include the results in its final message. Remind it that the controller (and user) only see the final output, so all relevant info must be included there.\n- Custom output formatting \u2013 adjust or enrich the subagent\u2019s response in code before handing it back to the main agent.\n- Example: pass specific state keys back to the main agent in addition to the final text.\n- This requires wrapping the result in a\nCommand\n(or equivalent structure) so you can merge custom state with the subagent\u2019s response.\nHandoffs\nIn handoffs, agents can directly pass control to each other. The \u201cactive\u201d agent changes, and the user interacts with whichever agent currently has control. Flow:- The current agent decides it needs help from another agent.\n- It passes control (and state) to the next agent.\n- The new agent interacts directly with the user until it decides to hand off again or finish.", "tokens": 999, "node_type": "child"}
{"id": 310, "chunk_id": "c16acf8a553ac24f5d77e7d3a1c6e493", "parent_id": 295, "url": "", "namespace": "langchain", "title": "oss-javascript-langchain-observability", "headers": ["oss-javascript-langchain-observability"], "section_index": 0, "chunk_index": 0, "text": "# oss-javascript-langchain-observability\n\n> Source: https://docs.langchain.com/oss/javascript/langchain/observability\n\nLangChain v1.0Welcome to the new LangChain documentation! If you encounter any issues or have feedback, please open an issue so we can improve. Archived v0 documentation can be found here.See the release notes and migration guide for a complete list of changes and instructions on how to upgrade your code.\ncreate_agent\n], you get built-in observability through LangSmith - a powerful platform for tracing, debugging, evaluating, and monitoring your LLM applications.\nTraces capture every step your agent takes, from the initial user input to the final response, including all tool calls, model interactions, and decision points. This enables you to debug your agents, evaluate performance, and monitor usage.\nPrerequisites\nBefore you begin, ensure you have the following:- A LangSmith account (free to sign up)\nEnable tracing\nAll LangChain agents automatically support LangSmith tracing. To enable it, set the following environment variables:Quick start\nNo extra code is needed to log a trace to LangSmith. Just run your agent code as you normally would:default\n. To configure a custom project name, see Log to a project.\nTrace selectively\nYou may opt to trace specific invocations or parts of your application using LangSmith\u2019stracing_context\ncontext manager:\nLog to a project\nStatically\nStatically\nYou can set a custom project name for your entire application by setting the\nLANGSMITH_PROJECT\nenvironment variable:Dynamically\nDynamically\nYou can set the project name programmatically for specific operations:\nAdd metadata to traces\nYou can annotate your traces with custom metadata and tags:tracing_context\nalso accepts tags and metadata for fine-grained control:", "tokens": 253, "node_type": "child"}
{"id": 311, "chunk_id": "c16acf8a553ac24f5d77e7d3a1c6e493", "parent_id": 296, "url": "", "namespace": "langchain", "title": "oss-javascript-langchain-overview", "headers": ["oss-javascript-langchain-overview"], "section_index": 0, "chunk_index": 0, "text": "# oss-javascript-langchain-overview\n\n> Source: https://docs.langchain.com/oss/javascript/langchain/overview\n\nLangChain v1.0Welcome to the new LangChain documentation! If you encounter any issues or have feedback, please open an issue so we can improve. Archived v0 documentation can be found here.See the release notes and migration guide for a complete list of changes and instructions on how to upgrade your code.\nInstall\nCreate an agent\nCore benefits\nStandard model interface\nDifferent providers have unique APIs for interacting with models, including the format of responses. LangChain standardizes how you interact with models so that you can seamlessly swap providers and avoid lock-in.\nEasy to use, highly flexible agent\nLangChain\u2019s agent abstraction is designed to be easy to get started with, letting you build a simple agent in under 10 lines of code. But it also provides enough flexibility to allow you to do all the context engineering your heart desires.\nBuilt on top of LangGraph\nLangChain\u2019s agents are built on top of LangGraph. This allows us to take advantage of LangGraph\u2019s durable execution, human-in-the-loop support, persistence, and more.\nDebug with LangSmith\nGain deep visibility into complex agent behavior with visualization tools that trace execution paths, capture state transitions, and provide detailed runtime metrics.", "tokens": 196, "node_type": "child"}
{"id": 312, "chunk_id": "c16acf8a553ac24f5d77e7d3a1c6e493", "parent_id": 297, "url": "", "namespace": "langchain", "title": "oss-javascript-langchain-philosophy", "headers": ["oss-javascript-langchain-philosophy"], "section_index": 0, "chunk_index": 0, "text": "# oss-javascript-langchain-philosophy\n\n> Source: https://docs.langchain.com/oss/javascript/langchain/philosophy\n\n- LLMs are great, powerful new technology.\n- LLMs are even better when you combine them with external sources of data.\n- LLMs will transform what the applications of the future look like. Specifically, the applications of the future will look more and more agentic.\n- It is still very early on in that transformation.\n- While it\u2019s easy to build a prototype of those agentic applications, it\u2019s still really hard to build agents that are reliable enough to put into production.\nWe want to enable developers to build with the best models.\nWe want to make it easy to use models to orchestrate more complex flows that interact with other data and computation.\nHistory\nGiven the constant rate of change in the field, LangChain has also evolved over time. Below is a brief timeline of how LangChain has changed over the years, evolving alongside what it means to build with LLMs:- LLM abstractions\n- \u201cChains\u201d, or predetermined steps of computation to run, for common use cases. For example - RAG: run a retrieval step, then run a generation step.\n@langchain/community\n.@langchain/core\nmessage format accordingly to allow developers to specify these multimodal inputs in a standard way.-\nComplete revamp of all chains and agents in\nlangchain\n. All chains and agents are now replaced with only one high level abstraction: an agent abstraction built on top of LangGraph. This was the high-level abstraction that was originally created in LangGraph, but just moved to LangChain. For users still using old LangChain chains/agents who do NOT want to upgrade (note: we recommend you do), you can continue using old LangChain by installing the@langchain/classic\npackage. - A standard message content format: Model APIs evolved from returning messages with a simple content string to more complex output types - reasoning blocks, citations, server-side tool calls, etc. LangChain evolved its message formats to standardize these across providers.", "tokens": 317, "node_type": "child"}
{"id": 313, "chunk_id": "c16acf8a553ac24f5d77e7d3a1c6e493", "parent_id": 298, "url": "", "namespace": "langchain", "title": "oss-javascript-langchain-quickstart", "headers": ["oss-javascript-langchain-quickstart"], "section_index": 0, "chunk_index": 0, "text": "# oss-javascript-langchain-quickstart\n\n> Source: https://docs.langchain.com/oss/javascript/langchain/quickstart\n\nLangChain v1.0Welcome to the new LangChain documentation! If you encounter any issues or have feedback, please open an issue so we can improve. Archived v0 documentation can be found here.See the release notes and migration guide for a complete list of changes and instructions on how to upgrade your code.\nBuild a basic agent\nStart by creating a simple agent that can answer questions and call tools. The agent will use Claude Sonnet 4.5 as its language model, a basic weather function as a tool, and a simple prompt to guide its behavior.Build a real-world agent\nNext, build a practical weather forecasting agent that demonstrates key production concepts:- Detailed system prompts for better agent behavior\n- Create tools that integrate with external data\n- Model configuration for consistent responses\n- Structured output for predictable results\n- Conversational memory for chat-like interactions\n- Create and run the agent create a fully functional agent\n1\nDefine the system prompt\nThe system prompt defines your agent\u2019s role and behavior. Keep it specific and actionable:\n2\nCreate tools\nTools are functions your agent can call. Oftentimes tools will want to connect to external systems, and will rely on runtime configuration to do so. Notice here how the\ngetUserLocation\ntool does exactly that:Zod is a library for validating and parsing pre-defined schemas. You can use it to define the input schema for your tools to make sure the agent only calls the tool with the correct arguments.Alternatively, you can define the\nschema\nproperty as a JSON schema object. Keep in mind that JSON schemas won\u2019t be validated at runtime.Example: Using JSON schema for tool input\nExample: Using JSON schema for tool input\n4\nDefine response format\nOptionally, define a structured response format if you need the agent responses to match\na specific schema.\n5\n6\nCreate and run the agent\nNow assemble your agent with all the components and run it!\n- Understand context and remember conversations\n- Use multiple tools intelligently\n- Provide structured responses in a consistent format\n- Handle user-specific information through context\n- Maintain conversation state across interactions", "tokens": 352, "node_type": "child"}
{"id": 314, "chunk_id": "c16acf8a553ac24f5d77e7d3a1c6e493", "parent_id": 299, "url": "", "namespace": "langchain", "title": "oss-javascript-langchain-rag", "headers": ["oss-javascript-langchain-rag"], "section_index": 0, "chunk_index": 0, "text": "# oss-javascript-langchain-rag > Source: https://docs.langchain.com/oss/javascript/langchain/rag LangGraph v1.0Welcome to the new LangGraph documentation! If you encounter any issues or have feedback, please open an issue so we can improve. Archived v0 documentation can be found here.See the release notes and migration guide for a complete list of changes and instructions on how to upgrade your code. Overview One of the most powerful applications enabled by LLMs is sophisticated question-answering (Q&A) chatbots. These are applications that can answer questions about specific source information. These applications use a technique known as Retrieval Augmented Generation, or RAG. This tutorial will show how to build a simple Q&A application over an unstructured text data source. We will demonstrate:- A RAG agent that executes searches with a simple tool. This is a good general-purpose implementation. - A two-step RAG chain that uses just a single LLM call per query. This is a fast and effective method for simple queries. Concepts We will cover the following concepts:- Indexing: a pipeline for ingesting data from a source and indexing it. This usually happens in a separate process. - Retrieval and generation: the actual RAG process, which takes the user query at run time and retrieves the relevant data from the index, then passes that to the model. The indexing portion of this tutorial will largely follow the semantic search tutorial.If your data is already available for search (i.e., you have a function to execute a search), or you\u2019re comfortable with the content from that tutorial, feel free to skip to the section on retrieval and generation Preview In this guide we\u2019ll build an app that answers questions about the website\u2019s content. The specific website we will use is the LLM Powered Autonomous Agents blog post by Lilian Weng, which allows us to ask questions about the contents of the post. We can create a simple indexing pipeline and RAG chain to do this in ~40 lines of code. See below for the full code snippet:Setup Installation This tutorial requires these langchain dependencies:LangSmith Many of the applications you build with LangChain will contain multiple steps with multiple invocations of LLM calls. As these applications get more complex, it becomes crucial to be able to inspect what exactly is going on inside your chain or agent. The best way to do this is with LangSmith. After you sign up at the link above, make sure to set your environment variables to start logging traces:Components We will need to select three components from LangChain\u2019s suite of integrations. Select a chat model: Select an embeddings model:- OpenAI - Azure - AWS - VertexAI - MistralAI - Cohere - Memory - Chroma - FAISS - MongoDB - PGVector - Pinecone - Qdrant 1. Indexing This section is an abbreviated version of the content in the semantic search tutorial.If your data is already indexed and available for search (i.e., you have a function to execute a search), or if you\u2019re comfortable with document loaders, embeddings, and vector stores, feel free to skip to the next section on retrieval and generation. - Load: First we need to load our data. This is done with Document Loaders. - Split: Text splitters break large Documents into smaller chunks. This is useful both for indexing data and passing it into a model, as large chunks are harder to search over and won\u2019t fit in a model\u2019s finite context window. - Store: We need somewhere to store and index our splits, so that they can be searched over later. This is often done using a VectorStore and Embeddings model. Loading documents We need to first load the blog post contents. We can use DocumentLoaders for this, which are objects that load in data from a source and return a list of Document objects.DocumentLoader : Object that loads data from a source as list of Documents . - Integrations: 160+ integrations to choose from. - Interface: API reference for the base interface. Splitting documents Our loaded document is over 42k characters which is too long to fit into the context window of many models. Even for those models that could fit the full post in their context window, models can struggle to find information in very long inputs. To handle this we\u2019ll split theDocument into chunks for embedding and vector storage. This should help us retrieve only the most relevant parts of the blog post at run time. As in the semantic search tutorial, we use a RecursiveCharacterTextSplitter , which will recursively split the document using common separators like new lines until each chunk is the appropriate size. This is the recommended text splitter for generic text use cases. Storing documents Now we need to index our 66 text chunks so that we can search over them at runtime. Following the semantic search tutorial, our approach is to embed the contents of each document split and insert these embeddings into a vector store. Given an input query, we can then use vector search to retrieve relevant documents. We can embed and store all of our document splits in a single command using the vector store and embeddings model selected at the start of the tutorial.Embeddings : Wrapper around a text embedding model, used for converting text to embeddings. - Integrations: 30+ integrations to choose from. - Interface: API reference for the base interface. VectorStore : Wrapper around a vector database, used for storing and querying embeddings. - Integrations: 40+ integrations to choose from. - Interface: API reference for the base interface. 2. Retrieval and Generation RAG applications commonly work as follows:- Retrieve: Given a user input, relevant splits are retrieved from storage using a Retriever. - Generate: A model produces an answer using a prompt that includes both the question with the retrieved data - A RAG agent that executes searches with a simple tool. This is a good general-purpose implementation. - A two-step RAG chain that uses just a single LLM call per query. This is a fast and effective method for simple queries. RAG agents One formulation of", "tokens": 1000, "node_type": "child"}
{"id": 315, "chunk_id": "9b0fd3b850698317bef57fa6e71a38ba", "parent_id": 299, "url": "", "namespace": "langchain", "title": "oss-javascript-langchain-rag", "headers": ["oss-javascript-langchain-rag"], "section_index": 0, "chunk_index": 1, "text": "the start of the tutorial.Embeddings : Wrapper around a text embedding model, used for converting text to embeddings. - Integrations: 30+ integrations to choose from. - Interface: API reference for the base interface. VectorStore : Wrapper around a vector database, used for storing and querying embeddings. - Integrations: 40+ integrations to choose from. - Interface: API reference for the base interface. 2. Retrieval and Generation RAG applications commonly work as follows:- Retrieve: Given a user input, relevant splits are retrieved from storage using a Retriever. - Generate: A model produces an answer using a prompt that includes both the question with the retrieved data - A RAG agent that executes searches with a simple tool. This is a good general-purpose implementation. - A two-step RAG chain that uses just a single LLM call per query. This is a fast and effective method for simple queries. RAG agents One formulation of a RAG application is as a simple agent with a tool that retrieves information. We can assemble a minimal RAG agent by implementing a tool that wraps our vector store:Here we specify the responseFormat to content_and_artifact to confiugre the tool to attach raw documents as artifacts to each ToolMessage. This will let us access document metadata in our application, separate from the stringified representation that is sent to the model.- Generates a query to search for a standard method for task decomposition; - Receiving the answer, generates a second query to search for common extensions of it; - Having received all necessary context, answers the question. You can add a deeper level of control and customization using the LangGraph framework directly\u2014 for example, you can add steps to grade document relevance and rewrite search queries. Check out LangGraph\u2019s Agentic RAG tutorial for more advanced formulations. RAG chains In the above agentic RAG formulation we allow the LLM to use its discretion in generating a tool call to help answer user queries. This is a good general-purpose solution, but comes with some trade-offs:| \u2705 Benefits | \u26a0\ufe0f Drawbacks | |---|---| | Search only when needed \u2013 The LLM can handle greetings, follow-ups, and simple queries without triggering unnecessary searches. | Two inference calls \u2013 When a search is performed, it requires one call to generate the query and another to produce the final response. | Contextual search queries \u2013 By treating search as a tool with a query input, the LLM crafts its own queries that incorporate conversational context. | Reduced control \u2013 The LLM may skip searches when they are actually needed, or issue extra searches when unnecessary. | | Multiple searches allowed \u2013 The LLM can execute several searches in support of a single user query. | Returning source documents Returning source documents The above RAG chain incorporates retrieved context into a single system message for that run.As in the agentic RAG formulation, we sometimes want to include raw source documents in the application state to have access to document metadata. We can do this for the two-step chain case by: - Adding a key to the state to store the retrieved documents - Adding a new node via a pre-model hook to populate that key (as well as inject the context). Next steps Now that we\u2019ve implemented a simple RAG application via @[create_agent ], we can easily incorporate new features and go deeper: - Stream tokens and other information for responsive user experiences - Add conversational memory to support multi-turn interactions - Add long-term memory to support memory across conversational threads - Add structured responses - Deploy your application with LangSmith Deployments", "tokens": 594, "node_type": "child"}
{"id": 316, "chunk_id": "c16acf8a553ac24f5d77e7d3a1c6e493", "parent_id": 300, "url": "", "namespace": "langchain", "title": "oss-javascript-langchain-retrieval", "headers": ["oss-javascript-langchain-retrieval"], "section_index": 0, "chunk_index": 0, "text": "# oss-javascript-langchain-retrieval\n\n> Source: https://docs.langchain.com/oss/javascript/langchain/retrieval\n\nLangChain v1.0Welcome to the new LangChain documentation! If you encounter any issues or have feedback, please open an issue so we can improve. Archived v0 documentation can be found here.See the release notes and migration guide for a complete list of changes and instructions on how to upgrade your code.\n- Finite context \u2014 they can\u2019t ingest entire corpora at once.\n- Static knowledge \u2014 their training data is frozen at a point in time.\nBuilding a knowledge base\nA knowledge base is a repository of documents or structured data used during retrieval. If you need a custom knowledge base, you can use LangChain\u2019s document loaders and vector stores to build one from your own data.If you already have a knowledge base (e.g., a SQL database, CRM, or internal documentation system), you do not need to rebuild it. You can:\n- Connect it as a tool for an agent in Agentic RAG.\n- Query it and supply the retrieved content as context to the LLM (2-Step RAG).\nTutorial: Semantic search\nLearn how to create a searchable knowledge base from your own data using LangChain\u2019s document loaders, embeddings, and vector stores.\nIn this tutorial, you\u2019ll build a search engine over a PDF, enabling retrieval of passages relevant to a query. You\u2019ll also implement a minimal RAG workflow on top of this engine to see how external knowledge can be integrated into LLM reasoning.\nFrom retrieval to RAG\nRetrieval allows LLMs to access relevant context at runtime. But most real-world applications go one step further: they integrate retrieval with generation to produce grounded, context-aware answers. This is the core idea behind Retrieval-Augmented Generation (RAG). The retrieval pipeline becomes a foundation for a broader system that combines search with generation.Retrieval Pipeline\nA typical retrieval workflow looks like this: Each component is modular: you can swap loaders, splitters, embeddings, or vector stores without rewriting the app\u2019s logic.Building Blocks\nEmbedding models\nAn embedding model turns text into a vector of numbers so that texts with similar meaning land close together in that vector space.\nVector stores\nSpecialized databases for storing and searching embeddings.\nRetrievers\nA retriever is an interface that returns documents given an unstructured query.\nRAG Architectures\nRAG can be implemented in multiple ways, depending on your system\u2019s needs. We outline each type in the sections below.| Architecture | Description | Control | Flexibility | Latency | Example Use Case |\n|---|---|---|---|---|---|\n| 2-Step RAG | Retrieval always happens before generation. Simple and predictable | \u2705 High | \u274c Low | \u26a1 Fast | FAQs, documentation bots |\n| Agentic RAG | An LLM-powered agent decides when and how to retrieve during reasoning | \u274c Low | \u2705 High | \u23f3 Variable | Research assistants with access to multiple tools |\n| Hybrid | Combines characteristics of both approaches with validation steps | \u2696\ufe0f Medium | \u2696\ufe0f Medium | \u23f3 Variable | Domain-specific Q&A with quality validation |\nLatency: Latency is generally more predictable in 2-Step RAG, as the maximum number of LLM calls is known and capped. This predictability assumes that LLM inference time is the dominant factor. However, real-world latency may also be affected by the performance of retrieval steps\u2014such as API response times, network delays, or database queries\u2014which can vary based on the tools and infrastructure in use.\n2-step RAG\nIn 2-Step RAG, the retrieval step is always executed before the generation step. This architecture is straightforward and predictable, making it suitable for many applications where the retrieval of relevant documents is a clear prerequisite for generating an answer.Tutorial: Retrieval-Augmented Generation (RAG)\nSee how to build a Q&A chatbot that can answer questions grounded in your data using Retrieval-Augmented Generation.\nThis tutorial walks through two approaches:\n- A RAG agent that runs searches with a flexible tool\u2014great for general-purpose use.\n- A 2-step RAG chain that requires just one LLM call per query\u2014fast and efficient for simpler tasks.\nAgentic RAG\nAgentic Retrieval-Augmented Generation (RAG) combines the strengths of Retrieval-Augmented Generation with agent-based reasoning. Instead of retrieving documents before answering, an agent (powered by an LLM) reasons step-by-step and decides when and how to retrieve information during the interaction.The only thing an agent needs to enable RAG behavior is access to one or more tools that can fetch external knowledge \u2014 such as documentation loaders, web APIs, or database queries.\nTutorial: Retrieval-Augmented Generation (RAG)\nSee how to build a Q&A chatbot that can answer questions grounded in your data using Retrieval-Augmented Generation.\nThis tutorial walks through two approaches:\n- A RAG agent that runs searches with a flexible tool\u2014great for general-purpose use.\n- A 2-step RAG chain that requires just one LLM call per query\u2014fast and efficient for simpler tasks.\nHybrid RAG\nHybrid RAG combines characteristics of both 2-Step and Agentic RAG. It introduces intermediate steps such as query preprocessing, retrieval validation, and post-generation checks. These systems offer more flexibility than fixed pipelines while maintaining some control over execution. Typical components include:- Query enhancement: Modify the input question to improve retrieval quality. This can involve rewriting unclear queries, generating multiple variations, or expanding queries with additional context.\n- Retrieval validation: Evaluate whether retrieved documents are relevant and sufficient. If not, the system may refine the query and retrieve again.\n- Answer validation: Check the generated answer for accuracy, completeness, and alignment with source content. If needed, the system can regenerate or revise the answer.\n- Applications with ambiguous or underspecified queries\n- Systems that require validation or quality control steps\n- Workflows involving multiple sources or iterative refinement\nTutorial: Agentic RAG with Self-Correction\nAn example of Hybrid RAG that combines agentic reasoning with retrieval and self-correction.", "tokens": 939, "node_type": "child"}
{"id": 317, "chunk_id": "c16acf8a553ac24f5d77e7d3a1c6e493", "parent_id": 301, "url": "", "namespace": "langchain", "title": "oss-javascript-langchain-runtime", "headers": ["oss-javascript-langchain-runtime"], "section_index": 0, "chunk_index": 0, "text": "# oss-javascript-langchain-runtime\n\n> Source: https://docs.langchain.com/oss/javascript/langchain/runtime\n\nLangChain v1.0Welcome to the new LangChain documentation! If you encounter any issues or have feedback, please open an issue so we can improve. Archived v0 documentation can be found here.See the release notes and migration guide for a complete list of changes and instructions on how to upgrade your code.\nOverview\nLangChain\u2019screateAgent\nruns on LangGraph\u2019s runtime under the hood.\nLangGraph exposes a Runtime object with the following information:\n- Context: static information like user id, db connections, or other dependencies for an agent invocation\n- Store: a BaseStore instance used for long-term memory\n- Stream writer: an object used for streaming information via the\n\"custom\"\nstream mode\nAccess\nWhen creating an agent withcreateAgent\n, you can specify a contextSchema\nto define the structure of the context\nstored in the agent Runtime.\nWhen invoking the agent, pass the context\nargument with the relevant configuration for the run:\nInside tools\nYou can access the runtime information inside tools to:- Access the context\n- Read or write long-term memory\n- Write to the custom stream (ex, tool progress / updates)\nruntime\nparameter to access the Runtime object inside a tool.\nInside middleware\nYou can access runtime information in middleware to create dynamic prompts, modify messages, or control agent behavior based on user context. Use theruntime\nparameter to access the Runtime object inside middleware.", "tokens": 225, "node_type": "child"}
{"id": 318, "chunk_id": "c16acf8a553ac24f5d77e7d3a1c6e493", "parent_id": 302, "url": "", "namespace": "langchain", "title": "oss-javascript-langchain-short-term-memory", "headers": ["oss-javascript-langchain-short-term-memory"], "section_index": 0, "chunk_index": 0, "text": "# oss-javascript-langchain-short-term-memory\n\n> Source: https://docs.langchain.com/oss/javascript/langchain/short-term-memory\n\nLangChain v1.0Welcome to the new LangChain documentation! If you encounter any issues or have feedback, please open an issue so we can improve. Archived v0 documentation can be found here.See the release notes and migration guide for a complete list of changes and instructions on how to upgrade your code.\nOverview\nMemory is a system that remembers information about previous interactions. For AI agents, memory is crucial because it lets them remember previous interactions, learn from feedback, and adapt to user preferences. As agents tackle more complex tasks with numerous user interactions, this capability becomes essential for both efficiency and user satisfaction. Short term memory lets your application remember previous interactions within a single thread or conversation.A thread organizes multiple interactions in a session, similar to the way email groups messages in a single conversation.\nUsage\nTo add short-term memory (thread-level persistence) to an agent, you need to specify acheckpointer\nwhen creating an agent.\nLangChain\u2019s agent manages short-term memory as a part of your agent\u2019s state.By storing these in the graph\u2019s state, the agent can access the full context for a given conversation while maintaining separation between different threads.State is persisted to a database (or memory) using a checkpointer so the thread can be resumed at any time.Short-term memory updates when the agent is invoked or a step (like a tool call) is completed, and the state is read at the start of each step.\nIn production\nIn production, use a checkpointer backed by a database:Customizing agent memory\nBy default, agents use @[AgentState\n] to manage short term memory, specifically the conversation history via a messages\nkey.\nYou can extend @[AgentState\n] to add additional fields. Custom state schemas are passed to @[create_agent\n] using the @[state_schema\n] parameter.\nCommon patterns\nWith short-term memory enabled, long conversations can exceed the LLM\u2019s context window. Common solutions are:Trim messages\nRemove first or last N messages (before calling LLM)\nDelete messages\nDelete messages from LangGraph state permanently\nSummarize messages\nSummarize earlier messages in the history and replace them with a summary\nCustom strategies\nCustom strategies (e.g., message filtering, etc.)\nTrim messages\nMost LLMs have a maximum supported context window (denominated in tokens). One way to decide when to truncate messages is to count the tokens in the message history and truncate whenever it approaches that limit. If you\u2019re using LangChain, you can use the trim messages utility and specify the number of tokens to keep from the list, as well as thestrategy\n(e.g., keep the last maxTokens\n) to use for handling the boundary.\nTo trim message history in an agent, use stateModifier\nwith the trimMessages\nfunction:\nDelete messages\nYou can delete messages from the graph state to manage the message history. This is useful when you want to remove specific messages or clear the entire message history. To delete messages from the graph state, you can use theRemoveMessage\n. For RemoveMessage\nto work, you need to use a state key with messagesStateReducer\nreducer, like MessagesZodState\n.\nTo remove specific messages:\nWhen deleting messages, make sure that the resulting message history is valid. Check the limitations of the LLM provider you\u2019re using. For example:\n- Some providers expect message history to start with a\nuser\nmessage - Most providers require\nassistant\nmessages with tool calls to be followed by correspondingtool\nresult messages.\nSummarize messages\nThe problem with trimming or removing messages, as shown above, is that you may lose information from culling of the message queue. Because of this, some applications benefit from a more sophisticated approach of summarizing the message history using a chat model. To summarize message history in an agent, use the built-insummarizationMiddleware\n:\nsummarizationMiddleware\nfor more configuration options.\nAccess memory\nYou can access and modify the short-term memory (state) of an agent in several ways:Tools\nRead short-term memory in a tool\nAccess short term memory (state) in a tool using theToolRuntime\nparameter.\nThe tool_runtime\nparameter is hidden from the tool signature (so the model doesn\u2019t see it), but the tool can access the state through it.\nWrite short-term memory from tools\nTo modify the agent\u2019s short-term memory (state) during execution, you can return state updates directly from the tools. This is useful for persisting intermediate results or making information accessible to subsequent tools or prompts.Prompt\nAccess short term memory (state) in middleware to create dynamic prompts based on conversation history or custom state fields.Before model\nAccess short term memory (state) in @[@before_model\n] middleware to process messages before model calls.\nAfter model\nAccess short term memory (state) in @[@after_model\n] middleware to process messages after model calls.", "tokens": 763, "node_type": "child"}
{"id": 319, "chunk_id": "c16acf8a553ac24f5d77e7d3a1c6e493", "parent_id": 303, "url": "", "namespace": "langchain", "title": "oss-javascript-langchain-sql-agent", "headers": ["oss-javascript-langchain-sql-agent"], "section_index": 0, "chunk_index": 0, "text": "# oss-javascript-langchain-sql-agent\n\n> Source: https://docs.langchain.com/oss/javascript/langchain/sql-agent\n\nOverview\nIn this tutorial, you will learn how to build an agent that can answer questions about a SQL database using LangChain agents. At a high level, the agent will:Fetch the available tables and schemas from the database\nDecide which tables are relevant to the question\nFetch the schemas for the relevant tables\nGenerate a query based on the question and information from the schemas\nDouble-check the query for common mistakes using an LLM\nExecute the query and return the results\nCorrect mistakes surfaced by the database engine until the query is successful\nFormulate a response based on the results\nConcepts\nWe will cover the following concepts:- Tools for reading from SQL databases\n- LangChain agents\n- Human-in-the-loop processes\nSetup\nInstallation\nLangSmith\nSet up LangSmith to inspect what is happening inside your chain or agent. Then set the following environment variables:1. Select an LLM\nSelect a model that supports tool-calling: The output shown in the examples below used OpenAI.2. Configure the database\nYou will be creating a SQLite database for this tutorial. SQLite is a lightweight database that is easy to set up and use. We will be loading thechinook\ndatabase, which is a sample database that represents a digital media store.\nFor convenience, we have hosted the database (Chinook.db\n) on a public GCS bucket.\n3. Add tools for database interactions\nUse theSqlDatabase\nwrapper available in the langchain/sql_db\nto interact with the database. The wrapper provides a simple interface to execute SQL queries and fetch results:\n6. Implement human-in-the-loop review\nIt can be prudent to check the agent\u2019s SQL queries before they are executed for any unintended actions or inefficiencies. LangChain agents feature support for built-in human-in-the-loop middleware to add oversight to agent tool calls. Let\u2019s configure the agent to pause for human review on calling thesql_db_query\ntool:\nsql_db_query\ntool:\n4. Execute SQL queries\nBefore running the command, do a check to check the LLM generated command in _safe_sql\n:\nrun\nfrom SQLDatabase\nto execute commands with an execute_sql\ntool:\n5. Use createAgent\nUse createAgent\nto build a ReAct agent with minimal code. The agent will interpret the request and generate a SQL command. The tools will check the command for safety and then try to execute the command. If the command has an error, the error message is returned to the model. The model can then examine the original request and the new error message and generate a new command. This can continue until the LLM generates the command successfully or reaches an end count. This pattern of providing a model with feedback - error messages in this case - is very powerful.\nInitialize the agent with a descriptive system prompt to customize its behavior:\n6. Run the agent\nRun the agent on a sample query and observe its behavior:(Optional) Use Studio\nStudio provides a \u201cclient side\u201d loop as well as memory so you can run this as a chat interface and query the database. You can ask questions like \u201cTell me the scheme of the database\u201d or \u201cShow me the invoices for the 5 top customers\u201d. You will see the SQL command that is generated and the resulting output. The details of how to get that started are below.Run your agent in Studio\nRun your agent in Studio\nlanggraph.json\nfile with the following contents:", "tokens": 552, "node_type": "child"}
{"id": 320, "chunk_id": "c16acf8a553ac24f5d77e7d3a1c6e493", "parent_id": 304, "url": "", "namespace": "langchain", "title": "oss-javascript-langchain-streaming", "headers": ["oss-javascript-langchain-streaming"], "section_index": 0, "chunk_index": 0, "text": "# oss-javascript-langchain-streaming\n\n> Source: https://docs.langchain.com/oss/javascript/langchain/streaming\n\nLangChain v1.0Welcome to the new LangChain documentation! If you encounter any issues or have feedback, please open an issue so we can improve. Archived v0 documentation can be found here.See the release notes and migration guide for a complete list of changes and instructions on how to upgrade your code.\nOverview\nLangChain\u2019s streaming system lets you surface live feedback from agent runs to your application. What\u2019s possible with LangChain streaming:- Stream agent progress \u2014 get state updates after each agent step.\n- Stream LLM tokens \u2014 stream language model tokens as they\u2019re generated.\n- Stream custom updates \u2014 emit user-defined signals (e.g.,\n\"Fetched 10/100 records\"\n). - Stream multiple modes \u2014 choose from\nupdates\n(agent progress),messages\n(LLM tokens + metadata), orcustom\n(arbitrary user data).\nAgent progress\nTo stream agent progress, use thestream()\nmethod with streamMode: \"updates\"\n. This emits an event after every agent step.\nFor example, if you have an agent that calls a tool once, you should see the following updates:\n- LLM node:\nAIMessage\nwith tool call requests - Tool node: @[\nToolMessage\n] with execution result - LLM node: Final AI response\nLLM tokens\nTo stream tokens as they are produced by the LLM, usestreamMode: \"messages\"\n:\nCustom updates\nTo stream updates from tools as they are executed, you can use thewriter\nparameter from the configuration.\nOutput\nIf you add the\nwriter\nparameter to your tool, you won\u2019t be able to invoke the tool outside of a LangGraph execution context without providing a writer function.Stream multiple modes\nYou can specify multiple streaming modes by passing streamMode as an array:streamMode: [\"updates\", \"messages\", \"custom\"]\n:", "tokens": 272, "node_type": "child"}
{"id": 321, "chunk_id": "c16acf8a553ac24f5d77e7d3a1c6e493", "parent_id": 305, "url": "", "namespace": "langchain", "title": "oss-javascript-langchain-structured-output", "headers": ["oss-javascript-langchain-structured-output"], "section_index": 0, "chunk_index": 0, "text": "# oss-javascript-langchain-structured-output\n\n> Source: https://docs.langchain.com/oss/javascript/langchain/structured-output\n\nLangChain v1.0Welcome to the new LangChain documentation! If you encounter any issues or have feedback, please open an issue so we can improve. Archived v0 documentation can be found here.See the release notes and migration guide for a complete list of changes and instructions on how to upgrade your code.\ncreateAgent\nhandles structured output automatically. The user sets their desired structured output schema, and when the model generates the structured data, it\u2019s captured, validated, and returned in the structuredResponse\nkey of the agent\u2019s state.\nResponse Format\nControls how the agent returns structured data. You can provide either a Zod object or JSON schema. By default, the agent uses a tool calling strategy, in which the output is created by an additional tool call. Certain models support native structured output, in which case the agent will use that strategy instead. You can control the behavior by wrappingResponseFormat\nin a toolStrategy\nor providerStrategy\nfunction call:\nstructuredResponse\nkey of the agent\u2019s final state.\nProvider strategy\nSome model providers support structured output natively through their APIs (currently only OpenAI and Grok). This is the most reliable method when available. To use this strategy, configure aProviderStrategy\n:\nThe schema defining the structured output format. Supports:\n- Zod Schema: A zod schema\n- JSON Schema: A JSON schema object\nProviderStrategy\nwhen you pass a schema type directly to createAgent.responseFormat\nand the model supports native structured output:\nIf the provider natively supports structured output for your model choice, it is functionally equivalent to write\nresponseFormat: contactInfoSchema\ninstead of responseFormat: toolStrategy(contactInfoSchema)\n. In either case, if structured output is not supported, the agent will fall back to a tool calling strategy.Tool calling strategy\nFor models that don\u2019t support native structured output, LangChain uses tool calling to achieve the same result. This works with all models that support tool calling, which is most modern models. To use this strategy, configure aToolStrategy\n:\nThe schema defining the structured output format. Supports:\n- Zod Schema: A zod schema\n- JSON Schema: A JSON schema object\nCustom content for the tool message returned when structured output is generated.\nIf not provided, defaults to a message showing the structured response data.\nOptions parameter containing an optional\nhandleError\nparameter for customizing the error handling strategy.true\n: Catch all errors with default error template (default)False\n: No retry, let exceptions propagate(error: ToolStrategyError) => string | Promise<string>\n: retry with the provided message or throw the error\nCustom tool message content\nThetoolMessageContent\nparameter allows you to customize the message that appears in the conversation history when structured output is generated:\ntoolMessageContent\n, we\u2019d see:\nError handling\nModels can make mistakes when generating structured output via tool calling. LangChain provides intelligent retry mechanisms to handle these errors automatically.Multiple structured outputs error\nWhen a model incorrectly calls multiple structured output tools, the agent provides error feedback in a @[ToolMessage\n] and prompts the model to retry:\nSchema validation error\nWhen structured output doesn\u2019t match the expected schema, the agent provides specific error feedback:Error handling strategies\nYou can customize how errors are handled using thehandleErrors\nparameter:\nCustom error message:", "tokens": 515, "node_type": "child"}
{"id": 322, "chunk_id": "c16acf8a553ac24f5d77e7d3a1c6e493", "parent_id": 306, "url": "", "namespace": "langchain", "title": "oss-javascript-langchain-studio", "headers": ["oss-javascript-langchain-studio"], "section_index": 0, "chunk_index": 0, "text": "# oss-javascript-langchain-studio\n\n> Source: https://docs.langchain.com/oss/javascript/langchain/studio\n\nLangChain v1.0Welcome to the new LangChain documentation! If you encounter any issues or have feedback, please open an issue so we can improve. Archived v0 documentation can be found here.See the release notes and migration guide for a complete list of changes and instructions on how to upgrade your code.\nPrerequisites\nBefore you begin, ensure you have the following:- An API key for LangSmith (free to sign up)\nSetup local LangGraph server\n1. Install the LangGraph CLI\n2. Prepare your agent\nWe\u2019ll use the following simple agent as an example:agent.py\n3. Environment variables\nCreate a.env\nfile in the root of your project and fill in the necessary API keys. We\u2019ll need to set the LANGSMITH_API_KEY\nenvironment variable to the API key you get from LangSmith.\nBe sure not to commit your\n.env\nto version control systems such as Git!.env\n4. Create a LangGraph config file\nInside your app\u2019s directory, create a configuration filelanggraph.json\n:\nlanggraph.json\ncreate_agent\nautomatically returns a compiled LangGraph graph that we can pass to the graphs\nkey in our configuration file.\nSo far, our project structure looks like this:\n5. Install dependencies\nIn the root of your new LangGraph app, install the dependencies:6. View your agent in Studio\nStart your LangGraph server:Safari blocks\nlocalhost\nconnections to Studio. To work around this, run the above command with --tunnel\nto access Studio via a secure tunnel.http://127.0.0.1:2024\n) and the Studio UI https://smith.langchain.com/studio/?baseUrl=http://127.0.0.1:2024\n:", "tokens": 239, "node_type": "child"}
{"id": 323, "chunk_id": "c16acf8a553ac24f5d77e7d3a1c6e493", "parent_id": 307, "url": "", "namespace": "langchain", "title": "oss-javascript-langchain-test", "headers": ["oss-javascript-langchain-test"], "section_index": 0, "chunk_index": 0, "text": "# oss-javascript-langchain-test\n\n> Source: https://docs.langchain.com/oss/javascript/langchain/test\n\n- Unit tests exercise small, deterministic pieces of your agent in isolation using in-memory fakes so you can assert exact behavior quickly and deterministically.\n- Integration tests test the agent using real network calls to confirm that components work together, credentials and schemas line up, and latency is acceptable.\nIntegration Testing\nMany agent behaviors only emerge when using a real LLM, such as which tool the agent decides to call, how it formats responses, or whether a prompt modification affects the entire execution trajectory. LangChain\u2019sagentevals\npackage provides evaluators specifically designed for testing agent trajectories with live models.\nAgentEvals lets you easily evaluate the trajectory of your agent (the exact sequence of messages, including tool calls) by performing a trajectory match or by using an LLM judge:\nTrajectory match\nLLM-as-judge\nInstalling AgentEvals\nTrajectory Match Evaluator\nAgentEvals offers thecreateTrajectoryMatchEvaluator\nfunction to match your agent\u2019s trajectory against a reference trajectory. There are four modes to choose from:\n| Mode | Description | Use Case |\n|---|---|---|\nstrict | Exact match of messages and tool calls in the same order | Testing specific sequences (e.g., policy lookup before authorization) |\nunordered | Same tool calls allowed in any order | Verifying information retrieval when order doesn\u2019t matter |\nsubset | Agent calls only tools from reference (no extras) | Ensuring agent doesn\u2019t exceed expected scope |\nsuperset | Agent calls at least the reference tools (extras allowed) | Verifying minimum required actions are taken |\nStrict match\nStrict match\nstrict\nmode ensures trajectories contain identical messages in the same order with the same tool calls, though it allows for differences in message content. This is useful when you need to enforce a specific sequence of operations, such as requiring a policy lookup before authorizing an action.Unordered match\nUnordered match\nunordered\nmode allows the same tool calls in any order, which is helpful when you want to verify that specific information was retrieved but don\u2019t care about the sequence. For example, an agent might need to check both weather and events for a city, but the order doesn\u2019t matter.Subset and superset match\nSubset and superset match\nsuperset\nand subset\nmodes match partial trajectories. The superset\nmode verifies that the agent called at least the tools in the reference trajectory, allowing additional tool calls. The subset\nmode ensures the agent did not call any tools beyond those in the reference.toolArgsMatchMode\nproperty and/or toolArgsMatchOverrides\nto customize how the evaluator considers equality between tool calls in the actual trajectory vs. the reference. By default, only tool calls with the same arguments to the same tool are considered equal. Visit the repository for more details.LLM-as-Judge Evaluator\nYou can also use an LLM to evaluate the agent\u2019s execution path with thecreateTrajectoryLLMAsJudge\nfunction. Unlike the trajectory match evaluators, it doesn\u2019t require a reference trajectory, but one can be provided if available.\nWithout reference trajectory\nWithout reference trajectory\nWith reference trajectory\nWith reference trajectory\nTRAJECTORY_ACCURACY_PROMPT_WITH_REFERENCE\nprompt and configure the reference_outputs\nvariable:LangSmith Integration\nFor tracking experiments over time, you can log evaluator results to LangSmith, a platform for building production-grade LLM applications that includes tracing, evaluation, and experimentation tools. First, set up LangSmith by setting the required environment variables:evaluate\nfunction.\nUsing vitest/jest integration\nUsing vitest/jest integration\nUsing the evaluate function\nUsing the evaluate function\nevaluate\nfunction:", "tokens": 548, "node_type": "child"}
{"id": 324, "chunk_id": "c16acf8a553ac24f5d77e7d3a1c6e493", "parent_id": 308, "url": "", "namespace": "langchain", "title": "oss-javascript-langchain-tools", "headers": ["oss-javascript-langchain-tools"], "section_index": 0, "chunk_index": 0, "text": "# oss-javascript-langchain-tools\n\n> Source: https://docs.langchain.com/oss/javascript/langchain/tools\n\nLangChain v1.0Welcome to the new LangChain documentation! If you encounter any issues or have feedback, please open an issue so we can improve. Archived v0 documentation can be found here.See the release notes and migration guide for a complete list of changes and instructions on how to upgrade your code.\nServer-side tool useSome chat models (e.g., OpenAI, Anthropic, and Gemini) feature built-in tools that are executed server-side, such as web search and code interpreters. Refer to the provider overview to learn how to access these tools with your specific chat model.\nCreate tools\nBasic tool definition\nThe simplest way to create a tool is by importing thetool\nfunction from the langchain\npackage. You can use zod to define the tool\u2019s input schema:\nAccessing Context\nWhy this matters: Tools are most powerful when they can access agent state, runtime context, and long-term memory. This enables tools to make context-aware decisions, personalize responses, and maintain information across conversations.\nToolRuntime\nparameter, which provides:\n- State - Mutable data that flows through execution (messages, counters, custom fields)\n- Context - Immutable configuration like user IDs, session details, or application-specific configuration\n- Store - Persistent long-term memory across conversations\n- Stream Writer - Stream custom updates as tools execute\n- Config - RunnableConfig for the execution\n- Tool Call ID - ID of the current tool call\nToolRuntime\nUseToolRuntime\nto access all runtime information in a single parameter. Simply add runtime: ToolRuntime\nto your tool signature, and it will be automatically injected without being exposed to the LLM.\nToolRuntime\n: A unified parameter that provides tools access to state, context, store, streaming, config, and tool call ID. This replaces the older pattern of using separate InjectedState\n, InjectedStore\n, get_runtime()\n, and InjectedToolCallId\nannotations.Context\nAccess immutable configuration and contextual data like user IDs, session details, or application-specific configuration throughruntime.context\n.\nTools can access an agent\u2019s runtime context through the config\nparameter:\nMemory (Store)\nAccess persistent data across conversations using the store. The store is accessed viaruntime.store\nand allows you to save and retrieve user-specific or application-specific data.\nStream Writer\nStream custom updates from tools as they execute usingruntime.stream_writer\n. This is useful for providing real-time feedback to users about what a tool is doing.", "tokens": 374, "node_type": "child"}
{"id": 325, "chunk_id": "c16acf8a553ac24f5d77e7d3a1c6e493", "parent_id": 309, "url": "", "namespace": "langchain", "title": "oss-javascript-langchain-ui", "headers": ["oss-javascript-langchain-ui"], "section_index": 0, "chunk_index": 0, "text": "# oss-javascript-langchain-ui\n\n> Source: https://docs.langchain.com/oss/javascript/langchain/ui\n\nLangChain v1.0Welcome to the new LangChain documentation! If you encounter any issues or have feedback, please open an issue so we can improve. Archived v0 documentation can be found here.See the release notes and migration guide for a complete list of changes and instructions on how to upgrade your code.\ncreate_agent()\n. This UI is designed to provide rich, interactive experiences for your agents with minimal setup, whether you\u2019re running locally or in a deployed context (such as LangSmith).\nAgent Chat UI\nAgent Chat UI is a Next.js application that provides a conversational interface for interacting with any LangChain agent. It supports real-time chat, tool visualization, and advanced features like time-travel debugging and state forking. Agent Chat UI is open source and can be adapted to your application needs.Features\nTool visualization\nTool visualization\nStudio automatically renders tool calls and results in an intuitive interface.\nTime-travel debugging\nTime-travel debugging\nNavigate through conversation history and fork from any point\nState inspection\nState inspection\nView and modify agent state at any point during execution\nHuman-in-the-loop\nHuman-in-the-loop\nBuilt-in support for reviewing and responding to agent requests\nQuick start\nThe fastest way to get started is using the hosted version:- Visit Agent Chat UI\n- Connect your agent by entering your deployment URL or local server address\n- Start chatting - the UI will automatically detect and render tool calls and interrupts\nLocal development\nFor customization or local development, you can run Agent Chat UI locally:Connect to your agent\nAgent Chat UI can connect to both local and deployed agents. After starting Agent Chat UI, you\u2019ll need to configure it to connect to your agent:- Graph ID: Enter your graph name (find this under\ngraphs\nin yourlanggraph.json\nfile) - Deployment URL: Your LangGraph server\u2019s endpoint (e.g.,\nhttp://localhost:2024\nfor local development, or your deployed agent\u2019s URL) - LangSmith API key (optional): Add your LangSmith API key (not required if you\u2019re using a local LangGraph server)", "tokens": 324, "node_type": "child"}
{"id": 326, "chunk_id": "c16acf8a553ac24f5d77e7d3a1c6e493", "parent_id": 310, "url": "", "namespace": "langchain", "title": "oss-javascript-langgraph-add-memory", "headers": ["oss-javascript-langgraph-add-memory"], "section_index": 0, "chunk_index": 0, "text": "# oss-javascript-langgraph-add-memory\n\n> Source: https://docs.langchain.com/oss/javascript/langgraph/add-memory\n\n- Add short-term memory as a part of your agent\u2019s state to enable multi-turn conversations.\n- Add long-term memory to store user-specific or application-level data across sessions.\nAdd short-term memory\nShort-term memory (thread-level persistence) enables agents to track multi-turn conversations. To add short-term memory:Use in production\nIn production, use a checkpointer backed by a database:Example: using Postgres checkpointer\nExample: using Postgres checkpointer\ncheckpointer.setup()\nthe first time you\u2019re using Postgres checkpointerUse in subgraphs\nIf your graph contains subgraphs, you only need to provide the checkpointer when compiling the parent graph. LangGraph will automatically propagate the checkpointer to the child subgraphs.Add long-term memory\nUse long-term memory to store user-specific or application-specific data across conversations.Use in production\nIn production, use a store backed by a database:Example: using Postgres store\nExample: using Postgres store\nstore.setup()\nthe first time you\u2019re using Postgres storeUse semantic search\nEnable semantic search in your graph\u2019s memory store to let graph agents search for items in the store by semantic similarity.Long-term memory with semantic search\nLong-term memory with semantic search\nManage short-term memory\nWith short-term memory enabled, long conversations can exceed the LLM\u2019s context window. Common solutions are:- Trim messages: Remove first or last N messages (before calling LLM)\n- Delete messages from LangGraph state permanently\n- Summarize messages: Summarize earlier messages in the history and replace them with a summary\n- Manage checkpoints to store and retrieve message history\n- Custom strategies (e.g., message filtering, etc.)\nTrim messages\nMost LLMs have a maximum supported context window (denominated in tokens). One way to decide when to truncate messages is to count the tokens in the message history and truncate whenever it approaches that limit. If you\u2019re using LangChain, you can use the trim messages utility and specify the number of tokens to keep from the list, as well as thestrategy\n(e.g., keep the last maxTokens\n) to use for handling the boundary.\nTo trim message history, use the trimMessages\nfunction:\nFull example: trim messages\nFull example: trim messages\nDelete messages\nYou can delete messages from the graph state to manage the message history. This is useful when you want to remove specific messages or clear the entire message history. To delete messages from the graph state, you can use theRemoveMessage\n. For RemoveMessage\nto work, you need to use a state key with messagesStateReducer\nreducer, like MessagesZodState\n.\nTo remove specific messages:\n- some providers expect message history to start with a\nuser\nmessage - most providers require\nassistant\nmessages with tool calls to be followed by correspondingtool\nresult messages.\nFull example: delete messages\nFull example: delete messages\nSummarize messages\nThe problem with trimming or removing messages, as shown above, is that you may lose information from culling of the message queue. Because of this, some applications benefit from a more sophisticated approach of summarizing the message history using a chat model. Prompting and orchestration logic can be used to summarize the message history. For example, in LangGraph you can include asummary\nkey in the state alongside the messages\nkey:\nsummarizeConversation\nnode can be called after some number of messages have accumulated in the messages\nstate key.\nFull example: summarize messages\nFull example: summarize messages", "tokens": 530, "node_type": "child"}
{"id": 327, "chunk_id": "c16acf8a553ac24f5d77e7d3a1c6e493", "parent_id": 311, "url": "", "namespace": "langchain", "title": "oss-javascript-langgraph-agentic-rag", "headers": ["oss-javascript-langgraph-agentic-rag"], "section_index": 0, "chunk_index": 0, "text": "# oss-javascript-langgraph-agentic-rag\n\n> Source: https://docs.langchain.com/oss/javascript/langgraph/agentic-rag\n\nLangGraph v1.0Welcome to the new LangGraph documentation! If you encounter any issues or have feedback, please open an issue so we can improve. Archived v0 documentation can be found here.See the release notes and migration guide for a complete list of changes and instructions on how to upgrade your code.\nOverview\nIn this tutorial we will build a retrieval agent using LangGraph. LangChain offers built-in agent implementations, implemented using LangGraph primitives. If deeper customization is required, agents can be implemented directly in LangGraph. This guide demonstrates an example implementation of a retrieval agent. Retrieval agents are useful when you want an LLM to make a decision about whether to retrieve context from a vectorstore or respond to the user directly. By the end of the tutorial we will have done the following:- Fetch and preprocess documents that will be used for retrieval.\n- Index those documents for semantic search and create a retriever tool for the agent.\n- Build an agentic RAG system that can decide when to use the retriever tool.\nConcepts\nWe will cover the following concepts:- Retrieval using document loaders, text splitters, embeddings, and vector stores\n- The LangGraph Graph API, including state, nodes, edges, and conditional edges.\nSetup\nLet\u2019s download the required packages and set our API keys:1. Preprocess documents\n- Fetch documents to use in our RAG system. We will use three of the most recent pages from Lilian Weng\u2019s excellent blog. We\u2019ll start by fetching the content of the pages using\nCheerioWebBaseLoader\n:\n- Split the fetched documents into smaller chunks for indexing into our vectorstore:\n2. Create a retriever tool\nNow that we have our split documents, we can index them into a vector store that we\u2019ll use for semantic search.- Use an in-memory vector store and OpenAI embeddings:\n- Create a retriever tool using LangChain\u2019s prebuilt\ncreateRetrieverTool\n:\n3. Generate query\nNow we will start building components (nodes and edges) for our agentic RAG graph.- Build a\ngenerateQueryOrRespond\nnode. It will call an LLM to generate a response based on the current graph state (list of messages). Given the input messages, it will decide to retrieve using the retriever tool, or respond directly to the user. Note that we\u2019re giving the chat model access to thetools\nwe created earlier via.bindTools\n:\n- Try it on a random input:\n- Ask a question that requires semantic search:\n4. Grade documents\n- Add a node \u2014\ngradeDocuments\n\u2014 to determine whether the retrieved documents are relevant to the question. We will use a model with structured output using Zod for document grading. We\u2019ll also add a conditional edge \u2014checkRelevance\n\u2014 that checks the grading result and returns the name of the node to go to (generate\norrewrite\n):\n- Run this with irrelevant documents in the tool response:\n- Confirm that the relevant documents are classified as such:\n5. Rewrite question\n- Build the\nrewrite\nnode. The retriever tool can return potentially irrelevant documents, which indicates a need to improve the original user question. To do so, we will call therewrite\nnode:\n- Try it out:\n6. Generate an answer\n- Build\ngenerate\nnode: if we pass the grader checks, we can generate the final answer based on the original question and the retrieved context:\n- Try it:\n7. Assemble the graph\nNow we\u2019ll assemble all the nodes and edges into a complete graph:- Start with a\ngenerateQueryOrRespond\nand determine if we need to call the retriever tool - Route to next step using a conditional edge:\n- If\ngenerateQueryOrRespond\nreturnedtool_calls\n, call the retriever tool to retrieve context - Otherwise, respond directly to the user\n- If\n- Grade retrieved document content for relevance to the question (\ngradeDocuments\n) and route to next step:- If not relevant, rewrite the question using\nrewrite\nand then callgenerateQueryOrRespond\nagain - If relevant, proceed to\ngenerate\nand generate final response using the @[ToolMessage\n] with the retrieved document context\n- If not relevant, rewrite the question using", "tokens": 666, "node_type": "child"}
{"id": 328, "chunk_id": "c16acf8a553ac24f5d77e7d3a1c6e493", "parent_id": 312, "url": "", "namespace": "langchain", "title": "oss-javascript-langgraph-application-structure", "headers": ["oss-javascript-langgraph-application-structure"], "section_index": 0, "chunk_index": 0, "text": "# oss-javascript-langgraph-application-structure\n\n> Source: https://docs.langchain.com/oss/javascript/langgraph/application-structure\n\nLangGraph v1.0Welcome to the new LangGraph documentation! If you encounter any issues or have feedback, please open an issue so we can improve. Archived v0 documentation can be found here.See the release notes and migration guide for a complete list of changes and instructions on how to upgrade your code.\nOverview\nA LangGraph application consists of one or more graphs, a configuration file (langgraph.json\n), a file that specifies dependencies, and an optional .env\nfile that specifies environment variables.\nThis guide shows a typical structure of an application and shows how the required information to deploy an application using the LangSmith is specified.\nKey Concepts\nTo deploy using the LangSmith, the following information should be provided:- A LangGraph configuration file (\nlanggraph.json\n) that specifies the dependencies, graphs, and environment variables to use for the application. - The graphs that implement the logic of the application.\n- A file that specifies dependencies required to run the application.\n- Environment variables that are required for the application to run.\nFile Structure\nBelow are examples of directory structures for applications:The directory structure of a LangGraph application can vary depending on the programming language and the package manager used.\nConfiguration File\nThelanggraph.json\nfile is a JSON file that specifies the dependencies, graphs, environment variables, and other settings required to deploy a LangGraph application.\nSee the LangGraph configuration file reference for details on all supported keys in the JSON file.\nExamples\n- The dependencies will be loaded from a dependency file in the local directory (e.g.,\npackage.json\n). - A single graph will be loaded from the file\n./your_package/your_file.js\nwith the functionagent\n. - The environment variable\nOPENAI_API_KEY\nis set inline.\nDependencies\nA LangGraph application may depend on other TypeScript/JavaScript libraries. You will generally need to specify the following information for dependencies to be set up correctly:-\nA file in the directory that specifies the dependencies (e.g.\npackage.json\n). -\nA\ndependencies\nkey in the LangGraph configuration file that specifies the dependencies required to run the LangGraph application. -\nAny additional binaries or system libraries can be specified using\ndockerfile_lines\nkey in the LangGraph configuration file.\nGraphs\nUse thegraphs\nkey in the LangGraph configuration file to specify which graphs will be available in the deployed LangGraph application.\nYou can specify one or more graphs in the configuration file. Each graph is identified by a name (which should be unique) and a path for either: (1) the compiled graph or (2) a function that makes a graph is defined.\nEnvironment Variables\nIf you\u2019re working with a deployed LangGraph application locally, you can configure environment variables in theenv\nkey of the LangGraph configuration file.\nFor a production deployment, you will typically want to configure the environment variables in the deployment environment.", "tokens": 460, "node_type": "child"}
{"id": 329, "chunk_id": "c16acf8a553ac24f5d77e7d3a1c6e493", "parent_id": 313, "url": "", "namespace": "langchain", "title": "oss-javascript-langgraph-case-studies", "headers": ["oss-javascript-langgraph-case-studies"], "section_index": 0, "chunk_index": 0, "text": "# oss-javascript-langgraph-case-studies\n\n> Source: https://docs.langchain.com/oss/javascript/langgraph/case-studies\n\nLangGraph v1.0Welcome to the new LangGraph documentation! If you encounter any issues or have feedback, please open an issue so we can improve. Archived v0 documentation can be found here.See the release notes and migration guide for a complete list of changes and instructions on how to upgrade your code.\n| Company | Industry | Use case | Reference |\n|---|---|---|---|\n| AirTop | Software & Technology (GenAI Native) | Browser automation for AI agents | Case study, 2024 |\n| AppFolio | Real Estate | Copilot for domain-specific task | Case study, 2024 |\n| Athena Intelligence | Software & Technology (GenAI Native) | Research & summarization | Case study, 2024 |\n| BlackRock | Financial Services | Copilot for domain-specific task | Interrupt talk, 2025 |\n| Captide | Software & Technology (GenAI Native) | Data extraction | Case study, 2025 |\n| Cisco CX | Software & Technology | Customer support | Interrupt Talk, 2025 |\n| Cisco Outshift | Software & Technology | DevOps | Video story, 2025; Case study, 2025; Blog post, 2025 |\n| Cisco TAC | Software & Technology | Customer support | Video story, 2025 |\n| City of Hope | Non-profit | Copilot for domain-specific task | Video story, 2025 |\n| C.H. Robinson | Logistics | Automation | Case study, 2025 |\n| Definely | Legal | Copilot for domain-specific task | Case study, 2025 |\n| Docent Pro | Travel | GenAI embedded product experiences | Case study, 2025 |\n| Elastic | Software & Technology | Copilot for domain-specific task | Blog post, 2025 |\n| Exa | Software & Technology (GenAI Native) | Search | Case study, 2025 |\n| GitLab | Software & Technology | Code generation | Duo workflow docs |\n| Harmonic | Software & Technology | Search | Case study, 2025 |\n| Inconvo | Software & Technology | Code generation | Case study, 2025 |\n| Infor | Software & Technology | GenAI embedded product experiences; customer support; copilot | Case study, 2025 |\n| J.P. Morgan | Financial Services | Copilot for domain-specific task | Interrupt talk, 2025 |\n| Klarna | Fintech | Copilot for domain-specific task | Case study, 2025 |\n| Komodo Health | Healthcare | Copilot for domain-specific task | Blog post |\n| Social Media | Code generation; Search & discovery | Interrupt talk, 2025; Blog post, 2025; Blog post, 2024 | |\n| Minimal | E-commerce | Customer support | Case study, 2025 |\n| Modern Treasury | Fintech | GenAI embedded product experiences | Video story, 2025 |\n| Monday | Software & Technology | GenAI embedded product experiences | Interrupt talk, 2025 |\n| Morningstar | Financial Services | Research & summarization | Video story, 2025 |\n| OpenRecovery | Healthcare | Copilot for domain-specific task | Case study, 2024 |\n| Pigment | Fintech | GenAI embedded product experiences | Video story, 2025 |\n| Prosper | Fintech | Customer support | Video story, 2025 |\n| Qodo | Software & Technology (GenAI Native) | Code generation | Blog post, 2025 |\n| Rakuten | E-commerce / Fintech | Copilot for domain-specific task | Video story, 2025; Blog post, 2025 |\n| Replit | Software & Technology | Code generation | Blog post, 2024; Breakout agent story, 2024; Fireside chat video, 2024 |\n| Rexera | Real Estate (GenAI Native) | Copilot for domain-specific task | Case study, 2024 |\n| Abu Dhabi Government | Government | Search | Case study, 2025 |\n| Tradestack | Software & Technology (GenAI Native) | Copilot for domain-specific task | Case study, 2024 |\n| Uber | Transportation | Developer productivity; Code generation | Interrupt talk, 2025; Presentation, 2024; Video, 2024 |\n| Unify | Software & Technology (GenAI Native) | Copilot for domain-specific task | Interrupt talk, 2025; Blog post, 2024 |\n| Vizient | Healthcare | Copilot for domain-specific task | Video story, 2025; Case study, 2025 |\n| Vodafone | Telecommunications | Code generation; internal search | Case study, 2025 |\n| WebToon | Media & Entertainment | Data extraction | Case study, 2025 |\n| 11x | Software & Technology (GenAI Native) | Research & outreach | Interrupt talk, 2025 |", "tokens": 715, "node_type": "child"}
{"id": 330, "chunk_id": "c16acf8a553ac24f5d77e7d3a1c6e493", "parent_id": 315, "url": "", "namespace": "langchain", "title": "oss-javascript-langgraph-deploy", "headers": ["oss-javascript-langgraph-deploy"], "section_index": 0, "chunk_index": 0, "text": "# oss-javascript-langgraph-deploy\n\n> Source: https://docs.langchain.com/oss/javascript/langgraph/deploy\n\nLangGraph v1.0Welcome to the new LangGraph documentation! If you encounter any issues or have feedback, please open an issue so we can improve. Archived v0 documentation can be found here.See the release notes and migration guide for a complete list of changes and instructions on how to upgrade your code.\nPrerequisites\nBefore you begin, ensure you have the following:- A GitHub account\n- A LangSmith account (free to sign up)\nDeploy your agent\n1. Create a repository on GitHub\nYour application\u2019s code must reside in a GitHub repository to be deployed on LangSmith. Both public and private repositories are supported. For this quickstart, first make sure your app is LangGraph-compatible by following the local server setup guide. Then, push your code to the repository.2. Deploy to LangSmith\n2\nCreate new deployment\nClick the + New Deployment button. A pane will open where you can fill in the required fields.\n3\nLink repository\nIf you are a first time user or adding a private repository that has not been previously connected, click the Add new account button and follow the instructions to connect your GitHub account.\n4\nDeploy repository\nSelect your application\u2019s repository. Click Submit to deploy. This may take about 15 minutes to complete. You can check the status in the Deployment details view.\n3. Test your application in Studio\nOnce your application is deployed:- Select the deployment you just created to view more details.\n- Click the Studio button in the top right corner. Studio will open to display your graph.\n4. Get the API URL for your deployment\n- In the Deployment details view in LangGraph, click the API URL to copy it to your clipboard.\n- Click the\nURL\nto copy it to the clipboard.\n5. Test the API\nYou can now test the API:- Python\n- Rest API\n- Install LangGraph Python:\n- Send a message to the agent:", "tokens": 318, "node_type": "child"}
{"id": 331, "chunk_id": "c16acf8a553ac24f5d77e7d3a1c6e493", "parent_id": 316, "url": "", "namespace": "langchain", "title": "oss-javascript-langgraph-durable-execution", "headers": ["oss-javascript-langgraph-durable-execution"], "section_index": 0, "chunk_index": 0, "text": "# oss-javascript-langgraph-durable-execution\n\n> Source: https://docs.langchain.com/oss/javascript/langgraph/durable-execution\n\nLangGraph v1.0Welcome to the new LangGraph documentation! If you encounter any issues or have feedback, please open an issue so we can improve. Archived v0 documentation can be found here.See the release notes and migration guide for a complete list of changes and instructions on how to upgrade your code.\nIf you are using LangGraph with a checkpointer, you already have durable execution enabled. You can pause and resume workflows at any point, even after interruptions or failures.\nTo make the most of durable execution, ensure that your workflow is designed to be deterministic and idempotent and wrap any side effects or non-deterministic operations inside tasks. You can use tasks from both the StateGraph (Graph API) and the Functional API.\nRequirements\nTo leverage durable execution in LangGraph, you need to:- Enable persistence in your workflow by specifying a checkpointer that will save workflow progress.\n- Specify a thread identifier when executing a workflow. This will track the execution history for a particular instance of the workflow.\n- Wrap any non-deterministic operations (e.g., random number generation) or operations with side effects (e.g., file writes, API calls) inside tasks to ensure that when a workflow is resumed, these operations are not repeated for the particular run, and instead their results are retrieved from the persistence layer. For more information, see Determinism and Consistent Replay.\nDeterminism and Consistent Replay\nWhen you resume a workflow run, the code does NOT resume from the same line of code where execution stopped; instead, it will identify an appropriate starting point from which to pick up where it left off. This means that the workflow will replay all steps from the starting point until it reaches the point where it was stopped. As a result, when you are writing a workflow for durable execution, you must wrap any non-deterministic operations (e.g., random number generation) and any operations with side effects (e.g., file writes, API calls) inside tasks or nodes. To ensure that your workflow is deterministic and can be consistently replayed, follow these guidelines:- Avoid Repeating Work: If a node contains multiple operations with side effects (e.g., logging, file writes, or network calls), wrap each operation in a separate task. This ensures that when the workflow is resumed, the operations are not repeated, and their results are retrieved from the persistence layer.\n- Encapsulate Non-Deterministic Operations: Wrap any code that might yield non-deterministic results (e.g., random number generation) inside tasks or nodes. This ensures that, upon resumption, the workflow follows the exact recorded sequence of steps with the same outcomes.\n- Use Idempotent Operations: When possible ensure that side effects (e.g., API calls, file writes) are idempotent. This means that if an operation is retried after a failure in the workflow, it will have the same effect as the first time it was executed. This is particularly important for operations that result in data writes. In the event that a task starts but fails to complete successfully, the workflow\u2019s resumption will re-run the task, relying on recorded outcomes to maintain consistency. Use idempotency keys or verify existing results to avoid unintended duplication, ensuring a smooth and predictable workflow execution.\nDurability modes\nLangGraph supports three durability modes that allow you to balance performance and data consistency based on your application\u2019s requirements. The durability modes, from least to most durable, are as follows: A higher durability mode adds more overhead to the workflow execution.Added in v0.6.0\nUse the\ndurability\nparameter instead of checkpoint_during\n(deprecated in v0.6.0) for persistence policy management:durability=\"async\"\nreplacescheckpoint_during=True\ndurability=\"exit\"\nreplacescheckpoint_during=False\ncheckpoint_during=True\n->durability=\"async\"\ncheckpoint_during=False\n->durability=\"exit\"\n\"exit\"\nChanges are persisted only when graph execution completes (either successfully or with an error). This provides the best performance for long-running graphs but means intermediate state is not saved, so you cannot recover from mid-execution failures or interrupt the graph execution.\n\"async\"\nChanges are persisted asynchronously while the next step executes. This provides good performance and durability, but there\u2019s a small risk that checkpoints might not be written if the process crashes during execution.\n\"sync\"\nChanges are persisted synchronously before the next step starts. This ensures that every checkpoint is written before continuing execution, providing high durability at the cost of some performance overhead.\nYou can specify the durability mode when calling any graph execution method:\nUsing tasks in nodes\nIf a node contains multiple operations, you may find it easier to convert each operation into a task rather than refactor the operations into individual nodes.- Original\n- With task\nResuming Workflows\nOnce you have enabled durable execution in your workflow, you can resume execution for the following scenarios:- Pausing and Resuming Workflows: Use the interrupt function to pause a workflow at specific points and the Command primitive to resume it with updated state. See Interrupts for more details.\n- Recovering from Failures: Automatically resume workflows from the last successful checkpoint after an exception (e.g., LLM provider outage). This involves executing the workflow with the same thread identifier by providing it with a\nnull\nas the input value (see this example with the functional API).\nStarting Points for Resuming Workflows\n- If you\u2019re using a StateGraph (Graph API), the starting point is the beginning of the node where execution stopped.\n- If you\u2019re making a subgraph call inside a node, the starting point will be the parent node that called the subgraph that was halted. Inside the subgraph, the starting point will be the specific node where execution stopped.\n- If you\u2019re using the Functional API, the starting point is the beginning of the entrypoint where execution stopped.", "tokens": 922, "node_type": "child"}
{"id": 332, "chunk_id": "c16acf8a553ac24f5d77e7d3a1c6e493", "parent_id": 317, "url": "", "namespace": "langchain", "title": "oss-javascript-langgraph-functional-api", "headers": ["oss-javascript-langgraph-functional-api"], "section_index": 0, "chunk_index": 0, "text": "# oss-javascript-langgraph-functional-api > Source: https://docs.langchain.com/oss/javascript/langgraph/functional-api LangGraph v1.0Welcome to the new LangGraph documentation! If you encounter any issues or have feedback, please open an issue so we can improve. Archived v0 documentation can be found here.See the release notes and migration guide for a complete list of changes and instructions on how to upgrade your code. if statements, for loops, and function calls. Unlike many data orchestration frameworks that require restructuring code into an explicit pipeline or DAG, the Functional API allows you to incorporate these capabilities without enforcing a rigid execution model. The Functional API uses two key building blocks: entrypoint \u2013 An entrypoint encapsulates workflow logic and manages execution flow, including handling long-running tasks and interrupts.task \u2013 Represents a discrete unit of work, such as an API call or data processing step, that can be executed asynchronously within an entrypoint. Tasks return a future-like object that can be awaited or resolved synchronously. Functional API vs. Graph API For users who prefer a more declarative approach, LangGraph\u2019s Graph API allows you to define workflows using a Graph paradigm. Both APIs share the same underlying runtime, so you can use them together in the same application. Here are some key differences:- Control flow: The Functional API does not require thinking about graph structure. You can use standard Python constructs to define workflows. This will usually trim the amount of code you need to write. - Short-term memory: The GraphAPI requires declaring a State and may require defining reducers to manage updates to the graph state. @entrypoint and@tasks do not require explicit state management as their state is scoped to the function and is not shared across functions. - Checkpointing: Both APIs generate and use checkpoints. In the Graph API a new checkpoint is generated after every superstep. In the Functional API, when tasks are executed, their results are saved to an existing checkpoint associated with the given entrypoint instead of creating a new checkpoint. - Visualization: The Graph API makes it easy to visualize the workflow as a graph which can be useful for debugging, understanding the workflow, and sharing with others. The Functional API does not support visualization as the graph is dynamically generated during runtime. Example Below we demonstrate a simple application that writes an essay and interrupts to request human review.Detailed Explanation Detailed Explanation This workflow will write an essay about the topic \u201ccat\u201d and then pause to get a review from a human. The workflow can be interrupted for an indefinite amount of time until a review is provided.When the workflow is resumed, it executes from the very start, but because the result of the An essay has been written and is ready for review. Once the review is provided, we can resume the workflow:The workflow has been completed and the review has been added to the essay. writeEssay task was already saved, the task result will be loaded from the checkpoint instead of being recomputed.Entrypoint Theentrypoint function can be used to create a workflow from a function. It encapsulates workflow logic and manages execution flow, including handling long-running tasks and interrupts. Definition An entrypoint is defined by calling theentrypoint function with configuration and a function. The function must accept a single positional argument, which serves as the workflow input. If you need to pass multiple pieces of data, use an object as the input type for the first argument. Creating an entrypoint with a function produces a workflow instance which helps to manage the execution of the workflow (e.g., handles streaming, resumption, and checkpointing). You will often want to pass a checkpointer to the entrypoint function to enable persistence and use features like human-in-the-loop. Executing Using theentrypoint function will return an object that can be executed using the invoke and stream methods. - Invoke - Stream Resuming Resuming an execution after an interrupt can be done by passing a resume value to theCommand primitive. - Invoke - Stream entrypoint with null and the same thread id (config). This assumes that the underlying error has been resolved and execution can proceed successfully. - Invoke - Stream Short-term memory When anentrypoint is defined with a checkpointer , it stores information between successive invocations on the same thread id in checkpoints. This allows accessing the state from the previous invocation using the getPreviousState function. By default, the getPreviousState function returns the return value of the previous invocation. entrypoint.final entrypoint.final is a special primitive that can be returned from an entrypoint and allows decoupling the value that is saved in the checkpoint from the return value of the entrypoint. The first value is the return value of the entrypoint, and the second value is the value that will be saved in the checkpoint. Task A task represents a discrete unit of work, such as an API call or data processing step. It has two key characteristics:- Asynchronous Execution: Tasks are designed to be executed asynchronously, allowing multiple operations to run concurrently without blocking. - Checkpointing: Task results are saved to a checkpoint, enabling resumption of the workflow from the last saved state. (See persistence for more details). Definition Tasks are defined using thetask function, which wraps a regular function. Serialization The outputs of tasks must be JSON-serializable to support checkpointing. Execution Tasks can only be called from within an entrypoint, another task, or a state graph node. Tasks cannot be called directly from the main application code. When you call a task, it returns a Promise that can be awaited.When to use a task Tasks are useful in the following scenarios:- Checkpointing: When you need to save the result of a long-running operation to a checkpoint, so you don\u2019t need to recompute it when resuming the workflow. - Human-in-the-loop: If you\u2019re building a workflow that requires human intervention, you MUST use tasks to encapsulate any randomness (e.g., API calls) to ensure that the workflow can be resumed correctly. See the determinism section for more details. - Parallel Execution: For I/O-bound tasks, tasks enable parallel execution, allowing multiple operations to run", "tokens": 1000, "node_type": "child"}
{"id": 333, "chunk_id": "9b0fd3b850698317bef57fa6e71a38ba", "parent_id": 317, "url": "", "namespace": "langchain", "title": "oss-javascript-langgraph-functional-api", "headers": ["oss-javascript-langgraph-functional-api"], "section_index": 0, "chunk_index": 1, "text": "defined using thetask function, which wraps a regular function. Serialization The outputs of tasks must be JSON-serializable to support checkpointing. Execution Tasks can only be called from within an entrypoint, another task, or a state graph node. Tasks cannot be called directly from the main application code. When you call a task, it returns a Promise that can be awaited.When to use a task Tasks are useful in the following scenarios:- Checkpointing: When you need to save the result of a long-running operation to a checkpoint, so you don\u2019t need to recompute it when resuming the workflow. - Human-in-the-loop: If you\u2019re building a workflow that requires human intervention, you MUST use tasks to encapsulate any randomness (e.g., API calls) to ensure that the workflow can be resumed correctly. See the determinism section for more details. - Parallel Execution: For I/O-bound tasks, tasks enable parallel execution, allowing multiple operations to run concurrently without blocking (e.g., calling multiple APIs). - Observability: Wrapping operations in tasks provides a way to track the progress of the workflow and monitor the execution of individual operations using LangSmith. - Retryable Work: When work needs to be retried to handle failures or inconsistencies, tasks provide a way to encapsulate and manage the retry logic. Serialization There are two key aspects to serialization in LangGraph:entrypoint inputs and outputs must be JSON-serializable.task outputs must be JSON-serializable. Determinism To utilize features like human-in-the-loop, any randomness should be encapsulated inside of tasks. This guarantees that when execution is halted (e.g., for human in the loop) and then resumed, it will follow the same sequence of steps, even if task results are non-deterministic. LangGraph achieves this behavior by persisting task and subgraph results as they execute. A well-designed workflow ensures that resuming execution follows the same sequence of steps, allowing previously computed results to be retrieved correctly without having to re-execute them. This is particularly useful for long-running tasks or tasks with non-deterministic results, as it avoids repeating previously done work and allows resuming from essentially the same. While different runs of a workflow can produce different results, resuming a specific run should always follow the same sequence of recorded steps. This allows LangGraph to efficiently look up task and subgraph results that were executed prior to the graph being interrupted and avoid recomputing them.Idempotency Idempotency ensures that running the same operation multiple times produces the same result. This helps prevent duplicate API calls and redundant processing if a step is rerun due to a failure. Always place API calls inside tasks functions for checkpointing, and design them to be idempotent in case of re-execution. Re-execution can occur if a task starts, but does not complete successfully. Then, if the workflow is resumed, the task will run again. Use idempotency keys or verify existing results to avoid duplication.Common Pitfalls Handling side effects Encapsulate side effects (e.g., writing to a file, sending an email) in tasks to ensure they are not executed multiple times when resuming a workflow.- Incorrect - Correct In this example, a side effect (writing to a file) is directly included in the workflow, so it will be executed a second time when resuming the workflow. Non-deterministic control flow Operations that might give different results each time (like getting current time or random numbers) should be encapsulated in tasks to ensure that on resume, the same result is returned.- In a task: Get random number (5) \u2192 interrupt \u2192 resume \u2192 (returns 5 again) \u2192 \u2026 - Not in a task: Get random number (5) \u2192 interrupt \u2192 resume \u2192 get new random number (7) \u2192 \u2026 interrupt call may be matched with the wrong resume value, leading to incorrect results. Please read the section on determinism for more details. - Incorrect - Correct In this example, the workflow uses the current time to determine which task to execute. This is non-deterministic because the result of the workflow depends on the time at which it is executed.", "tokens": 657, "node_type": "child"}
{"id": 334, "chunk_id": "c16acf8a553ac24f5d77e7d3a1c6e493", "parent_id": 318, "url": "", "namespace": "langchain", "title": "oss-javascript-langgraph-graph-api", "headers": ["oss-javascript-langgraph-graph-api"], "section_index": 0, "chunk_index": 0, "text": "# oss-javascript-langgraph-graph-api > Source: https://docs.langchain.com/oss/javascript/langgraph/graph-api LangGraph v1.0Welcome to the new LangGraph documentation! If you encounter any issues or have feedback, please open an issue so we can improve. Archived v0 documentation can be found here.See the release notes and migration guide for a complete list of changes and instructions on how to upgrade your code. Graphs At its core, LangGraph models agent workflows as graphs. You define the behavior of your agents using three key components:- State : A shared data structure that represents the current snapshot of your application. It can be any data type, but is typically defined using a shared state schema. - Nodes : Functions that encode the logic of your agents. They receive the current state as input, perform some computation or side-effect, and return an updated state. - Edges : Functions that determine whichNode to execute next based on the current state. They can be conditional branches or fixed transitions. Nodes and Edges , you can create complex, looping workflows that evolve the state over time. The real power, though, comes from how LangGraph manages that state. To emphasize: Nodes and Edges are nothing more than functions - they can contain an LLM or just good ol\u2019 code. In short: nodes do the work, edges tell what to do next. LangGraph\u2019s underlying graph algorithm uses message passing to define a general program. When a Node completes its operation, it sends messages along one or more edges to other node(s). These recipient nodes then execute their functions, pass the resulting messages to the next set of nodes, and the process continues. Inspired by Google\u2019s Pregel system, the program proceeds in discrete \u201csuper-steps.\u201d A super-step can be considered a single iteration over the graph nodes. Nodes that run in parallel are part of the same super-step, while nodes that run sequentially belong to separate super-steps. At the start of graph execution, all nodes begin in an inactive state. A node becomes active when it receives a new message (state) on any of its incoming edges (or \u201cchannels\u201d). The active node then runs its function and responds with updates. At the end of each super-step, nodes with no incoming messages vote to halt by marking themselves as inactive . The graph execution terminates when all nodes are inactive and no messages are in transit. StateGraph TheStateGraph class is the main graph class to use. This is parameterized by a user defined State object. Compiling your graph To build your graph, you first define the state, you then add nodes and edges, and then you compile it. What exactly is compiling your graph and why is it needed? Compiling is a pretty simple step. It provides a few basic checks on the structure of your graph (no orphaned nodes, etc). It is also where you can specify runtime args like checkpointers and breakpoints. You compile your graph by just calling the.compile method: State The first thing you do when you define a graph is define theState of the graph. The State consists of the schema of the graph as well as reducer functions which specify how to apply updates to the state. The schema of the State will be the input schema to all Nodes and Edges in the graph, and can be either a Zod schema or a schema built using Annotation.Root . All Nodes will emit updates to the State which are then applied using the specified reducer function. Schema The main documented way to specify the schema of a graph is by using Zod schemas. However, we also support using theAnnotation API to define the schema of the graph. By default, the graph will have the same input and output schemas. If you want to change this, you can also specify explicit input and output schemas directly. This is useful when you have a lot of keys, and some are explicitly for input and others for output. Multiple schemas Typically, all graph nodes communicate with a single schema. This means that they will read and write to the same state channels. But, there are cases where we want more control over this:- Internal nodes can pass information that is not required in the graph\u2019s input / output. - We may also want to use different input / output schemas for the graph. The output might, for example, only contain a single relevant output key. PrivateState . It is also possible to define explicit input and output schemas for a graph. In these cases, we define an \u201cinternal\u201d schema that contains all keys relevant to graph operations. But, we also define input and output schemas that are sub-sets of the \u201cinternal\u201d schema to constrain the input and output of the graph. See this guide for more detail. Let\u2019s look at an example: - We pass state as the input schema tonode1 . But, we write out tofoo , a channel inOverallState . How can we write out to a state channel that is not included in the input schema? This is because a node can write to any state channel in the graph state. The graph state is the union of the state channels defined at initialization, which includesOverallState and the filtersInputState andOutputState . - We initialize the graph with StateGraph({ state: OverallState, input: InputState, output: OutputState }) . So, how can we write toPrivateState innode2 ? How does the graph gain access to this schema if it was not passed in theStateGraph initialization? We can do this because nodes can also declare additional state channels as long as the state schema definition exists. In this case, thePrivateState schema is defined, so we can addbar as a new state channel in the graph and write to it. Reducers Reducers are key to understanding how updates from nodes are applied to theState . Each key in the State has its own independent reducer function. If no reducer function is explicitly specified then it is assumed that all updates to that key should override it. There are a", "tokens": 1000, "node_type": "child"}
{"id": 335, "chunk_id": "9b0fd3b850698317bef57fa6e71a38ba", "parent_id": 318, "url": "", "namespace": "langchain", "title": "oss-javascript-langgraph-graph-api", "headers": ["oss-javascript-langgraph-graph-api"], "section_index": 0, "chunk_index": 1, "text": "state is the union of the state channels defined at initialization, which includesOverallState and the filtersInputState andOutputState . - We initialize the graph with StateGraph({ state: OverallState, input: InputState, output: OutputState }) . So, how can we write toPrivateState innode2 ? How does the graph gain access to this schema if it was not passed in theStateGraph initialization? We can do this because nodes can also declare additional state channels as long as the state schema definition exists. In this case, thePrivateState schema is defined, so we can addbar as a new state channel in the graph and write to it. Reducers Reducers are key to understanding how updates from nodes are applied to theState . Each key in the State has its own independent reducer function. If no reducer function is explicitly specified then it is assumed that all updates to that key should override it. There are a few different types of reducers, starting with the default type of reducer: Default Reducer These two examples show how to use the default reducer: Example A:{ foo: 1, bar: [\"hi\"] } . Let\u2019s then assume the first Node returns { foo: 2 } . This is treated as an update to the state. Notice that the Node does not need to return the whole State schema - just an update. After applying this update, the State would then be { foo: 2, bar: [\"hi\"] } . If the second node returns { bar: [\"bye\"] } then the State would then be { foo: 2, bar: [\"bye\"] } Example B: bar ). Note that the first key remains unchanged. Let\u2019s assume the input to the graph is { foo: 1, bar: [\"hi\"] } . Let\u2019s then assume the first Node returns { foo: 2 } . This is treated as an update to the state. Notice that the Node does not need to return the whole State schema - just an update. After applying this update, the State would then be { foo: 2, bar: [\"hi\"] } . If the second node returns { bar: [\"bye\"] } then the State would then be { foo: 2, bar: [\"hi\", \"bye\"] } . Notice here that the bar key is updated by adding the two arrays together. Working with Messages in Graph State Why use messages? Most modern LLM providers have a chat model interface that accepts a list of messages as input. LangChain\u2019sChatModel in particular accepts a list of Message objects as inputs. These messages come in a variety of forms such as @[HumanMessage ] (user input) or AIMessage (LLM response). To read more about what message objects are, please refer to this conceptual guide. Using Messages in your Graph In many cases, it is helpful to store prior conversation history as a list of messages in your graph state. To do so, we can add a key (channel) to the graph state that stores a list ofMessage objects and annotate it with a reducer function (see messages key in the example below). The reducer function is vital to telling the graph how to update the list of Message objects in the state with each state update (for example, when a node sends an update). If you don\u2019t specify a reducer, every state update will overwrite the list of messages with the most recently provided value. If you wanted to simply append messages to the existing list, you could use a function that concatenates arrays as a reducer. However, you might also want to manually update messages in your graph state (e.g. human-in-the-loop). If you were to use a simple concatenation function, the manual state updates you send to the graph would be appended to the existing list of messages, instead of updating existing messages. To avoid that, you need a reducer that can keep track of message IDs and overwrite existing messages, if updated. To achieve this, you can use the prebuilt messagesStateReducer function or MessagesZodMeta when state schema is defined with Zod. For brand new messages, it will simply append to existing list, but it will also handle the updates for existing messages correctly. Serialization In addition to keeping track of message IDs,MessagesZodMeta will also try to deserialize messages into LangChain Message objects whenever a state update is received on the messages channel. This allows sending graph inputs / state updates in the following format: Messages when using MessagesZodMeta , you should use dot notation to access message attributes, like state.messages[state.messages.length - 1].content . Below is an example of a graph that uses MessagesZodMeta : MessagesZodState is defined with a single messages key which is a list of @[BaseMessage ] objects and uses the appropriate reducer. Typically, there is more state to track than just messages, so we see people extend this state and add more fields, like: Nodes In LangGraph, nodes are typically functions (sync or async) that accept the following arguments:state : The state of the graphconfig : ARunnableConfig object that contains configuration information likethread_id and tracing information liketags addNode method. START Node The START Node is a special node that represents the node that sends user input to the graph. The main purpose for referencing this node is to determine which nodes should be called first. END Node The END Node is a special node that represents a terminal node. This node is referenced when you want to denote which edges have no actions after they are done. Node Caching LangGraph supports caching of tasks/nodes based on the input to the node. To use caching:- Specify a cache when compiling a graph (or specifying an entrypoint) - Specify a cache policy for nodes. Each cache policy supports: keyFunc , which is used to generate a cache key based on the input to a node.ttl , the time to live for the cache in seconds. If not specified, the cache will never expire. Edges Edges define how the logic is routed and how the graph decides to stop. This is a big part of how your agents work", "tokens": 1000, "node_type": "child"}
{"id": 336, "chunk_id": "5345104ec252a41359fa4a3d2dfbec23", "parent_id": 318, "url": "", "namespace": "langchain", "title": "oss-javascript-langgraph-graph-api", "headers": ["oss-javascript-langgraph-graph-api"], "section_index": 0, "chunk_index": 2, "text": "to the graph. The main purpose for referencing this node is to determine which nodes should be called first. END Node The END Node is a special node that represents a terminal node. This node is referenced when you want to denote which edges have no actions after they are done. Node Caching LangGraph supports caching of tasks/nodes based on the input to the node. To use caching:- Specify a cache when compiling a graph (or specifying an entrypoint) - Specify a cache policy for nodes. Each cache policy supports: keyFunc , which is used to generate a cache key based on the input to a node.ttl , the time to live for the cache in seconds. If not specified, the cache will never expire. Edges Edges define how the logic is routed and how the graph decides to stop. This is a big part of how your agents work and how different nodes communicate with each other. There are a few key types of edges:- Normal Edges: Go directly from one node to the next. - Conditional Edges: Call a function to determine which node(s) to go to next. - Entry Point: Which node to call first when user input arrives. - Conditional Entry Point: Call a function to determine which node(s) to call first when user input arrives. Normal Edges If you always want to go from node A to node B, you can use theaddEdge method directly. Conditional Edges If you want to optionally route to 1 or more edges (or optionally terminate), you can use theaddConditionalEdges method. This method accepts the name of a node and a \u201crouting function\u201d to call after that node is executed: routingFunction accepts the current state of the graph and returns a value. By default, the return value routingFunction is used as the name of the node (or list of nodes) to send the state to next. All those nodes will be run in parallel as a part of the next superstep. You can optionally provide an object that maps the routingFunction \u2019s output to the name of the next node. Entry Point The entry point is the first node(s) that are run when the graph starts. You can use theaddEdge method from the virtual START node to the first node to execute to specify where to enter the graph. Conditional Entry Point A conditional entry point lets you start at different nodes depending on custom logic. You can useaddConditionalEdges from the virtual START node to accomplish this. routingFunction \u2019s output to the name of the next node. Send By default, Nodes and Edges are defined ahead of time and operate on the same shared state. However, there can be cases where the exact edges are not known ahead of time and/or you may want different versions of State to exist at the same time. A common example of this is with map-reduce design patterns. In this design pattern, a first node may generate a list of objects, and you may want to apply some other node to all those objects. The number of objects may be unknown ahead of time (meaning the number of edges may not be known) and the input State to the downstream Node should be different (one for each generated object). To support this design pattern, LangGraph supports returning Send objects from conditional edges. Send takes two arguments: first is the name of the node, and second is the state to pass to that node. Command It can be useful to combine control flow (edges) and state updates (nodes). For example, you might want to BOTH perform state updates AND decide which node to go to next in the SAME node. LangGraph provides a way to do so by returning a Command object from node functions: Command you can also achieve dynamic control flow behavior (identical to conditional edges): Command in your node functions, you must add the ends parameter when adding the node to specify which nodes it can route to: When returning Command in your node functions, you must add return type annotations with the list of node names the node is routing to, e.g. Command[Literal[\"my_other_node\"]] . This is necessary for the graph rendering and tells LangGraph that my_node can navigate to my_other_node .Command . When should I use Command instead of conditional edges? - Use Command when you need to both update the graph state and route to a different node. For example, when implementing multi-agent handoffs where it\u2019s important to route to a different agent and pass some information to that agent. - Use conditional edges to route between nodes conditionally without updating the state. Navigating to a node in a parent graph If you are using subgraphs, you might want to navigate from a node within a subgraph to a different subgraph (i.e. a different node in the parent graph). To do so, you can specifygraph: Command.PARENT in Command : Setting graph to Command.PARENT will navigate to the closest parent graph.When you send updates from a subgraph node to a parent graph node for a key that\u2019s shared by both parent and subgraph state schemas, you must define a reducer for the key you\u2019re updating in the parent graph state.graph: Command.PARENT in Command : Setting graph to Command.PARENT will navigate to the closest parent graph.When you send updates from a subgraph node to a parent graph node for a key that\u2019s shared by both parent and subgraph state schemas, you must define a reducer for the key you\u2019re updating in the parent graph state.Using inside tools A common use case is updating graph state from inside a tool. For example, in a customer support application you might want to look up customer information based on their account number or ID in the beginning of the conversation. Refer to this guide for detail.Human-in-the-loop Command is an important part of human-in-the-loop workflows: when using interrupt() to collect user input, Command is then used to supply the input and resume execution via new Command({", "tokens": 1000, "node_type": "child"}
{"id": 337, "chunk_id": "0f7e5a6eec52bd2b622d96bc793bb7d8", "parent_id": 318, "url": "", "namespace": "langchain", "title": "oss-javascript-langgraph-graph-api", "headers": ["oss-javascript-langgraph-graph-api"], "section_index": 0, "chunk_index": 3, "text": "and subgraph state schemas, you must define a reducer for the key you\u2019re updating in the parent graph state.graph: Command.PARENT in Command : Setting graph to Command.PARENT will navigate to the closest parent graph.When you send updates from a subgraph node to a parent graph node for a key that\u2019s shared by both parent and subgraph state schemas, you must define a reducer for the key you\u2019re updating in the parent graph state.Using inside tools A common use case is updating graph state from inside a tool. For example, in a customer support application you might want to look up customer information based on their account number or ID in the beginning of the conversation. Refer to this guide for detail.Human-in-the-loop Command is an important part of human-in-the-loop workflows: when using interrupt() to collect user input, Command is then used to supply the input and resume execution via new Command({ resume: \"User input\" }) . Check out the human-in-the-loop conceptual guide for more information. Graph Migrations LangGraph can easily handle migrations of graph definitions (nodes, edges, and state) even when using a checkpointer to track state.- For threads at the end of the graph (i.e. not interrupted) you can change the entire topology of the graph (i.e. all nodes and edges, remove, add, rename, etc) - For threads currently interrupted, we support all topology changes other than renaming / removing nodes (as that thread could now be about to enter a node that no longer exists) \u2014 if this is a blocker please reach out and we can prioritize a solution. - For modifying state, we have full backwards and forwards compatibility for adding and removing keys - State keys that are renamed lose their saved state in existing threads - State keys whose types change in incompatible ways could currently cause issues in threads with state from before the change \u2014 if this is a blocker please reach out and we can prioritize a solution. Runtime Context When creating a graph, you can specify acontextSchema for runtime context passed to nodes. This is useful for passing information to nodes that is not part of the graph state. For example, you might want to pass dependencies such as model name or a database connection. context property. Recursion Limit The recursion limit sets the maximum number of super-steps the graph can execute during a single execution. Once the limit is reached, LangGraph will raiseGraphRecursionError . By default this value is set to 25 steps. The recursion limit can be set on any graph at runtime, and is passed to invoke /stream via the config object. Importantly, recursionLimit is a standalone config key and should not be passed inside the configurable key as all other user-defined configuration. See the example below:", "tokens": 459, "node_type": "child"}
{"id": 338, "chunk_id": "c16acf8a553ac24f5d77e7d3a1c6e493", "parent_id": 319, "url": "", "namespace": "langchain", "title": "oss-javascript-langgraph-graph-recursion-limit", "headers": ["oss-javascript-langgraph-graph-recursion-limit"], "section_index": 0, "chunk_index": 0, "text": "# oss-javascript-langgraph-graph-recursion-limit\n\n> Source: https://docs.langchain.com/oss/javascript/langgraph/GRAPH_RECURSION_LIMIT\n\nLangGraph v1.0Welcome to the new LangGraph documentation! If you encounter any issues or have feedback, please open an issue so we can improve. Archived v0 documentation can be found here.See the release notes and migration guide for a complete list of changes and instructions on how to upgrade your code.\nStateGraph\nreached the maximum number of steps before hitting a stop condition.\nThis is often due to an infinite loop caused by code like the example below:\nTroubleshooting\n- If you are not expecting your graph to go through many iterations, you likely have a cycle. Check your logic for infinite loops.\n-\nIf you have a complex graph, you can pass in a higher\nrecursionLimit\nvalue into yourconfig\nobject when invoking your graph like this:", "tokens": 131, "node_type": "child"}
{"id": 339, "chunk_id": "c16acf8a553ac24f5d77e7d3a1c6e493", "parent_id": 320, "url": "", "namespace": "langchain", "title": "oss-javascript-langgraph-install", "headers": ["oss-javascript-langgraph-install"], "section_index": 0, "chunk_index": 0, "text": "# oss-javascript-langgraph-install\n\n> Source: https://docs.langchain.com/oss/javascript/langgraph/install\n\nLangGraph v1.0Welcome to the new LangGraph documentation! If you encounter any issues or have feedback, please open an issue so we can improve. Archived v0 documentation can be found here.See the release notes and migration guide for a complete list of changes and instructions on how to upgrade your code.\nTo install the base LangGraph package:\nCopy\nnpm install @langchain/langgraph @langchain/core\nTo use LangGraph you will usually want to access LLMs and define tools.\nYou can do this however you see fit.One way to do this (which we will use in the docs) is to use LangChain.Install LangChain with:\nCopy\nnpm install langchain\nTo work with specific LLM provider packages, you will need install them separately.Refer to the integrations page for provider-specific installation instructions.", "tokens": 129, "node_type": "child"}
{"id": 340, "chunk_id": "c16acf8a553ac24f5d77e7d3a1c6e493", "parent_id": 321, "url": "", "namespace": "langchain", "title": "oss-javascript-langgraph-interrupts", "headers": ["oss-javascript-langgraph-interrupts"], "section_index": 0, "chunk_index": 0, "text": "# oss-javascript-langgraph-interrupts > Source: https://docs.langchain.com/oss/javascript/langgraph/interrupts LangGraph v1.0Welcome to the new LangGraph documentation! If you encounter any issues or have feedback, please open an issue so we can improve. Archived v0 documentation can be found here.See the release notes and migration guide for a complete list of changes and instructions on how to upgrade your code. interrupt() function at any point in your graph nodes. The function accepts any JSON-serializable value which is surfaced to the caller. When you\u2019re ready to continue, you resume execution by re-invoking the graph using Command , which then becomes the return value of the interrupt() call from inside the node. Unlike static breakpoints (which pause before or after specific nodes), interrupts are dynamic\u2014they can be placed anywhere in your code and can be conditional based on your application logic. - Checkpointing keeps your place: the checkpointer writes the exact graph state so you can resume later, even when in an error state. thread_id is your pointer: use{ configurable: { thread_id: ... } } as options to theinvoke method to tell the checkpointer which state to load.- Interrupt payloads surface as __interrupt__ : the values you pass tointerrupt() return to the caller in the__interrupt__ field so you know what the graph is waiting on. thread_id you choose is effectively your persistent cursor. Reusing it resumes the same checkpoint; using a new value starts a brand-new thread with an empty state. Pause using interrupt The interrupt function pauses graph execution and returns a value to the caller. When you call interrupt within a node, LangGraph saves the current graph state and waits for you to resume execution with input. To use interrupt , you need: - A checkpointer to persist the graph state (use a durable checkpointer in production) - A thread ID in your config so the runtime knows which state to resume from - To call interrupt() where you want to pause (payload must be JSON-serializable) interrupt , here\u2019s what happens: - Graph execution gets suspended at the exact point where interrupt is called - State is saved using the checkpointer so execution can be resumed later, In production, this should be a persistent checkpointer (e.g. backed by a database) - Value is returned to the caller under __interrupt__ ; it can be any JSON-serializable value (string, object, array, etc.) - Graph waits indefinitely until you resume execution with a response - Response is passed back into the node when you resume, becoming the return value of the interrupt() call Resuming interrupts After an interrupt pauses execution, you resume the graph by invoking it again with aCommand that contains the resume value. The resume value is passed back to the interrupt call, allowing the node to continue execution with the external input. - You must use the same thread ID when resuming that was used when the interrupt occurred - The value passed to Command(resume=...) becomes the return value of theinterrupt call - The node restarts from the beginning of the node where the interrupt was called when resumed, so any code before theinterrupt runs again - You can pass any JSON-serializable value as the resume value Common patterns The key thing that interrupts unlock is the ability to pause execution and wait for external input. This is useful for a variety of use cases, including:- Approval workflows: Pause before executing critical actions (API calls, database changes, financial transactions) - Review and edit: Let humans review and modify LLM outputs or tool calls before continuing - Interrupting tool calls: Pause before executing tool calls to review and edit the tool call before execution - Validating human input: Pause before proceeding to the next step to validate human input Approve or reject One of the most common uses of interrupts is to pause before a critical action and ask for approval. For example, you might want to ask a human to approve an API call, a database change, or any other important decision.true to approve or false to reject: Full example Full example Review and edit state Sometimes you want to let a human review and edit part of the graph state before continuing. This is useful for correcting LLMs, adding missing information, or making adjustments.Full example Full example Interrupts in tools You can also place interrupts directly inside tool functions. This makes the tool itself pause for approval whenever it\u2019s called, and allows for human review and editing of the tool call before it is executed. First, define a tool that usesinterrupt : Full example Full example Validating human input Sometimes you need to validate input from humans and ask again if it\u2019s invalid. You can do this using multipleinterrupt calls in a loop. Full example Full example Rules of interrupts When you callinterrupt within a node, LangGraph suspends execution by raising an exception that signals the runtime to pause. This exception propagates up through the call stack and is caught by the runtime, which notifies the graph to save the current state and wait for external input. When execution resumes (after you provide the requested input), the runtime restarts the entire node from the beginning\u2014it does not resume from the exact line where interrupt was called. This means any code that ran before the interrupt will execute again. Because of this, there\u2019s a few important rules to follow when working with interrupts to ensure they behave as expected. Do not wrap interrupt calls in try/catch The way that interrupt pauses execution at the point of the call is by throwing a special exception. If you wrap the interrupt call in a try/catch block, you will catch this exception and the interrupt will not be passed back to the graph. - \u2705 Separate interrupt calls from error-prone code - \u2705 Conditionally catch errors if needed - \ud83d\udd34 Do not wrap interrupt calls in bare try/catch blocks Do not reorder interrupt calls within a node It\u2019s common to use multiple interrupts in a single node, however this can lead to unexpected behavior if not handled carefully.", "tokens": 1000, "node_type": "child"}
{"id": 341, "chunk_id": "9b0fd3b850698317bef57fa6e71a38ba", "parent_id": 321, "url": "", "namespace": "langchain", "title": "oss-javascript-langgraph-interrupts", "headers": ["oss-javascript-langgraph-interrupts"], "section_index": 0, "chunk_index": 1, "text": "beginning\u2014it does not resume from the exact line where interrupt was called. This means any code that ran before the interrupt will execute again. Because of this, there\u2019s a few important rules to follow when working with interrupts to ensure they behave as expected. Do not wrap interrupt calls in try/catch The way that interrupt pauses execution at the point of the call is by throwing a special exception. If you wrap the interrupt call in a try/catch block, you will catch this exception and the interrupt will not be passed back to the graph. - \u2705 Separate interrupt calls from error-prone code - \u2705 Conditionally catch errors if needed - \ud83d\udd34 Do not wrap interrupt calls in bare try/catch blocks Do not reorder interrupt calls within a node It\u2019s common to use multiple interrupts in a single node, however this can lead to unexpected behavior if not handled carefully. When a node contains multiple interrupt calls, LangGraph keeps a list of resume values specific to the task executing the node. Whenever execution resumes, it starts at the beginning of the node. For each interrupt encountered, LangGraph checks if a matching value exists in the task\u2019s resume list. Matching is strictly index-based, so the order of interrupt calls within the node is important. - \u2705 Keep interrupt calls consistent across node executions - \ud83d\udd34 Do not conditionally skip interrupt calls within a node - \ud83d\udd34 Do not loop interrupt calls using logic that isn\u2019t deterministic across executions Do not return complex values in interrupt calls Depending on which checkpointer is used, complex values may not be serializable (e.g. you can\u2019t serialize a function). To make your graphs adaptable to any deployment, it\u2019s best practice to only use values that can be reasonably serialized. - \u2705 Pass simple, JSON-serializable types to interrupt - \u2705 Pass dictionaries/objects with simple values - \ud83d\udd34 Do not pass functions, class instances, or other complex objects to interrupt Side effects called before interrupt must be idempotent Because interrupts work by re-running the nodes they were called from, side effects called before interrupt should (ideally) be idempotent. For context, idempotency means that the same operation can be applied multiple times without changing the result beyond the initial execution. As an example, you might have an API call to update a record inside of a node. If interrupt is called after that call is made, it will be re-run multiple times when the node is resumed, potentially overwriting the initial update or creating duplicate records. - \u2705 Use idempotent operations before interrupt - \u2705 Place side effects after interrupt calls - \u2705 Separate side effects into separate nodes when possible - \ud83d\udd34 Do not perform non-idempotent operations before interrupt - \ud83d\udd34 Do not create new records without checking if they exist Using with subgraphs called as functions When invoking a subgraph within a node, the parent graph will resume execution from the beginning of the node where the subgraph was invoked and theinterrupt was triggered. Similarly, the subgraph will also resume from the beginning of the node where interrupt was called. Debugging with interrupts To debug and test a graph, you can use static interrupts as breakpoints to step through the graph execution one node at a time. Static interrupts are triggered at defined points either before or after a node executes. You can set these by specifyinginterruptBefore and interruptAfter when compiling the graph. Static interrupts are not recommended for human-in-the-loop workflows. Use the interrupt method instead.- At compile time - At run time - The breakpoints are set during compile time. interruptBefore specifies the nodes where execution should pause before the node is executed.interruptAfter specifies the nodes where execution should pause after the node is executed.- A checkpointer is required to enable breakpoints. - The graph is run until the first breakpoint is hit. - The graph is resumed by passing in null for the input. This will run the graph until the next breakpoint is hit.", "tokens": 660, "node_type": "child"}
{"id": 342, "chunk_id": "c16acf8a553ac24f5d77e7d3a1c6e493", "parent_id": 322, "url": "", "namespace": "langchain", "title": "oss-javascript-langgraph-invalid-chat-history", "headers": ["oss-javascript-langgraph-invalid-chat-history"], "section_index": 0, "chunk_index": 0, "text": "# oss-javascript-langgraph-invalid-chat-history\n\n> Source: https://docs.langchain.com/oss/javascript/langgraph/INVALID_CHAT_HISTORY\n\nLangGraph v1.0Welcome to the new LangGraph documentation! If you encounter any issues or have feedback, please open an issue so we can improve. Archived v0 documentation can be found here.See the release notes and migration guide for a complete list of changes and instructions on how to upgrade your code.\ncallModel\ngraph node receives a malformed list of messages. Specifically, it is malformed when there are AIMessage\ns with tool_calls\n(LLM requesting to call a tool) that do not have a corresponding @[ToolMessage\n] (result of a tool invocation to return to the LLM).\nThere could be a few reasons you\u2019re seeing this error:\n- You manually passed a malformed list of messages when invoking the graph, e.g.\ngraph.invoke({messages: [new AIMessage({..., tool_calls: [...]})]})\n- The graph was interrupted before receiving updates from the\ntools\nnode (i.e. a list of @[ToolMessage\n]) and you invoked it with an input that is not null or a ToolMessage, e.g.graph.invoke({messages: [new HumanMessage(...)]}, config)\n. This interrupt could have been triggered in one of the following ways:\n- You manually set\ninterruptBefore: ['tools']\nincreateAgent\n- One of the tools raised an error that wasn\u2019t handled by the ToolNode (\n\"tools\"\n)\nTroubleshooting\nTo resolve this, you can do one of the following:- Don\u2019t invoke the graph with a malformed list of messages\n- In case of an interrupt (manual or due to an error) you can:\n- provide\nToolMessage\nobjects that match existing tool calls and callgraph.invoke({messages: [new ToolMessage(...)]})\n. NOTE: this will append the messages to the history and run the graph from the START node.- manually update the state and resume the graph from the interrupt:\n- get the list of most recent messages from the graph state with\ngraph.getState(config)\n- modify the list of messages to either remove unanswered tool calls from AIMessages\n- get the list of most recent messages from the graph state with\n- manually update the state and resume the graph from the interrupt:\nToolMessage\nobjects with toolCallId\ns that match unanswered tool calls 3. call graph.updateState(config, {messages: ...})\nwith the modified list of messages 4. resume the graph, e.g. call graph.invoke(null, config)", "tokens": 360, "node_type": "child"}
{"id": 343, "chunk_id": "c16acf8a553ac24f5d77e7d3a1c6e493", "parent_id": 323, "url": "", "namespace": "langchain", "title": "oss-javascript-langgraph-invalid-concurrent-graph-update", "headers": ["oss-javascript-langgraph-invalid-concurrent-graph-update"], "section_index": 0, "chunk_index": 0, "text": "# oss-javascript-langgraph-invalid-concurrent-graph-update\n\n> Source: https://docs.langchain.com/oss/javascript/langgraph/INVALID_CONCURRENT_GRAPH_UPDATE\n\nLangGraph v1.0Welcome to the new LangGraph documentation! If you encounter any issues or have feedback, please open an issue so we can improve. Archived v0 documentation can be found here.See the release notes and migration guide for a complete list of changes and instructions on how to upgrade your code.\nStateGraph\nreceived concurrent updates to its state from multiple nodes to a state property that doesn\u2019t\nsupport it.\nOne way this can occur is if you are using a fanout\nor other parallel execution in your graph and you have defined a graph like this:\n{ someKey: \"some_string_value\" }\n, this will overwrite the state value for someKey\nwith \"some_string_value\"\n.\nHowever, if multiple nodes in e.g. a fanout within a single step return values for someKey\n, the graph will throw this error because\nthere is uncertainty around how to update the internal state.\nTo get around this, you can define a reducer that combines multiple values:\nTroubleshooting\nThe following may help resolve this error:- If your graph executes nodes in parallel, make sure you have defined relevant state keys with a reducer.", "tokens": 189, "node_type": "child"}
{"id": 344, "chunk_id": "c16acf8a553ac24f5d77e7d3a1c6e493", "parent_id": 325, "url": "", "namespace": "langchain", "title": "oss-javascript-langgraph-local-server", "headers": ["oss-javascript-langgraph-local-server"], "section_index": 0, "chunk_index": 0, "text": "# oss-javascript-langgraph-local-server\n\n> Source: https://docs.langchain.com/oss/javascript/langgraph/local-server\n\nLangGraph v1.0Welcome to the new LangGraph documentation! If you encounter any issues or have feedback, please open an issue so we can improve. Archived v0 documentation can be found here.See the release notes and migration guide for a complete list of changes and instructions on how to upgrade your code.\nPrerequisites\nBefore you begin, ensure you have the following:- An API key for LangSmith - free to sign up\n1. Install the LangGraph CLI\n2. Create a LangGraph app \ud83c\udf31\nCreate a new app from thenew-langgraph-project-js\ntemplate. This template demonstrates a single-node application you can extend with your own logic.\n3. Install dependencies\nIn the root of your new LangGraph app, install the dependencies inedit\nmode so your local changes are used by the server:\n4. Create a .env\nfile\nYou will find a .env.example\nin the root of your new LangGraph app. Create a .env\nfile in the root of your new LangGraph app and copy the contents of the .env.example\nfile into it, filling in the necessary API keys:\n5. Launch LangGraph Server \ud83d\ude80\nStart the LangGraph API server locally:langgraph dev\ncommand starts LangGraph Server in an in-memory mode. This mode is suitable for development and testing purposes. For production use, deploy LangGraph Server with access to a persistent storage backend. For more information, see the Hosting overview.\n6. Test your application in Studio\nStudio is a specialized UI that you can connect to LangGraph API server to visualize, interact with, and debug your application locally. Test your graph in Studio by visiting the URL provided in the output of thelanggraph dev\ncommand:\nSafari compatibility\nSafari compatibility\nUse the\n--tunnel\nflag with your command to create a secure tunnel, as Safari has limitations when connecting to localhost servers:7. Test the API\n- Javascript SDK\n- Rest API\n- Install the LangGraph JS SDK:\n- Send a message to the assistant (threadless run):\nNext steps\nNow that you have a LangGraph app running locally, take your journey further by exploring deployment and advanced features:- Deployment quickstart: Deploy your LangGraph app using LangSmith.\n- LangSmith: Learn about foundational LangSmith concepts.\n- JS/TS SDK Reference: Explore the JS/TS SDK API Reference.", "tokens": 365, "node_type": "child"}
{"id": 345, "chunk_id": "c16acf8a553ac24f5d77e7d3a1c6e493", "parent_id": 326, "url": "", "namespace": "langchain", "title": "oss-javascript-langgraph-missing-checkpointer", "headers": ["oss-javascript-langgraph-missing-checkpointer"], "section_index": 0, "chunk_index": 0, "text": "# oss-javascript-langgraph-missing-checkpointer\n\n> Source: https://docs.langchain.com/oss/javascript/langgraph/MISSING_CHECKPOINTER\n\nLangGraph v1.0Welcome to the new LangGraph documentation! If you encounter any issues or have feedback, please open an issue so we can improve. Archived v0 documentation can be found here.See the release notes and migration guide for a complete list of changes and instructions on how to upgrade your code.\ncheckpointer\nis missing in the compile()\nmethod of StateGraph\nor entrypoint\n.\nTroubleshooting\nThe following may help resolve this error:- Initialize and pass a checkpointer to the\ncompile()\nmethod ofStateGraph\norentrypoint\n.\n- Use the LangGraph API so you don\u2019t need to implement or configure checkpointers manually. The API handles all persistence infrastructure for you.\nRelated\n- Read more about persistence.", "tokens": 116, "node_type": "child"}
{"id": 346, "chunk_id": "c16acf8a553ac24f5d77e7d3a1c6e493", "parent_id": 327, "url": "", "namespace": "langchain", "title": "oss-javascript-langgraph-multiple-subgraphs", "headers": ["oss-javascript-langgraph-multiple-subgraphs"], "section_index": 0, "chunk_index": 0, "text": "# oss-javascript-langgraph-multiple-subgraphs\n\n> Source: https://docs.langchain.com/oss/javascript/langgraph/MULTIPLE_SUBGRAPHS\n\nLangGraph v1.0Welcome to the new LangGraph documentation! If you encounter any issues or have feedback, please open an issue so we can improve. Archived v0 documentation can be found here.See the release notes and migration guide for a complete list of changes and instructions on how to upgrade your code.\nTroubleshooting\nThe following may help resolve this error:-\nIf you don\u2019t need to interrupt/resume from a subgraph, pass\ncheckpointer: false\nwhen compiling it like this:.compile({ checkpointer: false })\n-\nDon\u2019t imperatively call graphs multiple times in the same node, and instead use the\nSend\nAPI.", "tokens": 100, "node_type": "child"}
{"id": 347, "chunk_id": "c16acf8a553ac24f5d77e7d3a1c6e493", "parent_id": 328, "url": "", "namespace": "langchain", "title": "oss-javascript-langgraph-observability", "headers": ["oss-javascript-langgraph-observability"], "section_index": 0, "chunk_index": 0, "text": "# oss-javascript-langgraph-observability\n\n> Source: https://docs.langchain.com/oss/javascript/langgraph/observability\n\nLangGraph v1.0Welcome to the new LangGraph documentation! If you encounter any issues or have feedback, please open an issue so we can improve. Archived v0 documentation can be found here.See the release notes and migration guide for a complete list of changes and instructions on how to upgrade your code.\nPrerequisites\nBefore you begin, ensure you have the following:- A LangSmith account (free to sign up)\nEnable tracing\nTo enable tracing for your application, set the following environment variables:default\n. To configure a custom project name, see Log to a project.\nFor more information, see Trace with LangGraph.\nTrace selectively\nYou may opt to trace specific invocations or parts of your application using LangSmith\u2019stracing_context\ncontext manager:\nLog to a project\nStatically\nStatically\nYou can set a custom project name for your entire application by setting the\nLANGSMITH_PROJECT\nenvironment variable:Dynamically\nDynamically\nYou can set the project name programmatically for specific operations:\nAdd metadata to traces\nYou can annotate your traces with custom metadata and tags:tracing_context\nalso accepts tags and metadata for fine-grained control:\nUse anonymizers to prevent logging of sensitive data in traces\nYou may want to mask sensitive data to prevent it from being logged to LangSmith. You can create anonymizers and apply them to your graph using configuration. This example will redact anything matching the Social Security Number format XXX-XX-XXXX from traces sent to LangSmith.TypeScript", "tokens": 231, "node_type": "child"}
{"id": 348, "chunk_id": "c16acf8a553ac24f5d77e7d3a1c6e493", "parent_id": 329, "url": "", "namespace": "langchain", "title": "oss-javascript-langgraph-overview", "headers": ["oss-javascript-langgraph-overview"], "section_index": 0, "chunk_index": 0, "text": "# oss-javascript-langgraph-overview\n\n> Source: https://docs.langchain.com/oss/javascript/langgraph/overview\n\nLangGraph v1.0Welcome to the new LangGraph documentation! If you encounter any issues or have feedback, please open an issue so we can improve. Archived v0 documentation can be found here.See the release notes and migration guide for a complete list of changes and instructions on how to upgrade your code.\nInstall\nCore benefits\nLangGraph provides low-level supporting infrastructure for any long-running, stateful workflow or agent. LangGraph does not abstract prompts or architecture, and provides the following central benefits:- Durable execution: Build agents that persist through failures and can run for extended periods, resuming from where they left off.\n- Human-in-the-loop: Incorporate human oversight by inspecting and modifying agent state at any point.\n- Comprehensive memory: Create stateful agents with both short-term working memory for ongoing reasoning and long-term memory across sessions.\n- Debugging with LangSmith: Gain deep visibility into complex agent behavior with visualization tools that trace execution paths, capture state transitions, and provide detailed runtime metrics.\n- Production-ready deployment: Deploy sophisticated agent systems confidently with scalable infrastructure designed to handle the unique challenges of stateful, long-running workflows.\nLangGraph ecosystem\nWhile LangGraph can be used standalone, it also integrates seamlessly with any LangChain product, giving developers a full suite of tools for building agents. To improve your LLM application development, pair LangGraph with:- LangSmith \u2014 Helpful for agent evals and observability. Debug poor-performing LLM app runs, evaluate agent trajectories, gain visibility in production, and improve performance over time.\n- LangSmith \u2014 Deploy and scale agents effortlessly with a purpose-built deployment platform for long running, stateful workflows. Discover, reuse, configure, and share agents across teams \u2014 and iterate quickly with visual prototyping in Studio.\n- LangChain - Provides integrations and composable components to streamline LLM application development. Contains agent abstractions built on top of LangGraph.", "tokens": 299, "node_type": "child"}
{"id": 349, "chunk_id": "c16acf8a553ac24f5d77e7d3a1c6e493", "parent_id": 330, "url": "", "namespace": "langchain", "title": "oss-javascript-langgraph-persistence", "headers": ["oss-javascript-langgraph-persistence"], "section_index": 0, "chunk_index": 0, "text": "# oss-javascript-langgraph-persistence > Source: https://docs.langchain.com/oss/javascript/langgraph/persistence LangGraph v1.0Welcome to the new LangGraph documentation! If you encounter any issues or have feedback, please open an issue so we can improve. Archived v0 documentation can be found here.See the release notes and migration guide for a complete list of changes and instructions on how to upgrade your code. checkpoint of the graph state at every super-step. Those checkpoints are saved to a thread , which can be accessed after graph execution. Because threads allow access to graph\u2019s state after execution, several powerful capabilities including human-in-the-loop, memory, time travel, and fault-tolerance are all possible. Below, we\u2019ll discuss each of these concepts in more detail. LangGraph API handles checkpointing automatically When using the LangGraph API, you don\u2019t need to implement or configure checkpointers manually. The API handles all persistence infrastructure for you behind the scenes. Threads A thread is a unique ID or thread identifier assigned to each checkpoint saved by a checkpointer. It contains the accumulated state of a sequence of runs. When a run is executed, the state of the underlying graph of the assistant will be persisted to the thread. When invoking a graph with a checkpointer, you must specify athread_id as part of the configurable portion of the config. Checkpoints The state of a thread at a particular point in time is called a checkpoint. Checkpoint is a snapshot of the graph state saved at each super-step and is represented byStateSnapshot object with the following key properties: config : Config associated with this checkpoint.metadata : Metadata associated with this checkpoint.values : Values of the state channels at this point in time.next A tuple of the node names to execute next in the graph.tasks : A tuple ofPregelTask objects that contain information about next tasks to be executed. If the step was previously attempted, it will include error information. If a graph was interrupted dynamically from within a node, tasks will contain additional data associated with interrupts. - empty checkpoint with START as the next node to be executed - checkpoint with the user input {'foo': '', 'bar': []} andnodeA as the next node to be executed - checkpoint with the outputs of nodeA {'foo': 'a', 'bar': ['a']} andnodeB as the next node to be executed - checkpoint with the outputs of nodeB {'foo': 'b', 'bar': ['a', 'b']} and no next nodes to be executed bar channel values contain outputs from both nodes as we have a reducer for the bar channel. Get state When interacting with the saved graph state, you must specify a thread identifier. You can view the latest state of the graph by callinggraph.getState(config) . This will return a StateSnapshot object that corresponds to the latest checkpoint associated with the thread ID provided in the config or a checkpoint associated with a checkpoint ID for the thread, if provided. getState will look like this: Get state history You can get the full history of the graph execution for a given thread by callinggraph.getStateHistory(config) . This will return a list of StateSnapshot objects associated with the thread ID provided in the config. Importantly, the checkpoints will be ordered chronologically with the most recent checkpoint / StateSnapshot being the first in the list. getStateHistory will look like this: Replay It\u2019s also possible to play-back a prior graph execution. If weinvoke a graph with a thread_id and a checkpoint_id , then we will re-play the previously executed steps before a checkpoint that corresponds to the checkpoint_id , and only execute the steps after the checkpoint. thread_id is the ID of a thread.checkpoint_id is an identifier that refers to a specific checkpoint within a thread. configurable portion of the config: checkpoint_id . All of the steps after checkpoint_id will be executed (i.e., a new fork), even if they have been executed previously. See this how to guide on time-travel to learn more about replaying. Update state In addition to re-playing the graph from specificcheckpoints , we can also edit the graph state. We do this using graph.updateState() . This method accepts three different arguments: config The config should contain thread_id specifying which thread to update. When only the thread_id is passed, we update (or fork) the current state. Optionally, if we include checkpoint_id field, then we fork that selected checkpoint. values These are the values that will be used to update the state. Note that this update is treated exactly as any update from a node is treated. This means that these values will be passed to the reducer functions, if they are defined for some of the channels in the graph state. This means that update_state does NOT automatically overwrite the channel values for every channel, but only for the channels without reducers. Let\u2019s walk through an example. Let\u2019s assume you have defined the state of your graph with the following schema (see full example above): foo key (channel) is completely changed (because there is no reducer specified for that channel, so updateState overwrites it). However, there is a reducer specified for the bar key, and so it appends \"b\" to the state of bar . as_node The final thing you can optionally specify when calling updateState is asNode . If you provide it, the update will be applied as if it came from node asNode . If asNode is not provided, it will be set to the last node that updated the state, if not ambiguous. The reason this matters is that the next steps to execute depend on the last node to have given an update, so this can be used to control which node executes next. See this how to guide on time-travel to learn more about forking state. Memory Store A state schema specifies a set of keys that are populated as a graph is executed. As discussed above, state can be written by a checkpointer to a thread at each graph step, enabling state persistence. But, what if we want to retain some information across threads? Consider the case of a chatbot where we want to", "tokens": 1000, "node_type": "child"}
{"id": 350, "chunk_id": "9b0fd3b850698317bef57fa6e71a38ba", "parent_id": 330, "url": "", "namespace": "langchain", "title": "oss-javascript-langgraph-persistence", "headers": ["oss-javascript-langgraph-persistence"], "section_index": 0, "chunk_index": 1, "text": "can optionally specify when calling updateState is asNode . If you provide it, the update will be applied as if it came from node asNode . If asNode is not provided, it will be set to the last node that updated the state, if not ambiguous. The reason this matters is that the next steps to execute depend on the last node to have given an update, so this can be used to control which node executes next. See this how to guide on time-travel to learn more about forking state. Memory Store A state schema specifies a set of keys that are populated as a graph is executed. As discussed above, state can be written by a checkpointer to a thread at each graph step, enabling state persistence. But, what if we want to retain some information across threads? Consider the case of a chatbot where we want to retain specific information about the user across all chat conversations (e.g., threads) with that user! With checkpointers alone, we cannot share information across threads. This motivates the need for theStore interface. As an illustration, we can define an InMemoryStore to store information about a user across threads. We simply compile our graph with a checkpointer, as before, and with our new in_memory_store variable. LangGraph API handles stores automatically When using the LangGraph API, you don\u2019t need to implement or configure stores manually. The API handles all storage infrastructure for you behind the scenes. Basic Usage First, let\u2019s showcase this in isolation without using LangGraph.tuple , which in this specific example will be (<user_id>, \"memories\") . The namespace can be any length and represent anything, does not have to be user specific. store.put method to save memories to our namespace in the store. When we do this, we specify the namespace, as defined above, and a key-value pair for the memory: the key is simply a unique identifier for the memory (memory_id ) and the value (a dictionary) is the memory itself. store.search method, which will return all memories for a given user as a list. The most recent memory is the last in the list. value : The value of this memorykey : A unique key for this memory in this namespacenamespace : A list of strings, the namespace of this memory typecreatedAt : Timestamp for when this memory was createdupdatedAt : Timestamp for when this memory was updated Semantic Search Beyond simple retrieval, the store also supports semantic search, allowing you to find memories based on meaning rather than exact matches. To enable this, configure the store with an embedding model:fields parameter or by specifying the index parameter when storing memories: Using in LangGraph With this all in place, we use thememoryStore in LangGraph. The memoryStore works hand-in-hand with the checkpointer: the checkpointer saves state to threads, as discussed above, and the memoryStore allows us to store arbitrary information for access across threads. We compile the graph with both the checkpointer and the memoryStore as follows. thread_id , as before, and also with a user_id , which we\u2019ll use to namespace our memories to this particular user as we showed above. memoryStore and the user_id in any node by accessing config and store as node arguments. Here\u2019s how we might use semantic search in a node to find relevant memories: store.search method to get memories. Recall the memories are returned as a list of objects that can be converted to a dictionary. user_id is the same. langgraph.json file. For example: Checkpointer libraries Under the hood, checkpointing is powered by checkpointer objects that conform to BaseCheckpointSaver interface. LangGraph provides several checkpointer implementations, all implemented via standalone, installable libraries:@langchain/langgraph-checkpoint : The base interface for checkpointer savers (BaseCheckpointSaver ) and serialization/deserialization interface (SerializerProtocol ). Includes in-memory checkpointer implementation (MemorySaver ) for experimentation. LangGraph comes with@langchain/langgraph-checkpoint included.@langchain/langgraph-checkpoint-sqlite : An implementation of LangGraph checkpointer that uses SQLite database (SqliteSaver ). Ideal for experimentation and local workflows. Needs to be installed separately.@langchain/langgraph-checkpoint-postgres : An advanced checkpointer that uses Postgres database (PostgresSaver ), used in LangSmith. Ideal for using in production. Needs to be installed separately. Checkpointer interface Each checkpointer conforms to the BaseCheckpointSaver interface and implements the following methods:.put - Store a checkpoint with its configuration and metadata..putWrites - Store intermediate writes linked to a checkpoint (i.e. pending writes)..getTuple - Fetch a checkpoint tuple using for a given configuration (thread_id andcheckpoint_id ). This is used to populateStateSnapshot ingraph.getState() ..list - List checkpoints that match a given configuration and filter criteria. This is used to populate state history ingraph.getStateHistory() Serializer When checkpointers save the graph state, they need to serialize the channel values in the state. This is done using serializer objects.@langchain/langgraph-checkpoint defines protocol for implementing serializers and provides a default implementation that handles a wide variety of types, including LangChain and LangGraph primitives, datetimes, enums and more.", "tokens": 802, "node_type": "child"}
{"id": 351, "chunk_id": "c16acf8a553ac24f5d77e7d3a1c6e493", "parent_id": 331, "url": "", "namespace": "langchain", "title": "oss-javascript-langgraph-pregel", "headers": ["oss-javascript-langgraph-pregel"], "section_index": 0, "chunk_index": 0, "text": "# oss-javascript-langgraph-pregel\n\n> Source: https://docs.langchain.com/oss/javascript/langgraph/pregel\n\nLangGraph v1.0Welcome to the new LangGraph documentation! If you encounter any issues or have feedback, please open an issue so we can improve. Archived v0 documentation can be found here.See the release notes and migration guide for a complete list of changes and instructions on how to upgrade your code.\nNote: The Pregel runtime is named after Google\u2019s Pregel algorithm, which describes an efficient method for large-scale parallel computation using graphs.\nOverview\nIn LangGraph, Pregel combines actors and channels into a single application. Actors read data from channels and write data to channels. Pregel organizes the execution of the application into multiple steps, following the Pregel Algorithm/Bulk Synchronous Parallel model. Each step consists of three phases:- Plan: Determine which actors to execute in this step. For example, in the first step, select the actors that subscribe to the special input channels; in subsequent steps, select the actors that subscribe to channels updated in the previous step.\n- Execution: Execute all selected actors in parallel, until all complete, or one fails, or a timeout is reached. During this phase, channel updates are invisible to actors until the next step.\n- Update: Update the channels with the values written by the actors in this step.\nActors\nAn actor is aPregelNode\n. It subscribes to channels, reads data from them, and writes data to them. It can be thought of as an actor in the Pregel algorithm. PregelNodes\nimplement LangChain\u2019s Runnable interface.\nChannels\nChannels are used to communicate between actors (PregelNodes). Each channel has a value type, an update type, and an update function \u2013 which takes a sequence of updates and modifies the stored value. Channels can be used to send data from one chain to another, or to send data from a chain to itself in a future step. LangGraph provides a number of built-in channels:- @[LastValue]: The default channel, stores the last value sent to the channel, useful for input and output values, or for sending data from one step to the next.\n- @[Topic]: A configurable PubSub Topic, useful for sending multiple values between actors, or for accumulating output. Can be configured to deduplicate values or to accumulate values over the course of multiple steps.\n- BinaryOperatorAggregate: stores a persistent value, updated by applying a binary operator to the current value and each update sent to the channel, useful for computing aggregates over multiple steps; e.g.,\ntotal = BinaryOperatorAggregate(int, operator.add)\nExamples\nWhile most users will interact with Pregel through the StateGraph API or the entrypoint decorator, it is possible to interact with Pregel directly. Below are a few different examples to give you a sense of the Pregel API.- Single node\n- Multiple nodes\n- Topic\n- BinaryOperatorAggregate\n- Cycle\nHigh-level API\nLangGraph provides two high-level APIs for creating a Pregel application: the StateGraph (Graph API) and the Functional API.- StateGraph (Graph API)\n- Functional API\nThe StateGraph (Graph API) is a higher-level abstraction that simplifies the creation of Pregel applications. It allows you to define a graph of nodes and edges. When you compile the graph, the StateGraph API automatically creates the Pregel application for you.The compiled Pregel instance will be associated with a list of nodes and channels. You can inspect the nodes and channels by printing them.You will see something like this:You should see something like this", "tokens": 556, "node_type": "child"}
{"id": 352, "chunk_id": "c16acf8a553ac24f5d77e7d3a1c6e493", "parent_id": 332, "url": "", "namespace": "langchain", "title": "oss-javascript-langgraph-quickstart", "headers": ["oss-javascript-langgraph-quickstart"], "section_index": 0, "chunk_index": 0, "text": "# oss-javascript-langgraph-quickstart\n\n> Source: https://docs.langchain.com/oss/javascript/langgraph/quickstart\n\nLangGraph v1.0Welcome to the new LangGraph documentation! If you encounter any issues or have feedback, please open an issue so we can improve. Archived v0 documentation can be found here.See the release notes and migration guide for a complete list of changes and instructions on how to upgrade your code.\n- Use the Graph API if you prefer to define your agent as a graph of nodes and edges.\n- Use the Functional API if you prefer to define your agent as a single function.\n- Use the Graph API\n- Use the Functional API\n1. Define tools and model\nIn this example, we\u2019ll use the Claude Sonnet 4.5 model and define tools for addition, multiplication, and division.2. Define state\nThe graph\u2019s state is used to store the messages and the number of LLM calls.State in LangGraph persists throughout the agent\u2019s execution. The\nAnnotated\ntype with operator.add\nensures that new messages are appended to the existing list rather than replacing it.3. Define model node\nThe model node is used to call the LLM and decide whether to call a tool or not.4. Define tool node\nThe tool node is used to call the tools and return the results.5. Define end logic\nThe conditional edge function is used to route to the tool node or end based upon whether the LLM made a tool call.6. Build and compile the agent\nThe agent is built using theStateGraph\nclass and compiled using the compile\nmethod.Full code example\nFull code example", "tokens": 252, "node_type": "child"}
{"id": 353, "chunk_id": "c16acf8a553ac24f5d77e7d3a1c6e493", "parent_id": 333, "url": "", "namespace": "langchain", "title": "oss-javascript-langgraph-streaming", "headers": ["oss-javascript-langgraph-streaming"], "section_index": 0, "chunk_index": 0, "text": "# oss-javascript-langgraph-streaming\n\n> Source: https://docs.langchain.com/oss/javascript/langgraph/streaming\n\n- Stream graph state \u2014 get state updates / values with\nupdates\nandvalues\nmodes. - Stream subgraph outputs \u2014 include outputs from both the parent graph and any nested subgraphs.\n- Stream LLM tokens \u2014 capture token streams from anywhere: inside nodes, subgraphs, or tools.\n- Stream custom data \u2014 send custom updates or progress signals directly from tool functions.\n- Use multiple streaming modes \u2014 choose from\nvalues\n(full state),updates\n(state deltas),messages\n(LLM tokens + metadata),custom\n(arbitrary user data), ordebug\n(detailed traces).\nSupported stream modes\nPass one or more of the following stream modes as a list to thestream()\nmethod:\n| Mode | Description |\n|---|---|\nvalues | Streams the full value of the state after each step of the graph. |\nupdates | Streams the updates to the state after each step of the graph. If multiple updates are made in the same step (e.g., multiple nodes are run), those updates are streamed separately. |\ncustom | Streams custom data from inside your graph nodes. |\nmessages | Streams 2-tuples (LLM token, metadata) from any graph nodes where an LLM is invoked. |\ndebug | Streams as much information as possible throughout the execution of the graph. |\nBasic usage example\nLangGraph graphs expose the.stream()\nmethod to yield streamed outputs as iterators.\nExtended example: streaming updates\nExtended example: streaming updates\nStream multiple modes\nYou can pass an array as thestreamMode\nparameter to stream multiple modes at once.\nThe streamed outputs will be tuples of [mode, chunk]\nwhere mode\nis the name of the stream mode and chunk\nis the data streamed by that mode.\nStream graph state\nUse the stream modesupdates\nand values\nto stream the state of the graph as it executes.\nupdates\nstreams the updates to the state after each step of the graph.values\nstreams the full value of the state after each step of the graph.\n- updates\n- values\nStream subgraph outputs\nTo include outputs from subgraphs in the streamed outputs, you can setsubgraphs: true\nin the .stream()\nmethod of the parent graph. This will stream outputs from both the parent graph and any subgraphs.\nThe outputs will be streamed as tuples [namespace, data]\n, where namespace\nis a tuple with the path to the node where a subgraph is invoked, e.g. [\"parent_node:<task_id>\", \"child_node:<task_id>\"]\n.\nExtended example: streaming from subgraphs\nExtended example: streaming from subgraphs\nDebugging\nUse thedebug\nstreaming mode to stream as much information as possible throughout the execution of the graph. The streamed outputs include the name of the node as well as the full state.\nLLM tokens\nUse themessages\nstreaming mode to stream Large Language Model (LLM) outputs token by token from any part of your graph, including nodes, tools, subgraphs, or tasks.\nThe streamed output from messages\nmode is a tuple [message_chunk, metadata]\nwhere:\nmessage_chunk\n: the token or message segment from the LLM.metadata\n: a dictionary containing details about the graph node and LLM invocation.\nIf your LLM is not available as a LangChain integration, you can stream its outputs using custom\nmode instead. See use with any LLM for details.\nFilter by LLM invocation\nYou can associatetags\nwith LLM invocations to filter the streamed tokens by LLM invocation.\nExtended example: filtering by tags\nExtended example: filtering by tags\nFilter by node\nTo stream tokens only from specific nodes, usestream_mode=\"messages\"\nand filter the outputs by the langgraph_node\nfield in the streamed metadata:\nExtended example: streaming LLM tokens from specific nodes\nExtended example: streaming LLM tokens from specific nodes\nStream custom data\nTo send custom user-defined data from inside a LangGraph node or tool, follow these steps:- Use the\nwriter\nparameter from theLangGraphRunnableConfig\nto emit custom data. - Set\nstreamMode: \"custom\"\nwhen calling.stream()\nto get the custom data in the stream. You can combine multiple modes (e.g.,[\"updates\", \"custom\"]\n), but at least one must be\"custom\"\n.\n- node\n- tool\nUse with any LLM\nYou can usestreamMode: \"custom\"\nto stream data from any LLM API \u2014 even if that API does not implement the LangChain chat model interface.\nThis lets you integrate raw LLM clients or external services that provide their own streaming interfaces, making LangGraph highly flexible for custom setups.\nExtended example: streaming arbitrary chat model\nExtended example: streaming arbitrary chat model\nDisable streaming for specific chat models\nIf your application mixes models that support streaming with those that do not, you may need to explicitly disable streaming for models that do not support it. Setstreaming: false\nwhen initializing the model.", "tokens": 747, "node_type": "child"}
{"id": 354, "chunk_id": "c16acf8a553ac24f5d77e7d3a1c6e493", "parent_id": 334, "url": "", "namespace": "langchain", "title": "oss-javascript-langgraph-studio", "headers": ["oss-javascript-langgraph-studio"], "section_index": 0, "chunk_index": 0, "text": "# oss-javascript-langgraph-studio\n\n> Source: https://docs.langchain.com/oss/javascript/langgraph/studio\n\nLangGraph v1.0Welcome to the new LangGraph documentation! If you encounter any issues or have feedback, please open an issue so we can improve. Archived v0 documentation can be found here.See the release notes and migration guide for a complete list of changes and instructions on how to upgrade your code.\nPrerequisites\nBefore you begin, ensure you have the following:- An API key for LangSmith (free to sign up)\nSetup local LangGraph server\n1. Install the LangGraph CLI\n2. Prepare your agent\nWe\u2019ll use the following simple agent as an example:agent.py\n3. Environment variables\nCreate a.env\nfile in the root of your project and fill in the necessary API keys. We\u2019ll need to set the LANGSMITH_API_KEY\nenvironment variable to the API key you get from LangSmith.\nBe sure not to commit your\n.env\nto version control systems such as Git!.env\n4. Create a LangGraph config file\nInside your app\u2019s directory, create a configuration filelanggraph.json\n:\nlanggraph.json\ncreate_agent\nautomatically returns a compiled LangGraph graph that we can pass to the graphs\nkey in our configuration file.\nSo far, our project structure looks like this:\n5. Install dependencies\nIn the root of your new LangGraph app, install the dependencies:6. View your agent in Studio\nStart your LangGraph server:Safari blocks\nlocalhost\nconnections to Studio. To work around this, run the above command with --tunnel\nto access Studio via a secure tunnel.http://127.0.0.1:2024\n) and the Studio UI https://smith.langchain.com/studio/?baseUrl=http://127.0.0.1:2024\n:", "tokens": 239, "node_type": "child"}
{"id": 355, "chunk_id": "c16acf8a553ac24f5d77e7d3a1c6e493", "parent_id": 335, "url": "", "namespace": "langchain", "title": "oss-javascript-langgraph-test", "headers": ["oss-javascript-langgraph-test"], "section_index": 0, "chunk_index": 0, "text": "# oss-javascript-langgraph-test\n\n> Source: https://docs.langchain.com/oss/javascript/langgraph/test\n\nLangGraph v1.0Welcome to the new LangGraph documentation! If you encounter any issues or have feedback, please open an issue so we can improve. Archived v0 documentation can be found here.See the release notes and migration guide for a complete list of changes and instructions on how to upgrade your code.\ncreate_agent\n] instead.\nPrerequisites\nFirst, make sure you havevitest\ninstalled:\nGetting started\nBecause many LangGraph agents depend on state, a useful pattern is to create your graph before each test where you use it, then compile it within tests with a new checkpointer instance. The below example shows how this works with a simple, linear graph that progresses throughnode1\nand node2\n. Each node updates the single state key my_key\n:\nTesting individual nodes and edges\nCompiled LangGraph agents expose references to each individual node asgraph.nodes\n. You can take advantage of this to test individual nodes within your agent. Note that this will bypass any checkpointers passed when compiling the graph:\nPartial execution\nFor agents made up of larger graphs, you may wish to test partial execution paths within your agent rather than the entire flow end-to-end. In some cases, it may make semantic sense to restructure these sections as subgraphs, which you can invoke in isolation as normal. However, if you do not wish to make changes to your agent graph\u2019s overall structure, you can use LangGraph\u2019s persistence mechanisms to simulate a state where your agent is paused right before the beginning of the desired section, and will pause again at the end of the desired section. The steps are as follows:- Compile your agent with a checkpointer (the in-memory checkpointer\nMemorySaver\nwill suffice for testing). - Call your agent\u2019s\nupdate_state\nmethod with anasNode\nparameter set to the name of the node before the one you want to start your test. - Invoke your agent with the same\nthread_id\nyou used to update the state and aninterruptBefore\nparameter set to the name of the node you want to stop at.", "tokens": 337, "node_type": "child"}
{"id": 356, "chunk_id": "c16acf8a553ac24f5d77e7d3a1c6e493", "parent_id": 336, "url": "", "namespace": "langchain", "title": "oss-javascript-langgraph-thinking-in-langgraph", "headers": ["oss-javascript-langgraph-thinking-in-langgraph"], "section_index": 0, "chunk_index": 0, "text": "# oss-javascript-langgraph-thinking-in-langgraph > Source: https://docs.langchain.com/oss/javascript/langgraph/thinking-in-langgraph Start with the process you want to automate Imagine that you need to build an AI agent that handles customer support emails. Your product team has given you these requirements: The agent should:- Read incoming customer emails - Classify them by urgency and topic - Search relevant documentation to answer questions - Draft appropriate responses - Escalate complex issues to human agents - Schedule follow-ups when needed - Simple product question: \u201cHow do I reset my password?\u201d - Bug report: \u201cThe export feature crashes when I select PDF format\u201d - Urgent billing issue: \u201cI was charged twice for my subscription!\u201d - Feature request: \u201cCan you add dark mode to the mobile app?\u201d - Complex technical issue: \u201cOur API integration fails intermittently with 504 errors\u201d Step 1: Map out your workflow as discrete steps Start by identifying the distinct steps in your process. Each step will become a node (a function that does one specific thing). Then sketch how these steps connect to each other. The arrows show possible paths, but the actual decision of which path to take happens inside each node. Now that you\u2019ve identified the components in your workflow, let\u2019s understand what each node needs to do:- Read Email: Extract and parse the email content - Classify Intent: Use an LLM to categorize urgency and topic, then route to appropriate action - Doc Search: Query your knowledge base for relevant information - Bug Track: Create or update issue in tracking system - Draft Reply: Generate an appropriate response - Human Review: Escalate to human agent for approval or handling - Send Reply: Dispatch the email response Step 2: Identify what each step needs to do For each node in your graph, determine what type of operation it represents and what context it needs to work properly.LLM Steps Data Steps Action Steps User Input Steps LLM Steps When a step needs to understand, analyze, generate text, or make reasoning decisions:Classify Intent Node Classify Intent Node - Static context (prompt): Classification categories, urgency definitions, response format - Dynamic context (from state): Email content, sender information - Desired outcome: Structured classification that determines routing Draft Reply Node Draft Reply Node - Static context (prompt): Tone guidelines, company policies, response templates - Dynamic context (from state): Classification results, search results, customer history - Desired outcome: Professional email response ready for review Data Steps When a step needs to retrieve information from external sources:Doc Search Node Doc Search Node - Parameters: Query built from intent and topic - Retry strategy: Yes, with exponential backoff for transient failures - Caching: Could cache common queries to reduce API calls Customer History Lookup Customer History Lookup - Parameters: Customer email or ID from state - Retry strategy: Yes, but with fallback to basic info if unavailable - Caching: Yes, with time-to-live to balance freshness and performance Action Steps When a step needs to perform an external action:Send Reply Node Send Reply Node - When to execute: After approval (human or automated) - Retry strategy: Yes, with exponential backoff for network issues - Should not cache: Each send is a unique action Bug Track Node Bug Track Node - When to execute: Always when intent is \u201cbug\u201d - Retry strategy: Yes, critical to not lose bug reports - Returns: Ticket ID to include in response User Input Steps When a step needs human intervention:Human Review Node Human Review Node - Context for decision: Original email, draft response, urgency, classification - Expected input format: Approval boolean plus optional edited response - When triggered: High urgency, complex issues, or quality concerns Step 3: Design your state State is the shared memory accessible to all nodes in your agent. Think of it as the notebook your agent uses to keep track of everything it learns and decides as it works through the process.What belongs in state? Ask yourself these questions about each piece of data:Include in State Don't Store - The original email and sender info (can\u2019t reconstruct these) - Classification results (needed by multiple downstream nodes) - Search results and customer data (expensive to re-fetch) - The draft response (needs to persist through review) - Execution metadata (for debugging and recovery) Keep state raw, format prompts on-demand - Different nodes can format the same data differently for their needs - You can change prompt templates without modifying your state schema - Debugging is clearer - you see exactly what data each node received - Your agent can evolve without breaking existing state Step 4: Build your nodes Now we implement each step as a function. A node in LangGraph is just a JavaScript function that takes the current state and returns updates to it.Handle errors appropriately Different errors need different handling strategies:| Error Type | Who Fixes It | Strategy | When to Use | |---|---|---|---| | Transient errors (network issues, rate limits) | System (automatic) | Retry policy | Temporary failures that usually resolve on retry | | LLM-recoverable errors (tool failures, parsing issues) | LLM | Store error in state and loop back | LLM can see the error and adjust its approach | | User-fixable errors (missing information, unclear instructions) | Human | Pause with interrupt() | Need user input to proceed | | Unexpected errors | Developer | Let them bubble up | Unknown issues that need debugging | - Transient errors - LLM-recoverable - User-fixable - Unexpected Implementing our email agent nodes We\u2019ll implement each node as a simple function. Remember: nodes take state, do work, and return updates.Read and classify nodes Read and classify nodes Search and tracking nodes Search and tracking nodes Response nodes Response nodes Step 5: Wire it together Now we connect our nodes into a working graph. Since our nodes handle their own routing decisions, we only need a few essential edges. To enable human-in-the-loop withinterrupt() , we need to compile with a checkpointer to save state between runs: Graph compilation code Graph compilation code Command objects. Each node declares where it can go,", "tokens": 1000, "node_type": "child"}
{"id": 357, "chunk_id": "9b0fd3b850698317bef57fa6e71a38ba", "parent_id": 336, "url": "", "namespace": "langchain", "title": "oss-javascript-langgraph-thinking-in-langgraph", "headers": ["oss-javascript-langgraph-thinking-in-langgraph"], "section_index": 0, "chunk_index": 1, "text": "approach | | User-fixable errors (missing information, unclear instructions) | Human | Pause with interrupt() | Need user input to proceed | | Unexpected errors | Developer | Let them bubble up | Unknown issues that need debugging | - Transient errors - LLM-recoverable - User-fixable - Unexpected Implementing our email agent nodes We\u2019ll implement each node as a simple function. Remember: nodes take state, do work, and return updates.Read and classify nodes Read and classify nodes Search and tracking nodes Search and tracking nodes Response nodes Response nodes Step 5: Wire it together Now we connect our nodes into a working graph. Since our nodes handle their own routing decisions, we only need a few essential edges. To enable human-in-the-loop withinterrupt() , we need to compile with a checkpointer to save state between runs: Graph compilation code Graph compilation code Command objects. Each node declares where it can go, making the flow explicit and traceable. Try out your agent Let\u2019s run our agent with an urgent billing issue that needs human review:Testing the agent Testing the agent interrupt() , saves everything to the checkpointer, and waits. It can resume days later, picking up exactly where it left off. The thread_id ensures all state for this conversation is preserved together. Summary and next steps Key Insights Building this email agent has shown us the LangGraph way of thinking:Break into discrete steps State is shared memory Nodes are functions Errors are part of the flow Human input is first-class interrupt() function pauses execution indefinitely, saves all state, and resumes exactly where it left off when you provide input. When combined with other operations in a node, it must come first.Graph structure emerges naturally Advanced considerations Node granularity trade-offs Node granularity trade-offs - Isolation of external services: Doc Search and Bug Track are separate nodes because they call external APIs. If the search service is slow or fails, we want to isolate that from the LLM calls. We can add retry policies to these specific nodes without affecting others. - Intermediate visibility: Having Classify Intent as its own node lets us inspect what the LLM decided before taking action. This is valuable for debugging and monitoring\u2014you can see exactly when and why the agent routes to human review. - Different failure modes: LLM calls, database lookups, and email sending have different retry strategies. Separate nodes let you configure these independently. - Reusability and testing: Smaller nodes are easier to test in isolation and reuse in other workflows. \"async\" mode writes checkpoints in the background for good performance while maintaining durability. Use \"exit\" mode to checkpoint only at completion (faster for long-running graphs where mid-execution recovery isn\u2019t needed), or \"sync\" mode to guarantee checkpoints are written before proceeding to the next step (useful when you need to ensure state is persisted before continuing execution).", "tokens": 471, "node_type": "child"}
{"id": 358, "chunk_id": "c16acf8a553ac24f5d77e7d3a1c6e493", "parent_id": 337, "url": "", "namespace": "langchain", "title": "oss-javascript-langgraph-ui", "headers": ["oss-javascript-langgraph-ui"], "section_index": 0, "chunk_index": 0, "text": "# oss-javascript-langgraph-ui\n\n> Source: https://docs.langchain.com/oss/javascript/langgraph/ui\n\nLangGraph v1.0Welcome to the new LangGraph documentation! If you encounter any issues or have feedback, please open an issue so we can improve. Archived v0 documentation can be found here.See the release notes and migration guide for a complete list of changes and instructions on how to upgrade your code.\ncreate_agent()\n. This UI is designed to provide rich, interactive experiences for your agents with minimal setup, whether you\u2019re running locally or in a deployed context (such as LangSmith).\nAgent Chat UI\nAgent Chat UI is a Next.js application that provides a conversational interface for interacting with any LangChain agent. It supports real-time chat, tool visualization, and advanced features like time-travel debugging and state forking. Agent Chat UI is open source and can be adapted to your application needs.Features\nTool visualization\nTool visualization\nStudio automatically renders tool calls and results in an intuitive interface.\nTime-travel debugging\nTime-travel debugging\nNavigate through conversation history and fork from any point\nState inspection\nState inspection\nView and modify agent state at any point during execution\nHuman-in-the-loop\nHuman-in-the-loop\nBuilt-in support for reviewing and responding to agent requests\nQuick start\nThe fastest way to get started is using the hosted version:- Visit Agent Chat UI\n- Connect your agent by entering your deployment URL or local server address\n- Start chatting - the UI will automatically detect and render tool calls and interrupts\nLocal development\nFor customization or local development, you can run Agent Chat UI locally:Connect to your agent\nAgent Chat UI can connect to both local and deployed agents. After starting Agent Chat UI, you\u2019ll need to configure it to connect to your agent:- Graph ID: Enter your graph name (find this under\ngraphs\nin yourlanggraph.json\nfile) - Deployment URL: Your LangGraph server\u2019s endpoint (e.g.,\nhttp://localhost:2024\nfor local development, or your deployed agent\u2019s URL) - LangSmith API key (optional): Add your LangSmith API key (not required if you\u2019re using a local LangGraph server)", "tokens": 324, "node_type": "child"}
{"id": 359, "chunk_id": "c16acf8a553ac24f5d77e7d3a1c6e493", "parent_id": 338, "url": "", "namespace": "langchain", "title": "oss-javascript-langgraph-use-functional-api", "headers": ["oss-javascript-langgraph-use-functional-api"], "section_index": 0, "chunk_index": 0, "text": "# oss-javascript-langgraph-use-functional-api\n\n> Source: https://docs.langchain.com/oss/javascript/langgraph/use-functional-api\n\nCreating a simple workflow\nWhen defining anentrypoint\n, input is restricted to the first argument of the function. To pass multiple inputs, you can use a dictionary.\nExtended example: simple workflow\nExtended example: simple workflow\nExtended example: Compose an essay with an LLM\nExtended example: Compose an essay with an LLM\n@task\nand @entrypoint\ndecorators\nsyntactically. Given that a checkpointer is provided, the workflow results will\nbe persisted in the checkpointer.Parallel execution\nTasks can be executed in parallel by invoking them concurrently and waiting for the results. This is useful for improving performance in IO bound tasks (e.g., calling APIs for LLMs).Extended example: parallel LLM calls\nExtended example: parallel LLM calls\n@task\n. Each call generates a paragraph on a different topic, and results are joined into a single text output.Calling graphs\nThe Functional API and the Graph API can be used together in the same application as they share the same underlying runtime.Extended example: calling a simple graph from the functional API\nExtended example: calling a simple graph from the functional API\nCall other entrypoints\nYou can call other entrypoints from within an entrypoint or a task.Extended example: calling another entrypoint\nExtended example: calling another entrypoint\nStreaming\nThe Functional API uses the same streaming mechanism as the Graph API. Please read the streaming guide section for more details. Example of using the streaming API to stream both updates and custom data.- Emit custom data before computation begins.\n- Emit another custom message after computing the result.\n- Use\n.stream()\nto process streamed output. - Specify which streaming modes to use.\nRetry policy\nCaching Tasks\nttl\nis specified in seconds. The cache will be invalidated after this time.\nResuming after an error\nslowTask\nas its result is already saved in the checkpoint.\nHuman-in-the-loop\nThe functional API supports human-in-the-loop workflows using theinterrupt\nfunction and the Command\nprimitive.\nBasic human-in-the-loop workflow\nWe will create three tasks:- Append\n\"bar\"\n. - Pause for human input. When resuming, append human input.\n- Append\n\"qux\"\n.\nstep_1\n\u2014 are persisted, so that they are not run again following the interrupt\n.\nLet\u2019s send in a query string:\ninterrupt\nafter step_1\n. The interrupt provides instructions to resume the run. To resume, we issue a Command containing the data expected by the human_feedback\ntask.\nReview tool calls\nTo review tool calls before execution, we add areview_tool_call\nfunction that calls interrupt\n. When this function is called, execution will be paused until we issue a command to resume it.\nGiven a tool call, our function will interrupt\nfor human review. At that point we can either:\n- Accept the tool call\n- Revise the tool call and continue\n- Generate a custom tool message (e.g., instructing the model to re-format its tool call)\nToolMessage\n] supplied by the human. The results of prior tasks \u2014 in this case the initial model call \u2014 are persisted, so that they are not run again following the interrupt\n.\nShort-term memory\nShort-term memory allows storing information across different invocations of the same thread id. See short-term memory for more details.Manage checkpoints\nYou can view and delete the information stored by the checkpointer.View thread state\nView the history of the thread\nDecouple return value from saved value\nUseentrypoint.final\nto decouple what is returned to the caller from what is persisted in the checkpoint. This is useful when:\n- You want to return a computed result (e.g., a summary or status), but save a different internal value for use on the next invocation.\n- You need to control what gets passed to the previous parameter on the next run.\nChatbot example\nAn example of a simple chatbot using the functional API and theInMemorySaver\ncheckpointer.\nThe bot is able to remember the previous conversation and continue from where it left off.\nLong-term memory\nlong-term memory allows storing information across different thread ids. This could be useful for learning information about a given user in one conversation and using it in another.Workflows\n- Workflows and agent guide for more examples of how to build workflows using the Functional API.\nIntegrate with other libraries\n- Add LangGraph\u2019s features to other frameworks using the functional API: Add LangGraph features like persistence, memory and streaming to other agent frameworks that do not provide them out of the box.", "tokens": 716, "node_type": "child"}
{"id": 360, "chunk_id": "c16acf8a553ac24f5d77e7d3a1c6e493", "parent_id": 339, "url": "", "namespace": "langchain", "title": "oss-javascript-langgraph-use-graph-api", "headers": ["oss-javascript-langgraph-use-graph-api"], "section_index": 0, "chunk_index": 0, "text": "# oss-javascript-langgraph-use-graph-api > Source: https://docs.langchain.com/oss/javascript/langgraph/use-graph-api Setup Installlanggraph : Define and update state Here we show how to define and update state in LangGraph. We will demonstrate:- How to use state to define a graph\u2019s schema - How to use reducers to control how state updates are processed. Define state State in LangGraph can be defined using Zod schemas. Below we will use Zod. See this section for detail on using alternative approaches. By default, graphs will have the same input and output schema, and the state determines that schema. See this section for how to define distinct input and output schemas. Let\u2019s consider a simple example using messages. This represents a versatile formulation of state for many LLM applications. See our concepts page for more detail.Update state Let\u2019s build an example graph with a single node. Our node is just a TypeScript function that reads our graph\u2019s state and makes updates to it. The first argument to this function will always be the state:- We kicked off invocation by updating a single key of the state. - We receive the entire state in the invocation result. Process state updates with reducers Each key in the state can have its own independent reducer function, which controls how updates from nodes are applied. If no reducer function is explicitly specified then it is assumed that all updates to the key should override it. For Zod state schemas, we can define reducers by using the special.langgraph.reducer() method on the schema field. In the earlier example, our node updated the \"messages\" key in the state by appending a message to it. Below, we add a reducer to this key, such that updates are automatically appended: MessagesState In practice, there are additional considerations for updating lists of messages:- We may wish to update an existing message in the state. - We may want to accept short-hands for message formats, such as OpenAI format. MessagesZodMeta that handles these considerations: MessagesZodMeta for convenience, so that we can have: Define input and output schemas By default,StateGraph operates with a single schema, and all nodes are expected to communicate using that schema. However, it\u2019s also possible to define distinct input and output schemas for a graph. When distinct schemas are specified, an internal schema will still be used for communication between nodes. The input schema ensures that the provided input matches the expected structure, while the output schema filters the internal data to return only the relevant information according to the defined output schema. Below, we\u2019ll see how to define distinct input and output schema. Pass private state between nodes In some cases, you may want nodes to exchange information that is crucial for intermediate logic but doesn\u2019t need to be part of the main schema of the graph. This private data is not relevant to the overall input/output of the graph and should only be shared between certain nodes. Below, we\u2019ll create an example sequential graph consisting of three nodes (node_1, node_2 and node_3), where private data is passed between the first two steps (node_1 and node_2), while the third step (node_3) only has access to the public overall state.Alternative state definitions While Zod schemas are the recommended approach, LangGraph also supports other ways to define state schemas:Add runtime configuration Sometimes you want to be able to configure your graph when calling it. For example, you might want to be able to specify what LLM or system prompt to use at runtime, without polluting the graph state with these parameters. To add runtime configuration:- Specify a schema for your configuration - Add the configuration to the function signature for nodes or conditional edges - Pass the configuration into the graph. Extended example: specifying LLM at runtime Extended example: specifying LLM at runtime Extended example: specifying model and system message at runtime Extended example: specifying model and system message at runtime Add retry policies There are many use cases where you may wish for your node to have a custom retry policy, for example if you are calling an API, querying a database, or calling an LLM, etc. LangGraph lets you add retry policies to nodes. To configure a retry policy, pass theretryPolicy parameter to the addNode. The retryPolicy parameter takes in a RetryPolicy object. Below we instantiate a RetryPolicy object with the default parameters and associate it with a node: TypeError SyntaxError ReferenceError Extended example: customizing retry policies Extended example: customizing retry policies Create a sequence of steps Here we demonstrate how to construct a simple sequence of steps. We will show:- How to build a sequential graph - Built-in short-hand for constructing similar graphs. .addNode and .addEdge methods of our graph: Why split application steps into a sequence with LangGraph? Why split application steps into a sequence with LangGraph? - How state updates are checkpointed - How interruptions are resumed in human-in-the-loop workflows - How we can \u201crewind\u201d and branch-off executions using LangGraph\u2019s time travel features - Populate a value in a key of the state - Update the same value - Populate a different value .addNode :.addEdge takes the names of nodes, which for functions defaults tonode.name .- We must specify the entry point of the graph. For this we add an edge with the START node. - The graph halts when there are no more nodes to execute. - We kicked off invocation by providing a value for a single state key. We must always provide a value for at least one key. - The value we passed in was overwritten by the first node. - The second node updated the value. - The third node populated a different value. Create branches Parallel execution of nodes is essential to speed up overall graph operation. LangGraph offers native support for parallel execution of nodes, which can significantly enhance the performance of graph-based workflows. This parallelization is achieved through fan-out and fan-in mechanisms, utilizing both standard edges and conditional_edges. Below are some examples showing how to add create branching dataflows that work for you.Run graph nodes in", "tokens": 1000, "node_type": "child"}
{"id": 361, "chunk_id": "9b0fd3b850698317bef57fa6e71a38ba", "parent_id": 339, "url": "", "namespace": "langchain", "title": "oss-javascript-langgraph-use-graph-api", "headers": ["oss-javascript-langgraph-use-graph-api"], "section_index": 0, "chunk_index": 1, "text": "tonode.name .- We must specify the entry point of the graph. For this we add an edge with the START node. - The graph halts when there are no more nodes to execute. - We kicked off invocation by providing a value for a single state key. We must always provide a value for at least one key. - The value we passed in was overwritten by the first node. - The second node updated the value. - The third node populated a different value. Create branches Parallel execution of nodes is essential to speed up overall graph operation. LangGraph offers native support for parallel execution of nodes, which can significantly enhance the performance of graph-based workflows. This parallelization is achieved through fan-out and fan-in mechanisms, utilizing both standard edges and conditional_edges. Below are some examples showing how to add create branching dataflows that work for you.Run graph nodes in parallel In this example, we fan out fromNode A to B and C and then fan in to D . With our state, we specify the reducer add operation. This will combine or accumulate values for the specific key in the State, rather than simply overwriting the existing value. For lists, this means concatenating the new list with the existing list. See the above section on state reducers for more detail on updating state with reducers. \"b\" and \"c\" are executed concurrently in the same superstep. Because they are in the same step, node \"d\" executes after both \"b\" and \"c\" are finished.Importantly, updates from a parallel superstep may not be ordered consistently. If you need a consistent, predetermined ordering of updates from a parallel superstep, you should write the outputs to a separate field in the state together with a value with which to order them.Exception handling? Exception handling? - You can write regular python code within your node to catch and handle exceptions. - You can set a retry_policy to direct the graph to retry nodes that raise certain types of exceptions. Only failing branches are retried, so you needn\u2019t worry about performing redundant work. Conditional branching If your fan-out should vary at runtime based on the state, you can use addConditionalEdges to select one or more paths using the graph state. See example below, where nodea generates a state update that determines the following node. Map-Reduce and the Send API LangGraph supports map-reduce and other advanced branching patterns using the Send API. Here is an example of how to use it:Create and control loops When creating a graph with a loop, we require a mechanism for terminating execution. This is most commonly done by adding a conditional edge that routes to the END node once we reach some termination condition. You can also set the graph recursion limit when invoking or streaming the graph. The recursion limit sets the number of supersteps that the graph is allowed to execute before it raises an error. Read more about the concept of recursion limits here. Let\u2019s consider a simple graph with a loop to better understand how these mechanisms work.\"recursionLimit\" in the config. This will raise a GraphRecursionError , which you can catch and handle: \"a\" is a tool-calling model, and node \"b\" represents the tools. In our route conditional edge, we specify that we should end after the \"aggregate\" list in the state passes a threshold length. Invoking the graph, we see that we alternate between nodes \"a\" and \"b\" before terminating once we reach the termination condition. Impose a recursion limit In some applications, we may not have a guarantee that we will reach a given termination condition. In these cases, we can set the graph\u2019s recursion limit. This will raise aGraphRecursionError after a given number of supersteps. We can then catch and handle this exception: Combine control flow and state updates with Command It can be useful to combine control flow (edges) and state updates (nodes). For example, you might want to BOTH perform state updates AND decide which node to go to next in the SAME node. LangGraph provides a way to do so by returning a Command object from node functions: StateGraph with the above nodes. Notice that the graph doesn\u2019t have conditional edges for routing! This is because control flow is defined with Command inside nodeA . ends to specify which nodes nodeA can navigate to. This is necessary for the graph rendering and tells LangGraph that nodeA can navigate to nodeB and nodeC .Navigate to a node in a parent graph If you are using subgraphs, you might want to navigate from a node within a subgraph to a different subgraph (i.e. a different node in the parent graph). To do so, you can specifygraph=Command.PARENT in Command : nodeA in the above example into a single-node graph that we\u2019ll add as a subgraph to our parent graph. Command.PARENT When you send updates from a subgraph node to a parent graph node for a key that\u2019s shared by both parent and subgraph state schemas, you must define a reducer for the key you\u2019re updating in the parent graph state. See the example below.Use inside tools A common use case is updating graph state from inside a tool. For example, in a customer support application you might want to look up customer information based on their account number or ID in the beginning of the conversation. To update the graph state from the tool, you can returnCommand(update={\"my_custom_key\": \"foo\", \"messages\": [...]}) from the tool: messages (or any state key used for the message history) in Command.update when returning Command from a tool and the list of messages in messages MUST contain a ToolMessage . This is necessary for the resulting message history to be valid (LLM providers require AI messages with tool calls to be followed by the tool result messages).Command , we recommend using prebuilt ToolNode which automatically handles tools returning Command objects and propagates them to the graph state. If you\u2019re writing a custom node that calls tools, you would need to manually", "tokens": 1000, "node_type": "child"}
{"id": 362, "chunk_id": "5345104ec252a41359fa4a3d2dfbec23", "parent_id": 339, "url": "", "namespace": "langchain", "title": "oss-javascript-langgraph-use-graph-api", "headers": ["oss-javascript-langgraph-use-graph-api"], "section_index": 0, "chunk_index": 2, "text": "inside tools A common use case is updating graph state from inside a tool. For example, in a customer support application you might want to look up customer information based on their account number or ID in the beginning of the conversation. To update the graph state from the tool, you can returnCommand(update={\"my_custom_key\": \"foo\", \"messages\": [...]}) from the tool: messages (or any state key used for the message history) in Command.update when returning Command from a tool and the list of messages in messages MUST contain a ToolMessage . This is necessary for the resulting message history to be valid (LLM providers require AI messages with tool calls to be followed by the tool result messages).Command , we recommend using prebuilt ToolNode which automatically handles tools returning Command objects and propagates them to the graph state. If you\u2019re writing a custom node that calls tools, you would need to manually propagate Command objects returned by the tools as the update from the node. Visualize your graph Here we demonstrate how to visualize the graphs you create. You can visualize any arbitrary Graph, including StateGraph. Let\u2019s create a simple example graph to demonstrate visualization.Mermaid We can also convert a graph class into Mermaid syntax.PNG If preferred, we could render the Graph into a.png . This uses the Mermaid.ink API to generate the diagram.", "tokens": 222, "node_type": "child"}
{"id": 363, "chunk_id": "c16acf8a553ac24f5d77e7d3a1c6e493", "parent_id": 340, "url": "", "namespace": "langchain", "title": "oss-javascript-langgraph-use-subgraphs", "headers": ["oss-javascript-langgraph-use-subgraphs"], "section_index": 0, "chunk_index": 0, "text": "# oss-javascript-langgraph-use-subgraphs\n\n> Source: https://docs.langchain.com/oss/javascript/langgraph/use-subgraphs\n\n- Building multi-agent systems\n- Re-using a set of nodes in multiple graphs\n- Distributing development: when you want different teams to work on different parts of the graph independently, you can define each part as a subgraph, and as long as the subgraph interface (the input and output schemas) is respected, the parent graph can be built without knowing any details of the subgraph\n- Invoke a graph from a node \u2014 subgraphs are called from inside a node in the parent graph\n- Add a graph as a node \u2014 a subgraph is added directly as a node in the parent and shares state keys with the parent\nSetup\nInvoke a graph from a node\nA simple way to implement a subgraph is to invoke a graph from inside the node of another graph. In this case subgraphs can have completely different schemas from the parent graph (no shared keys). For example, you might want to keep a private message history for each of the agents in a multi-agent system. If that\u2019s the case for your application, you need to define a node function that invokes the subgraph. This function needs to transform the input (parent) state to the subgraph state before invoking the subgraph, and transform the results back to the parent state before returning the state update from the node.Full example: different state schemas\nFull example: different state schemas\n- Transform the state to the subgraph state\n- Transform response back to the parent state\nFull example: different state schemas (two levels of subgraphs)\nFull example: different state schemas (two levels of subgraphs)\n- We\u2019re transforming the state from the child state channels (\nmyChildKey\n) to the grandchild state channels (myGrandchildKey\n) - We\u2019re transforming the state from the grandchild state channels (\nmyGrandchildKey\n) back to the child state channels (myChildKey\n) - We\u2019re passing a function here instead of just compiled graph (\ngrandchildGraph\n) - We\u2019re transforming the state from the parent state channels (\nmyKey\n) to the child state channels (myChildKey\n) - We\u2019re transforming the state from the child state channels (\nmyChildKey\n) back to the parent state channels (myKey\n) - We\u2019re passing a function here instead of just a compiled graph (\nchildGraph\n)\nAdd a graph as a node\nWhen the parent graph and subgraph can communicate over a shared state key (channel) in the schema, you can add a graph as a node in another graph. For example, in multi-agent systems, the agents often communicate over a shared messages key. If your subgraph shares state keys with the parent graph, you can follow these steps to add it to your graph:- Define the subgraph workflow (\nsubgraphBuilder\nin the example below) and compile it - Pass compiled subgraph to the\n.addNode\nmethod when defining the parent graph workflow\nFull example: shared state schemas\nFull example: shared state schemas\nAdd persistence\nYou only need to provide the checkpointer when compiling the parent graph. LangGraph will automatically propagate the checkpointer to the child subgraphs.View subgraph state\nWhen you enable persistence, you can inspect the graph state (checkpoint) via the appropriate method. To view the subgraph state, you can use the subgraphs option. You can inspect the graph state viagraph.getState(config)\n. To view the subgraph state, you can use graph.getState(config, { subgraphs: true })\n.\nView interrupted subgraph state\nView interrupted subgraph state\nStream subgraph outputs\nTo include outputs from subgraphs in the streamed outputs, you can set the subgraphs option in the stream method of the parent graph. This will stream outputs from both the parent graph and any subgraphs.- Set\nsubgraphs: true\nto stream outputs from subgraphs.\nStream from subgraphs\nStream from subgraphs\n- Set\nsubgraphs: true\nto stream outputs from subgraphs.", "tokens": 632, "node_type": "child"}
{"id": 364, "chunk_id": "c16acf8a553ac24f5d77e7d3a1c6e493", "parent_id": 341, "url": "", "namespace": "langchain", "title": "oss-javascript-langgraph-use-time-travel", "headers": ["oss-javascript-langgraph-use-time-travel"], "section_index": 0, "chunk_index": 0, "text": "# oss-javascript-langgraph-use-time-travel\n\n> Source: https://docs.langchain.com/oss/javascript/langgraph/use-time-travel\n\nLangGraph v1.0Welcome to the new LangGraph documentation! If you encounter any issues or have feedback, please open an issue so we can improve. Archived v0 documentation can be found here.See the release notes and migration guide for a complete list of changes and instructions on how to upgrade your code.\n- Understand reasoning: Analyze the steps that led to a successful result.\n- Debug mistakes: Identify where and why errors occurred.\n- Explore alternatives: Test different paths to uncover better solutions.\n- Run the graph with initial inputs using\ninvoke\norstream\nmethods. - Identify a checkpoint in an existing thread: Use the\ngetStateHistory()\nmethod to retrieve the execution history for a specificthread_id\nand locate the desiredcheckpoint_id\n. Alternatively, set a breakpoint before the node(s) where you want execution to pause. You can then find the most recent checkpoint recorded up to that breakpoint. - Update the graph state (optional): Use the\nupdateState\nmethod to modify the graph\u2019s state at the checkpoint and resume execution from alternative state. - Resume execution from the checkpoint: Use the\ninvoke\norstream\nmethods with an input ofnull\nand a configuration containing the appropriatethread_id\nandcheckpoint_id\n.\nIn a workflow\nThis example builds a simple LangGraph workflow that generates a joke topic and writes a joke using an LLM. It demonstrates how to run the graph, retrieve past execution checkpoints, optionally modify the state, and resume execution from a chosen checkpoint to explore alternate outcomes.Setup\nFirst we need to install the packages required1. Run the graph\n2. Identify a checkpoint\n3. Update the state\nupdateState\nwill create a new checkpoint. The new checkpoint will be associated with the same thread, but a new checkpoint ID.", "tokens": 283, "node_type": "child"}
{"id": 365, "chunk_id": "c16acf8a553ac24f5d77e7d3a1c6e493", "parent_id": 342, "url": "", "namespace": "langchain", "title": "oss-javascript-langgraph-workflows-agents", "headers": ["oss-javascript-langgraph-workflows-agents"], "section_index": 0, "chunk_index": 0, "text": "# oss-javascript-langgraph-workflows-agents\n\n> Source: https://docs.langchain.com/oss/javascript/langgraph/workflows-agents\n\nLangGraph v1.0Welcome to the new LangGraph documentation! If you encounter any issues or have feedback, please open an issue so we can improve. Archived v0 documentation can be found here.See the release notes and migration guide for a complete list of changes and instructions on how to upgrade your code.\n- Workflows have predetermined code paths and are designed to operate in a certain order.\n- Agents are dynamic and define their own processes and tool usage.\nSetup\nTo build a workflow or agent, you can use any chat model that supports structured outputs and tool calling. The following example uses Anthropic:- Install dependencies\n- Initialize the LLM:\nLLMs and augmentations\nWorkflows and agentic systems are based on LLMs and the various augmentations you add to them. Tool calling, structured outputs, and short term memory are a few options for tailoring LLMs to your needs.Prompt chaining\nPrompt chaining is when each LLM call processes the output of the previous call. It\u2019s often used for performing well-defined tasks that can be broken down into smaller, verifiable steps. Some examples include:- Translating documents into different languages\n- Verifying generated content for consistency\nParallelization\nWith parallelization, LLMs work simultaneously on a task. This is either done by running multiple independent subtasks at the same time, or running the same task multiple times to check for different outputs. Parallelization is commonly used to:- Split up subtasks and run them in parallel, which increases speed\n- Run tasks multiple times to check for different outputs, which increases confidence\n- Running one subtask that processes a document for keywords, and a second subtask to check for formatting errors\n- Running a task multiple times that scores a document for accuracy based on different criteria, like the number of citations, the number of sources used, and the quality of the sources\nRouting\nRouting workflows process inputs and then directs them to context-specific tasks. This allows you to define specialized flows for complex tasks. For example, a workflow built to answer product related questions might process the type of question first, and then route the request to specific processes for pricing, refunds, returns, etc.Orchestrator-worker\nIn an orchestrator-worker configuration, the orchestrator:- Breaks down tasks into subtasks\n- Delegates subtasks to workers\n- Synthesizes worker outputs into a final result\nCreating workers in LangGraph\nOrchestrator-worker workflows are common and LangGraph has built-in support for them. TheSend\nAPI lets you dynamically create worker nodes and send them specific inputs. Each worker has its own state, and all worker outputs are written to a shared state key that is accessible to the orchestrator graph. This gives the orchestrator access to all worker output and allows it to synthesize them into a final output. The example below iterates over a list of sections and uses the Send\nAPI to send a section to each worker.\nEvaluator-optimizer\nIn evaluator-optimizer workflows, one LLM call creates a response and the other evaluates that response. If the evaluator or a human-in-the-loop determines the response needs refinement, feedback is provided and the response is recreated. This loop continues until an acceptable response is generated. Evaluator-optimizer workflows are commonly used when there\u2019s particular success criteria for a task, but iteration is required to meet that criteria. For example, there\u2019s not always a perfect match when translating text between two languages. It might take a few iterations to generate a translation with the same meaning across the two languages.Agents\nAgents are typically implemented as an LLM performing actions using tools. They operate in continuous feedback loops, and are used in situations where problems and solutions are unpredictable. Agents have more autonomy than workflows, and can make decisions about the tools they use and how to solve problems. You can still define the available toolset and guidelines for how agents behave.Using tools", "tokens": 636, "node_type": "child"}
{"id": 366, "chunk_id": "c16acf8a553ac24f5d77e7d3a1c6e493", "parent_id": 343, "url": "", "namespace": "langchain", "title": "oss-javascript-learn", "headers": ["oss-javascript-learn"], "section_index": 0, "chunk_index": 0, "text": "# oss-javascript-learn\n\n> Source: https://docs.langchain.com/oss/javascript/learn\n\nLangGraph v1.0Welcome to the new LangGraph documentation! If you encounter any issues or have feedback, please open an issue so we can improve. Archived v0 documentation can be found here.See the release notes and migration guide for a complete list of changes and instructions on how to upgrade your code.\nUse Cases\nBelow are tutorials for common use cases, organized by framework.LangChain\nLangChain agent implementations make it easy to get started for most use cases.Semantic Search\nBuild a semantic search engine over a PDF with LangChain components.\nRAG Agent\nCreate a Retrieval Augmented Generation (RAG) agent.\nSQL Agent\nBuild a SQL agent to interact with databases with human-in-the-loop review.\nLangGraph\nLangChain\u2019s agent implementations use LangGraph primitives. If deeper customization is required, agents can be implemented directly in LangGraph.Custom RAG Agent\nBuild a RAG agent using LangGraph primitives for fine-grained control.\nConceptual Overviews\nThese guides explain the core concepts and APIs underlying LangChain and LangGraph.Memory\nUnderstand persistence of interactions within and across threads.\nContext engineering\nLearn methods for providing AI applications the right information and tools to accomplish a task.\nGraph API\nExplore LangGraph\u2019s declarative graph-building API.\nFunctional API\nBuild agents as a single function.\nAdditional Resources\nLangChain Academy\nCourses and exercises to level up your LangChain skills.\nCase Studies\nSee how teams are using LangChain and LangGraph in production.", "tokens": 224, "node_type": "child"}
{"id": 367, "chunk_id": "c16acf8a553ac24f5d77e7d3a1c6e493", "parent_id": 344, "url": "", "namespace": "langchain", "title": "oss-javascript-migrate-langchain-v1", "headers": ["oss-javascript-migrate-langchain-v1"], "section_index": 0, "chunk_index": 0, "text": "# oss-javascript-migrate-langchain-v1 > Source: https://docs.langchain.com/oss/javascript/migrate/langchain-v1 createAgent In v1, the react agent prebuilt is now in the langchain package. The table below outlines what functionality has changed: | Section | What changed | |---|---| | Import path | Package moved from @langchain/langgraph/prebuilts to langchain | | Prompts | Parameter renamed to systemPrompt , dynamic prompts use middleware | | Pre-model hook | Replaced by middleware with beforeModel method | | Post-model hook | Replaced by middleware with afterModel method | | Custom state | Defined in middleware, zod objects only | | Model | Dynamic selection via middleware, pre-bound models not supported | | Tools | Tool error handling moved to middleware with wrapToolCall | | Structured output | prompted output removed, use toolStrategy /providerStrategy | | Streaming node name | Node name changed from \"agent\" to \"model\" | | Runtime context | context property instead of config.configurable | | Namespace | Streamlined to focus on agent building blocks, legacy code moved to @langchain/classic | Import path The import path for the react agent prebuilt has changed from@langchain/langgraph/prebuilts to langchain . The name of the function has changed from createReactAgent to createAgent : Prompts Static prompt rename Theprompt parameter has been renamed to systemPrompt : SystemMessage If using SystemMessage objects in the system prompt, the string content is now used directly: Dynamic prompts Dynamic prompts are a core context engineering pattern\u2014 they adapt what you tell the model based on the current conversation state. To do this, usedynamicSystemPromptMiddleware : Pre-model hook Pre-model hooks are now implemented as middleware with thebeforeModel method. This pattern is more extensible\u2014you can define multiple middlewares to run before the model is called and reuse them across agents. Common use cases include: - Summarizing conversation history - Trimming messages - Input guardrails, like PII redaction Post-model hook Post-model hooks are now implemented as middleware with theafterModel method. This lets you compose multiple handlers after the model responds. Common use cases include: - Human-in-the-loop approval - Output guardrails Custom state Custom state is now defined in middleware using thestateSchema property. Use Zod to declare additional state fields that are carried through the agent run. Model Dynamic model selection now happens via middleware. UsewrapModelCall to swap models (and tools) based on state or runtime context. In createReactAgent , this was done via a function passed to the model parameter. This functionality has been ported to the middleware interface in v1. Dynamic model selection Pre-bound models To better support structured output,createAgent should receive a plain model (string or instance) and a separate tools list. Avoid passing models pre-bound with tools when using structured output. Tools Thetools argument to createAgent accepts: - Functions created with tool - LangChain tool instances - Objects that represent built-in provider tools wrapToolCall to centralize error handling and logging for tools. Structured output Node changes Structured output used to be generated in a separate node from the main agent. This is no longer the case. Structured output is generated in the main loop (no extra LLM call), reducing cost and latency.Tool and provider strategies In v1, there are two strategies:toolStrategy uses artificial tool calling to generate structured outputproviderStrategy uses provider-native structured output generation Prompted output removed Prompted output via custom instructions inresponseFormat is removed in favor of the above strategies. Streaming node name rename When streaming events from agents, the node name was changed from\"agent\" to \"model\" to better reflect the node\u2019s purpose. Runtime context When invoking an agent, pass static, read-only configuration via thecontext config argument. This replaces patterns that used config.configurable . The old config.configurable pattern still works for backward compatibility, but using the new context parameter is recommended for new applications or applications migrating to v1.Standard content In v1, messages gain provider-agnostic standard content blocks. Access them viamessage.contentBlocks for a consistent, typed view across providers. The existing message.content field remains unchanged for strings or provider-native structures. What changed - New contentBlocks property on messages for normalized content. - New TypeScript types under ContentBlock for strong typing. - Optional serialization of standard blocks into content viaLC_OUTPUT_VERSION=v1 oroutputVersion: \"v1\" . Read standardized content Create multimodal messages Example block types Serialize standard content Standard content blocks are not serialized into thecontent attribute by default. If you need to access standard content blocks in the content attribute (e.g., when sending messages to a client), you can opt-in to serializing them into content . Simplified package Thelangchain package namespace is streamlined to focus on agent building blocks. Legacy functionality has moved to @langchain/classic . The new package exposes only the most useful and relevant functionality. Exports The v1 package includes:| Module | What\u2019s available | Notes | |---|---|---| | Agents | createAgent , AgentState | Core agent creation functionality | | Messages | Message types, content blocks, trimMessages | Re-exported from @langchain/core | | Tools | tool , tool classes | Re-exported from @langchain/core | | Chat models | initChatModel , BaseChatModel | Unified model initialization | @langchain/classic If you use legacy chains, the indexing API, or functionality previously re-exported from @langchain/community , install @langchain/classic and update imports: Breaking changes Dropped Node 18 support All LangChain packages now require Node.js 20 or higher. Node.js 18 reached end of life in March 2025.New build outputs Builds for all langchain packages now use a bundler based approach instead of using raw typescript outputs. If you were importing files from thedist/ directory (which is not recommended), you will need to update your imports to use the new module system. Legacy code moved to @langchain/classic Legacy functionality outside the focus of standard interfaces and agents has been moved to the @langchain/classic package. See the Simplified package section for details on what\u2019s available in the core langchain package and what moved to @langchain/classic . Removal of deprecated APIs Methods, functions, and other objects that were already deprecated and slated for removal in 1.0 have been deleted.View removed deprecated APIs View removed deprecated APIs The following deprecated APIs have been removed in v1: Core functionality TraceGroup - Use LangSmith tracing insteadBaseDocumentLoader.loadAndSplit - Use.load() followed", "tokens": 1000, "node_type": "child"}
{"id": 368, "chunk_id": "9b0fd3b850698317bef57fa6e71a38ba", "parent_id": 344, "url": "", "namespace": "langchain", "title": "oss-javascript-migrate-langchain-v1", "headers": ["oss-javascript-migrate-langchain-v1"], "section_index": 0, "chunk_index": 1, "text": "require Node.js 20 or higher. Node.js 18 reached end of life in March 2025.New build outputs Builds for all langchain packages now use a bundler based approach instead of using raw typescript outputs. If you were importing files from thedist/ directory (which is not recommended), you will need to update your imports to use the new module system. Legacy code moved to @langchain/classic Legacy functionality outside the focus of standard interfaces and agents has been moved to the @langchain/classic package. See the Simplified package section for details on what\u2019s available in the core langchain package and what moved to @langchain/classic . Removal of deprecated APIs Methods, functions, and other objects that were already deprecated and slated for removal in 1.0 have been deleted.View removed deprecated APIs View removed deprecated APIs The following deprecated APIs have been removed in v1: Core functionality TraceGroup - Use LangSmith tracing insteadBaseDocumentLoader.loadAndSplit - Use.load() followed by a text splitterRemoteRunnable - No longer supported Prompts BasePromptTemplate.serialize and.deserialize - Use JSON serialization directlyChatPromptTemplate.fromPromptMessages - UseChatPromptTemplate.fromMessages Retrievers BaseRetrieverInterface.getRelevantDocuments - Use.invoke() instead Runnables Runnable.bind - Use.bindTools() or other specific binding methodsRunnable.map - Use.batch() insteadRunnableBatchOptions.maxConcurrency - UsemaxConcurrency in the config object Chat models BaseChatModel.predictMessages - Use.invoke() insteadBaseChatModel.predict - Use.invoke() insteadBaseChatModel.serialize - Use JSON serialization directlyBaseChatModel.callPrompt - Use.invoke() insteadBaseChatModel.call - Use.invoke() instead LLMs BaseLLMParams.concurrency - UsemaxConcurrency in the config objectBaseLLM.call - Use.invoke() insteadBaseLLM.predict - Use.invoke() insteadBaseLLM.predictMessages - Use.invoke() insteadBaseLLM.serialize - Use JSON serialization directly Streaming createChatMessageChunkEncoderStream - Use.stream() method directly Tracing BaseTracer.runMap - Use LangSmith tracing APIsgetTracingCallbackHandler - Use LangSmith tracinggetTracingV2CallbackHandler - Use LangSmith tracingLangChainTracerV1 - Use LangSmith tracing Memory and storage BaseListChatMessageHistory.addAIChatMessage - Use.addMessage() withAIMessage BaseStoreInterface - Use specific store implementations Utilities getRuntimeEnvironmentSync - Use asyncgetRuntimeEnvironment()", "tokens": 276, "node_type": "child"}
{"id": 369, "chunk_id": "c16acf8a553ac24f5d77e7d3a1c6e493", "parent_id": 345, "url": "", "namespace": "langchain", "title": "oss-javascript-migrate-langgraph-v1", "headers": ["oss-javascript-migrate-langgraph-v1"], "section_index": 0, "chunk_index": 0, "text": "# oss-javascript-migrate-langgraph-v1\n\n> Source: https://docs.langchain.com/oss/javascript/migrate/langgraph-v1\n\nSummary of changes\n| Area | What changed |\n|---|---|\n| React prebuilt | createReactAgent deprecated; use LangChain createAgent |\n| Interrupts | Typed interrupts supported via interrupts config |\ntoLangGraphEventStream removed | Use graph.stream with the desired encoding format |\nuseStream | Supports custom transports |\nDeprecation: createReactAgent\n\u2192 createAgent\nLangGraph v1 deprecates the createReactAgent\nprebuilt. Use LangChain\u2019s createAgent\n, which runs on LangGraph and adds a flexible middleware system.\nSee the LangChain v1 docs for details:\nTyped interrupts\nYou can now define interrupt types at graph construction to strictly type the values passed to and received from interrupts.Event stream encoding\nThe low-leveltoLangGraphEventStream\nhelper is removed. Streaming responses are handled by the SDK; when using low-level clients, select the wire format via an encoding\noption passed to graph.stream\n.\nBreaking changes\nDropped Node 18 support\nAll LangGraph packages now require Node.js 20 or higher. Node.js 18 reached end of life in March 2025.New build outputs\nBuilds for all langgraph packages now use a bundler based approach instead of using raw typescript outputs. If you were importing files from thedist/\ndirectory (which is not recommended), you will need to update your imports to use the new module system.", "tokens": 203, "node_type": "child"}
{"id": 370, "chunk_id": "c16acf8a553ac24f5d77e7d3a1c6e493", "parent_id": 347, "url": "", "namespace": "langchain", "title": "oss-javascript-release-policy", "headers": ["oss-javascript-release-policy"], "section_index": 0, "chunk_index": 0, "text": "# oss-javascript-release-policy\n\n> Source: https://docs.langchain.com/oss/javascript/release-policy\n\nLangChain v1.0Welcome to the new LangChain documentation! If you encounter any issues or have feedback, please open an issue so we can improve. Archived v0 documentation can be found here.See the release notes and migration guide for a complete list of changes and instructions on how to upgrade your code.\n- LangChain\n- LangGraph\nThe LangChain ecosystem is composed of different component packages (e.g.,\n@langchain/core\n, langchain\n, @langchain/community\n, partner packages, etc.)Release cadence\nWe expect to space out minor releases (e.g., from0.2.x\nto 0.3.0\n) of langchain\nand @langchain/core\nby at least 2-3 months, as such releases may contain breaking changes.Patch versions are released frequently, up to a few times per week, as they contain bug fixes and new features.API stability\nThe development of LLM applications is a rapidly evolving field, and we are constantly learning from our users and the community. As such, we expect that the APIs inlangchain\nand @langchain/core\nwill continue to evolve to better serve the needs of our users.- Breaking changes to the public API will result in a minor version bump (the second digit)\n- Any bug fixes or new features will result in a patch version bump (the third digit)\nStability of other packages\nThe stability of other packages in the LangChain ecosystem may vary:-\n@langchain/community\nis a community maintained package that contains 3rd party integrations. While we do our best to review and test changes in@langchain/community\n,@langchain/community\nis expected to experience more breaking changes thanlangchain\nand@langchain/core\nas it contains many community contributions. - Partner packages may follow different stability and versioning policies, and users should refer to the documentation of those packages for more information; however, in general these packages are expected to be stable.\nDeprecation policy\nWe will generally avoid deprecating features until a better alternative is available.When a feature is deprecated, it will continue to work in the current and next minor version oflangchain\nand @langchain/core\n. After that, the feature will be removed.Since we\u2019re expecting to space out minor releases by at least 2-3 months, this means that a feature can be removed within 2-6 months of being deprecated.In some situations, we may allow the feature to remain in the code base for longer periods of time, if it\u2019s not causing issues in the packages, to reduce the burden on users.", "tokens": 388, "node_type": "child"}
{"id": 371, "chunk_id": "c16acf8a553ac24f5d77e7d3a1c6e493", "parent_id": 348, "url": "", "namespace": "langchain", "title": "oss-javascript-releases-langchain-v1", "headers": ["oss-javascript-releases-langchain-v1"], "section_index": 0, "chunk_index": 0, "text": "# oss-javascript-releases-langchain-v1\n\n> Source: https://docs.langchain.com/oss/javascript/releases/langchain-v1\n\ncreateAgent\nA new standard way to build agents in LangChain, replacing\ncreateReactAgent\nfrom LangGraph with a cleaner, more powerful API.Standard content blocks\nA new\ncontentBlocks\nproperty that provides unified access to modern LLM features across all providers.Simplified package\nThe\nlangchain\npackage has been streamlined to focus on essential building blocks for agents, with legacy functionality moved to @langchain/classic\n.createAgent\ncreateAgent\nis the standard way to build agents in LangChain 1.0. It provides a simpler interface than the prebuilt createReactAgent\nexported from LangGraph while offering greater customization potential by using middleware.\ncreateAgent\nis built on the basic agent loop \u2014 calling a model, letting it choose tools to execute, and then finishing when it calls no more tools:\nFor more information, see Agents.\nMiddleware\nMiddleware is the defining feature ofcreateAgent\n. It makes createAgent\nhighly customizable, raising the ceiling for what you can build.\nGreat agents require context engineering: getting the right information to the model at the right time. Middleware helps you control dynamic prompts, conversation summarization, selective tool access, state management, and guardrails through a composable abstraction.\nPrebuilt middleware\nLangChain provides a few prebuilt middlewares for common patterns, including:summarizationMiddleware\n: Condense conversation history when it gets too longhumanInTheLoopMiddleware\n: Require approval for sensitive tool callspiiRedactionMiddleware\n: Redact sensitive information before sending to the model\nCustom middleware\nYou can also build custom middleware to fit your specific needs. Build custom middleware by implementing any of these hooks using thecreateMiddleware\nfunction:\n| Hook | When it runs | Use cases |\n|---|---|---|\nbeforeAgent | Before calling the agent | Load memory, validate input |\nbeforeModel | Before each LLM call | Update prompts, trim messages |\nwrapModelCall | Around each LLM call | Intercept and modify requests/responses |\nwrapToolCall | Around each tool call | Intercept and modify tool execution |\nafterModel | After each LLM response | Validate output, apply guardrails |\nafterAgent | After agent completes | Save results, cleanup |\nBuilt on LangGraph\nBecausecreateAgent\nis built on LangGraph, you automatically get built in support for long running and reliable agents via:\nPersistence\nConversations automatically persist across sessions with built-in checkpointing\nStreaming\nStream tokens, tool calls, and reasoning traces in real-time\nHuman-in-the-loop\nPause agent execution for human approval before sensitive actions\nTime travel\nRewind conversations to any point and explore alternate paths and prompts\nStructured output\ncreateAgent\nhas improved structured output generation:\n- Main loop integration: Structured output is now generated in the main loop instead of requiring an additional LLM call\n- Structured output strategy: Models can choose between calling tools or using provider-side structured output generation\n- Cost reduction: Eliminates extra expense from additional LLM calls\nhandleErrors\nparameter to ToolStrategy\n:\n- Parsing errors: Model generates data that doesn\u2019t match desired structure\n- Multiple tool calls: Model generates 2+ tool calls for structured output schemas\nStandard content blocks\n1.0 releases are available for most packages. Only the following currently support new content blocks:\nlangchain\n@langchain/core\n@langchain/anthropic\n@langchain/openai\nBenefits\n- Provider agnostic: Access reasoning traces, citations, built-in tools (web search, code interpreters, etc.), and other features using the same API regardless of provider\n- Type safe: Full type hints for all content block types\n- Backward compatible: Standard content can be loaded lazily, so there are no associated breaking changes\nSimplified package\nLangChain v1 streamlines thelangchain\npackage namespace to focus on essential building blocks for agents. The package exposes only the most useful and relevant functionality:\nMost of these are re-exported from @langchain/core\nfor convenience, which gives you a focused API surface for building agents.\n@langchain/classic\nLegacy functionality has moved to @langchain/classic\nto keep the core package lean and focused.\nWhat\u2019s in @langchain/classic\n- Legacy chains and chain implementations\n- The indexing API\n@langchain/community\nexports- Other deprecated functionality\n@langchain/classic\n:\nReporting issues\nPlease report any issues discovered with 1.0 on GitHub using the'v1'\nlabel.\nAdditional resources\nLangChain 1.0\nRead the announcement\nMiddleware Guide\nDeep dive into middleware\nAgents Documentation\nFull agent documentation\nMessage Content\nNew content blocks API\nMigration guide\nHow to migrate to LangChain v1\nGitHub\nReport issues or contribute\nSee also\n- Versioning - Understanding version numbers\n- Release policy - Detailed release policies", "tokens": 693, "node_type": "child"}
{"id": 372, "chunk_id": "c16acf8a553ac24f5d77e7d3a1c6e493", "parent_id": 349, "url": "", "namespace": "langchain", "title": "oss-javascript-releases-langgraph-v1", "headers": ["oss-javascript-releases-langgraph-v1"], "section_index": 0, "chunk_index": 0, "text": "# oss-javascript-releases-langgraph-v1\n\n> Source: https://docs.langchain.com/oss/javascript/releases/langgraph-v1\n\ncreateAgent\nis built on LangGraph) so you can start high-level and drop down to granular control when needed.\nStable core APIs\nGraph primitives (state, nodes, edges) and the execution/runtime model are unchanged, making upgrades straightforward.\nReliability, by default\nDurable execution with checkpointing, persistence, streaming, and human-in-the-loop continues to be first-class.\nSeamless with LangChain v1\nLangChain\u2019s\ncreateAgent\nruns on LangGraph. Use LangChain for a fast start; drop to LangGraph for custom orchestration.Deprecation of createReactAgent\nThe LangGraph createReactAgent\nprebuilt has been deprecated in favor of LangChain\u2019s createAgent\n. It provides a simpler interface, and offers greater customization potential through the introduction of middleware.\n- For information on the new\ncreateAgent\nAPI, see the LangChain v1 release notes. - For information on migrating from\ncreateReactAgent\ntocreateAgent\n, see the LangChain v1 migration guide.\nTyped interrupts\nStateGraph\nnow accepts a map of interrupt types in the constructor to more closely constrain the types of interrupts that can be used within a graph.\nFrontend SDK enhancements\nLangGraph v1 comes with a few enhancements when interacting with a LangGraph application from the frontend.Event stream encoding\nThe low-leveltoLangGraphEventStream\nhelper has been removed. Streaming responses are now handled natively by the SDK, and you can select the wire format via passing in the encoding\nformat to graph.stream\n. This makes switching between SSE and normal JSON responses straightforward without changing UI logic.\nSee the migration guide for more information.\nCustom transports in useStream\nThe React useStream\nhook now supports pluggable transports so you can have more control over the network layer without changing UI code.\nReporting issues\nPlease report any issues discovered with 1.0 on GitHub using the'v1'\nlabel.\nAdditional resources\nLangGraph 1.0\nRead the announcement\nOverview\nWhat LangGraph is and when to use it\nGraph API\nBuild graphs with state, nodes, and edges\nLangChain Agents\nHigh-level agents built on LangGraph\nMigration guide\nHow to migrate to LangGraph v1\nGitHub\nReport issues or contribute\nSee also\n- Versioning - Understanding version numbers\n- Release policy - Detailed release policies", "tokens": 336, "node_type": "child"}
{"id": 373, "chunk_id": "c16acf8a553ac24f5d77e7d3a1c6e493", "parent_id": 350, "url": "", "namespace": "langchain", "title": "oss-javascript-versioning", "headers": ["oss-javascript-versioning"], "section_index": 0, "chunk_index": 0, "text": "# oss-javascript-versioning\n\n> Source: https://docs.langchain.com/oss/javascript/versioning\n\nLangChain v1.0Welcome to the new LangChain documentation! If you encounter any issues or have feedback, please open an issue so we can improve. Archived v0 documentation can be found here.See the release notes and migration guide for a complete list of changes and instructions on how to upgrade your code.\nMAJOR.MINOR.PATCH\n- Major: Breaking API updates that require code changes.\n- Minor: New features and improvements that maintain backward compatibility.\n- Patch: Bug fixes and minor improvements.\nVersion numbering\nLangChain and LangGraph follow Semantic Versioning principles:1.0.0\n: First stable release with production-ready APIs1.1.0\n: New features added in a backward-compatible manner1.0.1\n: Backward-compatible bug fixes\nAPI stability\nWe communicate the stability of our APIs as follows:Stable APIs\nAll APIs without special prefixes are considered stable and ready for production use. We maintain backward compatibility for stable features and only introduce breaking changes in major releases.Beta APIs\nAPIs marked asbeta\nare feature-complete but may undergo minor changes based on user feedback. They are safe for production use but may require small adjustments in future releases.\nAlpha APIs\nAPIs marked asalpha\nare experimental and subject to significant changes. Use these with caution in production environments.\nDeprecated APIs\nAPIs marked asdeprecated\nwill be removed in future major releases. When possible, we specify the intended version of removal. To handle deprecations:\n- Switch to the recommended alternative API\n- Follow the migration guide (released alongside major releases)\n- Use automated migration tools when available\nInternal APIs\nCertain APIs are explicitly marked as \u201cinternal\u201d in a couple of ways:- Some documentation refers to internals and mentions them as such. If the documentation says that something is internal, it may change.\n- Functions, methods, and other objects prefixed by a leading underscore (\n_\n). This is the standard Python convention of indicating that something is private; if any method starts with a single_\n, it\u2019s an internal API.- Exception: Certain methods are prefixed with\n_\n, but do not contain an implementation. These methods are meant to be overridden by sub-classes that provide the implementation. Such methods are generally part of the Public API of LangChain.\n- Exception: Certain methods are prefixed with\nRelease cycles\nMajor releases\nMajor releases\nMajor releases (e.g.,\n1.0.0\n\u2192 2.0.0\n) may include:- Breaking API changes\n- Removal of deprecated features\n- Significant architectural improvements\n- Detailed migration guides\n- Automated migration tools when possible\n- Extended support period for the previous major version\nMinor releases\nMinor releases\nMinor releases (e.g.,\n1.0.0\n\u2192 1.1.0\n) include:- New features and capabilities\n- Performance improvements\n- New optional parameters\n- Backward-compatible enhancements\nPatch releases\nPatch releases\nPatch releases (e.g.,\n1.0.0\n\u2192 1.0.1\n) include:- Bug fixes\n- Security updates\n- Documentation improvements\n- Performance optimizations without API changes\nVersion support policy\n- Latest major version: Full support with active development\n- Previous major version: Security updates and critical bug fixes for 12 months after the next major release\n- Older versions: Community support only\nCheck your version\nTo check your installed version:Upgrade\nPre-release versions\nWe occasionally release alpha and beta versions for early testing:- Alpha (e.g.,\n1.0.0a1\n): Early preview, significant changes expected - Beta (e.g.,\n1.0.0b1\n): Feature-complete, minor changes possible - Release Candidate (e.g.,\n1.0.0rc1\n): Final testing before stable release\nSee also\n- Release policy - Detailed release and deprecation policies\n- Releases - Version-specific release notes and migration guides", "tokens": 566, "node_type": "child"}
{"id": 374, "chunk_id": "c16acf8a553ac24f5d77e7d3a1c6e493", "parent_id": 351, "url": "", "namespace": "langchain", "title": "oss-python-concepts-context", "headers": ["oss-python-concepts-context"], "section_index": 0, "chunk_index": 0, "text": "# oss-python-concepts-context\n\n> Source: https://docs.langchain.com/oss/python/concepts/context\n\nLangGraph v1.0Welcome to the new LangGraph documentation! If you encounter any issues or have feedback, please open an issue so we can improve. Archived v0 documentation can be found here.See the release notes and migration guide for a complete list of changes and instructions on how to upgrade your code.\n- By mutability:\n- Static context: Immutable data that doesn\u2019t change during execution (e.g., user metadata, database connections, tools)\n- Dynamic context: Mutable data that evolves as the application runs (e.g., conversation history, intermediate results, tool call observations)\n- By lifetime:\n- Runtime context: Data scoped to a single run or invocation\n- Cross-conversation context: Data that persists across multiple conversations or sessions\nRuntime context refers to local context: data and dependencies your code needs to run. It does not refer to:\n- The LLM context, which is the data passed into the LLM\u2019s prompt.\n- The \u201ccontext window\u201d, which is the maximum number of tokens that can be passed to the LLM.\n| Context type | Description | Mutability | Lifetime | Access method |\n|---|---|---|---|---|\n| Static runtime context | User metadata, tools, db connections passed at startup | Static | Single run | context argument to invoke /stream |\n| Dynamic runtime context (state) | Mutable data that evolves during a single run | Dynamic | Single run | LangGraph state object |\n| Dynamic cross-conversation context (store) | Persistent data shared across conversations | Dynamic | Cross-conversation | LangGraph store |\nStatic runtime context\nStatic runtime context represents immutable data like user metadata, tools, and database connections that are passed to an application at the start of a run via thecontext\nargument to invoke\n/stream\n. This data does not change during execution.\nThe\nRuntime\nobject can be used to access static context and other utilities like the active store and stream writer.\nSee the [Runtime][langgraph.runtime.Runtime] documentation for details.Dynamic runtime context\nDynamic runtime context represents mutable data that can evolve during a single run and is managed through the LangGraph state object. This includes conversation history, intermediate results, and values derived from tools or LLM outputs. In LangGraph, the state object acts as short-term memory during a run.- In an agent\n- In a workflow\nTurning on memory\nPlease see the memory guide for more details on how to enable memory. This is a powerful feature that allows you to persist the agent\u2019s state across multiple invocations. Otherwise, the state is scoped only to a single run.\nDynamic cross-conversation context\nDynamic cross-conversation context represents persistent, mutable data that spans across multiple conversations or sessions and is managed through the LangGraph store. This includes user profiles, preferences, and historical interactions. The LangGraph store acts as long-term memory across multiple runs. This can be used to read or update persistent facts (e.g., user profiles, preferences, prior interactions).See also\n- Memory conceptual overview\n- Short-term memory in LangChain\n- Long-term memory in LangChain\n- Memory in LangGraph", "tokens": 493, "node_type": "child"}
{"id": 375, "chunk_id": "c16acf8a553ac24f5d77e7d3a1c6e493", "parent_id": 352, "url": "", "namespace": "langchain", "title": "oss-python-concepts-memory", "headers": ["oss-python-concepts-memory"], "section_index": 0, "chunk_index": 0, "text": "# oss-python-concepts-memory > Source: https://docs.langchain.com/oss/python/concepts/memory LangGraph v1.0Welcome to the new LangGraph documentation! If you encounter any issues or have feedback, please open an issue so we can improve. Archived v0 documentation can be found here.See the release notes and migration guide for a complete list of changes and instructions on how to upgrade your code. - Short-term memory, or thread-scoped memory, tracks the ongoing conversation by maintaining message history within a session. LangGraph manages short-term memory as a part of your agent\u2019s state. State is persisted to a database using a checkpointer so the thread can be resumed at any time. Short-term memory updates when the graph is invoked or a step is completed, and the State is read at the start of each step. - Long-term memory stores user-specific or application-level data across sessions and is shared across conversational threads. It can be recalled at any time and in any thread. Memories are scoped to any custom namespace, not just within a single thread ID. LangGraph provides stores (reference doc) to let you save and recall long-term memories. Short-term memory Short-term memory lets your application remember previous interactions within a single thread or conversation. A thread organizes multiple interactions in a session, similar to the way email groups messages in a single conversation. LangGraph manages short-term memory as part of the agent\u2019s state, persisted via thread-scoped checkpoints. This state can normally include the conversation history along with other stateful data, such as uploaded files, retrieved documents, or generated artifacts. By storing these in the graph\u2019s state, the bot can access the full context for a given conversation while maintaining separation between different threads.Manage short-term memory Conversation history is the most common form of short-term memory, and long conversations pose a challenge to today\u2019s LLMs. A full history may not fit inside an LLM\u2019s context window, resulting in an irrecoverable error. Even if your LLM supports the full context length, most LLMs still perform poorly over long contexts. They get \u201cdistracted\u201d by stale or off-topic content, all while suffering from slower response times and higher costs. Chat models accept context using messages, which include developer provided instructions (a system message) and user inputs (human messages). In chat applications, messages alternate between human inputs and model responses, resulting in a list of messages that grows longer over time. Because context windows are limited and token-rich message lists can be costly, many applications can benefit from using techniques to manually remove or forget stale information. For more information on common techniques for managing messages, see the Add and manage memory guide.Long-term memory Long-term memory in LangGraph allows systems to retain information across different conversations or sessions. Unlike short-term memory, which is thread-scoped, long-term memory is saved within custom \u201cnamespaces.\u201d Long-term memory is a complex challenge without a one-size-fits-all solution. However, the following questions provide a framework to help you navigate the different techniques:- What is the type of memory? Humans use memories to remember facts (semantic memory), experiences (episodic memory), and rules (procedural memory). AI agents can use memory in the same ways. For example, AI agents can use memory to remember specific facts about a user to accomplish a task. - When do you want to update memories? Memory can be updated as part of an agent\u2019s application logic (e.g., \u201con the hot path\u201d). In this case, the agent typically decides to remember facts before responding to a user. Alternatively, memory can be updated as a background task (logic that runs in the background / asynchronously and generates memories). We explain the tradeoffs between these approaches in the section below. Memory types Different applications require various types of memory. Although the analogy isn\u2019t perfect, examining human memory types can be insightful. Some research (e.g., the CoALA paper) have even mapped these human memory types to those used in AI agents.| Memory Type | What is Stored | Human Example | Agent Example | |---|---|---|---| | Semantic | Facts | Things I learned in school | Facts about a user | | Episodic | Experiences | Things I did | Past agent actions | | Procedural | Instructions | Instincts or motor skills | Agent system prompt | Semantic memory Semantic memory, both in humans and AI agents, involves the retention of specific facts and concepts. In humans, it can include information learned in school and the understanding of concepts and their relationships. For AI agents, semantic memory is often used to personalize applications by remembering facts or concepts from past interactions.Semantic memory is different from \u201csemantic search,\u201d which is a technique for finding similar content using \u201cmeaning\u201d (usually as embeddings). Semantic memory is a term from psychology, referring to storing facts and knowledge, while semantic search is a method for retrieving information based on meaning rather than exact matches. Profile Semantic memories can be managed in different ways. For example, memories can be a single, continuously updated \u201cprofile\u201d of well-scoped and specific information about a user, organization, or other entity (including the agent itself). A profile is generally just a JSON document with various key-value pairs you\u2019ve selected to represent your domain. When remembering a profile, you will want to make sure that you are updating the profile each time. As a result, you will want to pass in the previous profile and ask the model to generate a new profile (or some JSON patch to apply to the old profile). This can be become error-prone as the profile gets larger, and may benefit from splitting a profile into multiple documents or strict decoding when generating documents to ensure the memory schemas remains valid.Collection Alternatively, memories can be a collection of documents that are continuously updated and extended over time. Each individual memory can be more narrowly scoped and easier to generate, which means that you\u2019re less likely to lose information over time. It\u2019s easier for an LLM to generate new objects for new information than reconcile new information with an existing profile. As a result, a document collection tends to", "tokens": 1000, "node_type": "child"}
{"id": 376, "chunk_id": "9b0fd3b850698317bef57fa6e71a38ba", "parent_id": 352, "url": "", "namespace": "langchain", "title": "oss-python-concepts-memory", "headers": ["oss-python-concepts-memory"], "section_index": 0, "chunk_index": 1, "text": "domain. When remembering a profile, you will want to make sure that you are updating the profile each time. As a result, you will want to pass in the previous profile and ask the model to generate a new profile (or some JSON patch to apply to the old profile). This can be become error-prone as the profile gets larger, and may benefit from splitting a profile into multiple documents or strict decoding when generating documents to ensure the memory schemas remains valid.Collection Alternatively, memories can be a collection of documents that are continuously updated and extended over time. Each individual memory can be more narrowly scoped and easier to generate, which means that you\u2019re less likely to lose information over time. It\u2019s easier for an LLM to generate new objects for new information than reconcile new information with an existing profile. As a result, a document collection tends to lead to higher recall downstream. However, this shifts some complexity memory updating. The model must now delete or update existing items in the list, which can be tricky. In addition, some models may default to over-inserting and others may default to over-updating. See the Trustcall package for one way to manage this and consider evaluation (e.g., with a tool like LangSmith) to help you tune the behavior. Working with document collections also shifts complexity to memory search over the list. TheStore currently supports both semantic search and filtering by content. Finally, using a collection of memories can make it challenging to provide comprehensive context to the model. While individual memories may follow a specific schema, this structure might not capture the full context or relationships between memories. As a result, when using these memories to generate responses, the model may lack important contextual information that would be more readily available in a unified profile approach. Regardless of memory management approach, the central point is that the agent will use the semantic memories to ground its responses, which often leads to more personalized and relevant interactions. Episodic memory Episodic memory, in both humans and AI agents, involves recalling past events or actions. The CoALA paper frames this well: facts can be written to semantic memory, whereas experiences can be written to episodic memory. For AI agents, episodic memory is often used to help an agent remember how to accomplish a task. In practice, episodic memories are often implemented through few-shot example prompting, where agents learn from past sequences to perform tasks correctly. Sometimes it\u2019s easier to \u201cshow\u201d than \u201ctell\u201d and LLMs learn well from examples. Few-shot learning lets you \u201cprogram\u201d your LLM by updating the prompt with input-output examples to illustrate the intended behavior. While various best-practices can be used to generate few-shot examples, often the challenge lies in selecting the most relevant examples based on user input. Note that the memory store is just one way to store data as few-shot examples. If you want to have more developer involvement, or tie few-shots more closely to your evaluation harness, you can also use a LangSmith Dataset to store your data. Then dynamic few-shot example selectors can be used out-of-the box to achieve this same goal. LangSmith will index the dataset for you and enable retrieval of few shot examples that are most relevant to the user input based upon keyword similarity (using a BM25-like algorithm for keyword based similarity). See this how-to video for example usage of dynamic few-shot example selection in LangSmith. Also, see this blog post showcasing few-shot prompting to improve tool calling performance and this blog post using few-shot example to align an LLMs to human preferences.Procedural memory Procedural memory, in both humans and AI agents, involves remembering the rules used to perform tasks. In humans, procedural memory is like the internalized knowledge of how to perform tasks, such as riding a bike via basic motor skills and balance. Episodic memory, on the other hand, involves recalling specific experiences, such as the first time you successfully rode a bike without training wheels or a memorable bike ride through a scenic route. For AI agents, procedural memory is a combination of model weights, agent code, and agent\u2019s prompt that collectively determine the agent\u2019s functionality. In practice, it is fairly uncommon for agents to modify their model weights or rewrite their code. However, it is more common for agents to modify their own prompts. One effective approach to refining an agent\u2019s instructions is through \u201cReflection\u201d or meta-prompting. This involves prompting the agent with its current instructions (e.g., the system prompt) along with recent conversations or explicit user feedback. The agent then refines its own instructions based on this input. This method is particularly useful for tasks where instructions are challenging to specify upfront, as it allows the agent to learn and adapt from its interactions. For example, we built a Tweet generator using external feedback and prompt re-writing to produce high-quality paper summaries for Twitter. In this case, the specific summarization prompt was difficult to specify a priori, but it was fairly easy for a user to critique the generated Tweets and provide feedback on how to improve the summarization process. The below pseudo-code shows how you might implement this with the LangGraph memory store, using the store to save a prompt, theupdate_instructions node to get the current prompt (as well as feedback from the conversation with the user captured in state[\"messages\"] ), update the prompt, and save the new prompt back to the store. Then, the call_model get the updated prompt from the store and uses it to generate a response. Writing memories There are two primary methods for agents to write memories: \u201cin the hot path\u201d and \u201cin the background\u201d.In the hot path Creating memories during runtime offers both advantages and challenges. On the positive side, this approach allows for real-time updates, making new memories immediately available for use in subsequent interactions. It also enables transparency, as users can be notified when memories are created and stored. However, this method also presents challenges. It may increase complexity", "tokens": 1000, "node_type": "child"}
{"id": 377, "chunk_id": "5345104ec252a41359fa4a3d2dfbec23", "parent_id": 352, "url": "", "namespace": "langchain", "title": "oss-python-concepts-memory", "headers": ["oss-python-concepts-memory"], "section_index": 0, "chunk_index": 2, "text": "summarization process. The below pseudo-code shows how you might implement this with the LangGraph memory store, using the store to save a prompt, theupdate_instructions node to get the current prompt (as well as feedback from the conversation with the user captured in state[\"messages\"] ), update the prompt, and save the new prompt back to the store. Then, the call_model get the updated prompt from the store and uses it to generate a response. Writing memories There are two primary methods for agents to write memories: \u201cin the hot path\u201d and \u201cin the background\u201d.In the hot path Creating memories during runtime offers both advantages and challenges. On the positive side, this approach allows for real-time updates, making new memories immediately available for use in subsequent interactions. It also enables transparency, as users can be notified when memories are created and stored. However, this method also presents challenges. It may increase complexity if the agent requires a new tool to decide what to commit to memory. In addition, the process of reasoning about what to save to memory can impact agent latency. Finally, the agent must multitask between memory creation and its other responsibilities, potentially affecting the quantity and quality of memories created. As an example, ChatGPT uses a save_memories tool to upsert memories as content strings, deciding whether and how to use this tool with each user message. See our memory-agent template as an reference implementation.In the background Creating memories as a separate background task offers several advantages. It eliminates latency in the primary application, separates application logic from memory management, and allows for more focused task completion by the agent. This approach also provides flexibility in timing memory creation to avoid redundant work. However, this method has its own challenges. Determining the frequency of memory writing becomes crucial, as infrequent updates may leave other threads without new context. Deciding when to trigger memory formation is also important. Common strategies include scheduling after a set time period (with rescheduling if new events occur), using a cron schedule, or allowing manual triggers by users or the application logic. See our memory-service template as an reference implementation.Memory storage LangGraph stores long-term memories as JSON documents in a store. Each memory is organized under a customnamespace (similar to a folder) and a distinct key (like a file name). Namespaces often include user or org IDs or other labels that makes it easier to organize information. This structure enables hierarchical organization of memories. Cross-namespace searching is then supported through content filters.", "tokens": 416, "node_type": "child"}
{"id": 378, "chunk_id": "c16acf8a553ac24f5d77e7d3a1c6e493", "parent_id": 353, "url": "", "namespace": "langchain", "title": "oss-python-contributing-code", "headers": ["oss-python-contributing-code"], "section_index": 0, "chunk_index": 0, "text": "# oss-python-contributing-code\n\n> Source: https://docs.langchain.com/oss/python/contributing/code\n\nPhilosophy\nAim to follow these core principles for all code contributions:Backwards compatibility\nTesting first\nCode quality\nSecurity focused\nGetting started\nQuick fix: submit a bugfix\nFor simple bugfixes, you can get started immediately:Create a branch\nRun tests\nFull development setup\nFor ongoing development or larger contributions:Development environment\n- LangChain\n- LangGraph\nCore abstractions\nCore abstractions\nlangchain-core\n:Main package\nMain package\nlangchain\n:Repository structure\n- LangChain\n- LangGraph\nCore packages\nCore packages\nPartner packages\nPartner packages\nlibs/partners/\n, these are independently versioned packages for specific integrations. For example:langchain-openai\n: OpenAI integrationslangchain-anthropic\n: Anthropic integrationslangchain-google-genai\n: Google Generative AI integrations\nSupporting packages\nSupporting packages\nlangchain-text-splitters\n: Text splitting utilitieslangchain-standard-tests\n: Standard test suites for integrationslangchain-cli\n: Command line interfacelangchain-community\n: Community maintained integrations (located in a separate repo)\nDevelopment workflow\nTesting requirements\nUnit tests\ntests/unit_tests/\nRequirements:- No network calls allowed\n- Test all code paths including edge cases\n- Use mocks for external dependencies\nIntegration tests\ntests/integration_tests/\nRequirements:- Test real integrations with external services\n- Use environment variables for API keys\n- Skip gracefully if credentials unavailable\nTest quality checklist\n- Tests fail when your code is broken\n- Edge cases and error conditions are tested\n- Proper use of fixtures and mocks\nCode quality standards\nQuality requirements:- Type hints\n- Documentation\n- Code style\nManual formatting and linting\nFormat code\nRun linting checks\nVerify changes\nContribution guidelines\nBackwards compatibility\nStable interfaces\nStable interfaces\n- Function signatures and parameter names\n- Class interfaces and method names\n- Return value structure and types\n- Import paths for public APIs\nSafe changes\nSafe changes\n- Adding new optional parameters\n- Adding new methods to classes\n- Improving performance without changing behavior\n- Adding new modules or functions\nBefore making changes\nBefore making changes\n- Would this break existing user code?\n- Check if your target is public\n-\nIf needed, is it exported in\n__init__.py\n? - Are there existing usage patterns in tests?\nBugfixes\nFor bugfix contributions:Reproduce the issue\nWrite failing tests\nImplement the fix\nVerify the fix\nDocument the change\nNew features\nWe aim to keep the bar high for new features. We generally don\u2019t accept new core abstractions, changes to infra, changes to dependencies, or new agents/chains from outside contributors without an existing issue that demonstrates an acute need for them. In general, feature contribution requirements include:Design discussion\n- The problem you\u2019re solving\n- Proposed API design\n- Expected usage patterns\nImplementation\n- Follow existing code patterns\n- Include comprehensive tests and documentation\n- Consider security implications\nIntegration considerations\n- How does this interact with existing features?\n- Are there performance implications?\n- Does this introduce new dependencies?\nSecurity guidelines\nInput validation\nInput validation\n- Validate and sanitize all user inputs\n- Properly escape data in templates and queries\n- Never use\neval()\n,exec()\n, orpickle\non user data, as this can lead to arbitrary code execution vulnerabilities\nError handling\nError handling\n- Use specific exception types\n- Don\u2019t expose sensitive information in error messages\n- Implement proper resource cleanup\nDependencies\nDependencies\n- Avoid adding hard dependencies\n- Keep optional dependencies minimal\n- Review third-party packages for security issues\nTesting and validation\nRunning tests locally\nBefore submitting your PR, ensure you have completed the following steps. Note that the requirements differ slightly between LangChain and LangGraph.- LangChain\n- LangGraph\nUnit tests\nIntegration tests\nFormatting\nType checking\nPR submission\nTest writing guidelines\nIn order to write effective tests, there\u2019s a few good practices to follow:- Use natural language to describe the test in docstrings\n- Use descriptive variable names\n- Be exhaustive with assertions\n- Unit tests\n- Integration tests\n- Mock usage", "tokens": 606, "node_type": "child"}
{"id": 379, "chunk_id": "c16acf8a553ac24f5d77e7d3a1c6e493", "parent_id": 354, "url": "", "namespace": "langchain", "title": "oss-python-contributing-comarketing", "headers": ["oss-python-contributing-comarketing"], "section_index": 0, "chunk_index": 0, "text": "# oss-python-contributing-comarketing\n\n> Source: https://docs.langchain.com/oss/python/contributing/comarketing\n\nContent we\u2019re excited to promote\nEducational content\nEducational content\nBlogs, YouTube videos and other media showcasing educational content. Note that we prefer content that is NOT framed as \u201chere\u2019s how to use integration XYZ\u201d, but rather \u201chere\u2019s how to do ABC\u201d, as we find that is more educational and helpful for developers.\nEnd-to-end applications\nEnd-to-end applications\nEnd-to-end applications are great resources for developers looking to build. We prefer to highlight applications that are more complex/agentic in nature, and that use LangGraph as the orchestration framework. We get particularly excited about anything involving:\n- Long-term memory systems\n- Human-in-the-loop interaction patterns\n- Multi-agent architectures\nResearch\nResearch\nWe love highlighting novel research! Whether it is research built on top of LangChain or that integrates with it.", "tokens": 129, "node_type": "child"}
{"id": 380, "chunk_id": "c16acf8a553ac24f5d77e7d3a1c6e493", "parent_id": 355, "url": "", "namespace": "langchain", "title": "oss-python-contributing-documentation", "headers": ["oss-python-contributing-documentation"], "section_index": 0, "chunk_index": 0, "text": "# oss-python-contributing-documentation\n\n> Source: https://docs.langchain.com/oss/python/contributing/documentation\n\nConceptual guides\nReferences\nTutorials (Learn)\nHow-to guides\nGetting started\nQuick edit: fix a typo\nFor simple changes like fixing typos, you can edit directly on GitHub without setting up a local development environment:Find the page\nFork the repository\nMake your changes\nCreate pull request\nFull development IDE setup\nFor larger changes or ongoing contributions, it\u2019s important to set up a local development environment on your machine. Our documentation build pipeline offers local preview and live reload as you edit, important for ensuring your changes appear as intended before submitting. Please review the steps to set up your environment outlined in the docs repoREADME.md\n.\nDocumentation types\nConceptual guides\nConceptual guide cover core concepts abstractly, providing deep understanding.Characteristics\nCharacteristics\n- Understanding-focused: Explain why things work as they do\n- Broad perspective: Higher and wider view than other types\n- Design-oriented: Explain decisions and trade-offs\n- Context-rich: Use analogies and comparisons\nTips\nTips\n- Explain design decisions - \u201cwhy does concept X exist?\u201d\n- Use analogies and reference alternatives\n- Avoid blending in too much reference content\n- Link to related tutorials and how-to guides\n- Focus on the \u201cwhy\u201d rather than the \u201chow\u201d\nReferences\nReference documentation contains detailed, low-level information describing exactly what functionality exists and how to use it.Python reference\n- Describe what exists (all parameters, options, return values)\n- Be comprehensive and structured for easy lookup\n- Serve as the authoritative source for technical details\nLangChain reference best practices\nLangChain reference best practices\n- Be consistent; follow existing patterns for provider-specific documentation\n- Include both basic usage (code snippets) and common edge cases/failure modes\n- Note when features require specific versions\nWhen to create new reference documentation\nWhen to create new reference documentation\n- New integrations or providers need dedicated reference pages\n- Complex configuration options require detailed explanation\n- API changes introduce new parameters or behavior\n- Community frequently asks questions about specific functionality\nWriting standard\nMintlify components\nUse appropriate Mintlify components to enhance readability:- Callouts\n- Structure\n- Code\n<Note>\nfor helpful supplementary information<Warning>\nfor important cautions and breaking changes<Tip>\nfor best practices and advice<Info>\nfor neutral contextual information<Check>\nfor success confirmations\nPage structure\nEvery documentation page must begin with YAML frontmatter:Localization\nAll documentation must be localized in both Python and JavaScript/TypeScript when possible. To do so, we use a custom in-line syntax to differentiate between sections that should appear in one or both languages:Quality standards\nGeneral guidelines\nAvoid duplication\nAvoid duplication\nLink frequently\nLink frequently\nBe concise\nBe concise\nAccessibility requirements\nEnsure documentation is accessible to all users:- Structure content for easy scanning with headers and lists\n- Use specific, actionable link text instead of \u201cclick here\u201d\n- Include descriptive alt text for all images and diagrams\nTesting and validation\nBefore submitting documentation:Test all code\nCheck formatting\nBuild locally\nReview links\nIn-code documentation\nLanguage and style\nFollow these standards for all documentation:- Voice: Use second person (\u201cyou\u201d) for instructions\n- Tense: Use active voice and present tense\n- Clarity: Write clear, direct language for technical audiences\n- Consistency: Use consistent terminology throughout\n- Conciseness: Keep sentences concise while providing necessary context\nCode examples\nCompleteness\nRealism\nError handling\nDocumentation", "tokens": 527, "node_type": "child"}
{"id": 381, "chunk_id": "c16acf8a553ac24f5d77e7d3a1c6e493", "parent_id": 356, "url": "", "namespace": "langchain", "title": "oss-python-contributing-implement-langchain", "headers": ["oss-python-contributing-implement-langchain"], "section_index": 0, "chunk_index": 0, "text": "# oss-python-contributing-implement-langchain\n\n> Source: https://docs.langchain.com/oss/python/contributing/implement-langchain\n\nIntegration packages are Python packages that users can install for use in their projects. They implement one or more components that adhere to the LangChain interface standards.LangChain components are subclasses of base classes in langchain-core. Examples include chat models, tools, retrievers, and more.Your integration package will typically implement a subclass of at least one of these components. Expand the tabs below to see details on each.\nChat Models\nTools\nRetrievers\nVector Stores\nEmbeddings\nChat models are subclasses of the BaseChatModel class. They implement methods for generating chat completions, handling message formatting, and managing model parameters.\nThe chat model integration guide is currently WIP. In the meantime, read the chat model conceptual guide for details on how LangChain chat models function.", "tokens": 125, "node_type": "child"}
{"id": 382, "chunk_id": "c16acf8a553ac24f5d77e7d3a1c6e493", "parent_id": 358, "url": "", "namespace": "langchain", "title": "oss-python-contributing-integrations-langchain", "headers": ["oss-python-contributing-integrations-langchain"], "section_index": 0, "chunk_index": 0, "text": "# oss-python-contributing-integrations-langchain\n\n> Source: https://docs.langchain.com/oss/python/contributing/integrations-langchain\n\nWhy contribute an integration to LangChain?\nDiscoverability\nLangChain is the most used framework for building LLM applications, with over 20 million monthly downloads.\nInteroperability\nLangChain components expose a standard interface, allowing developers to easily swap them for each other. If you implement a LangChain integration, any developer using a different component will easily be able to swap yours in.\nBest Practices\nThrough their standard interface, LangChain components encourage and facilitate best practices (streaming, async, etc.) that improve developer experience and application performance.\nComponents to integrate\nWhile any component can be integrated into LangChain, there are specific types of integrations we encourage more: Integrate these \u2705:- Chat Models: Most actively used component type\n- Tools/Toolkits: Enable agent capabilities\n- Retrievers: Core to RAG applications\n- Embedding Models: Foundation for vector operations\n- Vector Stores: Essential for semantic search\n- LLMs (Text-Completion Models): Deprecated in favor of Chat Models\n- Document Loaders: High maintenance burden\n- Key-Value Stores: Limited usage\n- Document Transformers: Niche use cases\n- Model Caches: Infrastructure concerns\n- Graphs: Complex abstractions\n- Message Histories: Storage abstractions\n- Callbacks: System-level components\n- Chat Loaders: Limited demand\n- Adapters: Edge case utilities\nHow to contribute an integration\n1\n3\n5\nAdd documentation\nOpen a PR to add documentation for your integration to the official LangChain docs.\nIntegration documentation guide\nIntegration documentation guide\nAn integration is only as useful as its documentation. To ensure a consistent experience for users, docs are required for all new integrations. We have a standard starting-point template for each type of integration for you to copy and modify.In a new PR to the LangChain docs repo, create a new file in the relevant directory under\nsrc/oss/python/integrations/<component_type>/integration_name.mdx\nusing the appropriate template file:- Chat models\n- Tools and toolkits\n- Retrievers\n- Text splitters - Coming soon\n- Embedding models - Coming soon\n- Vector stores\n- Document loaders - Coming soon\n- Key-value stores - Coming soon", "tokens": 325, "node_type": "child"}
{"id": 383, "chunk_id": "c16acf8a553ac24f5d77e7d3a1c6e493", "parent_id": 360, "url": "", "namespace": "langchain", "title": "oss-python-contributing-overview", "headers": ["oss-python-contributing-overview"], "section_index": 0, "chunk_index": 0, "text": "# oss-python-contributing-overview\n\n> Source: https://docs.langchain.com/oss/python/contributing/overview\n\nLangGraph v1.0Welcome to the new LangGraph documentation! If you encounter any issues or have feedback, please open an issue so we can improve. Archived v0 documentation can be found here.See the release notes and migration guide for a complete list of changes and instructions on how to upgrade your code.\nWays to Contribute\nReport bugs\nReport bugs\nFound a bug? Please help us fix it by following these steps:If you are adding an issue, please try to keep it focused on a single topic. If two issues are related, or blocking, please link them rather than combining them. For example,\n1\n2\nCreate issue\nIf no issue exists, create a new one. When writing, be sure to follow the template provided and to include a minimal, reproducible, example. Attach any relevant labels to the final issue once created. If a project maintainer is unable to reproduce the issue, it is unlikely to be addressed in a timely manner.\n3\nWait\nA project maintainer will triage the issue and may ask for additional information. Please be patient as we manage a high volume of issues. Do not bump the issue unless you have new information to provide.\nSuggest features\nSuggest features\nHave an idea for a new feature or enhancement?\n1\n2\n3\nDescribe\nBe sure to describe the use case and why it would be valuable to others. If possible, provide examples or mockups where applicable. Outline test cases that should pass.\nImprove documentation\nImprove documentation\nContribute code\nContribute code\nWith a large userbase, it can be hard for our small team to keep up with all the feature requests and bug fixes. If you have the skills and time, we would love your help!If you start working on an issue, please assign it to yourself or ask a maintainer to do so. This helps avoid duplicate work.If you are looking for something to work on, check out the issues labeled \u201cgood first issue\u201d or \u201chelp wanted\u201d in our repos:\nHow to make your first Pull Request\nGuide", "tokens": 343, "node_type": "child"}
{"id": 384, "chunk_id": "c16acf8a553ac24f5d77e7d3a1c6e493", "parent_id": 361, "url": "", "namespace": "langchain", "title": "oss-python-contributing-publish-langchain", "headers": ["oss-python-contributing-publish-langchain"], "section_index": 0, "chunk_index": 0, "text": "# oss-python-contributing-publish-langchain\n\n> Source: https://docs.langchain.com/oss/python/contributing/publish-langchain\n\nImportant: New integrations should be standalone packages, not PRs to the LangChain monorepo.While LangChain maintains a small subset of first-party and high-usage integrations (like OpenAI, Anthropic, and Ollama) in the main repository, new integrations should be published as separate PyPI packages and repositories (e.g.,\nlangchain-yourservice\n) that users install alongside the core LangChain packages. You should not submit a PR to add your integration directly to the main LangChain repository.Publishing your package\nFor the purposes of this guide, we\u2019ll be using PyPI as the package registry. You may choose to publish to other registries if you prefer; instructions will vary.\nSetup credentials\nFirst, make sure you have a PyPI account:How to create a PyPI Token\nHow to create a PyPI Token\n2\nVerify email\nVerify your email address by clicking the link that PyPI emails to you\n3\nEnable 2FA\nGo to your account settings and click \u201cGenerate Recovery Codes\u201d to enable 2FA. To generate an API token, you must have 2FA enabled\nBuild and publish\nHow to publish a package\nHelpful guide from\nuv\non how to build and publish a package to PyPI.Adding documentation\nTo add documentation for your package to this site under the integrations tab, you will need to create the relevant documentation pages and open a PR in the LangChain docs repository.Writing docs\nDepending on the type of integration you have built, you will need to create different types of documentation pages. LangChain provides templates for different types of integrations to help you get started.To reference existing documentation, you can look at the list of integrations and find similar ones to yours.To view a given documentation page in raw markdown, use the dropdown button next to \u201cCopy page\u201d on the top right of the page and select \u201cView as Markdown\u201d.\nSubmitting a PR\nMake a fork of the LangChain docs repository under a personal GitHub account, and clone it locally. Create a new branch for your integration. Copy the template and modify them using your favorite markdown text editor. Make sure to refer to and follow the documentation guide when writing your documentation.We may reject PRs or ask for modification if:\n- CI checks fail\n- Severe grammatical errors or typos are present\n- Mintlify components are used incorrectly\n- Pages are missing a frontmatter\n- Localization is missing (where applicable)\n- Code examples do not run or have errors\n- Quality standards are not met\nNext steps\nCongratulations! Your integration is now published and documented, making it available to the entire LangChain community.Co-marketing\nGet in touch with the LangChain marketing team to explore co-marketing opportunities.", "tokens": 436, "node_type": "child"}
{"id": 385, "chunk_id": "c16acf8a553ac24f5d77e7d3a1c6e493", "parent_id": 363, "url": "", "namespace": "langchain", "title": "oss-python-contributing-standard-tests-langchain", "headers": ["oss-python-contributing-standard-tests-langchain"], "section_index": 0, "chunk_index": 0, "text": "# oss-python-contributing-standard-tests-langchain\n\n> Source: https://docs.langchain.com/oss/python/contributing/standard-tests-langchain\n\nSetup\nFirst, install the required dependencies:langchain-core\nDefines the interfaces we want to import to define our custom components\nlangchain-tests\nProvides the standard tests and\npytest\nplugins necessary to run themlangchain-tests\npackage:\nUnit tests\nUnit tests\nLocation:\nlangchain_tests.unit_tests\nDesigned to test the component in isolation and without access to external servicesIntegration tests\nIntegration tests\nLocation:\nlangchain_tests.integration_tests\nDesigned to test the component with access to external services (in particular, the external service that the component is designed to interact with)pytest\nclass-based test suites.\nImplementing standard tests\nDepending on your integration type, you will need to implement either or both unit and integration tests. By subclassing the standard test suite for your integration type, you get the full collection of standard tests for that type. For a test run to be successful, the a given test should pass only if the model supports the capability being tested. Otherwise, the test should be skipped. Because different integrations offer unique sets of features, most standard tests provided by LangChain are opt-in by default to prevent false positives. Consequently, you will need to override properties to indicate which features your integration supports - see the below example for an illustration.tests/integration_tests/test_standard.py\nYou should typically organize tests in these subdirectories relative to the root of your package:\ntests/unit_tests\nfor unit teststests/integration_tests\nfor integration tests\nRunning tests\nIf bootstrapping an integration from a template, aMakefile\nis provided that includes targets for running unit and integration tests:\nTroubleshooting\nFor a full list of the standard test suites that are available, as well as information on which tests are included and how to troubleshoot common issues, see the Standard Tests API Reference. You can find troubleshooting guides under the individual test suites listed in that API Reference. For example, here is the guide forChatModelIntegrationTests.test_usage_metadata\n.", "tokens": 299, "node_type": "child"}
{"id": 386, "chunk_id": "c16acf8a553ac24f5d77e7d3a1c6e493", "parent_id": 366, "url": "", "namespace": "langchain", "title": "oss-python-integrations-document-loaders-index", "headers": ["oss-python-integrations-document-loaders-index"], "section_index": 0, "chunk_index": 0, "text": "# oss-python-integrations-document-loaders-index\n\n> Source: https://docs.langchain.com/oss/python/integrations/document_loaders/index\n\nInterface\nEach document loader may define its own parameters, but they share a common API:.load()\n\u2013 Loads all documents at once..lazy_load()\n\u2013 Streams documents lazily, useful for large datasets.\nBy category\nWebpages\nThe below document loaders allow you to load webpages.| Document Loader | Description | Package/API |\n|---|---|---|\n| Web | Uses urllib and BeautifulSoup to load and parse HTML web pages | Package |\n| Unstructured | Uses Unstructured to load and parse web pages | Package |\n| RecursiveURL | Recursively scrapes all child links from a root URL | Package |\n| Sitemap | Scrapes all pages on a given sitemap | Package |\n| Spider | Crawler and scraper that returns LLM-ready data | API |\n| Firecrawl | API service that can be deployed locally | API |\n| Docling | Uses Docling to load and parse web pages | Package |\n| Hyperbrowser | Platform for running and scaling headless browsers, can be used to scrape/crawl any site | API |\n| AgentQL | Web interaction and structured data extraction from any web page using an AgentQL query or a Natural Language prompt | API |\nPDFs\nThe below document loaders allow you to load PDF documents.| Document Loader | Description | Package/API |\n|---|---|---|\n| PyPDF | Uses pypdf to load and parse PDFs | Package |\n| Unstructured | Uses Unstructured\u2019s open source library to load PDFs | Package |\n| Amazon Textract | Uses AWS API to load PDFs | API |\n| MathPix | Uses MathPix to load PDFs | Package |\n| PDFPlumber | Load PDF files using PDFPlumber | Package |\n| PyPDFDirectry | Load a directory with PDF files | Package |\n| PyPDFium2 | Load PDF files using PyPDFium2 | Package |\n| PyMuPDF | Load PDF files using PyMuPDF | Package |\n| PyMuPDF4LLM | Load PDF content to Markdown using PyMuPDF4LLM | Package |\n| PDFMiner | Load PDF files using PDFMiner | Package |\n| Upstage Document Parse Loader | Load PDF files using UpstageDocumentParseLoader | Package |\n| Docling | Load PDF files using Docling | Package |\n| UnDatasIO | Load PDF files using UnDatasIO | Package |\n| OpenDataLoader PDF | Load PDF files using OpenDataLoader PDF | Package |\nCloud Providers\nThe below document loaders allow you to load documents from your favorite cloud providers.| Document Loader | Description | Partner Package | API reference |\n|---|---|---|---|\n| AWS S3 Directory | Load documents from an AWS S3 directory | \u274c | S3DirectoryLoader |\n| AWS S3 File | Load documents from an AWS S3 file | \u274c | S3FileLoader |\n| Azure AI Data | Load documents from Azure AI services | \u274c | AzureAIDataLoader |\n| Azure Blob Storage Container | Load documents from an Azure Blob Storage container | \u274c | AzureBlobStorageContainerLoader |\n| Azure Blob Storage File | Load documents from an Azure Blob Storage file | \u274c | AzureBlobStorageFileLoader |\n| Dropbox | Load documents from Dropbox | \u274c | DropboxLoader |\n| Google Cloud Storage Directory | Load documents from GCS bucket | \u2705 | GCSDirectoryLoader |\n| Google Cloud Storage File | Load documents from GCS file object | \u2705 | GCSFileLoader |\n| Google Drive | Load documents from Google Drive (Google Docs only) | \u2705 | GoogleDriveLoader |\n| Huawei OBS Directory | Load documents from Huawei Object Storage Service Directory | \u274c | OBSDirectoryLoader |\n| Huawei OBS File | Load documents from Huawei Object Storage Service File | \u274c | OBSFileLoader |\n| Microsoft OneDrive | Load documents from Microsoft OneDrive | \u274c | OneDriveLoader |\n| Microsoft SharePoint | Load documents from Microsoft SharePoint | \u274c | SharePointLoader |\n| Tencent COS Directory | Load documents from Tencent Cloud Object Storage Directory | \u274c | TencentCOSDirectoryLoader |\n| Tencent COS File | Load documents from Tencent Cloud Object Storage File | \u274c | TencentCOSFileLoader |\nSocial Platforms\nThe below document loaders allow you to load documents from different social media platforms.| Document Loader | API reference |\n|---|---|\nTwitterTweetLoader | |\nRedditPostsLoader |\nMessaging Services\nThe below document loaders allow you to load data from different messaging platforms.| Document Loader | API reference |\n|---|---|\n| Telegram | TelegramChatFileLoader |\nWhatsAppChatLoader | |\n| Discord | DiscordChatLoader |\n| Facebook Chat | FacebookChatLoader |\n| Mastodon | MastodonTootsLoader |\nProductivity tools\nThe below document loaders allow you to load data from commonly used productivity tools.| Document Loader | API reference |\n|---|---|\n| Figma | FigmaFileLoader |\n| Notion | NotionDirectoryLoader |\n| Slack | SlackDirectoryLoader |\n| Quip | QuipLoader |\n| Trello | TrelloLoader |\n| Roam | RoamLoader |\n| GitHub | GithubFileLoader |\nCommon File Types\nThe below document loaders allow you to load data from common data formats.| Document Loader | Data Type |\n|---|---|\n| CSVLoader | CSV files |\n| Unstructured | Many file types (see https://docs.unstructured.io/platform/supported-file-types) |\n| JSONLoader | JSON files |\n| BSHTMLLoader | HTML files |\n| DoclingLoader | Various file types (see https://ds4sd.github.io/docling/) |", "tokens": 855, "node_type": "child"}
{"id": 387, "chunk_id": "c16acf8a553ac24f5d77e7d3a1c6e493", "parent_id": 367, "url": "", "namespace": "langchain", "title": "oss-python-integrations-providers-all-providers", "headers": ["oss-python-integrations-providers-all-providers"], "section_index": 0, "chunk_index": 0, "text": "# oss-python-integrations-providers-all-providers > Source: https://docs.langchain.com/oss/python/integrations/providers/all_providers Providers Abso Custom AI integration platform for enterprise workflows. Acreom Knowledge management platform with AI-powered organization. ActiveLoop DeepLake Vector database for AI applications with deep learning focus. Ads4GPTs Advertising platform for GPT applications and AI services. AgentQL Web scraping with natural language queries. AI21 AI21 Labs\u2019 Jurassic models for text generation. AIM Tracking Experiment tracking and management platform. AI/ML API Unified API for multiple AI and ML services. AI Network Decentralized AI computing network platform. Airbyte Data integration platform for ETL and ELT pipelines. Airtable Cloud-based spreadsheet and database platform. Alchemy Blockchain development platform and APIs. Aleph Alpha European AI company\u2019s multilingual language models. Alibaba Cloud Alibaba\u2019s cloud computing and AI services. AnalyticDB Alibaba Cloud\u2019s real-time analytics database. Anchor Browser Browser automation and web scraping tools. Annoy Approximate nearest neighbors search library. Anthropic Claude models for advanced reasoning and conversation. Anyscale Distributed computing platform for ML workloads. Apache Doris Real-time analytical database management system. Apache Apache Software Foundation tools and libraries. Apify Web scraping and automation platform. Apple Apple\u2019s machine learning and AI frameworks. ArangoDB Multi-model database with graph capabilities. Arcee Domain-specific language model training platform. ArcGIS Geographic information system platform. Argilla Data labeling and annotation platform for NLP. Arize ML observability and performance monitoring. Arthur Tracking AI model monitoring and governance platform. arXiv Academic paper repository and search platform. Ascend Data engineering and pipeline automation platform. Ask News Real-time news search and analysis API. AssemblyAI Speech-to-text and audio intelligence API. AstraDB DataStax Astra DB vector database platform. Atlas Data visualization and exploration platform. AwaDB Vector database for AI and ML applications. AWS Amazon Web Services cloud platform and AI services. AZLyrics Song lyrics database and search platform. Azure AI Microsoft Azure AI and cognitive services. BAAI Beijing Academy of AI research and models. Bagel Vector database and semantic search platform. BagelDB Multi-modal AI database and storage system. Baichuan Chinese language model from Baichuan AI. Baidu Baidu\u2019s AI services and language models. BananaDev Serverless GPU infrastructure for ML models. Baseten ML model deployment and serving platform. Beam Serverless GPU computing platform. Beautiful Soup HTML and XML parsing library for web scraping. BibTeX Bibliography management and citation format. Bilibili Chinese video sharing platform integration. Bittensor Decentralized AI network and incentive protocol. Blackboard Educational technology and learning management. Bodo DataFrames High-performance analytics and data processing. BookendAI AI-powered reading and research assistant. Box Cloud content management and collaboration. Brave Search Privacy-focused search engine API. Breebs AI knowledge management and retrieval platform. Brightdata Web data platform and proxy services. Browserbase Headless browser automation platform. Browserless Serverless browser automation service. ByteDance ByteDance\u2019s AI models and services. Cassandra Distributed NoSQL database management system. Cerebras AI compute platform with specialized processors. CerebriumAI Serverless GPU platform for AI applications. Chaindesk No-code AI chatbot and automation platform. Chroma Open-source embedding database for AI apps. Clarifai Computer vision and AI model platform. ClearML Tracking ML experiment tracking and automation. ClickHouse Fast columnar database for analytics. ClickUp Project management and productivity platform. Cloudflare Web infrastructure and security services. Clova Naver\u2019s AI assistant and NLP platform. CnosDB Time series database for IoT and analytics. Cognee Memory layer for AI applications and agents. CogniSwitch AI knowledge management and retrieval system. Cohere Language AI platform for enterprise applications. College Confidential College admissions and education platform. Comet Tracking ML experiment tracking and model management. Confident AI observability and monitoring platform. Confluence Team collaboration and documentation platform. Connery Plugin system for AI agents and applications. Context Context management for AI applications. Contextual Contextual AI and language understanding. Couchbase NoSQL cloud database platform. Coze Conversational AI platform and chatbot builder. CrateDB Distributed SQL database for machine data. CTransformers Python bindings for transformer models in C/C++. CTranslate2 Fast inference engine for Transformer models. Cube Semantic layer for building data applications. Dappier Real-time AI data platform and API. DashVector Alibaba Cloud\u2019s vector database service. Databricks Unified analytics platform for big data and ML. Datadog Monitoring and analytics platform for applications. Datadog Logs Log management and analysis platform. DataForSEO SEO and SERP data API platform. DataHerald Natural language to SQL query platform. Dedoc Document analysis and structure detection. DeepInfra Serverless inference for deep learning models. DeepLake Vector database for deep learning applications. DeepSeek Advanced reasoning and coding AI models. DeepSparse Inference runtime for sparse neural networks. Dell Dell Technologies AI and computing solutions. Diffbot Web data extraction and knowledge graph. Dingo Distributed vector database system. Discord Communication platform integration and bots. Discord Shikenso Discord analytics and moderation tools. DocArray Data structure for multimodal AI applications. Docling Document processing and AI integration. Doctran Document transformation and processing. Docugami Document AI and semantic processing. Docusaurus Documentation website generator and platform. Dria Decentralized knowledge retrieval network. Dropbox Cloud storage and file sharing platform. DuckDB In-process SQL OLAP database management system. DuckDuckGo Search Privacy-focused search engine integration. E2B Cloud development environment platform. EdenAI Unified API for multiple AI services. Elasticsearch Distributed search and analytics engine. ElevenLabs AI voice synthesis and speech platform. EmbedChain Framework for creating RAG applications. Epsilla Vector database for AI and ML applications. Etherscan Ethereum blockchain explorer and analytics. EverlyAI Serverless AI inference platform. Evernote Note-taking and organization platform. Exa Search AI-powered search engine for developers. Meta\u2019s social platform integration and APIs. FalkorDB Graph database with ultra-low latency. Fauna Serverless, globally distributed database. Featherless AI Fast and efficient AI model serving. Fiddler AI observability and monitoring platform. Figma Design collaboration and prototyping platform. FireCrawl Web scraping and crawling API service. Fireworks Fast inference platform for open-source models. Flyte Workflow orchestration for ML and data processing. FMP Data Financial market data and analytics API. ForefrontAI Fine-tuning platform for language models. Friendli Optimized serving engine for AI models. Galaxia Prompt-driven engineering assistant. Gel Knowledge extraction and NLP platform. GeoPandas Geographic data analysis with Python. Git Version control system integration. GitBook Documentation platform and knowledge base. GitHub Code hosting and collaboration platform. GitLab DevOps platform and code repository. GOAT Tool use framework for AI agents. Golden Knowledge graph and data platform. Goodfire Interpretable AI and model analysis. Google\u2019s AI", "tokens": 1000, "node_type": "child"}
{"id": 388, "chunk_id": "9b0fd3b850698317bef57fa6e71a38ba", "parent_id": 367, "url": "", "namespace": "langchain", "title": "oss-python-integrations-providers-all-providers", "headers": ["oss-python-integrations-providers-all-providers"], "section_index": 0, "chunk_index": 1, "text": "and organization platform. Exa Search AI-powered search engine for developers. Meta\u2019s social platform integration and APIs. FalkorDB Graph database with ultra-low latency. Fauna Serverless, globally distributed database. Featherless AI Fast and efficient AI model serving. Fiddler AI observability and monitoring platform. Figma Design collaboration and prototyping platform. FireCrawl Web scraping and crawling API service. Fireworks Fast inference platform for open-source models. Flyte Workflow orchestration for ML and data processing. FMP Data Financial market data and analytics API. ForefrontAI Fine-tuning platform for language models. Friendli Optimized serving engine for AI models. Galaxia Prompt-driven engineering assistant. Gel Knowledge extraction and NLP platform. GeoPandas Geographic data analysis with Python. Git Version control system integration. GitBook Documentation platform and knowledge base. GitHub Code hosting and collaboration platform. GitLab DevOps platform and code repository. GOAT Tool use framework for AI agents. Golden Knowledge graph and data platform. Goodfire Interpretable AI and model analysis. Google\u2019s AI services and cloud platform. Google Serper Google Search API service. GooseAI Fully managed NLP-as-a-Service platform. GPT4All Open-source LLM ecosystem for local deployment. Gradient AI model training and deployment platform. GradientAI Private AI model training platform. Graph RAG Graph-based retrieval augmented generation. GraphSignal AI observability and monitoring platform. GreenNode Sustainable AI computing platform. GROBID Machine learning library for bibliographic data. Groq Ultra-fast inference with specialized hardware. Gutenberg Project Gutenberg digital library access. Hacker News Tech news and discussion platform. Hazy Research Machine learning research and tools. Helicone LLM observability and monitoring platform. Hologres Real-time interactive analytics service. HTML2Text HTML to plain text conversion utility. Huawei Huawei Cloud AI services and models. Hugging Face Open platform for ML models and datasets. HyperBrowser Web automation and scraping platform. IBM IBM Watson AI and enterprise solutions. IEIT Systems Enterprise AI and system integration. iFixit Repair guides and technical documentation. iFlytek Chinese speech and language AI platform. IMSDb Internet Movie Script Database access. InfinispanVS Distributed cache and data grid platform. Infinity High-performance embedding inference server. Infino Observability and monitoring platform. Intel Intel\u2019s AI optimization tools and libraries. IUGU Brazilian payment processing platform. Jaguar Vector database and search platform. Javelin AI Gateway AI model gateway and management platform. Jenkins Automation server and CI/CD platform. Jina Neural search framework and cloud platform. John Snow Labs Enterprise NLP and healthcare AI platform. Joplin Open-source note taking and organization. KDB.AI Time-series vector database platform. Kinetica Real-time analytics and database platform. KoboldAI Browser-based AI writing assistant. Konko Generative AI platform and model hosting. KoNLPy Korean natural language processing toolkit. Kuzu Embedded graph database management system. Label Studio Data labeling and annotation platform. LakeFS Git-like version control for data lakes. LanceDB Developer-friendly embedded vector database. LangChain Decorators Syntactic sugar and utilities for LangChain. LangFair Bias testing framework for language models. LangFuse LLM engineering platform and observability. Lantern PostgreSQL vector database extension. Lindorm Alibaba Cloud\u2019s multi-model database service. LinkUp Real-time job market data and search. LiteLLM Unified interface for 100+ LLM APIs. LlamaIndex Data framework for LLM applications. LlamaCPP Port of Meta\u2019s LLaMA model in C/C++. LlamaEdge Edge computing platform for LLaMA models. LlamaFile Single-file executable for running LLMs. LLMonitor Observability platform for LLM applications. LocalAI Self-hosted OpenAI-compatible API server. Log10 LLM data management and observability. MariaDB Open-source relational database management. MaritALK Brazilian Portuguese language model. Marqo End-to-end vector search engine. MediaWiki Dump Wikipedia and MediaWiki data processing. Meilisearch Lightning-fast search engine platform. Memcached Distributed memory caching system. Memgraph Real-time graph database platform. Metal Managed vector search and retrieval. Microsoft Microsoft Azure AI and enterprise services. Milvus Open-source vector database for AI applications. MindsDB AI layer for databases and data platforms. Minimax Chinese AI company\u2019s language models. MistralAI Efficient open-source language models. MLflow ML lifecycle management platform. MLflow Tracking Experiment tracking and model registry. MLX Apple\u2019s machine learning framework. Modal Serverless cloud computing for data science. ModelScope Alibaba\u2019s open-source model hub. Modern Treasury Payment operations and treasury management. Momento Serverless cache and vector index. MongoDB Document-based NoSQL database platform. MongoDB Atlas Cloud-hosted MongoDB with vector search. MotherDuck Serverless analytics with DuckDB in the cloud. Motorhead Long-term memory for AI conversations. MyScale SQL-compatible vector database platform. Naver Naver\u2019s AI services and language models. Nebius AI cloud platform and infrastructure. Neo4j Native graph database and analytics platform. NetMind Decentralized AI computing network. Nimble Web intelligence and data extraction. NLP Cloud Production-ready NLP API platform. Nomic Open-source embedding models and tools. Notion All-in-one workspace and collaboration platform. Nuclia AI-powered search and understanding platform. NVIDIA NVIDIA\u2019s AI computing platform and models. Obsidian Connected note-taking and knowledge management. OceanBase Distributed relational database system. OCI Oracle Cloud Infrastructure AI services. OctoAI Efficient AI compute and model serving. Ollama Run large language models locally. Ontotext GraphDB RDF database and semantic graph platform. OpenAI GPT models and comprehensive AI platform. OpenDataLoader PDF Safe, Open, High-Performance \u2014 PDF for AI OpenGradient AI model training and fine-tuning platform. OpenLLM Operating LLMs in production environment. OpenSearch Distributed search and analytics suite. OpenWeatherMap Weather data and forecasting API. Oracle AI Oracle\u2019s AI and machine learning services. Outline Team knowledge base and wiki platform. Outlines Structured generation for language models. Oxylabs Web scraping and proxy services. Pandas Data analysis and manipulation library. Perigon Real-time news and media monitoring. Permit Authorization and access control platform. Perplexity AI-powered search and reasoning engine. Petals Distributed inference for large language models. PG Embedding PostgreSQL vector embedding extensions. pgvector Vector similarity search for PostgreSQL. Pinecone Managed vector database for ML applications. PipelineAI ML pipeline and model deployment platform. Pipeshift AI-powered content moderation platform. Portkey AI gateway and observability platform. Predibase Fine-tuning platform for large language models. PredictionGuard AI model security and compliance platform. PreMAI AI platform for model deployment and management. Prolog Logic programming language integration. PromptLayer Prompt engineering and observability platform. Psychic Universal API for SaaS integrations. PubMed Biomedical literature database access. Pull MD Markdown content extraction and processing. PygmalionAI Conversational AI model platform. PyMuPDF4LLM PDF processing optimized for LLM ingestion. Qdrant Vector similarity search engine. Ragatouille RAG toolkit with ColBERT indexing. Rank BM25 BM25 ranking algorithm implementation. Ray Serve Scalable model serving framework. Rebuff Prompt injection detection and", "tokens": 1000, "node_type": "child"}
{"id": 389, "chunk_id": "5345104ec252a41359fa4a3d2dfbec23", "parent_id": 367, "url": "", "namespace": "langchain", "title": "oss-python-integrations-providers-all-providers", "headers": ["oss-python-integrations-providers-all-providers"], "section_index": 0, "chunk_index": 2, "text": "and media monitoring. Permit Authorization and access control platform. Perplexity AI-powered search and reasoning engine. Petals Distributed inference for large language models. PG Embedding PostgreSQL vector embedding extensions. pgvector Vector similarity search for PostgreSQL. Pinecone Managed vector database for ML applications. PipelineAI ML pipeline and model deployment platform. Pipeshift AI-powered content moderation platform. Portkey AI gateway and observability platform. Predibase Fine-tuning platform for large language models. PredictionGuard AI model security and compliance platform. PreMAI AI platform for model deployment and management. Prolog Logic programming language integration. PromptLayer Prompt engineering and observability platform. Psychic Universal API for SaaS integrations. PubMed Biomedical literature database access. Pull MD Markdown content extraction and processing. PygmalionAI Conversational AI model platform. PyMuPDF4LLM PDF processing optimized for LLM ingestion. Qdrant Vector similarity search engine. Ragatouille RAG toolkit with ColBERT indexing. Rank BM25 BM25 ranking algorithm implementation. Ray Serve Scalable model serving framework. Rebuff Prompt injection detection and prevention. Social media platform integration and APIs. Redis In-memory data structure store and cache. Remembrall AI memory and context management. Replicate Cloud platform for running ML models. Roam Research and note-taking platform. Robocorp Python automation and RPA platform. Rockset Real-time analytics database platform. RunPod GPU cloud platform for AI workloads. Salesforce CRM platform and business automation. SambaNova AI platform with specialized hardware. SAP Enterprise software and AI solutions. ScrapeGraph AI-powered web scraping framework. Scrapeless Web scraping API and proxy service. SearchAPI Real-time search engine results API. SearX Privacy-respecting metasearch engine. SemaDB Vector database for semantic search. SerpAPI Google Search results scraping API. Shale Protocol Decentralized AI inference protocol. SingleStore Distributed database with vector capabilities. scikit-learn Machine learning library for Python. Slack Business communication and collaboration. Snowflake Cloud data platform and analytics. spaCy Industrial-strength NLP library. Spark Unified analytics engine for big data. SparkLLM iFlytek\u2019s multilingual language model. Spreedly Payment orchestration platform. SQLite Embedded relational database engine. StackExchange Q&A platform network integration. StarRocks High-performance analytical database. StochasticAI GPU cloud platform for ML acceleration. Streamlit Web app framework for data science. Stripe Online payment processing platform. Supabase Open-source Firebase alternative. SurrealDB Multi-model database for modern applications. Symbl.ai Nebula Conversation intelligence platform. Tableau Data visualization and business intelligence. Taiga Project management platform for agile teams. Tair Alibaba Cloud\u2019s in-memory database. Tavily AI-optimized search API for applications. Telegram Messaging platform and bot integration. Tencent Tencent Cloud AI services and models. TensorFlow Datasets Collection of ready-to-use datasets. TensorLake Data infrastructure for ML applications. TiDB Distributed SQL database platform. TigerGraph Scalable graph database and analytics. Tigris Globally distributed database platform. Tilores Entity resolution and data matching. Together Fast inference for open-source models. ToMarkdown HTML to Markdown conversion utility. Toolbox LangChain Extended toolkit for LangChain applications. Transwarp Big data platform and analytics suite. Trello Visual project management and collaboration. Trubrics LLM evaluation and analytics platform. TrueFoundry ML platform for model deployment. TrueLens Evaluation framework for LLM applications. Social media platform integration. Typesense Fast and typo-tolerant search engine. UnDatasIO Data extraction and processing platform. Unstructured Document processing and data extraction. Upstage Document AI and OCR platform. Upstash Serverless data platform for Redis and Kafka. UpTrain ML observability and evaluation platform. USearch Single-file vector search engine. Valthera AI platform for healthcare applications. Valyu AI-powered data analysis platform. VDMS Visual data management system. Vearch Distributed vector search engine. Vectara Neural search platform with built-in understanding. Vectorize Vector database and semantic search. Vespa Big data serving engine for vector search. VLite Simple vector database for embeddings. VoyageAI Embedding models and semantic search. Weights & Biases ML experiment tracking and collaboration. Weights & Biases Tracking Experiment tracking and model management. Weights & Biases Tracing LLM tracing and observability. Weather Weather data and forecasting services. Weaviate Open-source vector database with GraphQL. Messaging platform integration and automation. WhyLabs Profiling AI observability and data monitoring. Wikipedia Wikipedia content access and search. Wolfram Alpha Computational knowledge engine. Writer Enterprise AI writing platform. XAI xAI\u2019s Grok models for conversational AI. Xata Serverless database with vector search. Xinference Distributed inference framework for LLMs. Yahoo Yahoo services and data integration. Yandex Yandex AI services and language models. YDB Yandex Database distributed storage system. YeagerAI AI agent framework and development platform. Yellowbrick Data warehouse and analytics platform. Yi 01.AI\u2019s bilingual language models. You You.com search engine and AI platform. YouTube Video platform integration and content access. Zep Long-term memory for AI assistants. ZeusDB High-performance vector database. ZhipuAI ChatGLM and other Chinese language models. Zilliz Managed Milvus vector database service. Zotero Reference management and research tool.", "tokens": 735, "node_type": "child"}
{"id": 390, "chunk_id": "c16acf8a553ac24f5d77e7d3a1c6e493", "parent_id": 369, "url": "", "namespace": "langchain", "title": "oss-python-integrations-providers-aws", "headers": ["oss-python-integrations-providers-aws"], "section_index": 0, "chunk_index": 0, "text": "# oss-python-integrations-providers-aws > Source: https://docs.langchain.com/oss/python/integrations/providers/aws Chat models Bedrock Chat Amazon Bedrock is a fully managed service that offers a choice of high-performing foundation models (FMs) from leading AI companies likeSee a usage example.AI21 Labs ,Anthropic ,Cohere ,Meta ,Stability AI , andAmazon via a single API, along with a broad set of capabilities you need to build generative AI applications with security, privacy, and responsible AI. UsingAmazon Bedrock , you can easily experiment with and evaluate top FMs for your use case, privately customize them with your data using techniques such as fine-tuning andRetrieval Augmented Generation (RAG ), and build agents that execute tasks using your enterprise systems and data sources. SinceAmazon Bedrock is serverless, you don\u2019t have to manage any infrastructure, and you can securely integrate and deploy generative AI capabilities into your applications using the AWS services you are already familiar with. Bedrock Converse AWS Bedrock maintains a Converse API that provides a unified conversational interface for Bedrock models. This API does not yet support custom models. You can see a list of all models that are supported here. See a usage example. LLMs Bedrock See a usage example.Amazon API Gateway Amazon API Gateway is a fully managed service that makes it easy for developers to create, publish, maintain, monitor, and secure APIs at any scale. APIs act as the \u201cfront door\u201d for applications to access data, business logic, or functionality from your backend services. UsingSee a usage example.API Gateway , you can create RESTful APIs and WebSocket APIs that enable real-time two-way communication applications.API Gateway supports containerized and serverless workloads, as well as web applications.API Gateway handles all the tasks involved in accepting and processing up to hundreds of thousands of concurrent API calls, including traffic management, CORS support, authorization and access control, throttling, monitoring, and API version management.API Gateway has no minimum fees or startup costs. You pay for the API calls you receive and the amount of data transferred out and, with theAPI Gateway tiered pricing model, you can reduce your cost as your API usage scales. SageMaker Endpoint Amazon SageMaker is a system that can build, train, and deploy machine learning (ML) models with fully managed infrastructure, tools, and workflows.We use SageMaker to host our model and expose it as the SageMaker Endpoint . See a usage example. Embedding Models Bedrock See a usage example.SageMaker Endpoint See a usage example.Document loaders AWS S3 Directory and File Amazon Simple Storage Service (Amazon S3) is an object storage service. AWS S3 Directory AWS S3 BucketsSee a usage example for S3DirectoryLoader. See a usage example for S3FileLoader. Amazon Textract Amazon Textract is a machine learning (ML) service that automatically extracts text, handwriting, and data from scanned documents.See a usage example. Amazon Athena Amazon Athena is a serverless, interactive analytics service built on open-source frameworks, supporting open-table and file formats.See a usage example. AWS Glue The AWS Glue Data Catalog is a centralized metadata repository that allows you to manage, access, and share metadata about your data stored in AWS. It acts as a metadata store for your data assets, enabling various AWS services and your applications to query and connect to the data they need efficiently.See a usage example. Vector stores Amazon OpenSearch Service Amazon OpenSearch Service performs interactive log analytics, real-time application monitoring, website search, and more.We need to install several python libraries.OpenSearch is an open source, distributed search and analytics suite derived fromElasticsearch .Amazon OpenSearch Service offers the latest versions ofOpenSearch , support for many versions ofElasticsearch , as well as visualization capabilities powered byOpenSearch Dashboards andKibana . Amazon DocumentDB Vector Search Amazon DocumentDB (with MongoDB Compatibility) makes it easy to set up, operate, and scale MongoDB-compatible databases in the cloud. With Amazon DocumentDB, you can run the same application code and use the same drivers and tools that you use with MongoDB. Vector search for Amazon DocumentDB combines the flexibility and rich querying capability of a JSON-based document database with the power of vector search. Installation and Setup See detail configuration instructions. We need to install thepymongo python package. Deploy DocumentDB on AWS Amazon DocumentDB (with MongoDB Compatibility) is a fast, reliable, and fully managed database service. Amazon DocumentDB makes it easy to set up, operate, and scale MongoDB-compatible databases in the cloud. AWS offers services for computing, databases, storage, analytics, and other functionality. For an overview of all AWS services, see Cloud Computing with Amazon Web Services. See a usage example.Amazon MemoryDB Amazon MemoryDB is a durable, in-memory database service that delivers ultra-fast performance. MemoryDB is compatible with Redis OSS, a popular open source data store, enabling you to quickly build applications using the same flexible and friendly Redis OSS APIs, and commands that they already use today. InMemoryVectorStore class provides a vectorstore to connect with Amazon MemoryDB.Retrievers Amazon Kendra Amazon Kendra is an intelligent search service provided byAmazon Web Services (AWS ). It utilizes advanced natural language processing (NLP) and machine learning algorithms to enable powerful search capabilities across various data sources within an organization.Kendra is designed to help users find the information they need quickly and accurately, improving productivity and decision-making. With Kendra , we can search across a wide range of content types, including documents, FAQs, knowledge bases, manuals, and websites. It supports multiple languages and can understand complex queries, synonyms, and contextual meanings to provide highly relevant search results. We need to install the langchain-aws library. Amazon Bedrock (Knowledge Bases) Knowledge bases for Amazon Bedrock is anWe need to install theAmazon Web Services (AWS ) offering which lets you quickly build RAG applications by using your private data to customize foundation model response. langchain-aws library. Tools AWS Lambda We need to installAmazon AWS Lambda is a serverless computing service provided byAmazon Web Services (AWS ). It helps developers to build and run applications and services without provisioning or managing servers. This serverless architecture enables you to focus on writing and deploying code, while AWS automatically takes care of scaling, patching, and managing the infrastructure required to run your applications. boto3", "tokens": 1000, "node_type": "child"}
{"id": 391, "chunk_id": "9b0fd3b850698317bef57fa6e71a38ba", "parent_id": 369, "url": "", "namespace": "langchain", "title": "oss-python-integrations-providers-aws", "headers": ["oss-python-integrations-providers-aws"], "section_index": 0, "chunk_index": 1, "text": "Kendra , we can search across a wide range of content types, including documents, FAQs, knowledge bases, manuals, and websites. It supports multiple languages and can understand complex queries, synonyms, and contextual meanings to provide highly relevant search results. We need to install the langchain-aws library. Amazon Bedrock (Knowledge Bases) Knowledge bases for Amazon Bedrock is anWe need to install theAmazon Web Services (AWS ) offering which lets you quickly build RAG applications by using your private data to customize foundation model response. langchain-aws library. Tools AWS Lambda We need to installAmazon AWS Lambda is a serverless computing service provided byAmazon Web Services (AWS ). It helps developers to build and run applications and services without provisioning or managing servers. This serverless architecture enables you to focus on writing and deploying code, while AWS automatically takes care of scaling, patching, and managing the infrastructure required to run your applications. boto3 python library. Graphs Amazon Neptune Amazon Neptune is a high-performance graph analytics and serverless database for superior scalability and availability.For the Cypher and SPARQL integrations below, we need to install the langchain-aws library. Amazon Neptune with Cypher See a usage example.Amazon Neptune with SPARQL Callbacks Bedrock token usage SageMaker Tracking Amazon SageMaker is a fully managed service that is used to quickly and easily build, train and deploy machine learning (ML) models. Amazon SageMaker Experiments is a capability of Amazon SageMaker that lets you organize, track, compare and evaluate ML experiments and model versions. We need to install several python libraries. Chains Amazon Comprehend Moderation Chain Amazon Comprehend is a natural-language processing (NLP) service that uses machine learning to uncover valuable insights and connections in text.We need to install the boto3 and nltk libraries.", "tokens": 284, "node_type": "child"}
{"id": 392, "chunk_id": "c16acf8a553ac24f5d77e7d3a1c6e493", "parent_id": 370, "url": "", "namespace": "langchain", "title": "oss-python-integrations-providers-google", "headers": ["oss-python-integrations-providers-google"], "section_index": 0, "chunk_index": 0, "text": "# oss-python-integrations-providers-google > Source: https://docs.langchain.com/oss/python/integrations/providers/google Google Generative AI (Gemini API & AI Studio): Access Google Gemini models directly via the Gemini API. Use Google AI Studio for rapid prototyping and get started quickly with the langchain-google-genai package. This is often the best starting point for individual developers. Google Cloud (Vertex AI & other services): Access Gemini models, Vertex AI Model Garden and a wide range of cloud services (databases, storage, document AI, etc.) via the Google Cloud Platform. Use the langchain-google-vertexai package for Vertex AI models and specific packages (e.g., langchain-google-cloud-sql-pg, langchain-google-community) for other cloud services. This is ideal for developers already using Google Cloud or needing enterprise features like MLOps, specific model tuning or enterprise support. See Google\u2019s guide on migrating from the Gemini API to Vertex AI for more details on the differences.Integration packages for Gemini models and the Vertex AI platform are maintained in the langchain-google repository. You can find a host of LangChain integrations with other Google APIs and services in the googleapisGithub organization and the langchain-google-community package. Access Google Gemini models directly using the Gemini API, best suited for rapid development and experimentation. Gemini models are available in Google AI Studio. Use the ChatGoogleGenerativeAI class to interact with Gemini models. See details in this guide. Copy from langchain_google_genai import ChatGoogleGenerativeAIfrom langchain.messages import HumanMessagellm = ChatGoogleGenerativeAI(model=\"gemini-2.5-flash\")# Simple text invocationresult = llm.invoke(\"Sing a ballad of LangChain.\")print(result.content)# Multimodal invocation with gemini-pro-visionmessage = HumanMessage( content=[ { \"type\": \"text\", \"text\": \"What's in this image?\", }, {\"type\": \"image_url\", \"image_url\": \"https://picsum.photos/seed/picsum/200/300\"}, ])result = llm.invoke([message])print(result.content) The image_url can be a public URL, a GCS URI (gs://...), a local file path, a base64 encoded image string (data:image/png;base64,...), or a PIL Image object. Generate text embeddings using models like gemini-embedding-001 with the GoogleGenerativeAIEmbeddings class.See a usage example. Copy from langchain_google_genai import GoogleGenerativeAIEmbeddingsembeddings = GoogleGenerativeAIEmbeddings(model=\"models/gemini-embedding-001\")vector = embeddings.embed_query(\"What are embeddings?\")print(vector[:5]) Access the same Gemini models using the (legacy) LLM interface with the GoogleGenerativeAI class.See a usage example. Copy from langchain_google_genai import GoogleGenerativeAIllm = GoogleGenerativeAI(model=\"gemini-2.5-flash\")result = llm.invoke(\"Sing a ballad of LangChain.\")print(result) Access Gemini models, Vertex AI Model Garden and other Google Cloud services via Vertex AI and specific cloud integrations.Vertex AI models require the langchain-google-vertexai package. Other services might require additional packages like langchain-google-community, langchain-google-cloud-sql-pg, etc. Copy pip install langchain-google-vertexai# pip install langchain-google-community[...] # For other services Google Cloud integrations typically use Application Default Credentials (ADC). Refer to the Google Cloud authentication documentation for setup instructions (e.g., using gcloud auth application-default login). Google Cloud Document AI is a Google Cloud service that transforms unstructured data from documents into structured data, making it easier to understand, analyze, and consume. We need to set up a GCS bucket and create your own OCR processor The GCS_OUTPUT_PATH should be a path to a folder on GCS (starting with gs://) and a processor name should look like projects/PROJECT_NUMBER/locations/LOCATION/processors/PROCESSOR_ID. We can get it either programmatically or copy from the Prediction endpoint section of the Processor details tab in the Google Cloud Console. Google Translate is a multilingual neural machine translation service developed by Google to translate text, documents and websites from one language into another. The GoogleTranslateTransformer allows you to translate text and HTML with the Google Cloud Translation API.First, we need to install the langchain-google-community with translate dependencies. Google Cloud AlloyDB is a fully managed relational database service that offers high performance, seamless integration, and impressive scalability on Google Cloud. AlloyDB is 100% compatible with PostgreSQL. Google Cloud BigQuery, BigQuery is a serverless and cost-effective enterprise data warehouse in Google Cloud.Google Cloud BigQuery Vector Search BigQuery vector search lets you use GoogleSQL to do semantic search, using vector indexes for fast but approximate results, or using brute force for exact results. It can calculate Euclidean or Cosine distance. With LangChain, we default to use Euclidean distance. # Note: BigQueryVectorSearch might be in langchain or langchain_community depending on version# Check imports in the usage example.from langchain.vectorstores import BigQueryVectorSearch # Or langchain_community.vectorstores Google Cloud Vertex AI Vector Search from Google Cloud, formerly known as Vertex AI Matching Engine, provides the industry\u2019s leading high-scale low latency vector database. These vector databases are commonly referred to as vector similarity-matching or an approximate nearest neighbor (ANN) service. Build generative AI powered search engines using Vertex AI Search. from Google Cloud allows developers to quickly build generative AI powered search engines for customers and employees. See a usage example.Note: GoogleVertexAISearchRetriever is deprecated. Use the components below from langchain-google-community.Install the google-cloud-discoveryengine package for underlying access. from langchain_google_community import VertexAIMultiTurnSearchRetriever VertexAISearchRetriever Copy # Note: The example code shows VertexAIMultiTurnSearchRetriever, confirm if VertexAISearchRetriever is separate or related.# Assuming it might be related or a typo in the original doc:from langchain_google_community import VertexAISearchRetriever # Verify class name if needed VertexAISearchSummaryTool Copy from langchain_google_community import VertexAISearchSummaryTool Note: GoogleDocumentAIWarehouseRetriever (from langchain) is deprecated. Use DocumentAIWarehouseRetriever from langchain-google-community.Requires installation of relevant Document AI packages (check specific docs). Copy pip install langchain-google-community # Add specific docai dependencies if needed Copy from langchain_google_community.documentai_warehouse import DocumentAIWarehouseRetriever Google Cloud Text-to-Speech is a Google Cloud service that enables developers to synthesize natural-sounding speech with 100+ voices, available in multiple languages and variants. It applies DeepMind\u2019s groundbreaking research in WaveNet and Google\u2019s powerful neural networks to deliver the highest fidelity possible. from langchain_community.tools.google_jobs import GoogleJobsQueryRun# Note: Utilities might be shared, e.g., GoogleFinanceAPIWrapper was listed, verify correct utility# from langchain_community.utilities.google_jobs import GoogleJobsAPIWrapper # If exists # Note: GooglePlacesTool might be in langchain or langchain_community depending on versionfrom langchain.tools import GooglePlacesTool # Or langchain_community.tools MCP Toolbox provides a simple and efficient way to connect to your databases, including those on Google Cloud like Cloud SQL and AlloyDB. With MCP Toolbox, you can seamlessly integrate your database with LangChain to build powerful, data-driven applications. Evaluate a single prediction string using Vertex AI models. Copy # Note: Original doc listed VertexPairWiseStringEvaluator twice. Assuming this class exists.from langchain_google_vertexai.evaluators.evaluation import VertexStringEvaluator # Verify class name if needed Google ScaNN (Scalable Nearest Neighbors) is a python package.ScaNN is a method for efficient vector similarity search at scale. ScaNN includes search space pruning and quantization for Maximum Inner Product", "tokens": 1000, "node_type": "child"}
{"id": 393, "chunk_id": "9b0fd3b850698317bef57fa6e71a38ba", "parent_id": 370, "url": "", "namespace": "langchain", "title": "oss-python-integrations-providers-google", "headers": ["oss-python-integrations-providers-google"], "section_index": 0, "chunk_index": 1, "text": "Google\u2019s powerful neural networks to deliver the highest fidelity possible. from langchain_community.tools.google_jobs import GoogleJobsQueryRun# Note: Utilities might be shared, e.g., GoogleFinanceAPIWrapper was listed, verify correct utility# from langchain_community.utilities.google_jobs import GoogleJobsAPIWrapper # If exists # Note: GooglePlacesTool might be in langchain or langchain_community depending on versionfrom langchain.tools import GooglePlacesTool # Or langchain_community.tools MCP Toolbox provides a simple and efficient way to connect to your databases, including those on Google Cloud like Cloud SQL and AlloyDB. With MCP Toolbox, you can seamlessly integrate your database with LangChain to build powerful, data-driven applications. Evaluate a single prediction string using Vertex AI models. Copy # Note: Original doc listed VertexPairWiseStringEvaluator twice. Assuming this class exists.from langchain_google_vertexai.evaluators.evaluation import VertexStringEvaluator # Verify class name if needed Google ScaNN (Scalable Nearest Neighbors) is a python package.ScaNN is a method for efficient vector similarity search at scale. ScaNN includes search space pruning and quantization for Maximum Inner Product Search and also supports other distance functions such as Euclidean distance. The implementation is optimized for x86 processors with AVX2 support. See its Google Research github for more details. from langchain_community.tools.google_jobs import GoogleJobsQueryRun# Note: Utilities might be shared, e.g., GoogleFinanceAPIWrapper was listed, verify correct utility# from langchain_community.utilities.google_jobs import GoogleJobsAPIWrapper # If exists # Note: GooglePlacesTool might be in langchain or langchain_community depending on versionfrom langchain.tools import GooglePlacesTool # Or langchain_community.tools from langchain_community.document_loaders.blob_loaders.youtube_audio import YoutubeAudioLoader# Often used with whisper parsers:# from langchain_community.document_loaders.parsers import OpenAIWhisperParser, OpenAIWhisperParserLocal", "tokens": 234, "node_type": "child"}
{"id": 394, "chunk_id": "c16acf8a553ac24f5d77e7d3a1c6e493", "parent_id": 371, "url": "", "namespace": "langchain", "title": "oss-python-integrations-providers-huggingface", "headers": ["oss-python-integrations-providers-huggingface"], "section_index": 0, "chunk_index": 0, "text": "# oss-python-integrations-providers-huggingface\n\n> Source: https://docs.langchain.com/oss/python/integrations/providers/huggingface\n\nSkip to main content\nAll LangChain integrations with Hugging Face Hub and libraries like transformers , sentence transformers , and datasets .\nChat models\nChatHuggingFace\nWe can use the Hugging Face\nLLM classes or directly use the ChatHuggingFace\nclass.\nSee a usage example .\nLLMs\nHuggingFaceEndpoint\nWe can use the HuggingFaceEndpoint\nclass to run open source models via serverless Inference Providers or via dedicated Inference Endpoints .\nSee a usage example .\nHuggingFacePipeline\nWe can use the HuggingFacePipeline\nclass to run open source models locally.\nSee a usage example .\nEmbedding Models\nHuggingFaceEmbeddings\nWe can use the HuggingFaceEmbeddings\nclass to run open source embedding models locally.\nSee a usage example .\nHuggingFaceEndpointEmbeddings\nWe can use the HuggingFaceEndpointEmbeddings\nclass to run open source embedding models via a dedicated Inference Endpoint .\nSee a usage example .\nHuggingFaceInferenceAPIEmbeddings\nWe can use the HuggingFaceInferenceAPIEmbeddings\nclass to run open source embedding models via Inference Providers .\nSee a usage example .\nHuggingFaceInstructEmbeddings\nWe can use the HuggingFaceInstructEmbeddings\nclass to run open source embedding models locally.\nSee a usage example .\nHuggingFaceBgeEmbeddings\nBGE models on the HuggingFace are one of the best open-source embedding models .\nBGE model is created by the Beijing Academy of Artificial Intelligence (BAAI) . BAAI\nis a private non-profit organization engaged in AI research and development.\nSee a usage example .\nDocument loaders\nHugging Face dataset\nHugging Face Hub is home to over 75,000\ndatasets in more than 100 languages\nthat can be used for a broad range of tasks across NLP, Computer Vision, and Audio.\nThey used for a diverse range of tasks such as translation, automatic speech\nrecognition, and image classification.\nWe need to install datasets\npython package.\nSee a usage example .\nHugging Face model loader\nLoad model information from Hugging Face Hub\n, including README content.\nThis loader interfaces with the Hugging Face Models API\nto fetch\nand load model metadata and README files.\nThe API allows you to search and filter models based on\nspecific criteria such as model tags, authors, and more.\nImage captions\nIt uses the Hugging Face models to generate image captions.\nWe need to install several python packages.\nSee a usage example .\nHugging Face Tools\nsupport text I/O and are loaded using the load_huggingface_tool\nfunction.\nWe need to install several python packages.\nSee a usage example .\nHugging Face Text-to-Speech Model Inference.\nIt is a wrapper around OpenAI Text-to-Speech API\n.", "tokens": 406, "node_type": "child"}
{"id": 395, "chunk_id": "c16acf8a553ac24f5d77e7d3a1c6e493", "parent_id": 372, "url": "", "namespace": "langchain", "title": "oss-python-integrations-providers-microsoft", "headers": ["oss-python-integrations-providers-microsoft"], "section_index": 0, "chunk_index": 0, "text": "# oss-python-integrations-providers-microsoft > Source: https://docs.langchain.com/oss/python/integrations/providers/microsoft Microsoft Azure, often referred to as Azure is a cloud computing platform run by Microsoft, which offers access, management, and development of applications and services through global data centers. It provides a range of capabilities, including software as a service (SaaS), platform as a service (PaaS), and infrastructure as a service (IaaS). Microsoft Azure supports many programming languages, tools, and frameworks, including Microsoft-specific and third-party software and systems. Azure OpenAI is an Azure service with powerful language models from OpenAI including the GPT-3, Codex and Embeddings model series for content generation, summarization, semantic search, and natural language to code translation. Copy pip install langchain-openai Set the environment variables to get access to the Azure OpenAI service. Azure AI Foundry provides access to a wide range of models from various providers including Azure OpenAI, DeepSeek R1, Cohere, Phi and Mistral through the AzureAIChatCompletionsModel class. Azure AI Document Intelligence (formerly known as Azure Form Recognizer) is machine-learning based service that extracts texts (including handwriting), tables, document structures, and key-value-pairs from digital or scanned PDFs, images, Office and HTML files.Document Intelligence supports PDF, JPEG/JPG, PNG, BMP, TIFF, HEIF, DOCX, XLSX, PPTX and HTML. Azure Blob Storage is Microsoft\u2019s object storage solution for the cloud. Blob Storage is optimized for storing massive amounts of unstructured data. Unstructured data is data that doesn\u2019t adhere to a particular data model or definition, such as text or binary data. Azure Files offers fully managed file shares in the cloud that are accessible via the industry standard Server Message Block (SMB) protocol, Network File System (NFS) protocol, and Azure Files REST API. Azure Files are based on the Azure Blob Storage. Azure Blob Storage is designed for: Serving images or documents directly to a browser. Storing files for distributed access. Streaming video and audio. Writing to log files. Storing data for backup and restore, disaster recovery, and archiving. Storing data for analysis by an on-premises or Azure-hosted service. Microsoft Excel is a spreadsheet editor developed by Microsoft for Windows, macOS, Android, iOS and iPadOS. It features calculation or computation capabilities, graphing tools, pivot tables, and a macro programming language called Visual Basic for Applications (VBA). Excel forms part of the Microsoft 365 suite of software. The UnstructuredExcelLoader is used to load Microsoft Excel files. The loader works with both .xlsx and .xls files. The page content will be the raw text of the Excel file. If you use the loader in \"elements\" mode, an HTML representation of the Excel file will be available in the document metadata under the text_as_html key.See a usage example. Copy from langchain_community.document_loaders import UnstructuredExcelLoader Microsoft SharePoint is a website-based collaboration system that uses workflow applications, \u201clist\u201d databases, and other web parts and security features to empower business teams to work together developed by Microsoft. Playwright is an open-source automation tool developed by Microsoft that allows you to programmatically control and automate web browsers. It is designed for end-to-end testing, scraping, and automating tasks across various web browsers such as Chromium, Firefox, and WebKit. AI agents can rely on Azure Cosmos DB as a unified memory system solution, enjoying speed, scale, and simplicity. This service successfully enabled OpenAI\u2019s ChatGPT service to scale dynamically with high reliability and low maintenance. Powered by an atom-record-sequence engine, it is the world\u2019s first globally distributed NoSQL, relational, and vector database service that offers a serverless mode.Below are two available Azure Cosmos DB APIs that can provide vector store functionalities. Azure Cosmos DB for MongoDB vCore makes it easy to create a database with full native MongoDB support. You can apply your MongoDB experience and continue to use your favorite MongoDB drivers, SDKs, and tools by pointing your application to the API for MongoDB vCore account\u2019s connection string. Use vector search in Azure Cosmos DB for MongoDB vCore to seamlessly integrate your AI-based applications with your data that\u2019s stored in Azure Cosmos DB. Azure Cosmos DB for MongoDB vCore provides developers with a fully managed MongoDB-compatible database service for building modern applications with a familiar architecture.With Cosmos DB for MongoDB vCore, developers can enjoy the benefits of native Azure integrations, low total cost of ownership (TCO), and the familiar vCore architecture when migrating existing applications or building new ones.Sign Up for free to get started today.See a usage example. Copy from langchain_community.vectorstores import AzureCosmosDBVectorSearch Azure Cosmos DB for NoSQL now offers vector indexing and search in preview. This feature is designed to handle high-dimensional vectors, enabling efficient and accurate vector search at any scale. You can now store vectors directly in the documents alongside your data. This means that each document in your database can contain not only traditional schema-free data, but also high-dimensional vectors as other properties of the documents. This colocation of data and vectors allows for efficient indexing and searching, as the vectors are stored in the same logical unit as the data they represent. This simplifies data management, AI application architectures, and the efficiency of vector-based operations. Azure Cosmos DB offers a solution for modern apps and intelligent workloads by being very responsive with dynamic and elastic autoscale. It is available in every Azure region and can automatically replicate data closer to users. It has SLA guaranteed low-latency and high availability.Sign Up for free to get started today.See a usage example. Copy from langchain_community.vectorstores import AzureCosmosDBNoSQLVectorSearch Azure Database for PostgreSQL - Flexible Server is a relational database service based on the open-source Postgres database engine. It\u2019s a fully managed database-as-a-service that can handle mission-critical workloads with predictable performance, security, high availability, and dynamic scalability. See set up instructions for Azure Database for PostgreSQL.Simply use the connection string from your Azure Portal.Since Azure Database for PostgreSQL is open-source Postgres, you can use the LangChain\u2019s Postgres support to connect to Azure Database for PostgreSQL. Azure SQL Database is a robust service that combines scalability, security, and high availability, providing all the benefits of a modern database solution. It also provides a dedicated Vector data type & built-in functions that simplifies the storage", "tokens": 1000, "node_type": "child"}
{"id": 396, "chunk_id": "9b0fd3b850698317bef57fa6e71a38ba", "parent_id": 372, "url": "", "namespace": "langchain", "title": "oss-python-integrations-providers-microsoft", "headers": ["oss-python-integrations-providers-microsoft"], "section_index": 0, "chunk_index": 1, "text": "available in every Azure region and can automatically replicate data closer to users. It has SLA guaranteed low-latency and high availability.Sign Up for free to get started today.See a usage example. Copy from langchain_community.vectorstores import AzureCosmosDBNoSQLVectorSearch Azure Database for PostgreSQL - Flexible Server is a relational database service based on the open-source Postgres database engine. It\u2019s a fully managed database-as-a-service that can handle mission-critical workloads with predictable performance, security, high availability, and dynamic scalability. See set up instructions for Azure Database for PostgreSQL.Simply use the connection string from your Azure Portal.Since Azure Database for PostgreSQL is open-source Postgres, you can use the LangChain\u2019s Postgres support to connect to Azure Database for PostgreSQL. Azure SQL Database is a robust service that combines scalability, security, and high availability, providing all the benefits of a modern database solution. It also provides a dedicated Vector data type & built-in functions that simplifies the storage and querying of vector embeddings directly within a relational database. This eliminates the need for separate vector databases and related integrations, increasing the security of your solutions while reducing the overall complexity. By leveraging your current SQL Server databases for vector search, you can enhance data capabilities while minimizing expenses and avoiding the challenges of transitioning to new systems. Azure AI Search is a cloud search service that gives developers infrastructure, APIs, and tools for information retrieval of vector, keyword, and hybrid queries at scale. See here for usage examples. Copy from langchain_community.vectorstores.azuresearch import AzureSearch Azure AI Search (formerly known as Azure Search or Azure Cognitive Search ) is a cloud search service that gives developers infrastructure, APIs, and tools for building a rich search experience over private, heterogeneous content in web, mobile, and enterprise applications. Search is foundational to any app that surfaces text to users, where common scenarios include catalog or document search, online retail apps, or data exploration over proprietary content. When you create a search service, you\u2019ll work with the following capabilities: A search engine for full text search over a search index containing user-owned content Rich indexing, with lexical analysis and optional AI enrichment for content extraction and transformation Rich query syntax for text search, fuzzy search, autocomplete, geo-search and more Programmability through REST APIs and client libraries in Azure SDKs Azure integration at the data layer, machine learning layer, and AI (AI Services) Azure Database for PostgreSQL - Flexible Server is a relational database service based on the open-source Postgres database engine. It\u2019s a fully managed database-as-a-service that can handle mission-critical workloads with predictable performance, security, high availability, and dynamic scalability. We need to get the POOL_MANAGEMENT_ENDPOINT environment variable from the Azure Container Apps service. See the instructions here.We need to install a python package. Follow the documentation here to get a detail explanations and instructions of this tool.The environment variable BING_SUBSCRIPTION_KEY and BING_SEARCH_URL are required from Bing Search resource. Playwright is an open-source automation tool developed by Microsoft that allows you to programmatically control and automate web browsers. It is designed for end-to-end testing, scraping, and automating tasks across various web browsers such as Chromium, Firefox, and WebKit. Presidio (Origin from Latin praesidium \u2018protection, garrison\u2019) helps to ensure sensitive data is properly managed and governed. It provides fast identification and anonymization modules for private entities in text and images such as credit card numbers, names, locations, social security numbers, bitcoin wallets, US phone numbers, financial data and more. First, you need to install several python packages and download a SpaCy model.", "tokens": 578, "node_type": "child"}
{"id": 397, "chunk_id": "c16acf8a553ac24f5d77e7d3a1c6e493", "parent_id": 376, "url": "", "namespace": "langchain", "title": "oss-python-integrations-retrievers-index", "headers": ["oss-python-integrations-retrievers-index"], "section_index": 0, "chunk_index": 0, "text": "# oss-python-integrations-retrievers-index\n\n> Source: https://docs.langchain.com/oss/python/integrations/retrievers/index\n\nBring-your-own documents\nThe below retrievers allow you to index and search a custom corpus of documents.| Retriever | Self-host | Cloud offering | Package |\n|---|---|---|---|\nAmazonKnowledgeBasesRetriever | \u274c | \u2705 | langchain-aws |\nAzureAISearchRetriever | \u274c | \u2705 | langchain-community |\nElasticsearchRetriever | \u2705 | \u2705 | langchain-elasticsearch |\nVertexAISearchRetriever | \u274c | \u2705 | langchain-google-community |\nExternal index\nThe below retrievers will search over an external index (e.g., constructed from Internet data or similar).| Retriever | Source | Package |\n|---|---|---|\nArxivRetriever | Scholarly articles on arxiv.org | langchain-community |\nTavilySearchAPIRetriever | Internet search | langchain-community |\nWikipediaRetriever | Wikipedia articles | langchain-community |\nAll retrievers\nNote: The descriptions in the table below are truncated for readability.", "tokens": 124, "node_type": "child"}
{"id": 398, "chunk_id": "c16acf8a553ac24f5d77e7d3a1c6e493", "parent_id": 377, "url": "", "namespace": "langchain", "title": "oss-python-integrations-splitters-index", "headers": ["oss-python-integrations-splitters-index"], "section_index": 0, "chunk_index": 0, "text": "# oss-python-integrations-splitters-index\n\n> Source: https://docs.langchain.com/oss/python/integrations/splitters/index\n\nFor most use cases, start with the RecursiveCharacterTextSplitter. It provides a solid balance between keeping context intact and managing chunk size. This default strategy works well out of the box, and you should only consider adjusting it if you need to fine-tune performance for your specific application.\nText structure-based\nText is naturally organized into hierarchical units such as paragraphs, sentences, and words. We can leverage this inherent structure to inform our splitting strategy, creating split that maintain natural language flow, maintain semantic coherence within split, and adapts to varying levels of text granularity. LangChain\u2019sRecursiveCharacterTextSplitter\nimplements this concept:\n- The RecursiveCharacterTextSplitter attempts to keep larger units (e.g., paragraphs) intact.\n- If a unit exceeds the chunk size, it moves to the next level (e.g., sentences).\n- This process continues down to the word level if necessary.\nLength-based\nAn intuitive strategy is to split documents based on their length. This simple yet effective approach ensures that each chunk doesn\u2019t exceed a specified size limit. Key benefits of length-based splitting:- Straightforward implementation\n- Consistent chunk sizes\n- Easily adaptable to different model requirements\n- Token-based: Splits text based on the number of tokens, which is useful when working with language models.\n- Character-based: Splits text based on the number of characters, which can be more consistent across different types of text.\nDocument structure-based\nSome documents have an inherent structure, such as HTML, Markdown, or JSON files. In these cases, it\u2019s beneficial to split the document based on its structure, as it often naturally groups semantically related text. Key benefits of structure-based splitting:- Preserves the logical organization of the document\n- Maintains context within each chunk\n- Can be more effective for downstream tasks like retrieval or summarization\n- Markdown: Split based on headers (e.g., #, ##, ###)\n- HTML: Split using tags\n- JSON: Split by object or array elements\n- Code: Split by functions, classes, or logical blocks", "tokens": 321, "node_type": "child"}
{"id": 399, "chunk_id": "c16acf8a553ac24f5d77e7d3a1c6e493", "parent_id": 378, "url": "", "namespace": "langchain", "title": "oss-python-integrations-stores-index", "headers": ["oss-python-integrations-stores-index"], "section_index": 0, "chunk_index": 0, "text": "# oss-python-integrations-stores-index\n\n> Source: https://docs.langchain.com/oss/python/integrations/stores/index\n\nOverview\nLangChain provides a key-value store interface for storing and retrieving data by key. The key-value store interface in LangChain is primarily used for caching embeddings.Interface\nAllBaseStores\nsupport the following interface:\nmget(key: Sequence[str]) -> List[Optional[bytes]]\n: get the contents of multiple keys, returningNone\nif the key does not existmset(key_value_pairs: Sequence[Tuple[str, bytes]]) -> None\n: set the contents of multiple keysmdelete(key: Sequence[str]) -> None\n: delete multiple keysyield_keys(prefix: Optional[str] = None) -> Iterator[str]\n: yield all keys in the store, optionally filtering by a prefix\nBase stores are designed to work multiple key-value pairs at once for efficiency. This saves on network round-trips and may allow for more efficient batch operations in the underlying store.\nBuilt-in stores for local development\nCustom stores\nYou can also implement your own custom store by extending theBaseStore\nclass. See the store interface documentation for more details.", "tokens": 146, "node_type": "child"}
{"id": 400, "chunk_id": "c16acf8a553ac24f5d77e7d3a1c6e493", "parent_id": 379, "url": "", "namespace": "langchain", "title": "oss-python-integrations-text-embedding-index", "headers": ["oss-python-integrations-text-embedding-index"], "section_index": 0, "chunk_index": 0, "text": "# oss-python-integrations-text-embedding-index\n\n> Source: https://docs.langchain.com/oss/python/integrations/text_embedding/index\n\nOverview\nThis overview covers text-based embedding models. LangChain does not currently support multimodal embeddings.\nHow it works\n- Vectorization \u2014 The model encodes each input string as a high-dimensional vector.\n- Similarity scoring \u2014 Vectors are compared using mathematical metrics to measure how closely related the underlying texts are.\nSimilarity metrics\nSeveral metrics are commonly used to compare embeddings:- Cosine similarity \u2014 measures the angle between two vectors.\n- Euclidean distance \u2014 measures the straight-line distance between points.\n- Dot product \u2014 measures how much one vector projects onto another.\nInterface\nLangChain provides a standard interface for text embedding models (e.g., OpenAI, Cohere, Hugging Face) via the Embeddings interface. Two main methods are available:embed_documents(texts: List[str]) \u2192 List[List[float]]\n: Embeds a list of documents.embed_query(text: str) \u2192 List[float]\n: Embeds a single query.\nThe interface allows queries and documents to be embedded with different strategies, though most providers handle them the same way in practice.\nTop integrations\nCaching\nEmbeddings can be stored or temporarily cached to avoid needing to recompute them. Caching embeddings can be done using aCacheBackedEmbeddings\n. This wrapper stores embeddings in a key-value store, where the text is hashed and the hash is used as the key in the cache.\nThe main supported way to initialize a CacheBackedEmbeddings\nis from_bytes_store\n. It takes the following parameters:\nunderlying_embedder\n: The embedder to use for embedding.document_embedding_cache\n: AnyByteStore\nfor caching document embeddings.batch_size\n: (optional, defaults toNone\n) The number of documents to embed between store updates.namespace\n: (optional, defaults to\"\"\n) The namespace to use for the document cache. Helps avoid collisions (e.g., set it to the embedding model name).query_embedding_cache\n: (optional, defaults toNone\n) AByteStore\nfor caching query embeddings, orTrue\nto reuse the same store asdocument_embedding_cache\n.", "tokens": 292, "node_type": "child"}
{"id": 401, "chunk_id": "c16acf8a553ac24f5d77e7d3a1c6e493", "parent_id": 380, "url": "", "namespace": "langchain", "title": "oss-python-integrations-tools-index", "headers": ["oss-python-integrations-tools-index"], "section_index": 0, "chunk_index": 0, "text": "# oss-python-integrations-tools-index\n\n> Source: https://docs.langchain.com/oss/python/integrations/tools/index\n\nSearch\nThe following table shows tools that execute online searches in some shape or form:| Tool/Toolkit | Free/Paid | Return Data |\n|---|---|---|\n| Bing Search | Paid | URL, Snippet, Title |\n| Brave Search | Free | URL, Snippet, Title |\n| DuckDuckgoSearch | Free | URL, Snippet, Title |\n| Exa Search | 1000 free searches/month | URL, Author, Title, Published Date |\n| Google Search | Paid | URL, Snippet, Title |\n| Google Serper | Free | URL, Snippet, Title, Search Rank, Site Links |\n| Jina Search | 1M Response Tokens Free | URL, Snippet, Title, Page Content |\n| Mojeek Search | Paid | URL, Snippet, Title |\n| SearchApi | 100 Free Searches on Sign Up | URL, Snippet, Title, Search Rank, Site Links, Authors |\n| SearxNG Search | Free | URL, Snippet, Title, Category |\n| SerpAPI | 100 Free Searches/Month | Answer |\n| Tavily Search | 1000 free searches/month | URL, Content, Title, Images, Answer |\n| You.com Search | Free for 60 days | URL, Title, Page Content |\nCode Interpreter\nThe following table shows tools that can be used as code interpreters:| Tool/Toolkit | Supported Languages | Sandbox Lifetime | Supports File Uploads | Return Types | Supports Self-Hosting |\n|---|---|---|---|---|---|\n| Azure Container Apps dynamic sessions | Python | 1 Hour | \u2705 | Text, Images | \u274c |\n| Bearly Code Interpreter | Python | Resets on Execution | \u2705 | Text | \u274c |\n| Riza Code Interpreter | Python, JavaScript, PHP, Ruby | Resets on Execution | \u2705 | Text | \u2705 |\nProductivity\nThe following table shows tools that can be used to automate tasks in productivity tools:| Tool/Toolkit | Pricing |\n|---|---|\n| Github Toolkit | Free |\n| Gitlab Toolkit | Free for personal project |\n| Gmail Toolkit | Free, with limit of 250 quota units per user per second |\n| Infobip Tool | Free trial, with variable pricing after |\n| Jira Toolkit | Free, with rate limits |\n| Office365 Toolkit | Free with Office365, includes rate limits |\n| Slack Toolkit | Free |\n| Twilio Tool | Free trial, with pay-as-you-go pricing after |\nWeb Browsing\nThe following table shows tools that can be used to automate tasks in web browsers:| Tool/Toolkit | Pricing | Supports Interacting with the Browser |\n|---|---|---|\n| AgentQL Toolkit | Free trial, with pay-as-you-go and flat rate plans after | \u2705 |\n| Hyperbrowser Browser Agent Tools | Free trial, with flat rate plans and pre-paid credits after | \u2705 |\n| Hyperbrowser Web Scraping Tools | Free trial, with flat rate plans and pre-paid credits after | \u274c |\n| MultiOn Toolkit | 40 free requests/day | \u2705 |\n| Oxylabs Web Scraper API | Free trial, with flat rate plans and pre-paid credits after | \u274c |\n| PlayWright Browser Toolkit | Free | \u2705 |\n| Requests Toolkit | Free | \u274c |\nDatabase\nThe following table shows tools that can be used to automate tasks in databases:| Tool/Toolkit | Allowed Operations |\n|---|---|\n| Cassandra Database Toolkit | SELECT and schema introspection |\n| MCP Toolbox | Any SQL operation |\n| SQLDatabase Toolkit | Any SQL operation |\n| Spark SQL Toolkit | Any SQL operation |\nFinance\nThe following table shows tools that can be used to execute financial transactions such as payments, purchases, and more:| Tool/Toolkit | Pricing | Capabilities |\n|---|---|---|\n| GOAT | Free | Create and receive payments, purchase physical goods, make investments, and more. |", "tokens": 604, "node_type": "child"}
{"id": 402, "chunk_id": "c16acf8a553ac24f5d77e7d3a1c6e493", "parent_id": 381, "url": "", "namespace": "langchain", "title": "oss-python-integrations-vectorstores-index", "headers": ["oss-python-integrations-vectorstores-index"], "section_index": 0, "chunk_index": 0, "text": "# oss-python-integrations-vectorstores-index\n\n> Source: https://docs.langchain.com/oss/python/integrations/vectorstores/index\n\nOverview\nA vector store stores embedded data and performs similarity search.Interface\nLangChain provides a unified interface for vector stores, allowing you to:add_documents\n- Add documents to the store.delete\n- Remove stored documents by ID.similarity_search\n- Query for semantically similar documents.\nInitialization\nTo initialize a vector store, provide it with an embedding model:Adding documents\nAddDocument\nobjects (holding page_content\nand optional metadata) like so:\nDeleting documents\nDelete by specifying IDs:Similarity search\nIssue a semantic query usingsimilarity_search\n, which returns the closest embedded documents:\nk\n\u2014 number of results to returnfilter\n\u2014 conditional filtering based on metadata\nSimilarity metrics & indexing\nEmbedding similarity may be computed using:- Cosine similarity\n- Euclidean distance\n- Dot product\nMetadata filtering\nFiltering by metadata (e.g., source, date) can refine search results:Top integrations\nSelect embedding model:OpenAI\nOpenAI\nAzure\nAzure\nGoogle Gemini\nGoogle Gemini\nGoogle Vertex\nGoogle Vertex\nAWS\nAWS\nHuggingFace\nHuggingFace\nOllama\nOllama\nCohere\nCohere\nMistral AI\nMistral AI\nNomic\nNomic\nNVIDIA\nNVIDIA\nVoyage AI\nVoyage AI\nIBM watsonx\nIBM watsonx\nFake\nFake\nxAI\nxAI\nPerplexity\nPerplexity\nDeepSeek\nDeepSeek\nIn-memory\nIn-memory\nAstraDB\nAstraDB\nChroma\nChroma\nFAISS\nFAISS\nMilvus\nMilvus\nMongoDB\nMongoDB\nPGVector\nPGVector\nPGVectorStore\nPGVectorStore\nPinecone\nPinecone\nQdrant\nQdrant\n| Vectorstore | Delete by ID | Filtering | Search by Vector | Search with score | Async | Passes Standard Tests | Multi Tenancy | IDs in add Documents |\n|---|---|---|---|---|---|---|---|---|\nAstraDBVectorStore | \u2705 | \u2705 | \u2705 | \u2705 | \u2705 | \u274c | \u274c | \u2705 |\nChroma | \u2705 | \u2705 | \u2705 | \u2705 | \u2705 | \u2705 | \u2705 | \u2705 |\nClickhouse | \u2705 | \u2705 | \u274c | \u2705 | \u274c | \u274c | \u274c | \u2705 |\nCouchbaseSearchVectorStore | \u2705 | \u2705 | \u2705 | \u2705 | \u2705 | \u274c | \u2705 | \u2705 |\nDatabricksVectorSearch | \u2705 | \u2705 | \u2705 | \u2705 | \u2705 | \u274c | \u274c | \u2705 |\nElasticsearchStore | \u2705 | \u2705 | \u2705 | \u2705 | \u2705 | \u274c | \u274c | \u2705 |\nFAISS | \u2705 | \u2705 | \u2705 | \u2705 | \u2705 | \u274c | \u274c | \u2705 |\nInMemoryVectorStore | \u2705 | \u2705 | \u274c | \u2705 | \u2705 | \u274c | \u274c | \u2705 |\nMilvus | \u2705 | \u2705 | \u2705 | \u2705 | \u2705 | \u2705 | \u2705 | \u2705 |\nMoorcheh | \u2705 | \u2705 | \u2705 | \u2705 | \u2705 | \u2705 | \u2705 | \u2705 |\nMongoDBAtlasVectorSearch | \u2705 | \u2705 | \u2705 | \u2705 | \u2705 | \u2705 | \u2705 | \u2705 |\nopenGauss | \u2705 | \u2705 | \u2705 | \u2705 | \u274c | \u2705 | \u274c | \u2705 |\nPGVector | \u2705 | \u2705 | \u2705 | \u2705 | \u2705 | \u274c | \u274c | \u2705 |\nPGVectorStore | \u2705 | \u2705 | \u2705 | \u2705 | \u2705 | \u2705 | \u274c | \u2705 |\nPineconeVectorStore | \u2705 | \u2705 | \u2705 | \u274c | \u2705 | \u274c | \u274c | \u2705 |\nQdrantVectorStore | \u2705 | \u2705 | \u2705 | \u2705 | \u2705 | \u274c | \u2705 | \u2705 |\nWeaviate | \u2705 | \u2705 | \u2705 | \u2705 | \u2705 | \u274c | \u2705 | \u2705 |\nSQLServer | \u2705 | \u2705 | \u2705 | \u2705 | \u274c | \u274c | \u274c | \u2705 |\nZeusDB | \u2705 | \u2705 | \u2705 | \u2705 | \u2705 | \u2705 | \u274c | \u2705 |", "tokens": 570, "node_type": "child"}
{"id": 403, "chunk_id": "c16acf8a553ac24f5d77e7d3a1c6e493", "parent_id": 382, "url": "", "namespace": "langchain", "title": "oss-python-langchain-agents", "headers": ["oss-python-langchain-agents"], "section_index": 0, "chunk_index": 0, "text": "# oss-python-langchain-agents\n\n> Source: https://docs.langchain.com/oss/python/langchain/agents\n\nLangChain v1.0Welcome to the new LangChain documentation! If you encounter any issues or have feedback, please open an issue so we can improve. Archived v0 documentation can be found here.See the release notes and migration guide for a complete list of changes and instructions on how to upgrade your code.\ncreate_agent\nprovides a production-ready agent implementation.\nAn LLM Agent runs tools in a loop to achieve a goal.\nAn agent runs until a stop condition is met - i.e., when the model emits a final output or an iteration limit is reached.\ncreate_agent\nbuilds a graph-based agent runtime using LangGraph. A graph consists of nodes (steps) and edges (connections) that define how your agent processes information. The agent moves through this graph, executing nodes like the model node (which calls the model), the tools node (which executes tools), or middleware.Learn more about the Graph API.Core components\nModel\nThe model is the reasoning engine of your agent. It can be specified in multiple ways, supporting both static and dynamic model selection.Static model\nStatic models are configured once when creating the agent and remain unchanged throughout execution. This is the most common and straightforward approach. To initialize a static model from a :\nFor more control over the model configuration, initialize a model instance directly using the provider package:\ntemperature\n, max_tokens\n, timeouts\n, base_url\n, and other provider-specific settings. Refer to the reference to see available params and methods on your model.\nDynamic model\nDynamic models are selected at based on the current and context. This enables sophisticated routing logic and cost optimization. To use a dynamic model, create middleware with the@wrap_model_call\ndecorator that modifies the model in the request:\nPre-bound models (models with\nbind_tools\nalready called) are not supported when using structured output. If you need dynamic model selection with structured output, ensure the models passed to the middleware are not pre-bound.Tools\nTools give agents the ability to take actions. Agents go beyond simple model-only tool binding by facilitating:- Multiple tool calls in sequence (triggered by a single prompt)\n- Parallel tool calls when appropriate\n- Dynamic tool selection based on previous results\n- Tool retry logic and error handling\n- State persistence across tool calls\nDefining tools\nPass a list of tools to the agent.Tool error handling\nTo customize how tool errors are handled, use the@wrap_tool_call\ndecorator to create middleware:\nToolMessage\nwith the custom error message when a tool fails:\nTool use in the ReAct loop\nAgents follow the ReAct (\u201cReasoning + Acting\u201d) pattern, alternating between brief reasoning steps with targeted tool calls and feeding the resulting observations into subsequent decisions until they can deliver a final answer.Example of ReAct loop\nExample of ReAct loop\nPrompt: Identify the current most popular wireless headphones and verify availability.\n- Reasoning: \u201cPopularity is time-sensitive, I need to use the provided search tool.\u201d\n- Acting: Call\nsearch_products(\"wireless headphones\")\n- Reasoning: \u201cI need to confirm availability for the top-ranked item before answering.\u201d\n- Acting: Call\ncheck_inventory(\"WH-1000XM5\")\n- Reasoning: \u201cI have the most popular model and its stock status. I can now answer the user\u2019s question.\u201d\n- Acting: Produce final answer\nSystem prompt\nYou can shape how your agent approaches tasks by providing a prompt. Thesystem_prompt\nparameter can be provided as a string:\nsystem_prompt\nis provided, the agent will infer its task from the messages directly.\nDynamic system prompt\nFor more advanced use cases where you need to modify the system prompt based on runtime context or agent state, you can use middleware. The@dynamic_prompt\ndecorator creates middleware that generates system prompts dynamically based on the model request:\nInvocation\nYou can invoke an agent by passing an update to its state. All agents include a sequence of messages in their state; to invoke the agent, pass a new message:Advanced concepts\nStructured output\nIn some situations, you may want the agent to return an output in a specific format. LangChain provides strategies for structured output via theresponse_format\nparameter.\nToolStrategy\nToolStrategy\nuses artificial tool calling to generate structured output. This works with any model that supports tool calling:\nProviderStrategy\nProviderStrategy\nuses the model provider\u2019s native structured output generation. This is more reliable but only works with providers that support native structured output (e.g., OpenAI):\nIn v1, simply passing a schema (e.g.,\nresponse_format=ContactInfo\n) is no longer supported. You must explicitly use ToolStrategy\nor ProviderStrategy\n.Memory\nAgents maintain conversation history automatically through the message state. You can also configure the agent to use a custom state schema to remember additional information during the conversation. Information stored in the state can be thought of as the short-term memory of the agent: Custom state schemas must extendAgentState\nas a TypedDict\n.\nThere are two ways to define custom state:\n- Via middleware (preferred)\n- Via\nstate_schema\noncreate_agent\nDefining custom state via middleware is preferred over defining it via\nstate_schema\non create_agent\nbecause it allows you to keep state extensions conceptually scoped to the relevant middleware and tools.state_schema\nis still supported for backwards compatibility on create_agent\n.Defining state via middleware\nUse middleware to define custom state when your custom state needs to be accessed by specific middleware hooks and tools attached to said middleware.Defining state via state_schema\nUse the state_schema\nparameter as a shortcut to define custom state that is only used in tools.\nStreaming\nWe\u2019ve seen how the agent can be called withinvoke\nto get a final response. If the agent executes multiple steps, this may take a while. To show intermediate progress, we can stream back messages as they occur.\nMiddleware\nMiddleware provides powerful extensibility for customizing agent behavior at different stages of execution. You can use middleware to:- Process state before the model is called (e.g., message trimming, context injection)\n- Modify or validate the model\u2019s response (e.g., guardrails, content filtering)\n- Handle tool execution errors with custom logic\n- Implement dynamic model selection based on state or context\n- Add custom logging, monitoring, or analytics", "tokens": 983, "node_type": "child"}
{"id": 404, "chunk_id": "c16acf8a553ac24f5d77e7d3a1c6e493", "parent_id": 383, "url": "", "namespace": "langchain", "title": "oss-python-langchain-context-engineering", "headers": ["oss-python-langchain-context-engineering"], "section_index": 0, "chunk_index": 0, "text": "# oss-python-langchain-context-engineering\n\n> Source: https://docs.langchain.com/oss/python/langchain/context-engineering\n\nLangChain v1.0Welcome to the new LangChain documentation! If you encounter any issues or have feedback, please open an issue so we can improve. Archived v0 documentation can be found here.See the release notes and migration guide for a complete list of changes and instructions on how to upgrade your code.\n- The underlying LLM is just not good enough\n- The \u201cright\u201d context was not passed to the LLM\nThe core agent loop\nIt\u2019s important to understand the core agent loop to understand where context should be accessed and/or updated from. The core agent loop is quite simple:- Get user input\n- Call LLM, asking it to either respond or call tools\n- If it decides to call tools - then go and execute those tools\n- Repeat steps 2 and 3 until it decides to finish\nThe model\nThe model (including specific model parameters) that you use is a key part of the agent loop. It drives the whole agent\u2019s reasoning logic. One reason the agent could mess up is the model you are using is just not good enough. In order to build reliable agents, you have to have access to all the possible models. LangChain, with its standard model interfaces, supports this - we have over 50 different provider integrations. Model choice is also related to context engineering, in two ways. First, the way you pass the context to the LLM may depend on what LLM you are using. Some model providers are better at JSON, some at XML. The context engineering you do may be specific to the model choice. Second, the right model to use in the agent loop may depend on the context you want to pass it. As an obvious example - some models have different context windows. If the context in an agent builds up, you may want to use one model provider while the context is small, and then once it gets too large for that model\u2019s context window you may want to switch to another model.Types of context\nThere are a few different types of context that can be used to construct the context that is ultimately passed to the LLM. Instructions: Base instructions from the developer, commonly referred to as the system prompt. This may be static or dynamic. Tools: What tools the agent has access to. The names and descriptions and arguments of these are just as important as the text in the prompt. Structured output: What format the agent should respond in. The name and description and arguments of these are just as important as the text in the prompt. Session context: We also call this \u201cshort term memory\u201d in the docs. In the context of a conversation, this is most easily thought of the list of messages that make up the conversation. But there can often be other, more structured information that you may want the agent to access or update throughout the session. The agent can read and write this context. This context is often put directly into the context that is passed to the LLM. Examples include: messages, files. Long term memory: This is information that should persist across sessions (conversations). Examples include: extracted preferences Runtime configuration context: This is context that is not the \u201cstate\u201d or \u201cmemory\u201d of the agent, but rather configuration for a given agent run. This is not modified by the agent, and typically isn\u2019t passed into the LLM, but is used to guide the agent\u2019s behavior or look up other context. Examples include: user ID, DB connectionsContext engineering with LangChain\nNow we understand the basic agent loop, the importance of the model you use, and the different types of context that exist. Let\u2019s explore the concrete patterns LangChain provides for context engineering.Managing instructions (system prompts)\nStatic instructions\nFor fixed instructions that don\u2019t change, use thesystem_prompt\nparameter:\nDynamic instructions\nFor instructions that depend on context (user profile, preferences, session data), use the@dynamic_prompt\nmiddleware:\nWhen to use each:\n- Static prompts: Base instructions that never change\n- Dynamic prompts: Personalization, A/B testing, context-dependent behavior\nManaging conversation context (messages)\nLong conversations can exceed context windows or degrade model performance. Use middleware to manage conversation history:Trimming messages\nSummarizationMiddleware\nwhich automatically summarizes old messages when approaching token limits.\nSee Before model hook for more examples.\nContextual tool execution\nTools can access runtime context, session state, and long-term memory to make context-aware decisions:Dynamic tool selection\nControl which tools the agent can access based on context, state, or user permissions:Dynamic model selection\nSwitch models based on conversation complexity, context window needs, or cost optimization:Best practices\n- Start simple - Begin with static prompts and tools, add dynamics only when needed\n- Test incrementally - Add one context engineering feature at a time\n- Monitor performance - Track model calls, token usage, and latency\n- Use built-in middleware - Leverage\nSummarizationMiddleware\n,LLMToolSelectorMiddleware\n, etc. - Document your context strategy - Make it clear what context is being passed and why\nRelated resources\n- Middleware - Complete middleware guide\n- Tools - Tool creation and context access\n- Memory - Short-term and long-term memory patterns\n- Agents - Core agent concepts", "tokens": 858, "node_type": "child"}
{"id": 405, "chunk_id": "c16acf8a553ac24f5d77e7d3a1c6e493", "parent_id": 384, "url": "", "namespace": "langchain", "title": "oss-python-langchain-deploy", "headers": ["oss-python-langchain-deploy"], "section_index": 0, "chunk_index": 0, "text": "# oss-python-langchain-deploy\n\n> Source: https://docs.langchain.com/oss/python/langchain/deploy\n\nLangChain v1.0Welcome to the new LangChain documentation! If you encounter any issues or have feedback, please open an issue so we can improve. Archived v0 documentation can be found here.See the release notes and migration guide for a complete list of changes and instructions on how to upgrade your code.\nPrerequisites\nBefore you begin, ensure you have the following:- A GitHub account\n- A LangSmith account (free to sign up)\nDeploy your agent\n1. Create a repository on GitHub\nYour application\u2019s code must reside in a GitHub repository to be deployed on LangSmith. Both public and private repositories are supported. For this quickstart, first make sure your app is LangGraph-compatible by following the local server setup guide. Then, push your code to the repository.2. Deploy to LangSmith\n2\nCreate new deployment\nClick the + New Deployment button. A pane will open where you can fill in the required fields.\n3\nLink repository\nIf you are a first time user or adding a private repository that has not been previously connected, click the Add new account button and follow the instructions to connect your GitHub account.\n4\nDeploy repository\nSelect your application\u2019s repository. Click Submit to deploy. This may take about 15 minutes to complete. You can check the status in the Deployment details view.\n3. Test your application in Studio\nOnce your application is deployed:- Select the deployment you just created to view more details.\n- Click the Studio button in the top right corner. Studio will open to display your graph.\n4. Get the API URL for your deployment\n- In the Deployment details view in LangGraph, click the API URL to copy it to your clipboard.\n- Click the\nURL\nto copy it to the clipboard.\n5. Test the API\nYou can now test the API:- Python\n- Rest API\n- Install LangGraph Python:\n- Send a message to the agent:", "tokens": 318, "node_type": "child"}
{"id": 406, "chunk_id": "c16acf8a553ac24f5d77e7d3a1c6e493", "parent_id": 385, "url": "", "namespace": "langchain", "title": "oss-python-langchain-guardrails", "headers": ["oss-python-langchain-guardrails"], "section_index": 0, "chunk_index": 0, "text": "# oss-python-langchain-guardrails\n\n> Source: https://docs.langchain.com/oss/python/langchain/guardrails\n\nLangChain v1.0Welcome to the new LangChain documentation! If you encounter any issues or have feedback, please open an issue so we can improve. Archived v0 documentation can be found here.See the release notes and migration guide for a complete list of changes and instructions on how to upgrade your code.\n- Preventing PII leakage\n- Detecting and blocking prompt injection attacks\n- Blocking inappropriate or harmful content\n- Enforcing business rules and compliance requirements\n- Validating output quality and accuracy\nGuardrails can be implemented using two complementary approaches:\nDeterministic guardrails\nUse rule-based logic like regex patterns, keyword matching, or explicit checks. Fast, predictable, and cost-effective, but may miss nuanced violations.\nModel-based guardrails\nUse LLMs or classifiers to evaluate content with semantic understanding. Catch subtle issues that rules miss, but are slower and more expensive.\nBuilt-in guardrails\nPII detection\nLangChain provides built-in middleware for detecting and handling Personally Identifiable Information (PII) in conversations. This middleware can detect common PII types like emails, credit cards, IP addresses, and more. PII detection middleware is helpful for cases such as health care and financial applications with compliance requirements, customer service agents that need to sanitize logs, and generally any application handling sensitive user data. The PII middleware supports multiple strategies for handling detected PII:| Strategy | Description | Example |\n|---|---|---|\nredact | Replace with [REDACTED_TYPE] | [REDACTED_EMAIL] |\nmask | Partially obscure (e.g., last 4 digits) | ****-****-****-1234 |\nhash | Replace with deterministic hash | a8f5f167... |\nblock | Raise exception when detected | Error thrown |\nBuilt-in PII types and configuration\nBuilt-in PII types and configuration\nBuilt-in PII types:\nemail\n- Email addressescredit_card\n- Credit card numbers (Luhn validated)ip\n- IP addressesmac_address\n- MAC addressesurl\n- URLs\n| Parameter | Description | Default |\n|---|---|---|\npii_type | Type of PII to detect (built-in or custom) | Required |\nstrategy | How to handle detected PII (\"block\" , \"redact\" , \"mask\" , \"hash\" ) | \"redact\" |\ndetector | Custom detector function or regex pattern | None (uses built-in) |\napply_to_input | Check user messages before model call | True |\napply_to_output | Check AI messages after model call | False |\napply_to_tool_results | Check tool result messages after execution | False |\nHuman-in-the-loop\nLangChain provides built-in middleware for requiring human approval before executing sensitive operations. This is one of the most effective guardrails for high-stakes decisions. Human-in-the-loop middleware is helpful for cases such as financial transactions and transfers, deleting or modifying production data, sending communications to external parties, and any operation with significant business impact.Custom guardrails\nFor more sophisticated guardrails, you can create custom middleware that runs before or after the agent executes. This gives you full control over validation logic, content filtering, and safety checks.Before agent guardrails\nUse \u201cbefore agent\u201d hooks to validate requests once at the start of each invocation. This is useful for session-level checks like authentication, rate limiting, or blocking inappropriate requests before any processing begins.After agent guardrails\nUse \u201cafter agent\u201d hooks to validate final outputs once before returning to the user. This is useful for model-based safety checks, quality validation, or final compliance scans on the complete agent response.Combine multiple guardrails\nYou can stack multiple guardrails by adding them to the middleware array. They execute in order, allowing you to build layered protection:Additional resources\n- Middleware documentation - Complete guide to custom middleware\n- Middleware API reference - Complete guide to custom middleware\n- Human-in-the-loop - Add human review for sensitive operations\n- Testing agents - Strategies for testing safety mechanisms", "tokens": 590, "node_type": "child"}
{"id": 407, "chunk_id": "c16acf8a553ac24f5d77e7d3a1c6e493", "parent_id": 386, "url": "", "namespace": "langchain", "title": "oss-python-langchain-human-in-the-loop", "headers": ["oss-python-langchain-human-in-the-loop"], "section_index": 0, "chunk_index": 0, "text": "# oss-python-langchain-human-in-the-loop\n\n> Source: https://docs.langchain.com/oss/python/langchain/human-in-the-loop\n\nLangChain v1.0Welcome to the new LangChain documentation! If you encounter any issues or have feedback, please open an issue so we can improve. Archived v0 documentation can be found here.See the release notes and migration guide for a complete list of changes and instructions on how to upgrade your code.\napprove\n), modified before running (edit\n), or rejected with feedback (reject\n).\nInterrupt decision types\nThe middleware defines three built-in ways a human can respond to an interrupt:| Decision Type | Description | Example Use Case |\n|---|---|---|\n\u2705 approve | The action is approved as-is and executed without changes. | Send an email draft exactly as written |\n\u270f\ufe0f edit | The tool call is executed with modifications. | Change the recipient before sending an email |\n\u274c reject | The tool call is rejected, with an explanation added to the conversation. | Reject an email draft and explain how to rewrite it |\ninterrupt_on\n.\nWhen multiple tool calls are paused at the same time, each action requires a separate decision.\nDecisions must be provided in the same order as the actions appear in the interrupt request.\nWhen editing tool arguments, make changes conservatively. Significant modifications to the original arguments may cause the model to re-evaluate its approach and potentially execute the tool multiple times or take unexpected actions.\nConfiguring interrupts\nTo use HITL, add the middleware to the agent\u2019smiddleware\nlist when creating the agent.\nYou configure it with a mapping of tool actions to the decision types that are allowed for each action. The middleware will interrupt execution when a tool call matches an action in the mapping.\nYou must configure a checkpointer to persist the graph state across interrupts.\nIn production, use a persistent checkpointer like\nAsyncPostgresSaver\n. For testing or prototyping, use InMemorySaver\n.When invoking the agent, pass a config\nthat includes the thread ID to associate execution with a conversation thread.\nSee the LangGraph interrupts documentation for details.Responding to interrupts\nWhen you invoke the agent, it runs until it either completes or an interrupt is raised. An interrupt is triggered when a tool call matches the policy you configured ininterrupt_on\n. In that case, the invocation result will include an __interrupt__\nfield with the actions that require review. You can then present those actions to a reviewer and resume execution once decisions are provided.\nDecision types\n- \u2705 approve\n- \u270f\ufe0f edit\n- \u274c reject\nUse\napprove\nto approve the tool call as-is and execute it without changes.Execution lifecycle\nThe middleware defines anafter_model\nhook that runs after the model generates a response but before any tool calls are executed:\n- The agent invokes the model to generate a response.\n- The middleware inspects the response for tool calls.\n- If any calls require human input, the middleware builds a\nHITLRequest\nwithaction_requests\nandreview_configs\nand calls interrupt. - The agent waits for human decisions.\n- Based on the\nHITLResponse\ndecisions, the middleware executes approved or edited calls, synthesizes ToolMessage\u2019s for rejected calls, and resumes execution.", "tokens": 505, "node_type": "child"}
{"id": 408, "chunk_id": "c16acf8a553ac24f5d77e7d3a1c6e493", "parent_id": 388, "url": "", "namespace": "langchain", "title": "oss-python-langchain-knowledge-base", "headers": ["oss-python-langchain-knowledge-base"], "section_index": 0, "chunk_index": 0, "text": "# oss-python-langchain-knowledge-base > Source: https://docs.langchain.com/oss/python/langchain/knowledge-base LangGraph v1.0Welcome to the new LangGraph documentation! If you encounter any issues or have feedback, please open an issue so we can improve. Archived v0 documentation can be found here.See the release notes and migration guide for a complete list of changes and instructions on how to upgrade your code. Overview This tutorial will familiarize you with LangChain\u2019s document loader, embedding, and vector store abstractions. These abstractions are designed to support retrieval of data\u2014 from (vector) databases and other sources \u2014 for integration with LLM workflows. They are important for applications that fetch data to be reasoned over as part of model inference, as in the case of retrieval-augmented generation, or RAG. Here we will build a search engine over a PDF document. This will allow us to retrieve passages in the PDF that are similar to an input query. The guide also includes a minimal RAG implementation on top of the search engine.Concepts This guide focuses on retrieval of text data. We will cover the following concepts:Setup Installation This tutorial requires thelangchain-community and pypdf packages: LangSmith Many of the applications you build with LangChain will contain multiple steps with multiple invocations of LLM calls. As these applications get more and more complex, it becomes crucial to be able to inspect what exactly is going on inside your chain or agent. The best way to do this is with LangSmith. After you sign up at the link above, make sure to set your environment variables to start logging traces:1. Documents and Document Loaders LangChain implements a Document abstraction, which is intended to represent a unit of text and associated metadata. It has three attributes:page_content : a string representing the content;metadata : a dict containing arbitrary metadata;id : (optional) a string identifier for the document. metadata attribute can capture information about the source of the document, its relationship to other documents, and other information. Note that an individual Document object often represents a chunk of a larger document. We can generate sample documents when desired: Loading documents Let\u2019s load a PDF into a sequence ofDocument objects. Here is a sample PDF \u2014 a 10-k filing for Nike from 2023. We can consult the LangChain documentation for available PDF document loaders. PyPDFLoader loads one Document object per PDF page. For each, we can easily access: - The string content of the page; - Metadata containing the file name and page number. Splitting For both information retrieval and downstream question-answering purposes, a page may be too coarse a representation. Our goal in the end will be to retrieveDocument objects that answer an input query, and further splitting our PDF will help ensure that the meanings of relevant portions of the document are not \u201cwashed out\u201d by surrounding text. We can use text splitters for this purpose. Here we will use a simple text splitter that partitions based on characters. We will split our documents into chunks of 1000 characters with 200 characters of overlap between chunks. The overlap helps mitigate the possibility of separating a statement from important context related to it. We use the RecursiveCharacterTextSplitter , which will recursively split the document using common separators like new lines until each chunk is the appropriate size. This is the recommended text splitter for generic text use cases. We set add_start_index=True so that the character index where each split Document starts within the initial Document is preserved as metadata attribute \u201cstart_index\u201d. 2. Embeddings Vector search is a common way to store and search over unstructured data (such as unstructured text). The idea is to store numeric vectors that are associated with the text. Given a query, we can embed it as a vector of the same dimension and use vector similarity metrics (such as cosine similarity) to identify related text. LangChain supports embeddings from dozens of providers. These models specify how text should be converted into a numeric vector. Let\u2019s select a model:- OpenAI - Azure - Google Gemini - Google Vertex - AWS - HuggingFace - Ollama - Cohere - MistralAI - Nomic - NVIDIA - Voyage AI - IBM watsonx - Fake 3. Vector stores LangChain VectorStore objects contain methods for adding text andDocument objects to the store, and querying them using various similarity metrics. They are often initialized with embedding models, which determine how text data is translated to numeric vectors. LangChain includes a suite of integrations with different vector store technologies. Some vector stores are hosted by a provider (e.g., various cloud providers) and require specific credentials to use; some (such as Postgres) run in separate infrastructure that can be run locally or via a third-party; others can run in-memory for lightweight workloads. Let\u2019s select a vector store: - In-memory - AstraDB - Chroma - FAISS - Milvus - MongoDB - PGVector - PGVectorStore - Pinecone - Qdrant VectorStore that contains documents, we can query it. VectorStore includes methods for querying: - Synchronously and asynchronously; - By string query and by vector; - With and without returning similarity scores; - By similarity and @[maximum marginal relevance][VectorStore.max_marginal_relevance_search] (to balance similarity with query to diversity in retrieved results). 4. Retrievers LangChainVectorStore objects do not subclass @[Runnable]. LangChain @[Retrievers] are Runnables, so they implement a standard set of methods (e.g., synchronous and asynchronous invoke and batch operations). Although we can construct retrievers from vector stores, retrievers can interface with non-vector store sources of data, as well (such as external APIs). We can create a simple version of this ourselves, without subclassing Retriever . If we choose what method we wish to use to retrieve documents, we can create a runnable easily. Below we will build one around the similarity_search method: as_retriever method that will generate a Retriever, specifically a VectorStoreRetriever. These retrievers include specific search_type and search_kwargs attributes that identify what methods of the underlying vector store to call, and how to parameterize them. For instance, we can replicate the above with the following: VectorStoreRetriever supports search types of \"similarity\" (default), \"mmr\" (maximum marginal relevance, described above), and", "tokens": 1000, "node_type": "child"}
{"id": 409, "chunk_id": "9b0fd3b850698317bef57fa6e71a38ba", "parent_id": 388, "url": "", "namespace": "langchain", "title": "oss-python-langchain-knowledge-base", "headers": ["oss-python-langchain-knowledge-base"], "section_index": 0, "chunk_index": 1, "text": "results). 4. Retrievers LangChainVectorStore objects do not subclass @[Runnable]. LangChain @[Retrievers] are Runnables, so they implement a standard set of methods (e.g., synchronous and asynchronous invoke and batch operations). Although we can construct retrievers from vector stores, retrievers can interface with non-vector store sources of data, as well (such as external APIs). We can create a simple version of this ourselves, without subclassing Retriever . If we choose what method we wish to use to retrieve documents, we can create a runnable easily. Below we will build one around the similarity_search method: as_retriever method that will generate a Retriever, specifically a VectorStoreRetriever. These retrievers include specific search_type and search_kwargs attributes that identify what methods of the underlying vector store to call, and how to parameterize them. For instance, we can replicate the above with the following: VectorStoreRetriever supports search types of \"similarity\" (default), \"mmr\" (maximum marginal relevance, described above), and \"similarity_score_threshold\" . We can use the latter to threshold documents output by the retriever by similarity score. Retrievers can easily be incorporated into more complex applications, such as retrieval-augmented generation (RAG) applications that combine a given question with retrieved context into a prompt for a LLM. To learn more about building such an application, check out the RAG tutorial tutorial.", "tokens": 210, "node_type": "child"}
{"id": 410, "chunk_id": "c16acf8a553ac24f5d77e7d3a1c6e493", "parent_id": 389, "url": "", "namespace": "langchain", "title": "oss-python-langchain-long-term-memory", "headers": ["oss-python-langchain-long-term-memory"], "section_index": 0, "chunk_index": 0, "text": "# oss-python-langchain-long-term-memory\n\n> Source: https://docs.langchain.com/oss/python/langchain/long-term-memory\n\nLangChain v1.0Welcome to the new LangChain documentation! If you encounter any issues or have feedback, please open an issue so we can improve. Archived v0 documentation can be found here.See the release notes and migration guide for a complete list of changes and instructions on how to upgrade your code.\nOverview\nLangChain agents use LangGraph persistence to enable long-term memory. This is a more advanced topic and requires knowledge of LangGraph to use.Memory storage\nLangGraph stores long-term memories as JSON documents in a store. Each memory is organized under a customnamespace\n(similar to a folder) and a distinct key\n(like a file name). Namespaces often include user or org IDs or other labels that makes it easier to organize information.\nThis structure enables hierarchical organization of memories. Cross-namespace searching is then supported through content filters.\nRead long-term memory in tools\nA tool the agent can use to look up user information\nWrite long-term memory from tools\nExample of a tool that updates user information", "tokens": 169, "node_type": "child"}
{"id": 411, "chunk_id": "c16acf8a553ac24f5d77e7d3a1c6e493", "parent_id": 390, "url": "", "namespace": "langchain", "title": "oss-python-langchain-mcp", "headers": ["oss-python-langchain-mcp"], "section_index": 0, "chunk_index": 0, "text": "# oss-python-langchain-mcp\n\n> Source: https://docs.langchain.com/oss/python/langchain/mcp\n\nLangChain v1.0Welcome to the new LangChain documentation! If you encounter any issues or have feedback, please open an issue so we can improve. Archived v0 documentation can be found here.See the release notes and migration guide for a complete list of changes and instructions on how to upgrade your code.\nlangchain-mcp-adapters\nlibrary.\nInstall\nInstall thelangchain-mcp-adapters\nlibrary to use MCP tools in LangGraph:\nTransport types\nMCP supports different transport mechanisms for client-server communication:- stdio: Client launches server as a subprocess and communicates via standard input/output. Best for local tools and simple setups.\n- Streamable HTTP: Server runs as an independent process handling HTTP requests. Supports remote connections and multiple clients.\n- Server-Sent Events (SSE): a variant of streamable HTTP optimized for real-time streaming communication.\nUse MCP tools\nlangchain-mcp-adapters\nenables agents to use tools defined across one or more MCP server.\nAccessing multiple MCP servers\nMultiServerMCPClient\nis stateless by default. Each tool invocation creates a fresh MCP ClientSession\n, executes the tool, and then cleans up.Custom MCP servers\nTo create your own MCP servers, you can use themcp\nlibrary. This library provides a simple way to define tools and run them as servers.\nMath server (stdio transport)\nWeather server (streamable HTTP transport)\nStateful tool usage\nFor stateful servers that maintain context between tool calls, useclient.session()\nto create a persistent ClientSession\n.\nUsing MCP ClientSession for stateful tool usage", "tokens": 231, "node_type": "child"}
{"id": 412, "chunk_id": "c16acf8a553ac24f5d77e7d3a1c6e493", "parent_id": 391, "url": "", "namespace": "langchain", "title": "oss-python-langchain-messages", "headers": ["oss-python-langchain-messages"], "section_index": 0, "chunk_index": 0, "text": "# oss-python-langchain-messages\n\n> Source: https://docs.langchain.com/oss/python/langchain/messages\n\n- Role - Identifies the message type (e.g.\nsystem\n,user\n) - Content - Represents the actual content of the message (like text, images, audio, documents, etc.)\n- Metadata - Optional fields such as response information, message IDs, and token usage\nBasic usage\nThe simplest way to use messages is to create message objects and pass them to a model when invoking.Text prompts\nText prompts are strings - ideal for straightforward generation tasks where you don\u2019t need to retain conversation history.- You have a single, standalone request\n- You don\u2019t need conversation history\n- You want minimal code complexity\nMessage prompts\nAlternatively, you can pass in a list of messages to the model by providing a list of message objects.- Managing multi-turn conversations\n- Working with multimodal content (images, audio, files)\n- Including system instructions\nDictionary format\nYou can also specify messages directly in OpenAI chat completions format.Message types\n- System message - Tells the model how to behave and provide context for interactions\n- Human message - Represents user input and interactions with the model\n- AI message - Responses generated by the model, including text content, tool calls, and metadata\n- Tool message - Represents the outputs of tool calls\nSystem Message\nASystemMessage\nrepresent an initial set of instructions that primes the model\u2019s behavior. You can use a system message to set the tone, define the model\u2019s role, and establish guidelines for responses.\nHuman Message\nAHumanMessage\nrepresents user input and interactions. They can contain text, images, audio, files, and any other amount of multimodal content.\nText content\nMessage metadata\nAI Message\nAnAIMessage\nrepresents the output of a model invocation. They can include multimodal data, tool calls, and provider-specific metadata that you can later access.\nAIMessage\nobjects are returned by the model when calling it, which contains all of the associated metadata in the response. However, that doesn\u2019t mean that\u2019s the only place they can be created/ modified from.\nProviders weight/contextualize types of messages differently, which means it is sometimes helpful to create a new AIMessage\nobject and insert it into the message history as if it came from the model.\nAttributes\nAttributes\nTool calls\nWhen models make tool calls, they\u2019re included in theAIMessage\n:\nToken usage\nAnAIMessage\ncan hold token counts and other usage metadata in its usage_metadata\nfield:\nStreaming and chunks\nDuring streaming, you\u2019ll receiveAIMessageChunk\nobjects that can be combined into a full message:\nTool Message\nFor models that support tool calling, AI messages can contain tool calls. Tool messages are used to pass the results of a single tool execution back to the model. Tools can generateToolMessage\nobjects directly. Below, we show a simple example. Read more in the tools guide.\nAttributes\nAttributes\nartifact\nfield stores supplementary data that won\u2019t be sent to the model but can be accessed programmatically. This is useful for storing raw results, debugging information, or data for downstream processing without cluttering the model\u2019s context.Example: Using artifact for retrieval metadata\nExample: Using artifact for retrieval metadata\ncontent\ncontains text that the model will reference, an artifact\ncan contain document identifiers or other metadata that an application can use (e.g., to render a page). See example below:Content\nYou can think of a message\u2019s content as the payload of data that gets sent to the model. Messages have acontent\nattribute that is loosely-typed, supporting strings and lists of untyped objects (e.g., dictionaries). This allows support for provider-native structures directly in LangChain chat models, such as multimodal content and other data.\nSeparately, LangChain provides dedicated content types for text, reasoning, citations, multi-modal data, server-side tool calls, and other message content. See content blocks below.\nLangChain chat models accept message content in the content\nattribute, and can contain:\n- A string\n- A list of content blocks in a provider-native format\n- A list of LangChain\u2019s standard content blocks\ncontent_blocks\nwhen initializing a message will still populate message\ncontent\n, but provides a type-safe interface for doing so.Standard content blocks\nLangChain provides a standard representation for message content that works across providers. Message objects implement acontent_blocks\nproperty that will lazily parse the content\nattribute into a standard, type-safe representation. For example, messages generated from ChatAnthropic or ChatOpenAI will include thinking\nor reasoning\nblocks in the format of the respective provider, but can be lazily parsed into a consistent ReasoningContentBlock\nrepresentation:\n- Anthropic\n- OpenAI\nLC_OUTPUT_VERSION\nenvironment variable to v1\n. Or,\ninitialize any chat model with output_version=\"v1\"\n:Multimodal\nMultimodality refers to the ability to work with data that comes in different forms, such as text, audio, images, and video. LangChain includes standard types for these data that can be used across providers. Chat models can accept multimodal data as input and generate it as output. Below we show short examples of input messages featuring multimodal data:Content block reference\nContent blocks are represented (either when creating a message or accessing thecontent_blocks\nproperty) as a list of typed dictionaries. Each item in the list must adhere to one of the following block types:\nCore\nCore\nTextContentBlock\nTextContentBlock\nMultimodal\nMultimodal\nImageContentBlock\nImageContentBlock\nAudioContentBlock\nAudioContentBlock\nVideoContentBlock\nVideoContentBlock\nFileContentBlock\nFileContentBlock\nTool Calling\nTool Calling\nToolCall\nToolCall\nToolCallChunk\nToolCallChunk\nServer-Side Tool Execution\nServer-Side Tool Execution\nServerToolCall\nServerToolCall\nServerToolCallChunk\nServerToolCallChunk\nServerToolResult\nServerToolResult\ncontent\nproperty, but rather a new property that can be used to access the content of a message in a standardized format.Use with chat models\nChat models accept a sequence of message objects as input and return anAIMessage\nas output. Interactions are often stateless, so that a simple conversational loop involves invoking a model with a growing list of messages.\nRefer to the below guides to learn more:\n- Built-in features for persisting and managing conversation histories\n- Strategies for managing context windows, including trimming and summarizing messages", "tokens": 952, "node_type": "child"}
{"id": 413, "chunk_id": "c16acf8a553ac24f5d77e7d3a1c6e493", "parent_id": 392, "url": "", "namespace": "langchain", "title": "oss-python-langchain-middleware", "headers": ["oss-python-langchain-middleware"], "section_index": 0, "chunk_index": 0, "text": "# oss-python-langchain-middleware > Source: https://docs.langchain.com/oss/python/langchain/middleware What can middleware do? Monitor Modify Control Enforce create_agent : Built-in middleware LangChain provides prebuilt middleware for common use cases:Summarization Automatically summarize conversation history when approaching token limits.- Long-running conversations that exceed context windows - Multi-turn dialogues with extensive history - Applications where preserving full conversation context matters Configuration options Configuration options Human-in-the-loop Pause agent execution for human approval, editing, or rejection of tool calls before they execute.- High-stakes operations requiring human approval (database writes, financial transactions) - Compliance workflows where human oversight is mandatory - Long running conversations where human feedback is used to guide the agent Configuration options Configuration options True (interrupt with default config), False (auto-approve), or an InterruptOnConfig object.InterruptOnConfig options:Anthropic prompt caching Reduce costs by caching repetitive prompt prefixes with Anthropic models.- Applications with long, repeated system prompts - Agents that reuse the same context across invocations - Reducing API costs for high-volume deployments Configuration options Configuration options Model call limit Limit the number of model calls to prevent infinite loops or excessive costs.- Preventing runaway agents from making too many API calls - Enforcing cost controls on production deployments - Testing agent behavior within specific call budgets Configuration options Configuration options Tool call limit Limit the number of tool calls to specific tools or all tools.- Preventing excessive calls to expensive external APIs - Limiting web searches or database queries - Enforcing rate limits on specific tool usage Configuration options Configuration options Model fallback Automatically fallback to alternative models when the primary model fails.- Building resilient agents that handle model outages - Cost optimization by falling back to cheaper models - Provider redundancy across OpenAI, Anthropic, etc. Configuration options Configuration options PII detection Detect and handle Personally Identifiable Information in conversations.- Healthcare and financial applications with compliance requirements - Customer service agents that need to sanitize logs - Any application handling sensitive user data Configuration options Configuration options email , credit_card , ip , mac_address , url ) or a custom type name.\"block\" - Raise exception when detected\"redact\" - Replace with[REDACTED_TYPE] \"mask\" - Partially mask (e.g.,****-****-****-1234 )\"hash\" - Replace with deterministic hash Planning Add todo list management capabilities for complex multi-step tasks.write_todos tool and system prompts to guide effective task planning.Configuration options Configuration options LLM tool selector Use an LLM to intelligently select relevant tools before calling the main model.- Agents with many tools (10+) where most aren\u2019t relevant per query - Reducing token usage by filtering irrelevant tools - Improving model focus and accuracy Configuration options Configuration options BaseChatModel instance. Defaults to the agent\u2019s main model.Tool retry Automatically retry failed tool calls with configurable exponential backoff.- Handling transient failures in external API calls - Improving reliability of network-dependent tools - Building resilient agents that gracefully handle temporary errors Configuration options Configuration options None , applies to all tools.True if it should be retried.\"return_message\" - Return a ToolMessage with error details (allows LLM to handle failure)\"raise\" - Re-raise the exception (stops agent execution)- Custom callable - Function that takes the exception and returns a string for the ToolMessage content initial_delay * (backoff_factor ** retry_number) seconds. Set to 0.0 for constant delay.LLM tool emulator Emulate tool execution using an LLM for testing purposes, replacing actual tool calls with AI-generated responses.- Testing agent behavior without executing real tools - Developing agents when external tools are unavailable or expensive - Prototyping agent workflows before implementing actual tools Configuration options Configuration options None (default), ALL tools will be emulated. If empty list, no tools will be emulated.Context editing Manage conversation context by trimming, summarizing, or clearing tool uses.- Long conversations that need periodic context cleanup - Removing failed tool attempts from context - Custom context management strategies Configuration options Configuration options Custom middleware Build custom middleware by implementing hooks that run at specific points in the agent execution flow. You can create middleware in two ways:- Decorator-based - Quick and simple for single-hook middleware - Class-based - More powerful for complex middleware with multiple hooks Decorator-based middleware For simple middleware that only needs a single hook, decorators provide the quickest way to add functionality:Available decorators Node-style (run at specific execution points):@before_agent - Before agent starts (once per invocation)@before_model - Before each model call@after_model - After each model response@after_agent - After agent completes (once per invocation) @wrap_model_call - Around each model call@wrap_tool_call - Around each tool call @dynamic_prompt - Generates dynamic system prompts (equivalent to@wrap_model_call that modifies the prompt) When to use decorators Use decorators when - You need a single hook - No complex configuration Use classes when - Multiple hooks needed - Complex configuration - Reusable across projects (config on init) Class-based middleware Two hook styles Node-style hooks Wrap-style hooks Node-style hooks Run at specific points in the execution flow:before_agent - Before agent starts (once per invocation)before_model - Before each model callafter_model - After each model responseafter_agent - After agent completes (up to once per invocation) Wrap-style hooks Intercept execution and control when the handler is called:wrap_model_call - Around each model callwrap_tool_call - Around each tool call Custom state schema Middleware can extend the agent\u2019s state with custom properties. Define a custom state type and set it as thestate_schema : Execution order When using multiple middleware, understanding execution order is important:Execution flow (click to expand) Execution flow (click to expand) middleware1.before_agent() middleware2.before_agent() middleware3.before_agent() middleware1.before_model() middleware2.before_model() middleware3.before_model() middleware1.wrap_model_call() \u2192middleware2.wrap_model_call() \u2192middleware3.wrap_model_call() \u2192 model middleware3.after_model() middleware2.after_model() middleware1.after_model() middleware3.after_agent() middleware2.after_agent() middleware1.after_agent() before_* hooks: First to lastafter_* hooks: Last to first (reverse)wrap_* hooks: Nested (first middleware wraps all others) Agent jumps To exit early from middleware, return a dictionary withjump_to : \"end\" : Jump to the end of the agent execution\"tools\" : Jump to the tools node\"model\" : Jump to the model node (or the firstbefore_model hook) before_model or after_model , jumping to \"model\" will cause all before_model middleware to run again. To enable jumping, decorate your hook with @hook_config(can_jump_to=[...]) : Best practices - Keep middleware focused - each should do one thing well - Handle errors gracefully - don\u2019t let middleware errors crash the agent - Use", "tokens": 1000, "node_type": "child"}
{"id": 414, "chunk_id": "9b0fd3b850698317bef57fa6e71a38ba", "parent_id": 392, "url": "", "namespace": "langchain", "title": "oss-python-langchain-middleware", "headers": ["oss-python-langchain-middleware"], "section_index": 0, "chunk_index": 1, "text": "type and set it as thestate_schema : Execution order When using multiple middleware, understanding execution order is important:Execution flow (click to expand) Execution flow (click to expand) middleware1.before_agent() middleware2.before_agent() middleware3.before_agent() middleware1.before_model() middleware2.before_model() middleware3.before_model() middleware1.wrap_model_call() \u2192middleware2.wrap_model_call() \u2192middleware3.wrap_model_call() \u2192 model middleware3.after_model() middleware2.after_model() middleware1.after_model() middleware3.after_agent() middleware2.after_agent() middleware1.after_agent() before_* hooks: First to lastafter_* hooks: Last to first (reverse)wrap_* hooks: Nested (first middleware wraps all others) Agent jumps To exit early from middleware, return a dictionary withjump_to : \"end\" : Jump to the end of the agent execution\"tools\" : Jump to the tools node\"model\" : Jump to the model node (or the firstbefore_model hook) before_model or after_model , jumping to \"model\" will cause all before_model middleware to run again. To enable jumping, decorate your hook with @hook_config(can_jump_to=[...]) : Best practices - Keep middleware focused - each should do one thing well - Handle errors gracefully - don\u2019t let middleware errors crash the agent - Use appropriate hook types: - Node-style for sequential logic (logging, validation) - Wrap-style for control flow (retry, fallback, caching) - Clearly document any custom state properties - Unit test middleware independently before integrating - Consider execution order - place critical middleware first in the list - Use built-in middleware when possible, don\u2019t reinvent the wheel :) Examples Dynamically selecting tools Select relevant tools at runtime to improve performance and accuracy.- Shorter prompts - Reduce complexity by exposing only relevant tools - Better accuracy - Models choose correctly from fewer options - Permission control - Dynamically filter tools based on user access Additional resources - Middleware API reference - Complete guide to custom middleware - Human-in-the-loop - Add human review for sensitive operations - Testing agents - Strategies for testing safety mechanisms", "tokens": 280, "node_type": "child"}
{"id": 415, "chunk_id": "c16acf8a553ac24f5d77e7d3a1c6e493", "parent_id": 393, "url": "", "namespace": "langchain", "title": "oss-python-langchain-models", "headers": ["oss-python-langchain-models"], "section_index": 0, "chunk_index": 0, "text": "# oss-python-langchain-models > Source: https://docs.langchain.com/oss/python/langchain/models - Tool calling - calling external tools (like databases queries or API calls) and use results in their responses. - Structured output - where the model\u2019s response is constrained follow a defined format. - Multimodality - process and return data other than text, such as images, audio, and video. - Reasoning - models perform multi-step reasoning to arrive at a conclusion. Basic usage Models can be utilized in two ways:- With agents - Models can be dynamically specified when creating an agent. See the agents guide for more details on how to do this. - Standalone - Models can be called directly (outside of the agent loop) for tasks like text generation, classification, or extraction without the need for an agent framework. Initialize a model The easiest way to get started with a standalone model in LangChain is to useinit_chat_model to initialize one from a provider of your choice (examples below): init_chat_model for more detail, including information on how to pass model parameters. Key methods Invoke Stream Batch Parameters A chat model takes parameters that can be used to configure its behavior. The full set of supported parameters varies by model and provider, but standard ones include:init_chat_model , pass these parameters as inline : ChatOpenAI has use_responses_api to dictate whether to use the OpenAI Responses or Completions API.To find all the parameters supported by a given chat model, head to the chat model integrations page.Invocation A chat model must be invoked to generate an output. There are three primary invocation methods, each suited to different use cases.Invoke The most straightforward way to call a model is to useinvoke() with a single message or a list of messages. Stream Most models can stream their output content while it is being generated. By displaying output progressively, streaming significantly improves user experience, particularly for longer responses. Callingstream() returns an that yields output chunks as they are produced. You can use a loop to process each chunk in real-time: invoke() , which returns a single AIMessage after the model has finished generating its full response, stream() returns multiple AIMessageChunk objects, each containing a portion of the output text. Importantly, each chunk in a stream is designed to be gathered into a full message via summation: invoke() - for example, it can be aggregated into a message history and passed back to the model as conversational context. Advanced streaming topics Advanced streaming topics \"Auto-streaming\" chat models \"Auto-streaming\" chat models model.invoke() within nodes, but LangChain will automatically delegate to streaming if running in a streaming mode.How it works When youinvoke() a chat model, LangChain will automatically switch to an internal streaming mode if it detects that you are trying to stream the overall application. The result of the invocation will be the same as far as the code that was using invoke is concerned; however, while the chat model is being streamed, LangChain will take care of invoking on_llm_new_token events in LangChain\u2019s callback system.Callback events allow LangGraph stream() and astream_events() to surface the chat model\u2019s output in real-time.Streaming events Streaming events astream_events() .This simplifies filtering based on event types and other metadata, and will aggregate the full message in the background. See below for an example.Batch Batching a collection of independent requests to a model can significantly improve performance and reduce costs, as the processing can be done in parallel:batch() will only return the final output for the entire batch. If you want to receive the output for each individual input as it finishes generating, you can stream results with batch_as_completed() : batch() or batch_as_completed() , you may want to control the maximum number of parallel calls. This can be done by setting the max_concurrency attribute in the RunnableConfig dictionary.RunnableConfig reference for a full list of supported attributes.Tool calling Models can request to call tools that perform tasks such as fetching data from a database, searching the web, or running code. Tools are pairings of:- A schema, including the name of the tool, a description, and/or argument definitions (often a JSON schema) - A function or to execute. bind_tools() . In subsequent invocations, the model can choose to call any of the bound tools as needed. Some model providers offer built-in tools that can be enabled via model or invocation parameters (e.g. ChatOpenAI , ChatAnthropic ). Check the respective provider reference for details. Tool execution loop Tool execution loop ToolMessage returned by the tool includes a tool_call_id that matches the original tool call, helping the model correlate results with requests.Forcing tool calls Forcing tool calls Parallel tool calls Parallel tool calls Streaming tool calls Streaming tool calls ToolCallChunk . This allows you to see tool calls as they\u2019re being generated rather than waiting for the complete response.Structured outputs Models can be requested to provide their response in a format matching a given schema. This is useful for ensuring the output can be easily parsed and used in subsequent processing. LangChain supports multiple schema types and methods for enforcing structured outputs.- Pydantic - TypedDict - JSON Schema - Method parameter: Some providers support different methods ( 'json_schema' ,'function_calling' ,'json_mode' )'json_schema' typically refers to dedicated structured output features offered by a provider'function_calling' derives structured output by forcing a tool call following the given schema'json_mode' is a precursor to'json_schema' offered by some providers- it generates valid json, but the schema must be described in the prompt - Include raw: Use include_raw=True to get both the parsed output and the raw AI message - Validation: Pydantic models provide automatic validation, while TypedDict and JSON Schema require manual validation Example: Message output alongside parsed structure Example: Message output alongside parsed structure AIMessage object alongside the parsed representation to access response metadata such as token counts. To do this, set include_raw=True when calling with_structured_output :Example: Nested structures Example: Nested structures Supported models LangChain supports all major model providers, including OpenAI, Anthropic, Google, Azure, AWS Bedrock, and more. Each provider offers a variety of models with different capabilities. For a full list of supported models in LangChain, see", "tokens": 1000, "node_type": "child"}
{"id": 416, "chunk_id": "9b0fd3b850698317bef57fa6e71a38ba", "parent_id": 393, "url": "", "namespace": "langchain", "title": "oss-python-langchain-models", "headers": ["oss-python-langchain-models"], "section_index": 0, "chunk_index": 1, "text": "to dedicated structured output features offered by a provider'function_calling' derives structured output by forcing a tool call following the given schema'json_mode' is a precursor to'json_schema' offered by some providers- it generates valid json, but the schema must be described in the prompt - Include raw: Use include_raw=True to get both the parsed output and the raw AI message - Validation: Pydantic models provide automatic validation, while TypedDict and JSON Schema require manual validation Example: Message output alongside parsed structure Example: Message output alongside parsed structure AIMessage object alongside the parsed representation to access response metadata such as token counts. To do this, set include_raw=True when calling with_structured_output :Example: Nested structures Example: Nested structures Supported models LangChain supports all major model providers, including OpenAI, Anthropic, Google, Azure, AWS Bedrock, and more. Each provider offers a variety of models with different capabilities. For a full list of supported models in LangChain, see the integrations page.Advanced topics Multimodal Certain models can process and return non-textual data such as images, audio, and video. You can pass non-textual data to a model by providing content blocks.- Data in the cross-provider standard format (see our messages guide) - OpenAI chat completions format - Any format that is native to that specific provider (e.g., Anthropic models accept Anthropic native format) AIMessage will have content blocks with multimodal types. Reasoning Newer models are capable of performing multi-step reasoning to arrive at a conclusion. This involves breaking down complex problems into smaller, more manageable steps. If supported by the underlying model, you can surface this reasoning process to better understand how the model arrived at its final answer.'low' or 'high' ) or integer token budgets. For details, see the integrations page or reference for your respective chat model. Local models LangChain supports running models locally on your own hardware. This is useful for scenarios where either data privacy is critical, you want to invoke a custom model, or when you want to avoid the costs incurred when using a cloud-based model. Ollama is one of the easiest ways to run models locally. See the full list of local integrations on the integrations page.Prompt caching Many providers offer prompt caching features to reduce latency and cost on repeat processing of the same tokens. These features can be implicit or explicit:- Implicit prompt caching: providers will automatically pass on cost savings if a request hits a cache. Examples: OpenAI and Gemini (Gemini 2.5 and above). - Explicit caching: providers allow you to manually indicate cache points for greater control or to guarantee cost savings. Examples: ChatOpenAI (viaprompt_cache_key ), Anthropic, AWS Bedrock, Gemini. Server-side tool use Some providers support server-side tool-calling loops: models can interact with web search, code interpreters, and other tools and analyze the results in a single conversational turn. If a model invokes a tool server-side, the content of the response message will include content representing the invocation and result of the tool. Accessing the content blocks of the response will return the server-side tool calls and results in a provider-agnostic format:Rate limiting Many chat model providers impose a limit on the number of invocations that can be made in a given time period. If you hit a rate limit, you will typically receive a rate limit error response from the provider, and will need to wait before making more requests. To help manage rate limits, chat model integrations accept arate_limiter parameter that can be provided during initialization to control the rate at which requests are made. Initialize and use a rate limiter Initialize and use a rate limiter InMemoryRateLimiter . This limiter is thread safe and can be shared by multiple threads in the same process.Base URL or proxy For many chat model integrations, you can configure the base URL for API requests, which allows you to use model providers that have OpenAI-compatible APIs or to use a proxy server.Base URL Base URL Proxy configuration Proxy configuration Log probabilities Certain models can be configured to return token-level log probabilities representing the likelihood of a given token by setting thelogprobs parameter when initializing the model: Token usage A number of model providers return token usage information as part of the invocation response. When available, this information will be included on theAIMessage objects produced by the corresponding model. For more details, see the messages guide. - Callback handler - Context manager Invocation config When invoking a model, you can pass additional configuration through theconfig parameter using a RunnableConfig dictionary. This provides run-time control over execution behavior, callbacks, and metadata tracking. Common configuration options include: - Debugging with LangSmith tracing - Implementing custom logging or monitoring - Controlling resource usage in production - Tracking invocations across complex pipelines Key configuration attributes Key configuration attributes Configurable models You can also create a runtime-configurable model by specifyingconfigurable_fields . If you don\u2019t specify a model value, then 'model' and 'model_provider' will be configurable by default. Configurable model with default values Configurable model with default values Using a configurable model declaratively Using a configurable model declaratively bind_tools , with_structured_output , with_configurable , etc. on a configurable model and chain a configurable model in the same way that we would a regularly instantiated chat model object.", "tokens": 860, "node_type": "child"}
{"id": 417, "chunk_id": "c16acf8a553ac24f5d77e7d3a1c6e493", "parent_id": 394, "url": "", "namespace": "langchain", "title": "oss-python-langchain-multi-agent", "headers": ["oss-python-langchain-multi-agent"], "section_index": 0, "chunk_index": 0, "text": "# oss-python-langchain-multi-agent\n\n> Source: https://docs.langchain.com/oss/python/langchain/multi-agent\n\nLangChain v1.0Welcome to the new LangChain documentation! If you encounter any issues or have feedback, please open an issue so we can improve. Archived v0 documentation can be found here.See the release notes and migration guide for a complete list of changes and instructions on how to upgrade your code.\n- A single agent has too many tools and makes poor decisions about which to use.\n- Context or memory grows too large for one agent to track effectively.\n- Tasks require specialization (e.g., a planner, researcher, math expert).\nMulti-agent patterns\n| Pattern | How it works | Control flow | Example use case |\n|---|---|---|---|\n| Tool Calling | A supervisor agent calls other agents as tools. The \u201ctool\u201d agents don\u2019t talk to the user directly \u2014 they just run their task and return results. | Centralized: all routing passes through the calling agent. | Task orchestration, structured workflows. |\n| Handoffs | The current agent decides to transfer control to another agent. The active agent changes, and the user may continue interacting directly with the new agent. | Decentralized: agents can change who is active. | Multi-domain conversations, specialist takeover. |\nTutorial: Build a supervisor agent\nLearn how to build a personal assistant using the supervisor pattern, where a central supervisor agent coordinates specialized worker agents.\nThis tutorial demonstrates:\n- Creating specialized sub-agents for different domains (calendar and email)\n- Wrapping sub-agents as tools for centralized orchestration\n- Adding human-in-the-loop review for sensitive actions\nChoosing a pattern\n| Question | Tool Calling | Handoffs |\n|---|---|---|\n| Need centralized control over workflow? | \u2705 Yes | \u274c No |\n| Want agents to interact directly with the user? | \u274c No | \u2705 Yes |\n| Complex, human-like conversation between specialists? | \u274c Limited | \u2705 Strong |\nYou can mix both patterns \u2014 use handoffs for agent switching, and have each agent call subagents as tools for specialized tasks.\nCustomizing agent context\nAt the heart of multi-agent design is context engineering - deciding what information each agent sees. LangChain gives you fine-grained control over:- Which parts of the conversation or state are passed to each agent.\n- Specialized prompts tailored to subagents.\n- Inclusion/exclusion of intermediate reasoning.\n- Customizing input/output formats per agent.\nTool calling\nIn tool calling, one agent (the \u201ccontroller\u201d) treats other agents as tools to be invoked when needed. The controller manages orchestration, while tool agents perform specific tasks and return results. Flow:- The controller receives input and decides which tool (subagent) to call.\n- The tool agent runs its task based on the controller\u2019s instructions.\n- The tool agent returns results to the controller.\n- The controller decides the next step or finishes.\nAgents used as tools are generally not expected to continue conversation with the user.\nTheir role is to perform a task and return results to the controller agent.\nIf you need subagents to be able to converse with the user, use handoffs instead.\nImplementation\nBelow is a minimal example where the main agent is given access to a single subagent via a tool definition:- The main agent invokes\ncall_subagent1\nwhen it decides the task matches the subagent\u2019s description. - The subagent runs independently and returns its result.\n- The main agent receives the result and continues orchestration.\nWhere to customize\nThere are several points where you can control how context is passed between the main agent and its subagents:- Subagent name (\n\"subagent1_name\"\n): This is how the main agent refers to the subagent. Since it influences prompting, choose it carefully. - Subagent description (\n\"subagent1_description\"\n): This is what the main agent \u201cknows\u201d about the subagent. It directly shapes how the main agent decides when to call it. - Input to the subagent: You can customize this input to better shape how the subagent interprets tasks. In the example above, we pass the agent-generated\nquery\ndirectly. - Output from the subagent: This is the response passed back to the main agent. You can adjust what is returned to control how the main agent interprets results. In the example above, we return the final message text, but you could return additional state or metadata.\nControl the input to the subagent\nThere are two main levers to control the input that the main agent passes to a subagent:- Modify the prompt \u2013 Adjust the main agent\u2019s prompt or the tool metadata (i.e., sub-agent\u2019s name and description) to better guide when and how it calls the subagent.\n- Context injection \u2013 Add input that isn\u2019t practical to capture in a static prompt (e.g., full message history, prior results, task metadata) by adjusting the tool call to pull from the agent\u2019s state.\nControl the output from the subagent\nTwo common strategies for shaping what the main agent receives back from a subagent:- Modify the prompt \u2013 Refine the subagent\u2019s prompt to specify exactly what should be returned.\n- Useful when outputs are incomplete, too verbose, or missing key details.\n- A common failure mode is that the subagent performs tool calls or reasoning but does not include the results in its final message. Remind it that the controller (and user) only see the final output, so all relevant info must be included there.\n- Custom output formatting \u2013 adjust or enrich the subagent\u2019s response in code before handing it back to the main agent.\n- Example: pass specific state keys back to the main agent in addition to the final text.\n- This requires wrapping the result in a\nCommand\n(or equivalent structure) so you can merge custom state with the subagent\u2019s response.\nHandoffs\nIn handoffs, agents can directly pass control to each other. The \u201cactive\u201d agent changes, and the user interacts with whichever agent currently has control. Flow:- The current agent decides it needs help from another agent.\n- It passes control (and state) to the next agent.\n- The new agent interacts directly with the user until it decides to hand off again or finish.", "tokens": 999, "node_type": "child"}
{"id": 418, "chunk_id": "c16acf8a553ac24f5d77e7d3a1c6e493", "parent_id": 395, "url": "", "namespace": "langchain", "title": "oss-python-langchain-observability", "headers": ["oss-python-langchain-observability"], "section_index": 0, "chunk_index": 0, "text": "# oss-python-langchain-observability\n\n> Source: https://docs.langchain.com/oss/python/langchain/observability\n\nLangChain v1.0Welcome to the new LangChain documentation! If you encounter any issues or have feedback, please open an issue so we can improve. Archived v0 documentation can be found here.See the release notes and migration guide for a complete list of changes and instructions on how to upgrade your code.\ncreate_agent\n, you get built-in observability through LangSmith - a powerful platform for tracing, debugging, evaluating, and monitoring your LLM applications.\nTraces capture every step your agent takes, from the initial user input to the final response, including all tool calls, model interactions, and decision points. This enables you to debug your agents, evaluate performance, and monitor usage.\nPrerequisites\nBefore you begin, ensure you have the following:- A LangSmith account (free to sign up)\nEnable tracing\nAll LangChain agents automatically support LangSmith tracing. To enable it, set the following environment variables:Quick start\nNo extra code is needed to log a trace to LangSmith. Just run your agent code as you normally would:default\n. To configure a custom project name, see Log to a project.\nTrace selectively\nYou may opt to trace specific invocations or parts of your application using LangSmith\u2019stracing_context\ncontext manager:\nLog to a project\nStatically\nStatically\nYou can set a custom project name for your entire application by setting the\nLANGSMITH_PROJECT\nenvironment variable:Dynamically\nDynamically\nYou can set the project name programmatically for specific operations:\nAdd metadata to traces\nYou can annotate your traces with custom metadata and tags:tracing_context\nalso accepts tags and metadata for fine-grained control:", "tokens": 253, "node_type": "child"}
{"id": 419, "chunk_id": "c16acf8a553ac24f5d77e7d3a1c6e493", "parent_id": 396, "url": "", "namespace": "langchain", "title": "oss-python-langchain-overview", "headers": ["oss-python-langchain-overview"], "section_index": 0, "chunk_index": 0, "text": "# oss-python-langchain-overview\n\n> Source: https://docs.langchain.com/oss/python/langchain/overview\n\nLangChain v1.0Welcome to the new LangChain documentation! If you encounter any issues or have feedback, please open an issue so we can improve. Archived v0 documentation can be found here.See the release notes and migration guide for a complete list of changes and instructions on how to upgrade your code.\nInstall\nCreate an agent\nCore benefits\nStandard model interface\nDifferent providers have unique APIs for interacting with models, including the format of responses. LangChain standardizes how you interact with models so that you can seamlessly swap providers and avoid lock-in.\nEasy to use, highly flexible agent\nLangChain\u2019s agent abstraction is designed to be easy to get started with, letting you build a simple agent in under 10 lines of code. But it also provides enough flexibility to allow you to do all the context engineering your heart desires.\nBuilt on top of LangGraph\nLangChain\u2019s agents are built on top of LangGraph. This allows us to take advantage of LangGraph\u2019s durable execution, human-in-the-loop support, persistence, and more.\nDebug with LangSmith\nGain deep visibility into complex agent behavior with visualization tools that trace execution paths, capture state transitions, and provide detailed runtime metrics.", "tokens": 196, "node_type": "child"}
{"id": 420, "chunk_id": "c16acf8a553ac24f5d77e7d3a1c6e493", "parent_id": 397, "url": "", "namespace": "langchain", "title": "oss-python-langchain-philosophy", "headers": ["oss-python-langchain-philosophy"], "section_index": 0, "chunk_index": 0, "text": "# oss-python-langchain-philosophy\n\n> Source: https://docs.langchain.com/oss/python/langchain/philosophy\n\n- LLMs are great, powerful new technology.\n- LLMs are even better when you combine them with external sources of data.\n- LLMs will transform what the applications of the future look like. Specifically, the applications of the future will look more and more agentic.\n- It is still very early on in that transformation.\n- While it\u2019s easy to build a prototype of those agentic applications, it\u2019s still really hard to build agents that are reliable enough to put into production.\nWe want to enable developers to build with the best models.\nWe want to make it easy to use models to orchestrate more complex flows that interact with other data and computation.\nHistory\nGiven the constant rate of change in the field, LangChain has also evolved over time. Below is a brief timeline of how LangChain has changed over the years, evolving alongside what it means to build with LLMs:- LLM abstractions\n- \u201cChains\u201d, or predetermined steps of computation to run, for common use cases. For example - RAG: run a retrieval step, then run a generation step.\nlangchain-community\n.langchain-core\nmessage format accordingly to allow developers to specify these multimodal inputs in a standard way.-\nComplete revamp of all chains and agents in\nlangchain\n. All chains and agents are now replaced with only one high level abstraction: an agent abstraction built on top of LangGraph. This was the high-level abstraction that was originally created in LangGraph, but just moved to LangChain. For users still using old LangChain chains/agents who do NOT want to upgrade (note: we recommend you do), you can continue using old LangChain by installing thelangchain-classic\npackage. - A standard message content format: Model APIs evolved from returning messages with a simple content string to more complex output types - reasoning blocks, citations, server-side tool calls, etc. LangChain evolved its message formats to standardize these across providers.", "tokens": 317, "node_type": "child"}
{"id": 421, "chunk_id": "c16acf8a553ac24f5d77e7d3a1c6e493", "parent_id": 398, "url": "", "namespace": "langchain", "title": "oss-python-langchain-quickstart", "headers": ["oss-python-langchain-quickstart"], "section_index": 0, "chunk_index": 0, "text": "# oss-python-langchain-quickstart\n\n> Source: https://docs.langchain.com/oss/python/langchain/quickstart\n\nLangChain v1.0Welcome to the new LangChain documentation! If you encounter any issues or have feedback, please open an issue so we can improve. Archived v0 documentation can be found here.See the release notes and migration guide for a complete list of changes and instructions on how to upgrade your code.\nBuild a basic agent\nStart by creating a simple agent that can answer questions and call tools. The agent will use Claude Sonnet 4.5 as its language model, a basic weather function as a tool, and a simple prompt to guide its behavior.Build a real-world agent\nNext, build a practical weather forecasting agent that demonstrates key production concepts:- Detailed system prompts for better agent behavior\n- Create tools that integrate with external data\n- Model configuration for consistent responses\n- Structured output for predictable results\n- Conversational memory for chat-like interactions\n- Create and run the agent create a fully functional agent\n1\nDefine the system prompt\nThe system prompt defines your agent\u2019s role and behavior. Keep it specific and actionable:\n2\nCreate tools\nTools let a model interact with external systems by calling functions you define.\nTools can depend on runtime context and also interact with agent memory.Notice below how the\nget_user_location\ntool uses runtime context:Tools should be well-documented: their name, description, and argument names become part of the model\u2019s prompt.\nLangChain\u2019s\n@tool\ndecorator adds metadata and enables runtime injection via the ToolRuntime\nparameter.4\nDefine response format\nOptionally, define a structured response format if you need the agent responses to match\na specific schema.\n5\n6\nCreate and run the agent\nNow assemble your agent with all the components and run it!\n- Understand context and remember conversations\n- Use multiple tools intelligently\n- Provide structured responses in a consistent format\n- Handle user-specific information through context\n- Maintain conversation state across interactions", "tokens": 309, "node_type": "child"}
{"id": 422, "chunk_id": "c16acf8a553ac24f5d77e7d3a1c6e493", "parent_id": 399, "url": "", "namespace": "langchain", "title": "oss-python-langchain-rag", "headers": ["oss-python-langchain-rag"], "section_index": 0, "chunk_index": 0, "text": "# oss-python-langchain-rag > Source: https://docs.langchain.com/oss/python/langchain/rag LangGraph v1.0Welcome to the new LangGraph documentation! If you encounter any issues or have feedback, please open an issue so we can improve. Archived v0 documentation can be found here.See the release notes and migration guide for a complete list of changes and instructions on how to upgrade your code. Overview One of the most powerful applications enabled by LLMs is sophisticated question-answering (Q&A) chatbots. These are applications that can answer questions about specific source information. These applications use a technique known as Retrieval Augmented Generation, or RAG. This tutorial will show how to build a simple Q&A application over an unstructured text data source. We will demonstrate:- A RAG agent that executes searches with a simple tool. This is a good general-purpose implementation. - A two-step RAG chain that uses just a single LLM call per query. This is a fast and effective method for simple queries. Concepts We will cover the following concepts:- Indexing: a pipeline for ingesting data from a source and indexing it. This usually happens in a separate process. - Retrieval and generation: the actual RAG process, which takes the user query at run time and retrieves the relevant data from the index, then passes that to the model. The indexing portion of this tutorial will largely follow the semantic search tutorial.If your data is already available for search (i.e., you have a function to execute a search), or you\u2019re comfortable with the content from that tutorial, feel free to skip to the section on retrieval and generation Preview In this guide we\u2019ll build an app that answers questions about the website\u2019s content. The specific website we will use is the LLM Powered Autonomous Agents blog post by Lilian Weng, which allows us to ask questions about the contents of the post. We can create a simple indexing pipeline and RAG chain to do this in ~40 lines of code. See below for the full code snippet:Setup Installation This tutorial requires these langchain dependencies:LangSmith Many of the applications you build with LangChain will contain multiple steps with multiple invocations of LLM calls. As these applications get more complex, it becomes crucial to be able to inspect what exactly is going on inside your chain or agent. The best way to do this is with LangSmith. After you sign up at the link above, make sure to set your environment variables to start logging traces:Components We will need to select three components from LangChain\u2019s suite of integrations. Select a chat model: Select an embeddings model:- OpenAI - Azure - Google Gemini - Google Vertex - AWS - HuggingFace - Ollama - Cohere - MistralAI - Nomic - NVIDIA - Voyage AI - IBM watsonx - Fake - In-memory - AstraDB - Chroma - FAISS - Milvus - MongoDB - PGVector - PGVectorStore - Pinecone - Qdrant 1. Indexing This section is an abbreviated version of the content in the semantic search tutorial.If your data is already indexed and available for search (i.e., you have a function to execute a search), or if you\u2019re comfortable with document loaders, embeddings, and vector stores, feel free to skip to the next section on retrieval and generation. - Load: First we need to load our data. This is done with Document Loaders. - Split: Text splitters break large Documents into smaller chunks. This is useful both for indexing data and passing it into a model, as large chunks are harder to search over and won\u2019t fit in a model\u2019s finite context window. - Store: We need somewhere to store and index our splits, so that they can be searched over later. This is often done using a VectorStore and Embeddings model. Loading documents We need to first load the blog post contents. We can use DocumentLoaders for this, which are objects that load in data from a source and return a list of Document objects. In this case we\u2019ll use the WebBaseLoader, which usesurllib to load HTML from web URLs and BeautifulSoup to parse it to text. We can customize the HTML -> text parsing by passing in parameters into the BeautifulSoup parser via bs_kwargs (see BeautifulSoup docs). In this case only HTML tags with class \u201cpost-content\u201d, \u201cpost-title\u201d, or \u201cpost-header\u201d are relevant, so we\u2019ll remove all others. DocumentLoader : Object that loads data from a source as list of Documents . - Integrations: 160+ integrations to choose from. - Interface: API reference for the base interface. Splitting documents Our loaded document is over 42k characters which is too long to fit into the context window of many models. Even for those models that could fit the full post in their context window, models can struggle to find information in very long inputs. To handle this we\u2019ll split theDocument into chunks for embedding and vector storage. This should help us retrieve only the most relevant parts of the blog post at run time. As in the semantic search tutorial, we use a RecursiveCharacterTextSplitter , which will recursively split the document using common separators like new lines until each chunk is the appropriate size. This is the recommended text splitter for generic text use cases. TextSplitter : Object that splits a list of Document objects into smaller chunks for storage and retrieval. - Integrations - Interface: API reference for the base interface. Storing documents Now we need to index our 66 text chunks so that we can search over them at runtime. Following the semantic search tutorial, our approach is to embed the contents of each document split and insert these embeddings into a vector store. Given an input query, we can then use vector search to retrieve relevant documents. We can embed and store all of our document splits in a single command using the vector store and embeddings model selected at the start of the tutorial.Embeddings : Wrapper around a text embedding model, used for converting text to embeddings. - Integrations: 30+ integrations to choose from. - Interface: API reference for the base interface. VectorStore", "tokens": 1000, "node_type": "child"}
{"id": 423, "chunk_id": "9b0fd3b850698317bef57fa6e71a38ba", "parent_id": 399, "url": "", "namespace": "langchain", "title": "oss-python-langchain-rag", "headers": ["oss-python-langchain-rag"], "section_index": 0, "chunk_index": 1, "text": "recommended text splitter for generic text use cases. TextSplitter : Object that splits a list of Document objects into smaller chunks for storage and retrieval. - Integrations - Interface: API reference for the base interface. Storing documents Now we need to index our 66 text chunks so that we can search over them at runtime. Following the semantic search tutorial, our approach is to embed the contents of each document split and insert these embeddings into a vector store. Given an input query, we can then use vector search to retrieve relevant documents. We can embed and store all of our document splits in a single command using the vector store and embeddings model selected at the start of the tutorial.Embeddings : Wrapper around a text embedding model, used for converting text to embeddings. - Integrations: 30+ integrations to choose from. - Interface: API reference for the base interface. VectorStore : Wrapper around a vector database, used for storing and querying embeddings. - Integrations: 40+ integrations to choose from. - Interface: API reference for the base interface. 2. Retrieval and Generation RAG applications commonly work as follows:- Retrieve: Given a user input, relevant splits are retrieved from storage using a Retriever. - Generate: A model produces an answer using a prompt that includes both the question with the retrieved data - A RAG agent that executes searches with a simple tool. This is a good general-purpose implementation. - A two-step RAG chain that uses just a single LLM call per query. This is a fast and effective method for simple queries. RAG agents One formulation of a RAG application is as a simple agent with a tool that retrieves information. We can assemble a minimal RAG agent by implementing a tool that wraps our vector store:Here we use the @[tool decorator][tool] to configure the tool to attach raw documents as artifacts to each ToolMessage. This will let us access document metadata in our application, separate from the stringified representation that is sent to the model. Retrieval tools are not limited to a single string query argument, as in the above example. You can force the LLM to specify additional search parameters by adding arguments\u2014 for example, a category:- Generates a query to search for a standard method for task decomposition; - Receiving the answer, generates a second query to search for common extensions of it; - Having received all necessary context, answers the question. You can add a deeper level of control and customization using the LangGraph framework directly\u2014 for example, you can add steps to grade document relevance and rewrite search queries. Check out LangGraph\u2019s Agentic RAG tutorial for more advanced formulations. RAG chains In the above agentic RAG formulation we allow the LLM to use its discretion in generating a tool call to help answer user queries. This is a good general-purpose solution, but comes with some trade-offs:| \u2705 Benefits | \u26a0\ufe0f Drawbacks | |---|---| | Search only when needed \u2013 The LLM can handle greetings, follow-ups, and simple queries without triggering unnecessary searches. | Two inference calls \u2013 When a search is performed, it requires one call to generate the query and another to produce the final response. | Contextual search queries \u2013 By treating search as a tool with a query input, the LLM crafts its own queries that incorporate conversational context. | Reduced control \u2013 The LLM may skip searches when they are actually needed, or issue extra searches when unnecessary. | | Multiple searches allowed \u2013 The LLM can execute several searches in support of a single user query. | Returning source documents Returning source documents The above RAG chain incorporates retrieved context into a single system message for that run.As in the agentic RAG formulation, we sometimes want to include raw source documents in the application state to have access to document metadata. We can do this for the two-step chain case by: - Adding a key to the state to store the retrieved documents - Adding a new node via a pre-model hook to populate that key (as well as inject the context). Next steps Now that we\u2019ve implemented a simple RAG application viacreate_agent , we can easily incorporate new features and go deeper: - Stream tokens and other information for responsive user experiences - Add conversational memory to support multi-turn interactions - Add long-term memory to support memory across conversational threads - Add structured responses - Deploy your application with LangSmith Deployments", "tokens": 741, "node_type": "child"}
{"id": 424, "chunk_id": "c16acf8a553ac24f5d77e7d3a1c6e493", "parent_id": 400, "url": "", "namespace": "langchain", "title": "oss-python-langchain-retrieval", "headers": ["oss-python-langchain-retrieval"], "section_index": 0, "chunk_index": 0, "text": "# oss-python-langchain-retrieval\n\n> Source: https://docs.langchain.com/oss/python/langchain/retrieval\n\nLangChain v1.0Welcome to the new LangChain documentation! If you encounter any issues or have feedback, please open an issue so we can improve. Archived v0 documentation can be found here.See the release notes and migration guide for a complete list of changes and instructions on how to upgrade your code.\n- Finite context \u2014 they can\u2019t ingest entire corpora at once.\n- Static knowledge \u2014 their training data is frozen at a point in time.\nBuilding a knowledge base\nA knowledge base is a repository of documents or structured data used during retrieval. If you need a custom knowledge base, you can use LangChain\u2019s document loaders and vector stores to build one from your own data.If you already have a knowledge base (e.g., a SQL database, CRM, or internal documentation system), you do not need to rebuild it. You can:\n- Connect it as a tool for an agent in Agentic RAG.\n- Query it and supply the retrieved content as context to the LLM (2-Step RAG).\nTutorial: Semantic search\nLearn how to create a searchable knowledge base from your own data using LangChain\u2019s document loaders, embeddings, and vector stores.\nIn this tutorial, you\u2019ll build a search engine over a PDF, enabling retrieval of passages relevant to a query. You\u2019ll also implement a minimal RAG workflow on top of this engine to see how external knowledge can be integrated into LLM reasoning.\nFrom retrieval to RAG\nRetrieval allows LLMs to access relevant context at runtime. But most real-world applications go one step further: they integrate retrieval with generation to produce grounded, context-aware answers. This is the core idea behind Retrieval-Augmented Generation (RAG). The retrieval pipeline becomes a foundation for a broader system that combines search with generation.Retrieval Pipeline\nA typical retrieval workflow looks like this: Each component is modular: you can swap loaders, splitters, embeddings, or vector stores without rewriting the app\u2019s logic.Building Blocks\nText splitters\nBreak large docs into smaller chunks that will be retrievable individually and fit within a model\u2019s context window.\nEmbedding models\nAn embedding model turns text into a vector of numbers so that texts with similar meaning land close together in that vector space.\nVector stores\nSpecialized databases for storing and searching embeddings.\nRetrievers\nA retriever is an interface that returns documents given an unstructured query.\nRAG Architectures\nRAG can be implemented in multiple ways, depending on your system\u2019s needs. We outline each type in the sections below.| Architecture | Description | Control | Flexibility | Latency | Example Use Case |\n|---|---|---|---|---|---|\n| 2-Step RAG | Retrieval always happens before generation. Simple and predictable | \u2705 High | \u274c Low | \u26a1 Fast | FAQs, documentation bots |\n| Agentic RAG | An LLM-powered agent decides when and how to retrieve during reasoning | \u274c Low | \u2705 High | \u23f3 Variable | Research assistants with access to multiple tools |\n| Hybrid | Combines characteristics of both approaches with validation steps | \u2696\ufe0f Medium | \u2696\ufe0f Medium | \u23f3 Variable | Domain-specific Q&A with quality validation |\nLatency: Latency is generally more predictable in 2-Step RAG, as the maximum number of LLM calls is known and capped. This predictability assumes that LLM inference time is the dominant factor. However, real-world latency may also be affected by the performance of retrieval steps\u2014such as API response times, network delays, or database queries\u2014which can vary based on the tools and infrastructure in use.\n2-step RAG\nIn 2-Step RAG, the retrieval step is always executed before the generation step. This architecture is straightforward and predictable, making it suitable for many applications where the retrieval of relevant documents is a clear prerequisite for generating an answer.Tutorial: Retrieval-Augmented Generation (RAG)\nSee how to build a Q&A chatbot that can answer questions grounded in your data using Retrieval-Augmented Generation.\nThis tutorial walks through two approaches:\n- A RAG agent that runs searches with a flexible tool\u2014great for general-purpose use.\n- A 2-step RAG chain that requires just one LLM call per query\u2014fast and efficient for simpler tasks.\nAgentic RAG\nAgentic Retrieval-Augmented Generation (RAG) combines the strengths of Retrieval-Augmented Generation with agent-based reasoning. Instead of retrieving documents before answering, an agent (powered by an LLM) reasons step-by-step and decides when and how to retrieve information during the interaction.The only thing an agent needs to enable RAG behavior is access to one or more tools that can fetch external knowledge \u2014 such as documentation loaders, web APIs, or database queries.\nTutorial: Retrieval-Augmented Generation (RAG)\nSee how to build a Q&A chatbot that can answer questions grounded in your data using Retrieval-Augmented Generation.\nThis tutorial walks through two approaches:\n- A RAG agent that runs searches with a flexible tool\u2014great for general-purpose use.\n- A 2-step RAG chain that requires just one LLM call per query\u2014fast and efficient for simpler tasks.\nHybrid RAG\nHybrid RAG combines characteristics of both 2-Step and Agentic RAG. It introduces intermediate steps such as query preprocessing, retrieval validation, and post-generation checks. These systems offer more flexibility than fixed pipelines while maintaining some control over execution. Typical components include:- Query enhancement: Modify the input question to improve retrieval quality. This can involve rewriting unclear queries, generating multiple variations, or expanding queries with additional context.\n- Retrieval validation: Evaluate whether retrieved documents are relevant and sufficient. If not, the system may refine the query and retrieve again.\n- Answer validation: Check the generated answer for accuracy, completeness, and alignment with source content. If needed, the system can regenerate or revise the answer.\n- Applications with ambiguous or underspecified queries\n- Systems that require validation or quality control steps\n- Workflows involving multiple sources or iterative refinement\nTutorial: Agentic RAG with Self-Correction\nAn example of Hybrid RAG that combines agentic reasoning with retrieval and self-correction.", "tokens": 959, "node_type": "child"}
{"id": 425, "chunk_id": "c16acf8a553ac24f5d77e7d3a1c6e493", "parent_id": 401, "url": "", "namespace": "langchain", "title": "oss-python-langchain-runtime", "headers": ["oss-python-langchain-runtime"], "section_index": 0, "chunk_index": 0, "text": "# oss-python-langchain-runtime\n\n> Source: https://docs.langchain.com/oss/python/langchain/runtime\n\nLangChain v1.0Welcome to the new LangChain documentation! If you encounter any issues or have feedback, please open an issue so we can improve. Archived v0 documentation can be found here.See the release notes and migration guide for a complete list of changes and instructions on how to upgrade your code.\nOverview\nLangChain\u2019screate_agent\nruns on LangGraph\u2019s runtime under the hood.\nLangGraph exposes a Runtime object with the following information:\n- Context: static information like user id, db connections, or other dependencies for an agent invocation\n- Store: a BaseStore instance used for long-term memory\n- Stream writer: an object used for streaming information via the\n\"custom\"\nstream mode\nAccess\nWhen creating an agent withcreate_agent\n, you can specify a context_schema\nto define the structure of the context\nstored in the agent Runtime.\nWhen invoking the agent, pass the context\nargument with the relevant configuration for the run:\nInside tools\nYou can access the runtime information inside tools to:- Access the context\n- Read or write long-term memory\n- Write to the custom stream (ex, tool progress / updates)\nToolRuntime\nparameter to access the Runtime object inside a tool.\nInside middleware\nYou can access runtime information in middleware to create dynamic prompts, modify messages, or control agent behavior based on user context. Userequest.runtime\nto access the Runtime object inside middleware decorators. The runtime object is available in the ModelRequest\nparameter passed to middleware functions.", "tokens": 237, "node_type": "child"}
{"id": 426, "chunk_id": "c16acf8a553ac24f5d77e7d3a1c6e493", "parent_id": 402, "url": "", "namespace": "langchain", "title": "oss-python-langchain-short-term-memory", "headers": ["oss-python-langchain-short-term-memory"], "section_index": 0, "chunk_index": 0, "text": "# oss-python-langchain-short-term-memory\n\n> Source: https://docs.langchain.com/oss/python/langchain/short-term-memory\n\nLangChain v1.0Welcome to the new LangChain documentation! If you encounter any issues or have feedback, please open an issue so we can improve. Archived v0 documentation can be found here.See the release notes and migration guide for a complete list of changes and instructions on how to upgrade your code.\nOverview\nMemory is a system that remembers information about previous interactions. For AI agents, memory is crucial because it lets them remember previous interactions, learn from feedback, and adapt to user preferences. As agents tackle more complex tasks with numerous user interactions, this capability becomes essential for both efficiency and user satisfaction. Short term memory lets your application remember previous interactions within a single thread or conversation.A thread organizes multiple interactions in a session, similar to the way email groups messages in a single conversation.\nUsage\nTo add short-term memory (thread-level persistence) to an agent, you need to specify acheckpointer\nwhen creating an agent.\nLangChain\u2019s agent manages short-term memory as a part of your agent\u2019s state.By storing these in the graph\u2019s state, the agent can access the full context for a given conversation while maintaining separation between different threads.State is persisted to a database (or memory) using a checkpointer so the thread can be resumed at any time.Short-term memory updates when the agent is invoked or a step (like a tool call) is completed, and the state is read at the start of each step.\nIn production\nIn production, use a checkpointer backed by a database:Customizing agent memory\nBy default, agents useAgentState\nto manage short term memory, specifically the conversation history via a messages\nkey.\nYou can extend AgentState\nto add additional fields. Custom state schemas are passed to create_agent\nusing the state_schema\nparameter.\nCommon patterns\nWith short-term memory enabled, long conversations can exceed the LLM\u2019s context window. Common solutions are:Trim messages\nRemove first or last N messages (before calling LLM)\nDelete messages\nDelete messages from LangGraph state permanently\nSummarize messages\nSummarize earlier messages in the history and replace them with a summary\nCustom strategies\nCustom strategies (e.g., message filtering, etc.)\nTrim messages\nMost LLMs have a maximum supported context window (denominated in tokens). One way to decide when to truncate messages is to count the tokens in the message history and truncate whenever it approaches that limit. If you\u2019re using LangChain, you can use the trim messages utility and specify the number of tokens to keep from the list, as well as thestrategy\n(e.g., keep the last max_tokens\n) to use for handling the boundary.\nTo trim message history in an agent, use the @before_model\nmiddleware decorator:\nDelete messages\nYou can delete messages from the graph state to manage the message history. This is useful when you want to remove specific messages or clear the entire message history. To delete messages from the graph state, you can use theRemoveMessage\n.\nFor RemoveMessage\nto work, you need to use a state key with add_messages\nreducer.\nThe default AgentState\nprovides this.\nTo remove specific messages:\nWhen deleting messages, make sure that the resulting message history is valid. Check the limitations of the LLM provider you\u2019re using. For example:\n- Some providers expect message history to start with a\nuser\nmessage - Most providers require\nassistant\nmessages with tool calls to be followed by correspondingtool\nresult messages.\nSummarize messages\nThe problem with trimming or removing messages, as shown above, is that you may lose information from culling of the message queue. Because of this, some applications benefit from a more sophisticated approach of summarizing the message history using a chat model. To summarize message history in an agent, use the built-inSummarizationMiddleware\n:\nSummarizationMiddleware\nfor more configuration options.\nAccess memory\nYou can access and modify the short-term memory (state) of an agent in several ways:Tools\nRead short-term memory in a tool\nAccess short term memory (state) in a tool using theToolRuntime\nparameter.\nThe tool_runtime\nparameter is hidden from the tool signature (so the model doesn\u2019t see it), but the tool can access the state through it.\nWrite short-term memory from tools\nTo modify the agent\u2019s short-term memory (state) during execution, you can return state updates directly from the tools. This is useful for persisting intermediate results or making information accessible to subsequent tools or prompts.Prompt\nAccess short term memory (state) in middleware to create dynamic prompts based on conversation history or custom state fields.Output\nBefore model\nAccess short term memory (state) in@before_model\nmiddleware to process messages before model calls.\nAfter model\nAccess short term memory (state) in@after_model\nmiddleware to process messages after model calls.", "tokens": 756, "node_type": "child"}
{"id": 427, "chunk_id": "c16acf8a553ac24f5d77e7d3a1c6e493", "parent_id": 403, "url": "", "namespace": "langchain", "title": "oss-python-langchain-sql-agent", "headers": ["oss-python-langchain-sql-agent"], "section_index": 0, "chunk_index": 0, "text": "# oss-python-langchain-sql-agent\n\n> Source: https://docs.langchain.com/oss/python/langchain/sql-agent\n\nOverview\nIn this tutorial, you will learn how to build an agent that can answer questions about a SQL database using LangChain agents. At a high level, the agent will:Fetch the available tables and schemas from the database\nDecide which tables are relevant to the question\nFetch the schemas for the relevant tables\nGenerate a query based on the question and information from the schemas\nDouble-check the query for common mistakes using an LLM\nExecute the query and return the results\nCorrect mistakes surfaced by the database engine until the query is successful\nFormulate a response based on the results\nConcepts\nWe will cover the following concepts:- Tools for reading from SQL databases\n- LangChain agents\n- Human-in-the-loop processes\nSetup\nInstallation\nLangSmith\nSet up LangSmith to inspect what is happening inside your chain or agent. Then set the following environment variables:1. Select an LLM\nSelect a model that supports tool-calling: The output shown in the examples below used OpenAI.2. Configure the database\nYou will be creating a SQLite database for this tutorial. SQLite is a lightweight database that is easy to set up and use. We will be loading thechinook\ndatabase, which is a sample database that represents a digital media store.\nFor convenience, we have hosted the database (Chinook.db\n) on a public GCS bucket.\nlangchain_community\npackage to interact with the database. The wrapper provides a simple interface to execute SQL queries and fetch results:\n3. Add tools for database interactions\nUse theSQLDatabase\nwrapper available in the langchain_community\npackage to interact with the database. The wrapper provides a simple interface to execute SQL queries and fetch results:\n5. Use create_agent\nUse create_agent\nto build a ReAct agent with minimal code. The agent will interpret the request and generate a SQL command, which the tools will execute. If the command has an error, the error message is returned to the model. The model can then examine the original request and the new error message and generate a new command. This can continue until the LLM generates the command successfully or reaches an end count. This pattern of providing a model with feedback - error messages in this case - is very powerful.\nInitialize the agent with a descriptive system prompt to customize its behavior:\n6. Run the agent\nRun the agent on a sample query and observe its behavior:(Optional) Use Studio\nStudio provides a \u201cclient side\u201d loop as well as memory so you can run this as a chat interface and query the database. You can ask questions like \u201cTell me the scheme of the database\u201d or \u201cShow me the invoices for the 5 top customers\u201d. You will see the SQL command that is generated and the resulting output. The details of how to get that started are below.Run your agent in Studio\nRun your agent in Studio\nlanggraph.json\nfile with the following contents:sql_agent.py\nand insert this:6. Implement human-in-the-loop review\nIt can be prudent to check the agent\u2019s SQL queries before they are executed for any unintended actions or inefficiencies. LangChain agents feature support for built-in human-in-the-loop middleware to add oversight to agent tool calls. Let\u2019s configure the agent to pause for human review on calling thesql_db_query\ntool:\nsql_db_query\ntool:", "tokens": 535, "node_type": "child"}
{"id": 428, "chunk_id": "c16acf8a553ac24f5d77e7d3a1c6e493", "parent_id": 404, "url": "", "namespace": "langchain", "title": "oss-python-langchain-streaming", "headers": ["oss-python-langchain-streaming"], "section_index": 0, "chunk_index": 0, "text": "# oss-python-langchain-streaming\n\n> Source: https://docs.langchain.com/oss/python/langchain/streaming\n\nLangChain v1.0Welcome to the new LangChain documentation! If you encounter any issues or have feedback, please open an issue so we can improve. Archived v0 documentation can be found here.See the release notes and migration guide for a complete list of changes and instructions on how to upgrade your code.\nOverview\nLangChain\u2019s streaming system lets you surface live feedback from agent runs to your application. What\u2019s possible with LangChain streaming:- Stream agent progress \u2014 get state updates after each agent step.\n- Stream LLM tokens \u2014 stream language model tokens as they\u2019re generated.\n- Stream custom updates \u2014 emit user-defined signals (e.g.,\n\"Fetched 10/100 records\"\n). - Stream multiple modes \u2014 choose from\nupdates\n(agent progress),messages\n(LLM tokens + metadata), orcustom\n(arbitrary user data).\nAgent progress\nTo stream agent progress, use thestream()\nor astream()\nmethods with stream_mode=\"updates\"\n. This emits an event after every agent step.\nFor example, if you have an agent that calls a tool once, you should see the following updates:\n- LLM node:\nAIMessage\nwith tool call requests - Tool node:\nToolMessage\nwith execution result - LLM node: Final AI response\nStreaming agent progress\nOutput\nLLM tokens\nTo stream tokens as they are produced by the LLM, usestream_mode=\"messages\"\n. Below you can see the output of the agent streaming tool calls and the final response.\nStreaming LLM tokens\nOutput\nCustom updates\nTo stream updates from tools as they are executed, you can useget_stream_writer\n.\nStreaming custom updates\nOutput\nStream multiple modes\nYou can specify multiple streaming modes by passing stream mode as a list:stream_mode=[\"updates\", \"custom\"]\n:\nStreaming multiple modes\nOutput", "tokens": 269, "node_type": "child"}
{"id": 429, "chunk_id": "c16acf8a553ac24f5d77e7d3a1c6e493", "parent_id": 405, "url": "", "namespace": "langchain", "title": "oss-python-langchain-structured-output", "headers": ["oss-python-langchain-structured-output"], "section_index": 0, "chunk_index": 0, "text": "# oss-python-langchain-structured-output\n\n> Source: https://docs.langchain.com/oss/python/langchain/structured-output\n\nLangChain v1.0Welcome to the new LangChain documentation! If you encounter any issues or have feedback, please open an issue so we can improve. Archived v0 documentation can be found here.See the release notes and migration guide for a complete list of changes and instructions on how to upgrade your code.\ncreate_agent\nhandles structured output automatically. The user sets their desired structured output schema, and when the model generates the structured data, it\u2019s captured, validated, and returned in the 'structured_response'\nkey of the agent\u2019s state.\nResponse Format\nControls how the agent returns structured data:ToolStrategy[StructuredResponseT]\n: Uses tool calling for structured outputProviderStrategy[StructuredResponseT]\n: Uses provider-native structured outputtype[StructuredResponseT]\n: Schema type - automatically selects best strategy based on model capabilitiesNone\n: No structured output\nProviderStrategy\nfor models supporting native structured output (OpenAI, Grok)ToolStrategy\nfor all other models\nstructured_response\nkey of the agent\u2019s final state.\nProvider strategy\nSome model providers support structured output natively through their APIs (currently only OpenAI and Grok). This is the most reliable method when available. To use this strategy, configure aProviderStrategy\n:\nThe schema defining the structured output format. Supports:\n- Pydantic models:\nBaseModel\nsubclasses with field validation - Dataclasses: Python dataclasses with type annotations\n- TypedDict: Typed dictionary classes\n- JSON Schema: Dictionary with JSON schema specification\nProviderStrategy\nwhen you pass a schema type directly to create_agent.response_format\nand the model supports native structured output:\nIf the provider natively supports structured output for your model choice, it is functionally equivalent to write\nresponse_format=ProductReview\ninstead of response_format=ToolStrategy(ProductReview)\n. In either case, if structured output is not supported, the agent will fall back to a tool calling strategy.Tool calling strategy\nFor models that don\u2019t support native structured output, LangChain uses tool calling to achieve the same result. This works with all models that support tool calling, which is most modern models. To use this strategy, configure aToolStrategy\n:\nThe schema defining the structured output format. Supports:\n- Pydantic models:\nBaseModel\nsubclasses with field validation - Dataclasses: Python dataclasses with type annotations\n- TypedDict: Typed dictionary classes\n- JSON Schema: Dictionary with JSON schema specification\n- Union types: Multiple schema options. The model will choose the most appropriate schema based on the context.\nCustom content for the tool message returned when structured output is generated.\nIf not provided, defaults to a message showing the structured response data.\nError handling strategy for structured output validation failures. Defaults to\nTrue\n.True\n: Catch all errors with default error templatestr\n: Catch all errors with this custom messagetype[Exception]\n: Only catch this exception type with default messagetuple[type[Exception], ...]\n: Only catch these exception types with default messageCallable[[Exception], str]\n: Custom function that returns error messageFalse\n: No retry, let exceptions propagate\nCustom tool message content\nThetool_message_content\nparameter allows you to customize the message that appears in the conversation history when structured output is generated:\ntool_message_content\n, our final ToolMessage\nwould be:\nError handling\nModels can make mistakes when generating structured output via tool calling. LangChain provides intelligent retry mechanisms to handle these errors automatically.Multiple structured outputs error\nWhen a model incorrectly calls multiple structured output tools, the agent provides error feedback in aToolMessage\nand prompts the model to retry:\nSchema validation error\nWhen structured output doesn\u2019t match the expected schema, the agent provides specific error feedback:Error handling strategies\nYou can customize how errors are handled using thehandle_errors\nparameter:\nCustom error message:\nhandle_errors\nis a string, the agent will always prompt the model to re-try with a fixed tool message:\nhandle_errors\nis an exception type, the agent will only retry (using the default error message) if the exception raised is the specified type. In all other cases, the exception will be raised.\nHandle multiple exception types:\nhandle_errors\nis a tuple of exceptions, the agent will only retry (using the default error message) if the exception raised is one of the specified types. In all other cases, the exception will be raised.\nCustom error handler function:\nStructuredOutputValidationError\n:\nMultipleStructuredOutputsError\n:", "tokens": 658, "node_type": "child"}
{"id": 430, "chunk_id": "c16acf8a553ac24f5d77e7d3a1c6e493", "parent_id": 406, "url": "", "namespace": "langchain", "title": "oss-python-langchain-studio", "headers": ["oss-python-langchain-studio"], "section_index": 0, "chunk_index": 0, "text": "# oss-python-langchain-studio\n\n> Source: https://docs.langchain.com/oss/python/langchain/studio\n\nLangChain v1.0Welcome to the new LangChain documentation! If you encounter any issues or have feedback, please open an issue so we can improve. Archived v0 documentation can be found here.See the release notes and migration guide for a complete list of changes and instructions on how to upgrade your code.\nPrerequisites\nBefore you begin, ensure you have the following:- An API key for LangSmith (free to sign up)\nSetup local LangGraph server\n1. Install the LangGraph CLI\n2. Prepare your agent\nWe\u2019ll use the following simple agent as an example:agent.py\n3. Environment variables\nCreate a.env\nfile in the root of your project and fill in the necessary API keys. We\u2019ll need to set the LANGSMITH_API_KEY\nenvironment variable to the API key you get from LangSmith.\nBe sure not to commit your\n.env\nto version control systems such as Git!.env\n4. Create a LangGraph config file\nInside your app\u2019s directory, create a configuration filelanggraph.json\n:\nlanggraph.json\ncreate_agent\nautomatically returns a compiled LangGraph graph that we can pass to the graphs\nkey in our configuration file.\nSo far, our project structure looks like this:\n5. Install dependencies\nIn the root of your new LangGraph app, install the dependencies:6. View your agent in Studio\nStart your LangGraph server:Safari blocks\nlocalhost\nconnections to Studio. To work around this, run the above command with --tunnel\nto access Studio via a secure tunnel.http://127.0.0.1:2024\n) and the Studio UI https://smith.langchain.com/studio/?baseUrl=http://127.0.0.1:2024\n:", "tokens": 239, "node_type": "child"}
{"id": 431, "chunk_id": "c16acf8a553ac24f5d77e7d3a1c6e493", "parent_id": 407, "url": "", "namespace": "langchain", "title": "oss-python-langchain-supervisor", "headers": ["oss-python-langchain-supervisor"], "section_index": 0, "chunk_index": 0, "text": "# oss-python-langchain-supervisor\n\n> Source: https://docs.langchain.com/oss/python/langchain/supervisor\n\nLangGraph v1.0Welcome to the new LangGraph documentation! If you encounter any issues or have feedback, please open an issue so we can improve. Archived v0 documentation can be found here.See the release notes and migration guide for a complete list of changes and instructions on how to upgrade your code.\nOverview\nThe supervisor pattern is a multi-agent architecture where a central supervisor agent coordinates specialized worker agents. This approach excels when tasks require different types of expertise. Rather than building one agent that manages tool selection across domains, you create focused specialists coordinated by a supervisor who understands the overall workflow. In this tutorial, you\u2019ll build a personal assistant system that demonstrates these benefits through a realistic workflow. The system will coordinate two specialists with fundamentally different responsibilities:- A calendar agent that handles scheduling, availability checking, and event management.\n- An email agent that manages communication, drafts messages, and sends notifications.\nWhy use a supervisor?\nMulti-agent architectures allow you to partition tools across workers, each with their own individual prompts or instructions. Consider an agent with direct access to all calendar and email APIs: it must choose from many similar tools, understand exact formats for each API, and handle multiple domains simultaneously. If performance degrades, it may be helpful to separate related tools and associated prompts into logical groups (in part to manage iterative improvements).Concepts\nWe will cover the following concepts:Setup\nInstallation\nThis tutorial requires thelangchain\npackage:\nLangSmith\nSet up LangSmith to inspect what is happening inside your agent. Then set the following environment variables:Components\nWe will need to select a chat model from LangChain\u2019s suite of integrations:1. Define tools\nStart by defining the tools that require structured inputs. In real applications, these would call actual APIs (Google Calendar, SendGrid, etc.). For this tutorial, you\u2019ll use stubs to demonstrate the pattern.2. Create specialized sub-agents\nNext, we\u2019ll create specialized sub-agents that handle each domain.Create a calendar agent\nThe calendar agent understands natural language scheduling requests and translates them into precise API calls. It handles date parsing, availability checking, and event creation.create_calendar_event\n, and returns a natural language confirmation.\nCreate an email agent\nThe email agent handles message composition and sending. It focuses on extracting recipient information, crafting appropriate subject lines and body text, and managing email communication.send_email\n, and returns a confirmation. Each sub-agent has a narrow focus with domain-specific tools and prompts, allowing it to excel at its specific task.\n3. Wrap sub-agents as tools\nNow wrap each sub-agent as a tool that the supervisor can invoke. This is the key architectural step that creates the layered system. The supervisor will see high-level tools like \u201cschedule_event\u201d, not low-level tools like \u201ccreate_calendar_event\u201d.4. Create the supervisor agent\nNow create the supervisor that orchestrates the sub-agents. The supervisor only sees high-level tools and makes routing decisions at the domain level, not the individual API level.5. Use the supervisor\nNow test your complete system with complex requests that require coordination across multiple domains:Example 1: Simple single-domain request\nschedule_event\n, and the calendar agent handles date parsing and event creation.\nExample 2: Complex multi-domain request\nschedule_event\nfor the meeting, then calls manage_email\nfor the reminder. Each sub-agent completes its task, and the supervisor synthesizes both results into a coherent response.\nComplete working example\nHere\u2019s everything together in a runnable script:Understanding the architecture\nYour system has three layers. The bottom layer contains rigid API tools that require exact formats. The middle layer contains sub-agents that accept natural language, translate it to structured API calls, and return natural language confirmations. The top layer contains the supervisor that routes to high-level capabilities and synthesizes results. This separation of concerns provides several benefits: each layer has a focused responsibility, you can add new domains without affecting existing ones, and you can test and iterate on each layer independently.6. Add human-in-the-loop review\nIt can be prudent to incorporate human-in-the-loop review of sensitive actions. LangChain includes built-in middleware to review tool calls, in this case the tools invoked by sub-agents. Let\u2019s add human-in-the-loop review to both sub-agents:- We configure the\ncreate_calendar_event\nandsend_email\ntools to interrupt, permitting all response types (approve\n,edit\n,reject\n) - We add a checkpointer only to the top-level agent. This is required to pause and resume execution.\nCommand\n. Refer to the human-in-the-loop guide for additional details. For demonstration purposes, here we will accept the calendar event, but edit the subject of the outbound email:\n7. Advanced: Control information flow\nBy default, sub-agents receive only the request string from the supervisor. You might want to pass additional context, such as conversation history or user preferences.Pass additional conversational context to sub-agents\nControl what supervisor receives\nYou can also customize what information flows back to the supervisor:8. Key takeaways\nThe supervisor pattern creates layers of abstraction where each layer has a clear responsibility. When designing a supervisor system, start with clear domain boundaries and give each sub-agent focused tools and prompts. Write clear tool descriptions for the supervisor, test each layer independently before integration, and control information flow based on your specific needs.When to use the supervisor patternUse the supervisor pattern when you have multiple distinct domains (calendar, email, CRM, database), each domain has multiple tools or complex logic, you want centralized workflow control, and sub-agents don\u2019t need to converse directly with users.For simpler cases with just a few tools, use a single agent. When agents need to have conversations with users, use handoffs instead. For peer-to-peer collaboration between agents, consider other multi-agent patterns.", "tokens": 910, "node_type": "child"}
{"id": 432, "chunk_id": "c16acf8a553ac24f5d77e7d3a1c6e493", "parent_id": 408, "url": "", "namespace": "langchain", "title": "oss-python-langchain-test", "headers": ["oss-python-langchain-test"], "section_index": 0, "chunk_index": 0, "text": "# oss-python-langchain-test\n\n> Source: https://docs.langchain.com/oss/python/langchain/test\n\n- Unit tests exercise small, deterministic pieces of your agent in isolation using in-memory fakes so you can assert exact behavior quickly and deterministically.\n- Integration tests test the agent using real network calls to confirm that components work together, credentials and schemas line up, and latency is acceptable.\nUnit Testing\nMocking Chat Model\nFor logic not requiring API calls, you can use an in-memory stub for mocking responses. LangChain providesGenericFakeChatModel\nfor mocking text responses. It accepts an iterator of responses (AIMessages or strings) and returns one per invocation. It supports both regular and streaming usage.\nInMemorySaver Checkpointer\nTo enable persistence during testing, you can use theInMemorySaver\ncheckpointer. This allows you to simulate multiple turns to test state-dependent behavior:\nIntegration Testing\nMany agent behaviors only emerge when using a real LLM, such as which tool the agent decides to call, how it formats responses, or whether a prompt modification affects the entire execution trajectory. LangChain\u2019sagentevals\npackage provides evaluators specifically designed for testing agent trajectories with live models.\nAgentEvals lets you easily evaluate the trajectory of your agent (the exact sequence of messages, including tool calls) by performing a trajectory match or by using an LLM judge:\nTrajectory match\nLLM-as-judge\nInstalling AgentEvals\nTrajectory Match Evaluator\nAgentEvals offers thecreate_trajectory_match_evaluator\nfunction to match your agent\u2019s trajectory against a reference trajectory. There are four modes to choose from:\n| Mode | Description | Use Case |\n|---|---|---|\nstrict | Exact match of messages and tool calls in the same order | Testing specific sequences (e.g., policy lookup before authorization) |\nunordered | Same tool calls allowed in any order | Verifying information retrieval when order doesn\u2019t matter |\nsubset | Agent calls only tools from reference (no extras) | Ensuring agent doesn\u2019t exceed expected scope |\nsuperset | Agent calls at least the reference tools (extras allowed) | Verifying minimum required actions are taken |\nStrict match\nStrict match\nstrict\nmode ensures trajectories contain identical messages in the same order with the same tool calls, though it allows for differences in message content. This is useful when you need to enforce a specific sequence of operations, such as requiring a policy lookup before authorizing an action.Unordered match\nUnordered match\nunordered\nmode allows the same tool calls in any order, which is helpful when you want to verify that specific information was retrieved but don\u2019t care about the sequence. For example, an agent might need to check both weather and events for a city, but the order doesn\u2019t matter.Subset and superset match\nSubset and superset match\nsuperset\nand subset\nmodes match partial trajectories. The superset\nmode verifies that the agent called at least the tools in the reference trajectory, allowing additional tool calls. The subset\nmode ensures the agent did not call any tools beyond those in the reference.tool_args_match_mode\nproperty and/or tool_args_match_overrides\nto customize how the evaluator considers equality between tool calls in the actual trajectory vs. the reference. By default, only tool calls with the same arguments to the same tool are considered equal. Visit the repository for more details.LLM-as-Judge Evaluator\nYou can also use an LLM to evaluate the agent\u2019s execution path with thecreate_trajectory_llm_as_judge\nfunction. Unlike the trajectory match evaluators, it doesn\u2019t require a reference trajectory, but one can be provided if available.\nWithout reference trajectory\nWithout reference trajectory\nWith reference trajectory\nWith reference trajectory\nTRAJECTORY_ACCURACY_PROMPT_WITH_REFERENCE\nprompt and configure the reference_outputs\nvariable:Async Support\nAllagentevals\nevaluators support Python asyncio. For evaluators that use factory functions, async versions are available by adding async\nafter create_\nin the function name.\nAsync judge and evaluator example\nAsync judge and evaluator example\nLangSmith Integration\nFor tracking experiments over time, you can log evaluator results to LangSmith, a platform for building production-grade LLM applications that includes tracing, evaluation, and experimentation tools. First, set up LangSmith by setting the required environment variables:evaluate\nfunction.\nUsing pytest integration\nUsing pytest integration\nUsing the evaluate function\nUsing the evaluate function\nevaluate\nfunction:Recording & Replaying HTTP Calls\nIntegration tests that call real LLM APIs can be slow and expensive, especially when run frequently in CI/CD pipelines. We recommend using a library for recording HTTP requests and responses, then replaying them in subsequent runs without making actual network calls. You can usevcrpy\nto achieve this. If you\u2019re using pytest\n, the pytest-recording\nplugin provides a simple way to enable this with minimal configuration. Request/responses are recorded in cassettes, which are then used to mock the real network calls in subsequent runs.\nSet up your conftest.py\nfile to filter out sensitive information from the cassettes:\nvcr\nmarker:\n--record-mode=once\noption records HTTP interactions on the first run and replays them on subsequent runs.vcr\nmarker:\ntest_agent_trajectory.yaml\nin the tests/cassettes\ndirectory. Subsequent runs will use that cassette to mock the real network calls, granted the agent\u2019s requests don\u2019t change from the previous run. If they do, the test will fail and you\u2019ll need to delete the cassette and rerun the test to record fresh interactions.", "tokens": 821, "node_type": "child"}
{"id": 433, "chunk_id": "c16acf8a553ac24f5d77e7d3a1c6e493", "parent_id": 409, "url": "", "namespace": "langchain", "title": "oss-python-langchain-tools", "headers": ["oss-python-langchain-tools"], "section_index": 0, "chunk_index": 0, "text": "# oss-python-langchain-tools\n\n> Source: https://docs.langchain.com/oss/python/langchain/tools\n\nLangChain v1.0Welcome to the new LangChain documentation! If you encounter any issues or have feedback, please open an issue so we can improve. Archived v0 documentation can be found here.See the release notes and migration guide for a complete list of changes and instructions on how to upgrade your code.\nServer-side tool useSome chat models (e.g., OpenAI, Anthropic, and Gemini) feature built-in tools that are executed server-side, such as web search and code interpreters. Refer to the provider overview to learn how to access these tools with your specific chat model.\nCreate tools\nBasic tool definition\nThe simplest way to create a tool is with the@tool\ndecorator. By default, the function\u2019s docstring becomes the tool\u2019s description that helps the model understand when to use it:\nCustomize tool properties\nCustom tool name\nBy default, the tool name comes from the function name. Override it when you need something more descriptive:Custom tool description\nOverride the auto-generated tool description for clearer model guidance:Advanced schema definition\nDefine complex inputs with Pydantic models or JSON schemas:Accessing Context\nWhy this matters: Tools are most powerful when they can access agent state, runtime context, and long-term memory. This enables tools to make context-aware decisions, personalize responses, and maintain information across conversations.\nToolRuntime\nparameter, which provides:\n- State - Mutable data that flows through execution (messages, counters, custom fields)\n- Context - Immutable configuration like user IDs, session details, or application-specific configuration\n- Store - Persistent long-term memory across conversations\n- Stream Writer - Stream custom updates as tools execute\n- Config - RunnableConfig for the execution\n- Tool Call ID - ID of the current tool call\nToolRuntime\nUseToolRuntime\nto access all runtime information in a single parameter. Simply add runtime: ToolRuntime\nto your tool signature, and it will be automatically injected without being exposed to the LLM.\nToolRuntime\n: A unified parameter that provides tools access to state, context, store, streaming, config, and tool call ID. This replaces the older pattern of using separate InjectedState\n, InjectedStore\n, get_runtime()\n, and InjectedToolCallId\nannotations.ToolRuntime\n:\nThe\ntool_runtime\nparameter is hidden from the model. For the example above, the model only sees pref_name\nin the tool schema - tool_runtime\nis not included in the request.Command\nto update the agent\u2019s state or control the graph\u2019s execution flow:\nContext\nAccess immutable configuration and contextual data like user IDs, session details, or application-specific configuration throughruntime.context\n.\nTools can access runtime context through ToolRuntime\n:\nMemory (Store)\nAccess persistent data across conversations using the store. The store is accessed viaruntime.store\nand allows you to save and retrieve user-specific or application-specific data.\nTools can access and update the store through ToolRuntime\n:\nStream Writer\nStream custom updates from tools as they execute usingruntime.stream_writer\n. This is useful for providing real-time feedback to users about what a tool is doing.", "tokens": 471, "node_type": "child"}
{"id": 434, "chunk_id": "c16acf8a553ac24f5d77e7d3a1c6e493", "parent_id": 410, "url": "", "namespace": "langchain", "title": "oss-python-langchain-ui", "headers": ["oss-python-langchain-ui"], "section_index": 0, "chunk_index": 0, "text": "# oss-python-langchain-ui\n\n> Source: https://docs.langchain.com/oss/python/langchain/ui\n\nLangChain v1.0Welcome to the new LangChain documentation! If you encounter any issues or have feedback, please open an issue so we can improve. Archived v0 documentation can be found here.See the release notes and migration guide for a complete list of changes and instructions on how to upgrade your code.\ncreate_agent\n. This UI is designed to provide rich, interactive experiences for your agents with minimal setup, whether you\u2019re running locally or in a deployed context (such as LangSmith).\nAgent Chat UI\nAgent Chat UI is a Next.js application that provides a conversational interface for interacting with any LangChain agent. It supports real-time chat, tool visualization, and advanced features like time-travel debugging and state forking. Agent Chat UI is open source and can be adapted to your application needs.Features\nTool visualization\nTool visualization\nStudio automatically renders tool calls and results in an intuitive interface.\nTime-travel debugging\nTime-travel debugging\nNavigate through conversation history and fork from any point\nState inspection\nState inspection\nView and modify agent state at any point during execution\nHuman-in-the-loop\nHuman-in-the-loop\nBuilt-in support for reviewing and responding to agent requests\nQuick start\nThe fastest way to get started is using the hosted version:- Visit Agent Chat UI\n- Connect your agent by entering your deployment URL or local server address\n- Start chatting - the UI will automatically detect and render tool calls and interrupts\nLocal development\nFor customization or local development, you can run Agent Chat UI locally:Connect to your agent\nAgent Chat UI can connect to both local and deployed agents. After starting Agent Chat UI, you\u2019ll need to configure it to connect to your agent:- Graph ID: Enter your graph name (find this under\ngraphs\nin yourlanggraph.json\nfile) - Deployment URL: Your LangGraph server\u2019s endpoint (e.g.,\nhttp://localhost:2024\nfor local development, or your deployed agent\u2019s URL) - LangSmith API key (optional): Add your LangSmith API key (not required if you\u2019re using a local LangGraph server)", "tokens": 324, "node_type": "child"}
{"id": 435, "chunk_id": "c16acf8a553ac24f5d77e7d3a1c6e493", "parent_id": 411, "url": "", "namespace": "langchain", "title": "oss-python-langgraph-add-memory", "headers": ["oss-python-langgraph-add-memory"], "section_index": 0, "chunk_index": 0, "text": "# oss-python-langgraph-add-memory\n\n> Source: https://docs.langchain.com/oss/python/langgraph/add-memory\n\n- Add short-term memory as a part of your agent\u2019s state to enable multi-turn conversations.\n- Add long-term memory to store user-specific or application-level data across sessions.\nAdd short-term memory\nShort-term memory (thread-level persistence) enables agents to track multi-turn conversations. To add short-term memory:Use in production\nIn production, use a checkpointer backed by a database:Example: using Postgres checkpointer\nExample: using Postgres checkpointer\ncheckpointer.setup()\nthe first time you\u2019re using Postgres checkpointer- Sync\n- Async\nExample: using [MongoDB](https://pypi.org/project/langgraph-checkpoint-mongodb/) checkpointer\nExample: using [MongoDB](https://pypi.org/project/langgraph-checkpoint-mongodb/) checkpointer\nExample: using [Redis](https://pypi.org/project/langgraph-checkpoint-redis/) checkpointer\nExample: using [Redis](https://pypi.org/project/langgraph-checkpoint-redis/) checkpointer\ncheckpointer.setup()\nthe first time you\u2019re using Redis checkpointer- Sync\n- Async\nUse in subgraphs\nIf your graph contains subgraphs, you only need to provide the checkpointer when compiling the parent graph. LangGraph will automatically propagate the checkpointer to the child subgraphs.Add long-term memory\nUse long-term memory to store user-specific or application-specific data across conversations.Use in production\nIn production, use a store backed by a database:Example: using Postgres store\nExample: using Postgres store\nstore.setup()\nthe first time you\u2019re using Postgres store- Sync\n- Async\nExample: using [Redis](https://pypi.org/project/langgraph-checkpoint-redis/) store\nExample: using [Redis](https://pypi.org/project/langgraph-checkpoint-redis/) store\nstore.setup()\nthe first time you\u2019re using Redis store- Sync\n- Async\nUse semantic search\nEnable semantic search in your graph\u2019s memory store to let graph agents search for items in the store by semantic similarity.Long-term memory with semantic search\nLong-term memory with semantic search\nManage short-term memory\nWith short-term memory enabled, long conversations can exceed the LLM\u2019s context window. Common solutions are:- Trim messages: Remove first or last N messages (before calling LLM)\n- Delete messages from LangGraph state permanently\n- Summarize messages: Summarize earlier messages in the history and replace them with a summary\n- Manage checkpoints to store and retrieve message history\n- Custom strategies (e.g., message filtering, etc.)\nTrim messages\nMost LLMs have a maximum supported context window (denominated in tokens). One way to decide when to truncate messages is to count the tokens in the message history and truncate whenever it approaches that limit. If you\u2019re using LangChain, you can use the trim messages utility and specify the number of tokens to keep from the list, as well as thestrategy\n(e.g., keep the last max_tokens\n) to use for handling the boundary.\nTo trim message history, use the trim_messages\nfunction:\nFull example: trim messages\nFull example: trim messages\nDelete messages\nYou can delete messages from the graph state to manage the message history. This is useful when you want to remove specific messages or clear the entire message history. To delete messages from the graph state, you can use theRemoveMessage\n. For RemoveMessage\nto work, you need to use a state key with add_messages\nreducer, like MessagesState\n.\nTo remove specific messages:\n- some providers expect message history to start with a\nuser\nmessage - most providers require\nassistant\nmessages with tool calls to be followed by correspondingtool\nresult messages.\nFull example: delete messages\nFull example: delete messages\nSummarize messages\nThe problem with trimming or removing messages, as shown above, is that you may lose information from culling of the message queue. Because of this, some applications benefit from a more sophisticated approach of summarizing the message history using a chat model. Prompting and orchestration logic can be used to summarize the message history. For example, in LangGraph you can extend theMessagesState\nto include a summary\nkey:\nsummarize_conversation\nnode can be called after some number of messages have accumulated in the messages\nstate key.\nFull example: summarize messages\nFull example: summarize messages\n- We will keep track of our running summary in the\ncontext\nfield\nSummarizationNode\n).- Define private state that will be used only for filtering\ncall_model\nnode.- We\u2019re passing a private input state here to isolate the messages returned by the summarization node\nManage checkpoints\nYou can view and delete the information stored by the checkpointer.View thread state\n- Graph/Functional API\n- Checkpointer API\nView the history of the thread\n- Graph/Functional API\n- Checkpointer API", "tokens": 657, "node_type": "child"}
{"id": 436, "chunk_id": "c16acf8a553ac24f5d77e7d3a1c6e493", "parent_id": 412, "url": "", "namespace": "langchain", "title": "oss-python-langgraph-agentic-rag", "headers": ["oss-python-langgraph-agentic-rag"], "section_index": 0, "chunk_index": 0, "text": "# oss-python-langgraph-agentic-rag\n\n> Source: https://docs.langchain.com/oss/python/langgraph/agentic-rag\n\nLangGraph v1.0Welcome to the new LangGraph documentation! If you encounter any issues or have feedback, please open an issue so we can improve. Archived v0 documentation can be found here.See the release notes and migration guide for a complete list of changes and instructions on how to upgrade your code.\nOverview\nIn this tutorial we will build a retrieval agent using LangGraph. LangChain offers built-in agent implementations, implemented using LangGraph primitives. If deeper customization is required, agents can be implemented directly in LangGraph. This guide demonstrates an example implementation of a retrieval agent. Retrieval agents are useful when you want an LLM to make a decision about whether to retrieve context from a vectorstore or respond to the user directly. By the end of the tutorial we will have done the following:- Fetch and preprocess documents that will be used for retrieval.\n- Index those documents for semantic search and create a retriever tool for the agent.\n- Build an agentic RAG system that can decide when to use the retriever tool.\nConcepts\nWe will cover the following concepts:- Retrieval using document loaders, text splitters, embeddings, and vector stores\n- The LangGraph Graph API, including state, nodes, edges, and conditional edges.\nSetup\nLet\u2019s download the required packages and set our API keys:1. Preprocess documents\n- Fetch documents to use in our RAG system. We will use three of the most recent pages from Lilian Weng\u2019s excellent blog. We\u2019ll start by fetching the content of the pages using\nWebBaseLoader\nutility:\n- Split the fetched documents into smaller chunks for indexing into our vectorstore:\n2. Create a retriever tool\nNow that we have our split documents, we can index them into a vector store that we\u2019ll use for semantic search.- Use an in-memory vector store and OpenAI embeddings:\n- Create a retriever tool using LangChain\u2019s prebuilt\ncreate_retriever_tool\n:\n- Test the tool:\n3. Generate query\nNow we will start building components (nodes and edges) for our agentic RAG graph. Note that the components will operate on theMessagesState\n\u2014 graph state that contains a messages\nkey with a list of chat messages.\n- Build a\ngenerate_query_or_respond\nnode. It will call an LLM to generate a response based on the current graph state (list of messages). Given the input messages, it will decide to retrieve using the retriever tool, or respond directly to the user. Note that we\u2019re giving the chat model access to theretriever_tool\nwe created earlier via.bind_tools\n:\n- Try it on a random input:\n- Ask a question that requires semantic search:\n4. Grade documents\n- Add a conditional edge \u2014\ngrade_documents\n\u2014 to determine whether the retrieved documents are relevant to the question. We will use a model with a structured output schemaGradeDocuments\nfor document grading. Thegrade_documents\nfunction will return the name of the node to go to based on the grading decision (generate_answer\norrewrite_question\n):\n- Run this with irrelevant documents in the tool response:\n- Confirm that the relevant documents are classified as such:\n5. Rewrite question\n- Build the\nrewrite_question\nnode. The retriever tool can return potentially irrelevant documents, which indicates a need to improve the original user question. To do so, we will call therewrite_question\nnode:\n- Try it out:\n6. Generate an answer\n- Build\ngenerate_answer\nnode: if we pass the grader checks, we can generate the final answer based on the original question and the retrieved context:\n- Try it:\n7. Assemble the graph\nNow we\u2019ll assemble all the nodes and edges into a complete graph:- Start with a\ngenerate_query_or_respond\nand determine if we need to callretriever_tool\n- Route to next step using\ntools_condition\n:- If\ngenerate_query_or_respond\nreturnedtool_calls\n, callretriever_tool\nto retrieve context - Otherwise, respond directly to the user\n- If\n- Grade retrieved document content for relevance to the question (\ngrade_documents\n) and route to next step:- If not relevant, rewrite the question using\nrewrite_question\nand then callgenerate_query_or_respond\nagain - If relevant, proceed to\ngenerate_answer\nand generate final response using theToolMessage\nwith the retrieved document context\n- If not relevant, rewrite the question using", "tokens": 678, "node_type": "child"}
{"id": 437, "chunk_id": "c16acf8a553ac24f5d77e7d3a1c6e493", "parent_id": 413, "url": "", "namespace": "langchain", "title": "oss-python-langgraph-application-structure", "headers": ["oss-python-langgraph-application-structure"], "section_index": 0, "chunk_index": 0, "text": "# oss-python-langgraph-application-structure\n\n> Source: https://docs.langchain.com/oss/python/langgraph/application-structure\n\nLangGraph v1.0Welcome to the new LangGraph documentation! If you encounter any issues or have feedback, please open an issue so we can improve. Archived v0 documentation can be found here.See the release notes and migration guide for a complete list of changes and instructions on how to upgrade your code.\nOverview\nA LangGraph application consists of one or more graphs, a configuration file (langgraph.json\n), a file that specifies dependencies, and an optional .env\nfile that specifies environment variables.\nThis guide shows a typical structure of an application and shows how the required information to deploy an application using the LangSmith is specified.\nKey Concepts\nTo deploy using the LangSmith, the following information should be provided:- A LangGraph configuration file (\nlanggraph.json\n) that specifies the dependencies, graphs, and environment variables to use for the application. - The graphs that implement the logic of the application.\n- A file that specifies dependencies required to run the application.\n- Environment variables that are required for the application to run.\nFile Structure\nBelow are examples of directory structures for applications:- Python (requirements.txt)\n- Python (pyproject.toml)\nThe directory structure of a LangGraph application can vary depending on the programming language and the package manager used.\nConfiguration File\nThelanggraph.json\nfile is a JSON file that specifies the dependencies, graphs, environment variables, and other settings required to deploy a LangGraph application.\nSee the LangGraph configuration file reference for details on all supported keys in the JSON file.\nExamples\n- The dependencies involve a custom local package and the\nlangchain_openai\npackage. - A single graph will be loaded from the file\n./your_package/your_file.py\nwith the variablevariable\n. - The environment variables are loaded from the\n.env\nfile.\nDependencies\nA LangGraph application may depend on other Python packages. You will generally need to specify the following information for dependencies to be set up correctly:-\nA file in the directory that specifies the dependencies (e.g.\nrequirements.txt\n,pyproject.toml\n, orpackage.json\n). -\nA\ndependencies\nkey in the LangGraph configuration file that specifies the dependencies required to run the LangGraph application. -\nAny additional binaries or system libraries can be specified using\ndockerfile_lines\nkey in the LangGraph configuration file.\nGraphs\nUse thegraphs\nkey in the LangGraph configuration file to specify which graphs will be available in the deployed LangGraph application.\nYou can specify one or more graphs in the configuration file. Each graph is identified by a name (which should be unique) and a path for either: (1) the compiled graph or (2) a function that makes a graph is defined.\nEnvironment Variables\nIf you\u2019re working with a deployed LangGraph application locally, you can configure environment variables in theenv\nkey of the LangGraph configuration file.\nFor a production deployment, you will typically want to configure the environment variables in the deployment environment.", "tokens": 466, "node_type": "child"}
{"id": 438, "chunk_id": "c16acf8a553ac24f5d77e7d3a1c6e493", "parent_id": 414, "url": "", "namespace": "langchain", "title": "oss-python-langgraph-case-studies", "headers": ["oss-python-langgraph-case-studies"], "section_index": 0, "chunk_index": 0, "text": "# oss-python-langgraph-case-studies\n\n> Source: https://docs.langchain.com/oss/python/langgraph/case-studies\n\nLangGraph v1.0Welcome to the new LangGraph documentation! If you encounter any issues or have feedback, please open an issue so we can improve. Archived v0 documentation can be found here.See the release notes and migration guide for a complete list of changes and instructions on how to upgrade your code.\n| Company | Industry | Use case | Reference |\n|---|---|---|---|\n| AirTop | Software & Technology (GenAI Native) | Browser automation for AI agents | Case study, 2024 |\n| AppFolio | Real Estate | Copilot for domain-specific task | Case study, 2024 |\n| Athena Intelligence | Software & Technology (GenAI Native) | Research & summarization | Case study, 2024 |\n| BlackRock | Financial Services | Copilot for domain-specific task | Interrupt talk, 2025 |\n| Captide | Software & Technology (GenAI Native) | Data extraction | Case study, 2025 |\n| Cisco CX | Software & Technology | Customer support | Interrupt Talk, 2025 |\n| Cisco Outshift | Software & Technology | DevOps | Video story, 2025; Case study, 2025; Blog post, 2025 |\n| Cisco TAC | Software & Technology | Customer support | Video story, 2025 |\n| City of Hope | Non-profit | Copilot for domain-specific task | Video story, 2025 |\n| C.H. Robinson | Logistics | Automation | Case study, 2025 |\n| Definely | Legal | Copilot for domain-specific task | Case study, 2025 |\n| Docent Pro | Travel | GenAI embedded product experiences | Case study, 2025 |\n| Elastic | Software & Technology | Copilot for domain-specific task | Blog post, 2025 |\n| Exa | Software & Technology (GenAI Native) | Search | Case study, 2025 |\n| GitLab | Software & Technology | Code generation | Duo workflow docs |\n| Harmonic | Software & Technology | Search | Case study, 2025 |\n| Inconvo | Software & Technology | Code generation | Case study, 2025 |\n| Infor | Software & Technology | GenAI embedded product experiences; customer support; copilot | Case study, 2025 |\n| J.P. Morgan | Financial Services | Copilot for domain-specific task | Interrupt talk, 2025 |\n| Klarna | Fintech | Copilot for domain-specific task | Case study, 2025 |\n| Komodo Health | Healthcare | Copilot for domain-specific task | Blog post |\n| Social Media | Code generation; Search & discovery | Interrupt talk, 2025; Blog post, 2025; Blog post, 2024 | |\n| Minimal | E-commerce | Customer support | Case study, 2025 |\n| Modern Treasury | Fintech | GenAI embedded product experiences | Video story, 2025 |\n| Monday | Software & Technology | GenAI embedded product experiences | Interrupt talk, 2025 |\n| Morningstar | Financial Services | Research & summarization | Video story, 2025 |\n| OpenRecovery | Healthcare | Copilot for domain-specific task | Case study, 2024 |\n| Pigment | Fintech | GenAI embedded product experiences | Video story, 2025 |\n| Prosper | Fintech | Customer support | Video story, 2025 |\n| Qodo | Software & Technology (GenAI Native) | Code generation | Blog post, 2025 |\n| Rakuten | E-commerce / Fintech | Copilot for domain-specific task | Video story, 2025; Blog post, 2025 |\n| Replit | Software & Technology | Code generation | Blog post, 2024; Breakout agent story, 2024; Fireside chat video, 2024 |\n| Rexera | Real Estate (GenAI Native) | Copilot for domain-specific task | Case study, 2024 |\n| Abu Dhabi Government | Government | Search | Case study, 2025 |\n| Tradestack | Software & Technology (GenAI Native) | Copilot for domain-specific task | Case study, 2024 |\n| Uber | Transportation | Developer productivity; Code generation | Interrupt talk, 2025; Presentation, 2024; Video, 2024 |\n| Unify | Software & Technology (GenAI Native) | Copilot for domain-specific task | Interrupt talk, 2025; Blog post, 2024 |\n| Vizient | Healthcare | Copilot for domain-specific task | Video story, 2025; Case study, 2025 |\n| Vodafone | Telecommunications | Code generation; internal search | Case study, 2025 |\n| WebToon | Media & Entertainment | Data extraction | Case study, 2025 |\n| 11x | Software & Technology (GenAI Native) | Research & outreach | Interrupt talk, 2025 |", "tokens": 715, "node_type": "child"}
{"id": 439, "chunk_id": "c16acf8a553ac24f5d77e7d3a1c6e493", "parent_id": 416, "url": "", "namespace": "langchain", "title": "oss-python-langgraph-deploy", "headers": ["oss-python-langgraph-deploy"], "section_index": 0, "chunk_index": 0, "text": "# oss-python-langgraph-deploy\n\n> Source: https://docs.langchain.com/oss/python/langgraph/deploy\n\nLangGraph v1.0Welcome to the new LangGraph documentation! If you encounter any issues or have feedback, please open an issue so we can improve. Archived v0 documentation can be found here.See the release notes and migration guide for a complete list of changes and instructions on how to upgrade your code.\nPrerequisites\nBefore you begin, ensure you have the following:- A GitHub account\n- A LangSmith account (free to sign up)\nDeploy your agent\n1. Create a repository on GitHub\nYour application\u2019s code must reside in a GitHub repository to be deployed on LangSmith. Both public and private repositories are supported. For this quickstart, first make sure your app is LangGraph-compatible by following the local server setup guide. Then, push your code to the repository.2. Deploy to LangSmith\n2\nCreate new deployment\nClick the + New Deployment button. A pane will open where you can fill in the required fields.\n3\nLink repository\nIf you are a first time user or adding a private repository that has not been previously connected, click the Add new account button and follow the instructions to connect your GitHub account.\n4\nDeploy repository\nSelect your application\u2019s repository. Click Submit to deploy. This may take about 15 minutes to complete. You can check the status in the Deployment details view.\n3. Test your application in Studio\nOnce your application is deployed:- Select the deployment you just created to view more details.\n- Click the Studio button in the top right corner. Studio will open to display your graph.\n4. Get the API URL for your deployment\n- In the Deployment details view in LangGraph, click the API URL to copy it to your clipboard.\n- Click the\nURL\nto copy it to the clipboard.\n5. Test the API\nYou can now test the API:- Python\n- Rest API\n- Install LangGraph Python:\n- Send a message to the agent:", "tokens": 318, "node_type": "child"}
{"id": 440, "chunk_id": "c16acf8a553ac24f5d77e7d3a1c6e493", "parent_id": 417, "url": "", "namespace": "langchain", "title": "oss-python-langgraph-durable-execution", "headers": ["oss-python-langgraph-durable-execution"], "section_index": 0, "chunk_index": 0, "text": "# oss-python-langgraph-durable-execution\n\n> Source: https://docs.langchain.com/oss/python/langgraph/durable-execution\n\nLangGraph v1.0Welcome to the new LangGraph documentation! If you encounter any issues or have feedback, please open an issue so we can improve. Archived v0 documentation can be found here.See the release notes and migration guide for a complete list of changes and instructions on how to upgrade your code.\nIf you are using LangGraph with a checkpointer, you already have durable execution enabled. You can pause and resume workflows at any point, even after interruptions or failures.\nTo make the most of durable execution, ensure that your workflow is designed to be deterministic and idempotent and wrap any side effects or non-deterministic operations inside tasks. You can use tasks from both the StateGraph (Graph API) and the Functional API.\nRequirements\nTo leverage durable execution in LangGraph, you need to:- Enable persistence in your workflow by specifying a checkpointer that will save workflow progress.\n- Specify a thread identifier when executing a workflow. This will track the execution history for a particular instance of the workflow.\n- Wrap any non-deterministic operations (e.g., random number generation) or operations with side effects (e.g., file writes, API calls) inside tasks to ensure that when a workflow is resumed, these operations are not repeated for the particular run, and instead their results are retrieved from the persistence layer. For more information, see Determinism and Consistent Replay.\nDeterminism and Consistent Replay\nWhen you resume a workflow run, the code does NOT resume from the same line of code where execution stopped; instead, it will identify an appropriate starting point from which to pick up where it left off. This means that the workflow will replay all steps from the starting point until it reaches the point where it was stopped. As a result, when you are writing a workflow for durable execution, you must wrap any non-deterministic operations (e.g., random number generation) and any operations with side effects (e.g., file writes, API calls) inside tasks or nodes. To ensure that your workflow is deterministic and can be consistently replayed, follow these guidelines:- Avoid Repeating Work: If a node contains multiple operations with side effects (e.g., logging, file writes, or network calls), wrap each operation in a separate task. This ensures that when the workflow is resumed, the operations are not repeated, and their results are retrieved from the persistence layer.\n- Encapsulate Non-Deterministic Operations: Wrap any code that might yield non-deterministic results (e.g., random number generation) inside tasks or nodes. This ensures that, upon resumption, the workflow follows the exact recorded sequence of steps with the same outcomes.\n- Use Idempotent Operations: When possible ensure that side effects (e.g., API calls, file writes) are idempotent. This means that if an operation is retried after a failure in the workflow, it will have the same effect as the first time it was executed. This is particularly important for operations that result in data writes. In the event that a task starts but fails to complete successfully, the workflow\u2019s resumption will re-run the task, relying on recorded outcomes to maintain consistency. Use idempotency keys or verify existing results to avoid unintended duplication, ensuring a smooth and predictable workflow execution.\nDurability modes\nLangGraph supports three durability modes that allow you to balance performance and data consistency based on your application\u2019s requirements. The durability modes, from least to most durable, are as follows: A higher durability mode adds more overhead to the workflow execution.Added in v0.6.0\nUse the\ndurability\nparameter instead of checkpoint_during\n(deprecated in v0.6.0) for persistence policy management:durability=\"async\"\nreplacescheckpoint_during=True\ndurability=\"exit\"\nreplacescheckpoint_during=False\ncheckpoint_during=True\n->durability=\"async\"\ncheckpoint_during=False\n->durability=\"exit\"\n\"exit\"\nChanges are persisted only when graph execution completes (either successfully or with an error). This provides the best performance for long-running graphs but means intermediate state is not saved, so you cannot recover from mid-execution failures or interrupt the graph execution.\n\"async\"\nChanges are persisted asynchronously while the next step executes. This provides good performance and durability, but there\u2019s a small risk that checkpoints might not be written if the process crashes during execution.\n\"sync\"\nChanges are persisted synchronously before the next step starts. This ensures that every checkpoint is written before continuing execution, providing high durability at the cost of some performance overhead.\nYou can specify the durability mode when calling any graph execution method:\nUsing tasks in nodes\nIf a node contains multiple operations, you may find it easier to convert each operation into a task rather than refactor the operations into individual nodes.- Original\n- With task\nResuming Workflows\nOnce you have enabled durable execution in your workflow, you can resume execution for the following scenarios:- Pausing and Resuming Workflows: Use the interrupt function to pause a workflow at specific points and the Command primitive to resume it with updated state. See Interrupts for more details.\n- Recovering from Failures: Automatically resume workflows from the last successful checkpoint after an exception (e.g., LLM provider outage). This involves executing the workflow with the same thread identifier by providing it with a\nNone\nas the input value (see this example with the functional API).\nStarting Points for Resuming Workflows\n- If you\u2019re using a StateGraph (Graph API), the starting point is the beginning of the node where execution stopped.\n- If you\u2019re making a subgraph call inside a node, the starting point will be the parent node that called the subgraph that was halted. Inside the subgraph, the starting point will be the specific node where execution stopped.\n- If you\u2019re using the Functional API, the starting point is the beginning of the entrypoint where execution stopped.", "tokens": 922, "node_type": "child"}
{"id": 441, "chunk_id": "c16acf8a553ac24f5d77e7d3a1c6e493", "parent_id": 418, "url": "", "namespace": "langchain", "title": "oss-python-langgraph-functional-api", "headers": ["oss-python-langgraph-functional-api"], "section_index": 0, "chunk_index": 0, "text": "# oss-python-langgraph-functional-api > Source: https://docs.langchain.com/oss/python/langgraph/functional-api LangGraph v1.0Welcome to the new LangGraph documentation! If you encounter any issues or have feedback, please open an issue so we can improve. Archived v0 documentation can be found here.See the release notes and migration guide for a complete list of changes and instructions on how to upgrade your code. if statements, for loops, and function calls. Unlike many data orchestration frameworks that require restructuring code into an explicit pipeline or DAG, the Functional API allows you to incorporate these capabilities without enforcing a rigid execution model. The Functional API uses two key building blocks: @entrypoint \u2013 Marks a function as the starting point of a workflow, encapsulating logic and managing execution flow, including handling long-running tasks and interrupts.@task \u2013 Represents a discrete unit of work, such as an API call or data processing step, that can be executed asynchronously within an entrypoint. Tasks return a future-like object that can be awaited or resolved synchronously. Functional API vs. Graph API For users who prefer a more declarative approach, LangGraph\u2019s Graph API allows you to define workflows using a Graph paradigm. Both APIs share the same underlying runtime, so you can use them together in the same application. Here are some key differences:- Control flow: The Functional API does not require thinking about graph structure. You can use standard Python constructs to define workflows. This will usually trim the amount of code you need to write. - Short-term memory: The GraphAPI requires declaring a State and may require defining reducers to manage updates to the graph state. @entrypoint and@tasks do not require explicit state management as their state is scoped to the function and is not shared across functions. - Checkpointing: Both APIs generate and use checkpoints. In the Graph API a new checkpoint is generated after every superstep. In the Functional API, when tasks are executed, their results are saved to an existing checkpoint associated with the given entrypoint instead of creating a new checkpoint. - Visualization: The Graph API makes it easy to visualize the workflow as a graph which can be useful for debugging, understanding the workflow, and sharing with others. The Functional API does not support visualization as the graph is dynamically generated during runtime. Example Below we demonstrate a simple application that writes an essay and interrupts to request human review.Detailed Explanation Detailed Explanation This workflow will write an essay about the topic \u201ccat\u201d and then pause to get a review from a human. The workflow can be interrupted for an indefinite amount of time until a review is provided.When the workflow is resumed, it executes from the very start, but because the result of the An essay has been written and is ready for review. Once the review is provided, we can resume the workflow:The workflow has been completed and the review has been added to the essay. writeEssay task was already saved, the task result will be loaded from the checkpoint instead of being recomputed.Entrypoint The@entrypoint decorator can be used to create a workflow from a function. It encapsulates workflow logic and manages execution flow, including handling long-running tasks and interrupts. Definition An entrypoint is defined by decorating a function with the@entrypoint decorator. The function must accept a single positional argument, which serves as the workflow input. If you need to pass multiple pieces of data, use a dictionary as the input type for the first argument. Decorating a function with an entrypoint produces a Pregel instance which helps to manage the execution of the workflow (e.g., handles streaming, resumption, and checkpointing). You will usually want to pass a checkpointer to the @entrypoint decorator to enable persistence and use features like human-in-the-loop. - Sync - Async Injectable parameters When declaring anentrypoint , you can request access to additional parameters that will be injected automatically at run time. These parameters include: | Parameter | Description | |---|---| | previous | Access the state associated with the previous checkpoint for the given thread. See short-term-memory. | | store | An instance of [BaseStore][langgraph.store.base.BaseStore]. Useful for long-term memory. | | writer | Use to access the StreamWriter when working with Async Python < 3.11. See streaming with functional API for details. | | config | For accessing run time configuration. See RunnableConfig for information. | Declare the parameters with the appropriate name and type annotation. Requesting Injectable Parameters Requesting Injectable Parameters Executing Using the@entrypoint yields a Pregel object that can be executed using the invoke , ainvoke , stream , and astream methods. - Invoke - Async Invoke - Stream - Async Stream Resuming Resuming an execution after an interrupt can be done by passing a resume value to the Command primitive.- Invoke - Async Invoke - Stream - Async Stream entrypoint with a None and the same thread id (config). This assumes that the underlying error has been resolved and execution can proceed successfully. - Invoke - Async Invoke - Stream - Async Stream Short-term memory When anentrypoint is defined with a checkpointer , it stores information between successive invocations on the same thread id in checkpoints. This allows accessing the state from the previous invocation using the previous parameter. By default, the previous parameter is the return value of the previous invocation. entrypoint.final entrypoint.final is a special primitive that can be returned from an entrypoint and allows decoupling the value that is saved in the checkpoint from the return value of the entrypoint. The first value is the return value of the entrypoint, and the second value is the value that will be saved in the checkpoint. The type annotation is entrypoint.final[return_type, save_type] . Task A task represents a discrete unit of work, such as an API call or data processing step. It has two key characteristics:- Asynchronous Execution: Tasks are designed to be executed asynchronously, allowing multiple operations to run concurrently without blocking. - Checkpointing: Task results are saved to a checkpoint, enabling resumption of the workflow from the last saved state. (See persistence for more details). Definition Tasks", "tokens": 1000, "node_type": "child"}
{"id": 442, "chunk_id": "9b0fd3b850698317bef57fa6e71a38ba", "parent_id": 418, "url": "", "namespace": "langchain", "title": "oss-python-langgraph-functional-api", "headers": ["oss-python-langgraph-functional-api"], "section_index": 0, "chunk_index": 1, "text": "state from the previous invocation using the previous parameter. By default, the previous parameter is the return value of the previous invocation. entrypoint.final entrypoint.final is a special primitive that can be returned from an entrypoint and allows decoupling the value that is saved in the checkpoint from the return value of the entrypoint. The first value is the return value of the entrypoint, and the second value is the value that will be saved in the checkpoint. The type annotation is entrypoint.final[return_type, save_type] . Task A task represents a discrete unit of work, such as an API call or data processing step. It has two key characteristics:- Asynchronous Execution: Tasks are designed to be executed asynchronously, allowing multiple operations to run concurrently without blocking. - Checkpointing: Task results are saved to a checkpoint, enabling resumption of the workflow from the last saved state. (See persistence for more details). Definition Tasks are defined using the@task decorator, which wraps a regular Python function. Serialization The outputs of tasks must be JSON-serializable to support checkpointing. Execution Tasks can only be called from within an entrypoint, another task, or a state graph node. Tasks cannot be called directly from the main application code. When you call a task, it returns immediately with a future object. A future is a placeholder for a result that will be available later. To obtain the result of a task, you can either wait for it synchronously (usingresult() ) or await it asynchronously (using await ). - Synchronous Invocation - Asynchronous Invocation When to use a task Tasks are useful in the following scenarios:- Checkpointing: When you need to save the result of a long-running operation to a checkpoint, so you don\u2019t need to recompute it when resuming the workflow. - Human-in-the-loop: If you\u2019re building a workflow that requires human intervention, you MUST use tasks to encapsulate any randomness (e.g., API calls) to ensure that the workflow can be resumed correctly. See the determinism section for more details. - Parallel Execution: For I/O-bound tasks, tasks enable parallel execution, allowing multiple operations to run concurrently without blocking (e.g., calling multiple APIs). - Observability: Wrapping operations in tasks provides a way to track the progress of the workflow and monitor the execution of individual operations using LangSmith. - Retryable Work: When work needs to be retried to handle failures or inconsistencies, tasks provide a way to encapsulate and manage the retry logic. Serialization There are two key aspects to serialization in LangGraph:entrypoint inputs and outputs must be JSON-serializable.task outputs must be JSON-serializable. Determinism To utilize features like human-in-the-loop, any randomness should be encapsulated inside of tasks. This guarantees that when execution is halted (e.g., for human in the loop) and then resumed, it will follow the same sequence of steps, even if task results are non-deterministic. LangGraph achieves this behavior by persisting task and subgraph results as they execute. A well-designed workflow ensures that resuming execution follows the same sequence of steps, allowing previously computed results to be retrieved correctly without having to re-execute them. This is particularly useful for long-running tasks or tasks with non-deterministic results, as it avoids repeating previously done work and allows resuming from essentially the same. While different runs of a workflow can produce different results, resuming a specific run should always follow the same sequence of recorded steps. This allows LangGraph to efficiently look up task and subgraph results that were executed prior to the graph being interrupted and avoid recomputing them.Idempotency Idempotency ensures that running the same operation multiple times produces the same result. This helps prevent duplicate API calls and redundant processing if a step is rerun due to a failure. Always place API calls inside tasks functions for checkpointing, and design them to be idempotent in case of re-execution. Re-execution can occur if a task starts, but does not complete successfully. Then, if the workflow is resumed, the task will run again. Use idempotency keys or verify existing results to avoid duplication.Common Pitfalls Handling side effects Encapsulate side effects (e.g., writing to a file, sending an email) in tasks to ensure they are not executed multiple times when resuming a workflow.- Incorrect - Correct In this example, a side effect (writing to a file) is directly included in the workflow, so it will be executed a second time when resuming the workflow. Non-deterministic control flow Operations that might give different results each time (like getting current time or random numbers) should be encapsulated in tasks to ensure that on resume, the same result is returned.- In a task: Get random number (5) \u2192 interrupt \u2192 resume \u2192 (returns 5 again) \u2192 \u2026 - Not in a task: Get random number (5) \u2192 interrupt \u2192 resume \u2192 get new random number (7) \u2192 \u2026 interrupt call may be matched with the wrong resume value, leading to incorrect results. Please read the section on determinism for more details. - Incorrect - Correct In this example, the workflow uses the current time to determine which task to execute. This is non-deterministic because the result of the workflow depends on the time at which it is executed.", "tokens": 851, "node_type": "child"}
{"id": 443, "chunk_id": "c16acf8a553ac24f5d77e7d3a1c6e493", "parent_id": 419, "url": "", "namespace": "langchain", "title": "oss-python-langgraph-graph-api", "headers": ["oss-python-langgraph-graph-api"], "section_index": 0, "chunk_index": 0, "text": "# oss-python-langgraph-graph-api > Source: https://docs.langchain.com/oss/python/langgraph/graph-api LangGraph v1.0Welcome to the new LangGraph documentation! If you encounter any issues or have feedback, please open an issue so we can improve. Archived v0 documentation can be found here.See the release notes and migration guide for a complete list of changes and instructions on how to upgrade your code. Graphs At its core, LangGraph models agent workflows as graphs. You define the behavior of your agents using three key components:- State : A shared data structure that represents the current snapshot of your application. It can be any data type, but is typically defined using a shared state schema. - Nodes : Functions that encode the logic of your agents. They receive the current state as input, perform some computation or side-effect, and return an updated state. - Edges : Functions that determine whichNode to execute next based on the current state. They can be conditional branches or fixed transitions. Nodes and Edges , you can create complex, looping workflows that evolve the state over time. The real power, though, comes from how LangGraph manages that state. To emphasize: Nodes and Edges are nothing more than functions - they can contain an LLM or just good ol\u2019 code. In short: nodes do the work, edges tell what to do next. LangGraph\u2019s underlying graph algorithm uses message passing to define a general program. When a Node completes its operation, it sends messages along one or more edges to other node(s). These recipient nodes then execute their functions, pass the resulting messages to the next set of nodes, and the process continues. Inspired by Google\u2019s Pregel system, the program proceeds in discrete \u201csuper-steps.\u201d A super-step can be considered a single iteration over the graph nodes. Nodes that run in parallel are part of the same super-step, while nodes that run sequentially belong to separate super-steps. At the start of graph execution, all nodes begin in an inactive state. A node becomes active when it receives a new message (state) on any of its incoming edges (or \u201cchannels\u201d). The active node then runs its function and responds with updates. At the end of each super-step, nodes with no incoming messages vote to halt by marking themselves as inactive . The graph execution terminates when all nodes are inactive and no messages are in transit. StateGraph TheStateGraph class is the main graph class to use. This is parameterized by a user defined State object. Compiling your graph To build your graph, you first define the state, you then add nodes and edges, and then you compile it. What exactly is compiling your graph and why is it needed? Compiling is a pretty simple step. It provides a few basic checks on the structure of your graph (no orphaned nodes, etc). It is also where you can specify runtime args like checkpointers and breakpoints. You compile your graph by just calling the.compile method: State The first thing you do when you define a graph is define theState of the graph. The State consists of the schema of the graph as well as reducer functions which specify how to apply updates to the state. The schema of the State will be the input schema to all Nodes and Edges in the graph, and can be either a TypedDict or a Pydantic model. All Nodes will emit updates to the State which are then applied using the specified reducer function. Schema The main documented way to specify the schema of a graph is by using aTypedDict . If you want to provide default values in your state, use a dataclass . We also support using a Pydantic BaseModel as your graph state if you want recursive data validation (though note that pydantic is less performant than a TypedDict or dataclass ). By default, the graph will have the same input and output schemas. If you want to change this, you can also specify explicit input and output schemas directly. This is useful when you have a lot of keys, and some are explicitly for input and others for output. See the guide here for how to use. Multiple schemas Typically, all graph nodes communicate with a single schema. This means that they will read and write to the same state channels. But, there are cases where we want more control over this:- Internal nodes can pass information that is not required in the graph\u2019s input / output. - We may also want to use different input / output schemas for the graph. The output might, for example, only contain a single relevant output key. PrivateState . It is also possible to define explicit input and output schemas for a graph. In these cases, we define an \u201cinternal\u201d schema that contains all keys relevant to graph operations. But, we also define input and output schemas that are sub-sets of the \u201cinternal\u201d schema to constrain the input and output of the graph. See this guide for more detail. Let\u2019s look at an example: - We pass state: InputState as the input schema tonode_1 . But, we write out tofoo , a channel inOverallState . How can we write out to a state channel that is not included in the input schema? This is because a node can write to any state channel in the graph state. The graph state is the union of the state channels defined at initialization, which includesOverallState and the filtersInputState andOutputState . - We initialize the graph with StateGraph(OverallState,input_schema=InputState,output_schema=OutputState) . So, how can we write toPrivateState innode_2 ? How does the graph gain access to this schema if it was not passed in theStateGraph initialization? We can do this because nodes can also declare additional state channels as long as the state schema definition exists. In this case, thePrivateState schema is defined, so we can addbar as a new state channel in the graph and write to it. Reducers Reducers are key to understanding how updates from nodes are applied to theState . Each key in the State has", "tokens": 1000, "node_type": "child"}
{"id": 444, "chunk_id": "9b0fd3b850698317bef57fa6e71a38ba", "parent_id": 419, "url": "", "namespace": "langchain", "title": "oss-python-langgraph-graph-api", "headers": ["oss-python-langgraph-graph-api"], "section_index": 0, "chunk_index": 1, "text": ". How can we write out to a state channel that is not included in the input schema? This is because a node can write to any state channel in the graph state. The graph state is the union of the state channels defined at initialization, which includesOverallState and the filtersInputState andOutputState . - We initialize the graph with StateGraph(OverallState,input_schema=InputState,output_schema=OutputState) . So, how can we write toPrivateState innode_2 ? How does the graph gain access to this schema if it was not passed in theStateGraph initialization? We can do this because nodes can also declare additional state channels as long as the state schema definition exists. In this case, thePrivateState schema is defined, so we can addbar as a new state channel in the graph and write to it. Reducers Reducers are key to understanding how updates from nodes are applied to theState . Each key in the State has its own independent reducer function. If no reducer function is explicitly specified then it is assumed that all updates to that key should override it. There are a few different types of reducers, starting with the default type of reducer: Default Reducer These two examples show how to use the default reducer: Example A:{\"foo\": 1, \"bar\": [\"hi\"]} . Let\u2019s then assume the first Node returns {\"foo\": 2} . This is treated as an update to the state. Notice that the Node does not need to return the whole State schema - just an update. After applying this update, the State would then be {\"foo\": 2, \"bar\": [\"hi\"]} . If the second node returns {\"bar\": [\"bye\"]} then the State would then be {\"foo\": 2, \"bar\": [\"bye\"]} Example B: Annotated type to specify a reducer function (operator.add ) for the second key (bar ). Note that the first key remains unchanged. Let\u2019s assume the input to the graph is {\"foo\": 1, \"bar\": [\"hi\"]} . Let\u2019s then assume the first Node returns {\"foo\": 2} . This is treated as an update to the state. Notice that the Node does not need to return the whole State schema - just an update. After applying this update, the State would then be {\"foo\": 2, \"bar\": [\"hi\"]} . If the second node returns {\"bar\": [\"bye\"]} then the State would then be {\"foo\": 2, \"bar\": [\"hi\", \"bye\"]} . Notice here that the bar key is updated by adding the two lists together. Working with Messages in Graph State Why use messages? Most modern LLM providers have a chat model interface that accepts a list of messages as input. LangChain\u2019sChatModel in particular accepts a list of Message objects as inputs. These messages come in a variety of forms such as HumanMessage (user input) or AIMessage (LLM response). To read more about what message objects are, please refer to this conceptual guide. Using Messages in your Graph In many cases, it is helpful to store prior conversation history as a list of messages in your graph state. To do so, we can add a key (channel) to the graph state that stores a list ofMessage objects and annotate it with a reducer function (see messages key in the example below). The reducer function is vital to telling the graph how to update the list of Message objects in the state with each state update (for example, when a node sends an update). If you don\u2019t specify a reducer, every state update will overwrite the list of messages with the most recently provided value. If you wanted to simply append messages to the existing list, you could use operator.add as a reducer. However, you might also want to manually update messages in your graph state (e.g. human-in-the-loop). If you were to use operator.add , the manual state updates you send to the graph would be appended to the existing list of messages, instead of updating existing messages. To avoid that, you need a reducer that can keep track of message IDs and overwrite existing messages, if updated. To achieve this, you can use the prebuilt add_messages function. For brand new messages, it will simply append to existing list, but it will also handle the updates for existing messages correctly. Serialization In addition to keeping track of message IDs, theadd_messages function will also try to deserialize messages into LangChain Message objects whenever a state update is received on the messages channel. See more information on LangChain serialization/deserialization here. This allows sending graph inputs / state updates in the following format: Messages when using add_messages , you should use dot notation to access message attributes, like state[\"messages\"][-1].content . Below is an example of a graph that uses add_messages as its reducer function. MessagesState Since having a list of messages in your state is so common, there exists a prebuilt state calledMessagesState which makes it easy to use messages. MessagesState is defined with a single messages key which is a list of AnyMessage objects and uses the add_messages reducer. Typically, there is more state to track than just messages, so we see people subclass this state and add more fields, like: Nodes In LangGraph, nodes are Python functions (either synchronous or asynchronous) that accept the following arguments:state : The state of the graphconfig : ARunnableConfig object that contains configuration information likethread_id and tracing information liketags runtime : ARuntime object that contains runtimecontext and other information likestore andstream_writer NetworkX , you add these nodes to a graph using the add_node method: START Node The START Node is a special node that represents the node that sends user input to the graph. The main purpose for referencing this node is to determine which nodes should be called first. END Node The END Node is a special node that represents a terminal node. This node is referenced when you want to denote which edges have no actions after they are done. Node Caching LangGraph supports caching of tasks/nodes based on the input to the node. To use caching:- Specify a cache when compiling a graph (or specifying an entrypoint) - Specify a cache policy for nodes.", "tokens": 1000, "node_type": "child"}
{"id": 445, "chunk_id": "5345104ec252a41359fa4a3d2dfbec23", "parent_id": 419, "url": "", "namespace": "langchain", "title": "oss-python-langgraph-graph-api", "headers": ["oss-python-langgraph-graph-api"], "section_index": 0, "chunk_index": 2, "text": "accept the following arguments:state : The state of the graphconfig : ARunnableConfig object that contains configuration information likethread_id and tracing information liketags runtime : ARuntime object that contains runtimecontext and other information likestore andstream_writer NetworkX , you add these nodes to a graph using the add_node method: START Node The START Node is a special node that represents the node that sends user input to the graph. The main purpose for referencing this node is to determine which nodes should be called first. END Node The END Node is a special node that represents a terminal node. This node is referenced when you want to denote which edges have no actions after they are done. Node Caching LangGraph supports caching of tasks/nodes based on the input to the node. To use caching:- Specify a cache when compiling a graph (or specifying an entrypoint) - Specify a cache policy for nodes. Each cache policy supports: key_func used to generate a cache key based on the input to a node, which defaults to ahash of the input with pickle.ttl , the time to live for the cache in seconds. If not specified, the cache will never expire. - First run takes two seconds to run (due to mocked expensive computation). - Second run utilizes cache and returns quickly. Edges Edges define how the logic is routed and how the graph decides to stop. This is a big part of how your agents work and how different nodes communicate with each other. There are a few key types of edges:- Normal Edges: Go directly from one node to the next. - Conditional Edges: Call a function to determine which node(s) to go to next. - Entry Point: Which node to call first when user input arrives. - Conditional Entry Point: Call a function to determine which node(s) to call first when user input arrives. Normal Edges If you always want to go from node A to node B, you can use the add_edge method directly.Conditional Edges If you want to optionally route to 1 or more edges (or optionally terminate), you can use the add_conditional_edges method. This method accepts the name of a node and a \u201crouting function\u201d to call after that node is executed:routing_function accepts the current state of the graph and returns a value. By default, the return value routing_function is used as the name of the node (or list of nodes) to send the state to next. All those nodes will be run in parallel as a part of the next superstep. You can optionally provide a dictionary that maps the routing_function \u2019s output to the name of the next node. Entry Point The entry point is the first node(s) that are run when the graph starts. You can use theadd_edge method from the virtual START node to the first node to execute to specify where to enter the graph. Conditional Entry Point A conditional entry point lets you start at different nodes depending on custom logic. You can useadd_conditional_edges from the virtual START node to accomplish this. routing_function \u2019s output to the name of the next node. Send By default, Nodes and Edges are defined ahead of time and operate on the same shared state. However, there can be cases where the exact edges are not known ahead of time and/or you may want different versions of State to exist at the same time. A common example of this is with map-reduce design patterns. In this design pattern, a first node may generate a list of objects, and you may want to apply some other node to all those objects. The number of objects may be unknown ahead of time (meaning the number of edges may not be known) and the input State to the downstream Node should be different (one for each generated object). To support this design pattern, LangGraph supports returning Send objects from conditional edges. Send takes two arguments: first is the name of the node, and second is the state to pass to that node. Command It can be useful to combine control flow (edges) and state updates (nodes). For example, you might want to BOTH perform state updates AND decide which node to go to next in the SAME node. LangGraph provides a way to do so by returning a Command object from node functions: Command you can also achieve dynamic control flow behavior (identical to conditional edges): When returning Command in your node functions, you must add return type annotations with the list of node names the node is routing to, e.g. Command[Literal[\"my_other_node\"]] . This is necessary for the graph rendering and tells LangGraph that my_node can navigate to my_other_node .Command . When should I use Command instead of conditional edges? - Use Command when you need to both update the graph state and route to a different node. For example, when implementing multi-agent handoffs where it\u2019s important to route to a different agent and pass some information to that agent. - Use conditional edges to route between nodes conditionally without updating the state. Navigating to a node in a parent graph If you are using subgraphs, you might want to navigate from a node within a subgraph to a different subgraph (i.e. a different node in the parent graph). To do so, you can specifygraph=Command.PARENT in Command : Setting graph to Command.PARENT will navigate to the closest parent graph.When you send updates from a subgraph node to a parent graph node for a key that\u2019s shared by both parent and subgraph state schemas, you must define a reducer for the key you\u2019re updating in the parent graph state. See this example.Using inside tools A common use case is updating graph state from inside a tool. For example, in a customer support application you might want to look up customer information based on their account number or ID in the beginning of the conversation. Refer to this guide for detail.Human-in-the-loop Command is an important part of human-in-the-loop workflows: when using interrupt() to collect", "tokens": 1000, "node_type": "child"}
{"id": 446, "chunk_id": "0f7e5a6eec52bd2b622d96bc793bb7d8", "parent_id": 419, "url": "", "namespace": "langchain", "title": "oss-python-langgraph-graph-api", "headers": ["oss-python-langgraph-graph-api"], "section_index": 0, "chunk_index": 3, "text": "you are using subgraphs, you might want to navigate from a node within a subgraph to a different subgraph (i.e. a different node in the parent graph). To do so, you can specifygraph=Command.PARENT in Command : Setting graph to Command.PARENT will navigate to the closest parent graph.When you send updates from a subgraph node to a parent graph node for a key that\u2019s shared by both parent and subgraph state schemas, you must define a reducer for the key you\u2019re updating in the parent graph state. See this example.Using inside tools A common use case is updating graph state from inside a tool. For example, in a customer support application you might want to look up customer information based on their account number or ID in the beginning of the conversation. Refer to this guide for detail.Human-in-the-loop Command is an important part of human-in-the-loop workflows: when using interrupt() to collect user input, Command is then used to supply the input and resume execution via Command(resume=\"User input\") . Check out this conceptual guide for more information. Graph Migrations LangGraph can easily handle migrations of graph definitions (nodes, edges, and state) even when using a checkpointer to track state.- For threads at the end of the graph (i.e. not interrupted) you can change the entire topology of the graph (i.e. all nodes and edges, remove, add, rename, etc) - For threads currently interrupted, we support all topology changes other than renaming / removing nodes (as that thread could now be about to enter a node that no longer exists) \u2014 if this is a blocker please reach out and we can prioritize a solution. - For modifying state, we have full backwards and forwards compatibility for adding and removing keys - State keys that are renamed lose their saved state in existing threads - State keys whose types change in incompatible ways could currently cause issues in threads with state from before the change \u2014 if this is a blocker please reach out and we can prioritize a solution. Runtime Context When creating a graph, you can specify acontext_schema for runtime context passed to nodes. This is useful for passing information to nodes that is not part of the graph state. For example, you might want to pass dependencies such as model name or a database connection. context parameter of the invoke method. Recursion Limit The recursion limit sets the maximum number of super-steps the graph can execute during a single execution. Once the limit is reached, LangGraph will raiseGraphRecursionError . By default this value is set to 25 steps. The recursion limit can be set on any graph at runtime, and is passed to invoke /stream via the config dictionary. Importantly, recursion_limit is a standalone config key and should not be passed inside the configurable key as all other user-defined configuration. See the example below:", "tokens": 474, "node_type": "child"}
{"id": 447, "chunk_id": "c16acf8a553ac24f5d77e7d3a1c6e493", "parent_id": 420, "url": "", "namespace": "langchain", "title": "oss-python-langgraph-graph-recursion-limit", "headers": ["oss-python-langgraph-graph-recursion-limit"], "section_index": 0, "chunk_index": 0, "text": "# oss-python-langgraph-graph-recursion-limit\n\n> Source: https://docs.langchain.com/oss/python/langgraph/GRAPH_RECURSION_LIMIT\n\nLangGraph v1.0Welcome to the new LangGraph documentation! If you encounter any issues or have feedback, please open an issue so we can improve. Archived v0 documentation can be found here.See the release notes and migration guide for a complete list of changes and instructions on how to upgrade your code.\nStateGraph\nreached the maximum number of steps before hitting a stop condition.\nThis is often due to an infinite loop caused by code like the example below:\nTroubleshooting\n- If you are not expecting your graph to go through many iterations, you likely have a cycle. Check your logic for infinite loops.\n-\nIf you have a complex graph, you can pass in a higher\nrecursion_limit\nvalue into yourconfig\nobject when invoking your graph like this:", "tokens": 131, "node_type": "child"}
{"id": 448, "chunk_id": "c16acf8a553ac24f5d77e7d3a1c6e493", "parent_id": 421, "url": "", "namespace": "langchain", "title": "oss-python-langgraph-install", "headers": ["oss-python-langgraph-install"], "section_index": 0, "chunk_index": 0, "text": "# oss-python-langgraph-install\n\n> Source: https://docs.langchain.com/oss/python/langgraph/install\n\nLangGraph v1.0Welcome to the new LangGraph documentation! If you encounter any issues or have feedback, please open an issue so we can improve. Archived v0 documentation can be found here.See the release notes and migration guide for a complete list of changes and instructions on how to upgrade your code.\nTo install the base LangGraph package:\nCopy\npip install -U langgraph\nTo use LangGraph you will usually want to access LLMs and define tools.\nYou can do this however you see fit.One way to do this (which we will use in the docs) is to use LangChain.Install LangChain with:\nCopy\npip install -U langchain\nTo work with specific LLM provider packages, you will need install them separately.Refer to the integrations page for provider-specific installation instructions.", "tokens": 130, "node_type": "child"}
{"id": 449, "chunk_id": "c16acf8a553ac24f5d77e7d3a1c6e493", "parent_id": 422, "url": "", "namespace": "langchain", "title": "oss-python-langgraph-interrupts", "headers": ["oss-python-langgraph-interrupts"], "section_index": 0, "chunk_index": 0, "text": "# oss-python-langgraph-interrupts > Source: https://docs.langchain.com/oss/python/langgraph/interrupts LangGraph v1.0Welcome to the new LangGraph documentation! If you encounter any issues or have feedback, please open an issue so we can improve. Archived v0 documentation can be found here.See the release notes and migration guide for a complete list of changes and instructions on how to upgrade your code. interrupt() function at any point in your graph nodes. The function accepts any JSON-serializable value which is surfaced to the caller. When you\u2019re ready to continue, you resume execution by re-invoking the graph using Command , which then becomes the return value of the interrupt() call from inside the node. Unlike static breakpoints (which pause before or after specific nodes), interrupts are dynamic\u2014they can be placed anywhere in your code and can be conditional based on your application logic. - Checkpointing keeps your place: the checkpointer writes the exact graph state so you can resume later, even when in an error state. thread_id is your pointer: setconfig={\"configurable\": {\"thread_id\": ...}} to tell the checkpointer which state to load.- Interrupt payloads surface as __interrupt__ : the values you pass tointerrupt() return to the caller in the__interrupt__ field so you know what the graph is waiting on. thread_id you choose is effectively your persistent cursor. Reusing it resumes the same checkpoint; using a new value starts a brand-new thread with an empty state. Pause using interrupt The interrupt function pauses graph execution and returns a value to the caller. When you call interrupt within a node, LangGraph saves the current graph state and waits for you to resume execution with input. To use interrupt , you need: - A checkpointer to persist the graph state (use a durable checkpointer in production) - A thread ID in your config so the runtime knows which state to resume from - To call interrupt() where you want to pause (payload must be JSON-serializable) interrupt , here\u2019s what happens: - Graph execution gets suspended at the exact point where interrupt is called - State is saved using the checkpointer so execution can be resumed later, In production, this should be a persistent checkpointer (e.g. backed by a database) - Value is returned to the caller under __interrupt__ ; it can be any JSON-serializable value (string, object, array, etc.) - Graph waits indefinitely until you resume execution with a response - Response is passed back into the node when you resume, becoming the return value of the interrupt() call Resuming interrupts After an interrupt pauses execution, you resume the graph by invoking it again with aCommand that contains the resume value. The resume value is passed back to the interrupt call, allowing the node to continue execution with the external input. - You must use the same thread ID when resuming that was used when the interrupt occurred - The value passed to Command(resume=...) becomes the return value of theinterrupt call - The node restarts from the beginning of the node where the interrupt was called when resumed, so any code before theinterrupt runs again - You can pass any JSON-serializable value as the resume value Common patterns The key thing that interrupts unlock is the ability to pause execution and wait for external input. This is useful for a variety of use cases, including:- Approval workflows: Pause before executing critical actions (API calls, database changes, financial transactions) - Review and edit: Let humans review and modify LLM outputs or tool calls before continuing - Interrupting tool calls: Pause before executing tool calls to review and edit the tool call before execution - Validating human input: Pause before proceeding to the next step to validate human input Approve or reject One of the most common uses of interrupts is to pause before a critical action and ask for approval. For example, you might want to ask a human to approve an API call, a database change, or any other important decision.true to approve or false to reject: Full example Full example Review and edit state Sometimes you want to let a human review and edit part of the graph state before continuing. This is useful for correcting LLMs, adding missing information, or making adjustments.Full example Full example Interrupts in tools You can also place interrupts directly inside tool functions. This makes the tool itself pause for approval whenever it\u2019s called, and allows for human review and editing of the tool call before it is executed. First, define a tool that usesinterrupt : Full example Full example Validating human input Sometimes you need to validate input from humans and ask again if it\u2019s invalid. You can do this using multipleinterrupt calls in a loop. Full example Full example Rules of interrupts When you callinterrupt within a node, LangGraph suspends execution by raising an exception that signals the runtime to pause. This exception propagates up through the call stack and is caught by the runtime, which notifies the graph to save the current state and wait for external input. When execution resumes (after you provide the requested input), the runtime restarts the entire node from the beginning\u2014it does not resume from the exact line where interrupt was called. This means any code that ran before the interrupt will execute again. Because of this, there\u2019s a few important rules to follow when working with interrupts to ensure they behave as expected. Do not wrap interrupt calls in try/except The way that interrupt pauses execution at the point of the call is by throwing a special exception. If you wrap the interrupt call in a try/except block, you will catch this exception and the interrupt will not be passed back to the graph. - \u2705 Separate interrupt calls from error-prone code - \u2705 Use specific exception types in try/except blocks - \ud83d\udd34 Do not wrap interrupt calls in bare try/except blocks Do not reorder interrupt calls within a node It\u2019s common to use multiple interrupts in a single node, however this can lead to unexpected behavior if not handled carefully. When a node contains multiple interrupt calls,", "tokens": 1000, "node_type": "child"}
{"id": 450, "chunk_id": "9b0fd3b850698317bef57fa6e71a38ba", "parent_id": 422, "url": "", "namespace": "langchain", "title": "oss-python-langgraph-interrupts", "headers": ["oss-python-langgraph-interrupts"], "section_index": 0, "chunk_index": 1, "text": "interrupt was called. This means any code that ran before the interrupt will execute again. Because of this, there\u2019s a few important rules to follow when working with interrupts to ensure they behave as expected. Do not wrap interrupt calls in try/except The way that interrupt pauses execution at the point of the call is by throwing a special exception. If you wrap the interrupt call in a try/except block, you will catch this exception and the interrupt will not be passed back to the graph. - \u2705 Separate interrupt calls from error-prone code - \u2705 Use specific exception types in try/except blocks - \ud83d\udd34 Do not wrap interrupt calls in bare try/except blocks Do not reorder interrupt calls within a node It\u2019s common to use multiple interrupts in a single node, however this can lead to unexpected behavior if not handled carefully. When a node contains multiple interrupt calls, LangGraph keeps a list of resume values specific to the task executing the node. Whenever execution resumes, it starts at the beginning of the node. For each interrupt encountered, LangGraph checks if a matching value exists in the task\u2019s resume list. Matching is strictly index-based, so the order of interrupt calls within the node is important. - \u2705 Keep interrupt calls consistent across node executions - \ud83d\udd34 Do not conditionally skip interrupt calls within a node - \ud83d\udd34 Do not loop interrupt calls using logic that isn\u2019t deterministic across executions Do not return complex values in interrupt calls Depending on which checkpointer is used, complex values may not be serializable (e.g. you can\u2019t serialize a function). To make your graphs adaptable to any deployment, it\u2019s best practice to only use values that can be reasonably serialized. - \u2705 Pass simple, JSON-serializable types to interrupt - \u2705 Pass dictionaries/objects with simple values - \ud83d\udd34 Do not pass functions, class instances, or other complex objects to interrupt Side effects called before interrupt must be idempotent Because interrupts work by re-running the nodes they were called from, side effects called before interrupt should (ideally) be idempotent. For context, idempotency means that the same operation can be applied multiple times without changing the result beyond the initial execution. As an example, you might have an API call to update a record inside of a node. If interrupt is called after that call is made, it will be re-run multiple times when the node is resumed, potentially overwriting the initial update or creating duplicate records. - \u2705 Use idempotent operations before interrupt - \u2705 Place side effects after interrupt calls - \u2705 Separate side effects into separate nodes when possible - \ud83d\udd34 Do not perform non-idempotent operations before interrupt - \ud83d\udd34 Do not create new records without checking if they exist Using with subgraphs called as functions When invoking a subgraph within a node, the parent graph will resume execution from the beginning of the node where the subgraph was invoked and theinterrupt was triggered. Similarly, the subgraph will also resume from the beginning of the node where interrupt was called. Debugging with interrupts To debug and test a graph, you can use static interrupts as breakpoints to step through the graph execution one node at a time. Static interrupts are triggered at defined points either before or after a node executes. You can set these by specifyinginterrupt_before and interrupt_after when compiling the graph. Static interrupts are not recommended for human-in-the-loop workflows. Use the interrupt method instead.- At compile time - At run time - The breakpoints are set during compile time. interrupt_before specifies the nodes where execution should pause before the node is executed.interrupt_after specifies the nodes where execution should pause after the node is executed.- A checkpointer is required to enable breakpoints. - The graph is run until the first breakpoint is hit. - The graph is resumed by passing in None for the input. This will run the graph until the next breakpoint is hit.", "tokens": 653, "node_type": "child"}
{"id": 451, "chunk_id": "c16acf8a553ac24f5d77e7d3a1c6e493", "parent_id": 423, "url": "", "namespace": "langchain", "title": "oss-python-langgraph-invalid-chat-history", "headers": ["oss-python-langgraph-invalid-chat-history"], "section_index": 0, "chunk_index": 0, "text": "# oss-python-langgraph-invalid-chat-history\n\n> Source: https://docs.langchain.com/oss/python/langgraph/INVALID_CHAT_HISTORY\n\nLangGraph v1.0Welcome to the new LangGraph documentation! If you encounter any issues or have feedback, please open an issue so we can improve. Archived v0 documentation can be found here.See the release notes and migration guide for a complete list of changes and instructions on how to upgrade your code.\ncall_model\ngraph node receives a malformed list of messages. Specifically, it is malformed when there are AIMessages\nwith tool_calls\n(LLM requesting to call a tool) that do not have a corresponding ToolMessage\n(result of a tool invocation to return to the LLM).\nThere could be a few reasons you\u2019re seeing this error:\n- You manually passed a malformed list of messages when invoking the graph, e.g.\ngraph.invoke({'messages': [AIMessage(..., tool_calls=[...])]})\n- The graph was interrupted before receiving updates from the\ntools\nnode (i.e. a list ofToolMessage\n) and you invoked it with an input that is not None or a ToolMessage, e.g.graph.invoke({'messages': [HumanMessage(...)]}, config)\n. This interrupt could have been triggered in one of the following ways:- You manually set\ninterrupt_before = ['tools']\nincreate_agent\n- You manually set\n- One of the tools raised an error that wasn\u2019t handled by the ToolNode (\n\"tools\"\n)\nTroubleshooting\nTo resolve this, you can do one of the following:- Don\u2019t invoke the graph with a malformed list of messages\n- In case of an interrupt (manual or due to an error) you can:\n- provide\nToolMessage\nobjects that match existing tool calls and callgraph.invoke({'messages': [ToolMessage(...)]})\n. NOTE: this will append the messages to the history and run the graph from the START node.- manually update the state and resume the graph from the interrupt:\n- get the list of most recent messages from the graph state with\ngraph.get_state(config)\n- modify the list of messages to either remove unanswered tool calls from AIMessages\n- get the list of most recent messages from the graph state with\n- manually update the state and resume the graph from the interrupt:\nToolMessage\nobjects with tool_call_ids\nthat match unanswered tool calls 3. call graph.update_state(config, {'messages': ...})\nwith the modified list of messages 4. resume the graph, e.g. call graph.invoke(None, config)", "tokens": 356, "node_type": "child"}
{"id": 452, "chunk_id": "c16acf8a553ac24f5d77e7d3a1c6e493", "parent_id": 424, "url": "", "namespace": "langchain", "title": "oss-python-langgraph-invalid-concurrent-graph-update", "headers": ["oss-python-langgraph-invalid-concurrent-graph-update"], "section_index": 0, "chunk_index": 0, "text": "# oss-python-langgraph-invalid-concurrent-graph-update\n\n> Source: https://docs.langchain.com/oss/python/langgraph/INVALID_CONCURRENT_GRAPH_UPDATE\n\nLangGraph v1.0Welcome to the new LangGraph documentation! If you encounter any issues or have feedback, please open an issue so we can improve. Archived v0 documentation can be found here.See the release notes and migration guide for a complete list of changes and instructions on how to upgrade your code.\nStateGraph\nreceived concurrent updates to its state from multiple nodes to a state property that doesn\u2019t\nsupport it.\nOne way this can occur is if you are using a fanout\nor other parallel execution in your graph and you have defined a graph like this:\n{ \"some_key\": \"some_string_value\" }\n, this will overwrite the state value for \"some_key\"\nwith \"some_string_value\"\n.\nHowever, if multiple nodes in e.g. a fanout within a single step return values for \"some_key\"\n, the graph will throw this error because\nthere is uncertainty around how to update the internal state.\nTo get around this, you can define a reducer that combines multiple values:\nTroubleshooting\nThe following may help resolve this error:- If your graph executes nodes in parallel, make sure you have defined relevant state keys with a reducer.", "tokens": 189, "node_type": "child"}
{"id": 453, "chunk_id": "c16acf8a553ac24f5d77e7d3a1c6e493", "parent_id": 426, "url": "", "namespace": "langchain", "title": "oss-python-langgraph-local-server", "headers": ["oss-python-langgraph-local-server"], "section_index": 0, "chunk_index": 0, "text": "# oss-python-langgraph-local-server\n\n> Source: https://docs.langchain.com/oss/python/langgraph/local-server\n\nLangGraph v1.0Welcome to the new LangGraph documentation! If you encounter any issues or have feedback, please open an issue so we can improve. Archived v0 documentation can be found here.See the release notes and migration guide for a complete list of changes and instructions on how to upgrade your code.\nPrerequisites\nBefore you begin, ensure you have the following:- An API key for LangSmith - free to sign up\n1. Install the LangGraph CLI\n2. Create a LangGraph app \ud83c\udf31\nCreate a new app from thenew-langgraph-project-python\ntemplate. This template demonstrates a single-node application you can extend with your own logic.\nAdditional templates\nIf you use\nlanggraph new\nwithout specifying a template, you will be presented with an interactive menu that will allow you to choose from a list of available templates.3. Install dependencies\nIn the root of your new LangGraph app, install the dependencies inedit\nmode so your local changes are used by the server:\n4. Create a .env\nfile\nYou will find a .env.example\nin the root of your new LangGraph app. Create a .env\nfile in the root of your new LangGraph app and copy the contents of the .env.example\nfile into it, filling in the necessary API keys:\n5. Launch LangGraph Server \ud83d\ude80\nStart the LangGraph API server locally:langgraph dev\ncommand starts LangGraph Server in an in-memory mode. This mode is suitable for development and testing purposes. For production use, deploy LangGraph Server with access to a persistent storage backend. For more information, see the Hosting overview.\n6. Test your application in Studio\nStudio is a specialized UI that you can connect to LangGraph API server to visualize, interact with, and debug your application locally. Test your graph in Studio by visiting the URL provided in the output of thelanggraph dev\ncommand:\nSafari compatibility\nSafari compatibility\nUse the\n--tunnel\nflag with your command to create a secure tunnel, as Safari has limitations when connecting to localhost servers:7. Test the API\n- Python SDK (async)\n- Python SDK (sync)\n- Rest API\n- Install the LangGraph Python SDK:\n- Send a message to the assistant (threadless run):\nNext steps\nNow that you have a LangGraph app running locally, take your journey further by exploring deployment and advanced features:- Deployment quickstart: Deploy your LangGraph app using LangSmith.\n- LangSmith: Learn about foundational LangSmith concepts.\n- Python SDK Reference: Explore the Python SDK API Reference.", "tokens": 400, "node_type": "child"}
{"id": 454, "chunk_id": "c16acf8a553ac24f5d77e7d3a1c6e493", "parent_id": 427, "url": "", "namespace": "langchain", "title": "oss-python-langgraph-missing-checkpointer", "headers": ["oss-python-langgraph-missing-checkpointer"], "section_index": 0, "chunk_index": 0, "text": "# oss-python-langgraph-missing-checkpointer\n\n> Source: https://docs.langchain.com/oss/python/langgraph/MISSING_CHECKPOINTER\n\nLangGraph v1.0Welcome to the new LangGraph documentation! If you encounter any issues or have feedback, please open an issue so we can improve. Archived v0 documentation can be found here.See the release notes and migration guide for a complete list of changes and instructions on how to upgrade your code.\ncheckpointer\nis missing in the compile()\nmethod of StateGraph\nor entrypoint\n.\nTroubleshooting\nThe following may help resolve this error:- Initialize and pass a checkpointer to the\ncompile()\nmethod ofStateGraph\norentrypoint\n.\n- Use the LangGraph API so you don\u2019t need to implement or configure checkpointers manually. The API handles all persistence infrastructure for you.\nRelated\n- Read more about persistence.", "tokens": 116, "node_type": "child"}
{"id": 455, "chunk_id": "c16acf8a553ac24f5d77e7d3a1c6e493", "parent_id": 429, "url": "", "namespace": "langchain", "title": "oss-python-langgraph-observability", "headers": ["oss-python-langgraph-observability"], "section_index": 0, "chunk_index": 0, "text": "# oss-python-langgraph-observability\n\n> Source: https://docs.langchain.com/oss/python/langgraph/observability\n\nLangGraph v1.0Welcome to the new LangGraph documentation! If you encounter any issues or have feedback, please open an issue so we can improve. Archived v0 documentation can be found here.See the release notes and migration guide for a complete list of changes and instructions on how to upgrade your code.\nPrerequisites\nBefore you begin, ensure you have the following:- A LangSmith account (free to sign up)\nEnable tracing\nTo enable tracing for your application, set the following environment variables:default\n. To configure a custom project name, see Log to a project.\nFor more information, see Trace with LangGraph.\nTrace selectively\nYou may opt to trace specific invocations or parts of your application using LangSmith\u2019stracing_context\ncontext manager:\nLog to a project\nStatically\nStatically\nYou can set a custom project name for your entire application by setting the\nLANGSMITH_PROJECT\nenvironment variable:Dynamically\nDynamically\nYou can set the project name programmatically for specific operations:\nAdd metadata to traces\nYou can annotate your traces with custom metadata and tags:tracing_context\nalso accepts tags and metadata for fine-grained control:\nUse anonymizers to prevent logging of sensitive data in traces\nYou may want to mask sensitive data to prevent it from being logged to LangSmith. You can create anonymizers and apply them to your graph using configuration. This example will redact anything matching the Social Security Number format XXX-XX-XXXX from traces sent to LangSmith.Python", "tokens": 231, "node_type": "child"}
{"id": 456, "chunk_id": "c16acf8a553ac24f5d77e7d3a1c6e493", "parent_id": 430, "url": "", "namespace": "langchain", "title": "oss-python-langgraph-overview", "headers": ["oss-python-langgraph-overview"], "section_index": 0, "chunk_index": 0, "text": "# oss-python-langgraph-overview\n\n> Source: https://docs.langchain.com/oss/python/langgraph/overview\n\nLangGraph v1.0Welcome to the new LangGraph documentation! If you encounter any issues or have feedback, please open an issue so we can improve. Archived v0 documentation can be found here.See the release notes and migration guide for a complete list of changes and instructions on how to upgrade your code.\nInstall\nCore benefits\nLangGraph provides low-level supporting infrastructure for any long-running, stateful workflow or agent. LangGraph does not abstract prompts or architecture, and provides the following central benefits:- Durable execution: Build agents that persist through failures and can run for extended periods, resuming from where they left off.\n- Human-in-the-loop: Incorporate human oversight by inspecting and modifying agent state at any point.\n- Comprehensive memory: Create stateful agents with both short-term working memory for ongoing reasoning and long-term memory across sessions.\n- Debugging with LangSmith: Gain deep visibility into complex agent behavior with visualization tools that trace execution paths, capture state transitions, and provide detailed runtime metrics.\n- Production-ready deployment: Deploy sophisticated agent systems confidently with scalable infrastructure designed to handle the unique challenges of stateful, long-running workflows.\nLangGraph ecosystem\nWhile LangGraph can be used standalone, it also integrates seamlessly with any LangChain product, giving developers a full suite of tools for building agents. To improve your LLM application development, pair LangGraph with:- LangSmith \u2014 Helpful for agent evals and observability. Debug poor-performing LLM app runs, evaluate agent trajectories, gain visibility in production, and improve performance over time.\n- LangSmith \u2014 Deploy and scale agents effortlessly with a purpose-built deployment platform for long running, stateful workflows. Discover, reuse, configure, and share agents across teams \u2014 and iterate quickly with visual prototyping in Studio.\n- LangChain - Provides integrations and composable components to streamline LLM application development. Contains agent abstractions built on top of LangGraph.", "tokens": 299, "node_type": "child"}
{"id": 457, "chunk_id": "c16acf8a553ac24f5d77e7d3a1c6e493", "parent_id": 431, "url": "", "namespace": "langchain", "title": "oss-python-langgraph-persistence", "headers": ["oss-python-langgraph-persistence"], "section_index": 0, "chunk_index": 0, "text": "# oss-python-langgraph-persistence > Source: https://docs.langchain.com/oss/python/langgraph/persistence LangGraph v1.0Welcome to the new LangGraph documentation! If you encounter any issues or have feedback, please open an issue so we can improve. Archived v0 documentation can be found here.See the release notes and migration guide for a complete list of changes and instructions on how to upgrade your code. checkpoint of the graph state at every super-step. Those checkpoints are saved to a thread , which can be accessed after graph execution. Because threads allow access to graph\u2019s state after execution, several powerful capabilities including human-in-the-loop, memory, time travel, and fault-tolerance are all possible. Below, we\u2019ll discuss each of these concepts in more detail. LangGraph API handles checkpointing automatically When using the LangGraph API, you don\u2019t need to implement or configure checkpointers manually. The API handles all persistence infrastructure for you behind the scenes. Threads A thread is a unique ID or thread identifier assigned to each checkpoint saved by a checkpointer. It contains the accumulated state of a sequence of runs. When a run is executed, the state of the underlying graph of the assistant will be persisted to the thread. When invoking a graph with a checkpointer, you must specify athread_id as part of the configurable portion of the config. Checkpoints The state of a thread at a particular point in time is called a checkpoint. Checkpoint is a snapshot of the graph state saved at each super-step and is represented byStateSnapshot object with the following key properties: config : Config associated with this checkpoint.metadata : Metadata associated with this checkpoint.values : Values of the state channels at this point in time.next A tuple of the node names to execute next in the graph.tasks : A tuple ofPregelTask objects that contain information about next tasks to be executed. If the step was previously attempted, it will include error information. If a graph was interrupted dynamically from within a node, tasks will contain additional data associated with interrupts. - empty checkpoint with START as the next node to be executed - checkpoint with the user input {'foo': '', 'bar': []} andnode_a as the next node to be executed - checkpoint with the outputs of node_a {'foo': 'a', 'bar': ['a']} andnode_b as the next node to be executed - checkpoint with the outputs of node_b {'foo': 'b', 'bar': ['a', 'b']} and no next nodes to be executed bar channel values contain outputs from both nodes as we have a reducer for bar channel. Get state When interacting with the saved graph state, you must specify a thread identifier. You can view the latest state of the graph by callinggraph.get_state(config) . This will return a StateSnapshot object that corresponds to the latest checkpoint associated with the thread ID provided in the config or a checkpoint associated with a checkpoint ID for the thread, if provided. get_state will look like this: Get state history You can get the full history of the graph execution for a given thread by callinggraph.get_state_history(config) . This will return a list of StateSnapshot objects associated with the thread ID provided in the config. Importantly, the checkpoints will be ordered chronologically with the most recent checkpoint / StateSnapshot being the first in the list. get_state_history will look like this: Replay It\u2019s also possible to play-back a prior graph execution. If weinvoke a graph with a thread_id and a checkpoint_id , then we will re-play the previously executed steps before a checkpoint that corresponds to the checkpoint_id , and only execute the steps after the checkpoint. thread_id is the ID of a thread.checkpoint_id is an identifier that refers to a specific checkpoint within a thread. configurable portion of the config: checkpoint_id . All of the steps after checkpoint_id will be executed (i.e., a new fork), even if they have been executed previously. See this how to guide on time-travel to learn more about replaying. Update state In addition to re-playing the graph from specificcheckpoints , we can also edit the graph state. We do this using graph.update_state() . This method accepts three different arguments: config The config should contain thread_id specifying which thread to update. When only the thread_id is passed, we update (or fork) the current state. Optionally, if we include checkpoint_id field, then we fork that selected checkpoint. values These are the values that will be used to update the state. Note that this update is treated exactly as any update from a node is treated. This means that these values will be passed to the reducer functions, if they are defined for some of the channels in the graph state. This means that update_state does NOT automatically overwrite the channel values for every channel, but only for the channels without reducers. Let\u2019s walk through an example. Let\u2019s assume you have defined the state of your graph with the following schema (see full example above): foo key (channel) is completely changed (because there is no reducer specified for that channel, so update_state overwrites it). However, there is a reducer specified for the bar key, and so it appends \"b\" to the state of bar . as_node The final thing you can optionally specify when calling update_state is as_node . If you provided it, the update will be applied as if it came from node as_node . If as_node is not provided, it will be set to the last node that updated the state, if not ambiguous. The reason this matters is that the next steps to execute depend on the last node to have given an update, so this can be used to control which node executes next. See this how to guide on time-travel to learn more about forking state. Memory Store A state schema specifies a set of keys that are populated as a graph is executed. As discussed above, state can be written by a checkpointer to a thread at each graph step, enabling state persistence. But, what if we want to retain some information across threads? Consider the case of a chatbot where we want to retain", "tokens": 1000, "node_type": "child"}
{"id": 458, "chunk_id": "9b0fd3b850698317bef57fa6e71a38ba", "parent_id": 431, "url": "", "namespace": "langchain", "title": "oss-python-langgraph-persistence", "headers": ["oss-python-langgraph-persistence"], "section_index": 0, "chunk_index": 1, "text": "optionally specify when calling update_state is as_node . If you provided it, the update will be applied as if it came from node as_node . If as_node is not provided, it will be set to the last node that updated the state, if not ambiguous. The reason this matters is that the next steps to execute depend on the last node to have given an update, so this can be used to control which node executes next. See this how to guide on time-travel to learn more about forking state. Memory Store A state schema specifies a set of keys that are populated as a graph is executed. As discussed above, state can be written by a checkpointer to a thread at each graph step, enabling state persistence. But, what if we want to retain some information across threads? Consider the case of a chatbot where we want to retain specific information about the user across all chat conversations (e.g., threads) with that user! With checkpointers alone, we cannot share information across threads. This motivates the need for theStore interface. As an illustration, we can define an InMemoryStore to store information about a user across threads. We simply compile our graph with a checkpointer, as before, and with our new in_memory_store variable. LangGraph API handles stores automatically When using the LangGraph API, you don\u2019t need to implement or configure stores manually. The API handles all storage infrastructure for you behind the scenes. Basic Usage First, let\u2019s showcase this in isolation without using LangGraph.tuple , which in this specific example will be (<user_id>, \"memories\") . The namespace can be any length and represent anything, does not have to be user specific. store.put method to save memories to our namespace in the store. When we do this, we specify the namespace, as defined above, and a key-value pair for the memory: the key is simply a unique identifier for the memory (memory_id ) and the value (a dictionary) is the memory itself. store.search method, which will return all memories for a given user as a list. The most recent memory is the last in the list. Item ) with certain attributes. We can access it as a dictionary by converting via .dict as above. The attributes it has are: value : The value (itself a dictionary) of this memorykey : A unique key for this memory in this namespacenamespace : A list of strings, the namespace of this memory typecreated_at : Timestamp for when this memory was createdupdated_at : Timestamp for when this memory was updated Semantic Search Beyond simple retrieval, the store also supports semantic search, allowing you to find memories based on meaning rather than exact matches. To enable this, configure the store with an embedding model:fields parameter or by specifying the index parameter when storing memories: Using in LangGraph With this all in place, we use thein_memory_store in LangGraph. The in_memory_store works hand-in-hand with the checkpointer: the checkpointer saves state to threads, as discussed above, and the in_memory_store allows us to store arbitrary information for access across threads. We compile the graph with both the checkpointer and the in_memory_store as follows. thread_id , as before, and also with a user_id , which we\u2019ll use to namespace our memories to this particular user as we showed above. in_memory_store and the user_id in any node by passing store: BaseStore and config: RunnableConfig as node arguments. Here\u2019s how we might use semantic search in a node to find relevant memories: store.search method to get memories. Recall the memories are returned as a list of objects that can be converted to a dictionary. user_id is the same. langgraph.json file. For example: Checkpointer libraries Under the hood, checkpointing is powered by checkpointer objects that conform to BaseCheckpointSaver interface. LangGraph provides several checkpointer implementations, all implemented via standalone, installable libraries:langgraph-checkpoint : The base interface for checkpointer savers (BaseCheckpointSaver) and serialization/deserialization interface (SerializerProtocol ). Includes in-memory checkpointer implementation (InMemorySaver ) for experimentation. LangGraph comes withlanggraph-checkpoint included.langgraph-checkpoint-sqlite : An implementation of LangGraph checkpointer that uses SQLite database (SqliteSaver /AsyncSqliteSaver ). Ideal for experimentation and local workflows. Needs to be installed separately.langgraph-checkpoint-postgres : An advanced checkpointer that uses Postgres database (PostgresSaver /AsyncPostgresSaver ), used in LangSmith. Ideal for using in production. Needs to be installed separately. Checkpointer interface Each checkpointer conforms to BaseCheckpointSaver interface and implements the following methods:.put - Store a checkpoint with its configuration and metadata..put_writes - Store intermediate writes linked to a checkpoint (i.e. pending writes)..get_tuple - Fetch a checkpoint tuple using for a given configuration (thread_id andcheckpoint_id ). This is used to populateStateSnapshot ingraph.get_state() ..list - List checkpoints that match a given configuration and filter criteria. This is used to populate state history ingraph.get_state_history() .ainvoke , .astream , .abatch ), asynchronous versions of the above methods will be used (.aput , .aput_writes , .aget_tuple , .alist ). Serializer When checkpointers save the graph state, they need to serialize the channel values in the state. This is done using serializer objects.langgraph_checkpoint defines protocol for implementing serializers provides a default implementation (JsonPlusSerializer) that handles a wide variety of types, including LangChain and LangGraph primitives, datetimes, enums and more. Serialization with pickle The default serializer, JsonPlusSerializer , uses ormsgpack and JSON under the hood, which is not suitable for all types of objects. If you want to fallback to pickle for objects not currently supported by our msgpack encoder (such as Pandas dataframes), you can use the pickle_fallback argument of the JsonPlusSerializer : Encryption Checkpointers can optionally encrypt all persisted state. To enable this, pass an instance ofEncryptedSerializer to the serde argument of any BaseCheckpointSaver implementation. The easiest way to create an encrypted serializer is via from_pycryptodome_aes , which reads the AES key from the LANGGRAPH_AES_KEY environment variable (or accepts a key argument): LANGGRAPH_AES_KEY is present, so you only need to provide the environment variable. Other encryption schemes can be used by implementing CipherProtocol and supplying it to EncryptedSerializer .", "tokens": 983, "node_type": "child"}
{"id": 459, "chunk_id": "c16acf8a553ac24f5d77e7d3a1c6e493", "parent_id": 432, "url": "", "namespace": "langchain", "title": "oss-python-langgraph-pregel", "headers": ["oss-python-langgraph-pregel"], "section_index": 0, "chunk_index": 0, "text": "# oss-python-langgraph-pregel\n\n> Source: https://docs.langchain.com/oss/python/langgraph/pregel\n\nLangGraph v1.0Welcome to the new LangGraph documentation! If you encounter any issues or have feedback, please open an issue so we can improve. Archived v0 documentation can be found here.See the release notes and migration guide for a complete list of changes and instructions on how to upgrade your code.\nNote: The Pregel runtime is named after Google\u2019s Pregel algorithm, which describes an efficient method for large-scale parallel computation using graphs.\nOverview\nIn LangGraph, Pregel combines actors and channels into a single application. Actors read data from channels and write data to channels. Pregel organizes the execution of the application into multiple steps, following the Pregel Algorithm/Bulk Synchronous Parallel model. Each step consists of three phases:- Plan: Determine which actors to execute in this step. For example, in the first step, select the actors that subscribe to the special input channels; in subsequent steps, select the actors that subscribe to channels updated in the previous step.\n- Execution: Execute all selected actors in parallel, until all complete, or one fails, or a timeout is reached. During this phase, channel updates are invisible to actors until the next step.\n- Update: Update the channels with the values written by the actors in this step.\nActors\nAn actor is aPregelNode\n. It subscribes to channels, reads data from them, and writes data to them. It can be thought of as an actor in the Pregel algorithm. PregelNodes\nimplement LangChain\u2019s Runnable interface.\nChannels\nChannels are used to communicate between actors (PregelNodes). Each channel has a value type, an update type, and an update function \u2013 which takes a sequence of updates and modifies the stored value. Channels can be used to send data from one chain to another, or to send data from a chain to itself in a future step. LangGraph provides a number of built-in channels:- LastValue: The default channel, stores the last value sent to the channel, useful for input and output values, or for sending data from one step to the next.\n- Topic: A configurable PubSub Topic, useful for sending multiple values between actors, or for accumulating output. Can be configured to deduplicate values or to accumulate values over the course of multiple steps.\n- BinaryOperatorAggregate: stores a persistent value, updated by applying a binary operator to the current value and each update sent to the channel, useful for computing aggregates over multiple steps; e.g.,\ntotal = BinaryOperatorAggregate(int, operator.add)\nExamples\nWhile most users will interact with Pregel through the StateGraph API or the entrypoint decorator, it is possible to interact with Pregel directly. Below are a few different examples to give you a sense of the Pregel API.- Single node\n- Multiple nodes\n- Topic\n- BinaryOperatorAggregate\n- Cycle\nHigh-level API\nLangGraph provides two high-level APIs for creating a Pregel application: the StateGraph (Graph API) and the Functional API.- StateGraph (Graph API)\n- Functional API\nThe StateGraph (Graph API) is a higher-level abstraction that simplifies the creation of Pregel applications. It allows you to define a graph of nodes and edges. When you compile the graph, the StateGraph API automatically creates the Pregel application for you.The compiled Pregel instance will be associated with a list of nodes and channels. You can inspect the nodes and channels by printing them.You will see something like this:You should see something like this", "tokens": 556, "node_type": "child"}
{"id": 460, "chunk_id": "c16acf8a553ac24f5d77e7d3a1c6e493", "parent_id": 433, "url": "", "namespace": "langchain", "title": "oss-python-langgraph-quickstart", "headers": ["oss-python-langgraph-quickstart"], "section_index": 0, "chunk_index": 0, "text": "# oss-python-langgraph-quickstart\n\n> Source: https://docs.langchain.com/oss/python/langgraph/quickstart\n\nLangGraph v1.0Welcome to the new LangGraph documentation! If you encounter any issues or have feedback, please open an issue so we can improve. Archived v0 documentation can be found here.See the release notes and migration guide for a complete list of changes and instructions on how to upgrade your code.\n- Use the Graph API if you prefer to define your agent as a graph of nodes and edges.\n- Use the Functional API if you prefer to define your agent as a single function.\n- Use the Graph API\n- Use the Functional API\n1. Define tools and model\nIn this example, we\u2019ll use the Claude Sonnet 4.5 model and define tools for addition, multiplication, and division.2. Define state\nThe graph\u2019s state is used to store the messages and the number of LLM calls.State in LangGraph persists throughout the agent\u2019s execution. The\nAnnotated\ntype with operator.add\nensures that new messages are appended to the existing list rather than replacing it.3. Define model node\nThe model node is used to call the LLM and decide whether to call a tool or not.4. Define tool node\nThe tool node is used to call the tools and return the results.5. Define end logic\nThe conditional edge function is used to route to the tool node or end based upon whether the LLM made a tool call.6. Build and compile the agent\nThe agent is built using theStateGraph\nclass and compiled using the compile\nmethod.Full code example\nFull code example", "tokens": 252, "node_type": "child"}
{"id": 461, "chunk_id": "c16acf8a553ac24f5d77e7d3a1c6e493", "parent_id": 434, "url": "", "namespace": "langchain", "title": "oss-python-langgraph-sql-agent", "headers": ["oss-python-langgraph-sql-agent"], "section_index": 0, "chunk_index": 0, "text": "# oss-python-langgraph-sql-agent\n\n> Source: https://docs.langchain.com/oss/python/langgraph/sql-agent\n\nLangGraph v1.0Welcome to the new LangGraph documentation! If you encounter any issues or have feedback, please open an issue so we can improve. Archived v0 documentation can be found here.See the release notes and migration guide for a complete list of changes and instructions on how to upgrade your code.\nBuilding Q&A systems of SQL databases requires executing model-generated SQL queries. There are inherent risks in doing this. Make sure that your database connection permissions are always scoped as narrowly as possible for your agent\u2019s needs. This will mitigate, though not eliminate, the risks of building a model-driven system.\nConcepts\nWe will cover the following concepts:- Tools for reading from SQL databases\n- The LangGraph Graph API, including state, nodes, edges, and conditional edges.\n- Human-in-the-loop processes\nSetup\nInstallation\nLangSmith\nSet up LangSmith to inspect what is happening inside your chain or agent. Then set the following environment variables:1. Select an LLM\nSelect a model that supports tool-calling: The output shown in the examples below used OpenAI.2. Configure the database\nYou will be creating a SQLite database for this tutorial. SQLite is a lightweight database that is easy to set up and use. We will be loading thechinook\ndatabase, which is a sample database that represents a digital media store.\nFor convenience, we have hosted the database (Chinook.db\n) on a public GCS bucket.\nlangchain_community\npackage to interact with the database. The wrapper provides a simple interface to execute SQL queries and fetch results:\n3. Add tools for database interactions\nUse theSQLDatabase\nwrapper available in the langchain_community\npackage to interact with the database. The wrapper provides a simple interface to execute SQL queries and fetch results:\n4. Define application steps\nWe construct dedicated nodes for the following steps:- Listing DB tables\n- Calling the \u201cget schema\u201d tool\n- Generating a query\n- Checking the query\n5. Implement the agent\nWe can now assemble these steps into a workflow using the Graph API. We define a conditional edge at the query generation step that will route to the query checker if a query is generated, or end if there are no tool calls present, such that the LLM has delivered a response to the query.6. Implement human-in-the-loop review\nIt can be prudent to check the agent\u2019s SQL queries before they are executed for any unintended actions or inefficiencies. Here we leverage LangGraph\u2019s human-in-the-loop features to pause the run before executing a SQL query and wait for human review. Using LangGraph\u2019s persistence layer, we can pause the run indefinitely (or at least as long as the persistence layer is alive). Let\u2019s wrap thesql_db_query\ntool in a node that receives human input. We can implement this using the interrupt function. Below, we allow for input to approve the tool call, edit its arguments, or provide user feedback.\nLet\u2019s now re-assemble our graph. We will replace the programmatic check with human review. Note that we now include a checkpointer; this is required to pause and resume the run.", "tokens": 499, "node_type": "child"}
{"id": 462, "chunk_id": "c16acf8a553ac24f5d77e7d3a1c6e493", "parent_id": 435, "url": "", "namespace": "langchain", "title": "oss-python-langgraph-streaming", "headers": ["oss-python-langgraph-streaming"], "section_index": 0, "chunk_index": 0, "text": "# oss-python-langgraph-streaming\n\n> Source: https://docs.langchain.com/oss/python/langgraph/streaming\n\n- Stream graph state \u2014 get state updates / values with\nupdates\nandvalues\nmodes. - Stream subgraph outputs \u2014 include outputs from both the parent graph and any nested subgraphs.\n- Stream LLM tokens \u2014 capture token streams from anywhere: inside nodes, subgraphs, or tools.\n- Stream custom data \u2014 send custom updates or progress signals directly from tool functions.\n- Use multiple streaming modes \u2014 choose from\nvalues\n(full state),updates\n(state deltas),messages\n(LLM tokens + metadata),custom\n(arbitrary user data), ordebug\n(detailed traces).\nSupported stream modes\nPass one or more of the following stream modes as a list to thestream()\nor astream()\nmethods:\n| Mode | Description |\n|---|---|\nvalues | Streams the full value of the state after each step of the graph. |\nupdates | Streams the updates to the state after each step of the graph. If multiple updates are made in the same step (e.g., multiple nodes are run), those updates are streamed separately. |\ncustom | Streams custom data from inside your graph nodes. |\nmessages | Streams 2-tuples (LLM token, metadata) from any graph nodes where an LLM is invoked. |\ndebug | Streams as much information as possible throughout the execution of the graph. |\nBasic usage example\nLangGraph graphs expose the.stream()\n(sync) and .astream()\n(async) methods to yield streamed outputs as iterators.\nExtended example: streaming updates\nExtended example: streaming updates\nStream multiple modes\nYou can pass a list as thestream_mode\nparameter to stream multiple modes at once.\nThe streamed outputs will be tuples of (mode, chunk)\nwhere mode\nis the name of the stream mode and chunk\nis the data streamed by that mode.\nStream graph state\nUse the stream modesupdates\nand values\nto stream the state of the graph as it executes.\nupdates\nstreams the updates to the state after each step of the graph.values\nstreams the full value of the state after each step of the graph.\n- updates\n- values\nStream subgraph outputs\nTo include outputs from subgraphs in the streamed outputs, you can setsubgraphs=True\nin the .stream()\nmethod of the parent graph. This will stream outputs from both the parent graph and any subgraphs.\nThe outputs will be streamed as tuples (namespace, data)\n, where namespace\nis a tuple with the path to the node where a subgraph is invoked, e.g. (\"parent_node:<task_id>\", \"child_node:<task_id>\")\n.\nExtended example: streaming from subgraphs\nExtended example: streaming from subgraphs\nDebugging\nUse thedebug\nstreaming mode to stream as much information as possible throughout the execution of the graph. The streamed outputs include the name of the node as well as the full state.\nLLM tokens\nUse themessages\nstreaming mode to stream Large Language Model (LLM) outputs token by token from any part of your graph, including nodes, tools, subgraphs, or tasks.\nThe streamed output from messages\nmode is a tuple (message_chunk, metadata)\nwhere:\nmessage_chunk\n: the token or message segment from the LLM.metadata\n: a dictionary containing details about the graph node and LLM invocation.\nIf your LLM is not available as a LangChain integration, you can stream its outputs using custom\nmode instead. See use with any LLM for details.\nRunnableConfig\nto ainvoke()\nto enable proper streaming. See Async with Python < 3.11 for details or upgrade to Python 3.11+.Filter by LLM invocation\nYou can associatetags\nwith LLM invocations to filter the streamed tokens by LLM invocation.\nExtended example: filtering by tags\nExtended example: filtering by tags\nFilter by node\nTo stream tokens only from specific nodes, usestream_mode=\"messages\"\nand filter the outputs by the langgraph_node\nfield in the streamed metadata:\nExtended example: streaming LLM tokens from specific nodes\nExtended example: streaming LLM tokens from specific nodes\nStream custom data\nTo send custom user-defined data from inside a LangGraph node or tool, follow these steps:- Use\nget_stream_writer\nto access the stream writer and emit custom data. - Set\nstream_mode=\"custom\"\nwhen calling.stream()\nor.astream()\nto get the custom data in the stream. You can combine multiple modes (e.g.,[\"updates\", \"custom\"]\n), but at least one must be\"custom\"\n.\nget_stream_writer\nin async for Python < 3.11\nIn async code running on Python < 3.11, get_stream_writer\nwill not work.\nInstead, add a writer\nparameter to your node or tool and pass it manually.\nSee Async with Python < 3.11 for usage examples.- node\n- tool\nUse with any LLM\nYou can usestream_mode=\"custom\"\nto stream data from any LLM API \u2014 even if that API does not implement the LangChain chat model interface.\nThis lets you integrate raw LLM clients or external services that provide their own streaming interfaces, making LangGraph highly flexible for custom setups.\nExtended example: streaming arbitrary chat model\nExtended example: streaming arbitrary chat model\nDisable streaming for specific chat models\nIf your application mixes models that support streaming with those that do not, you may need to explicitly disable streaming for models that do not support it. Setdisable_streaming=True\nwhen initializing the model.\n- init_chat_model\n- chat model interface\nAsync with Python < 3.11\nIn Python versions < 3.11, asyncio tasks do not support thecontext\nparameter.\nThis limits LangGraph ability to automatically propagate context, and affects LangGraph\u2019s streaming mechanisms in two key ways:\n- You must explicitly pass\nRunnableConfig\ninto async LLM calls (e.g.,ainvoke()\n), as callbacks are not automatically propagated. - You cannot use\nget_stream_writer\nin async nodes or tools \u2014 you must pass awriter\nargument directly.\nExtended example: async LLM call with manual config\nExtended example: async LLM call with manual config\nExtended example: async custom streaming with stream writer\nExtended example: async custom streaming with stream writer", "tokens": 918, "node_type": "child"}
{"id": 463, "chunk_id": "c16acf8a553ac24f5d77e7d3a1c6e493", "parent_id": 436, "url": "", "namespace": "langchain", "title": "oss-python-langgraph-studio", "headers": ["oss-python-langgraph-studio"], "section_index": 0, "chunk_index": 0, "text": "# oss-python-langgraph-studio\n\n> Source: https://docs.langchain.com/oss/python/langgraph/studio\n\nLangGraph v1.0Welcome to the new LangGraph documentation! If you encounter any issues or have feedback, please open an issue so we can improve. Archived v0 documentation can be found here.See the release notes and migration guide for a complete list of changes and instructions on how to upgrade your code.\nPrerequisites\nBefore you begin, ensure you have the following:- An API key for LangSmith (free to sign up)\nSetup local LangGraph server\n1. Install the LangGraph CLI\n2. Prepare your agent\nWe\u2019ll use the following simple agent as an example:agent.py\n3. Environment variables\nCreate a.env\nfile in the root of your project and fill in the necessary API keys. We\u2019ll need to set the LANGSMITH_API_KEY\nenvironment variable to the API key you get from LangSmith.\nBe sure not to commit your\n.env\nto version control systems such as Git!.env\n4. Create a LangGraph config file\nInside your app\u2019s directory, create a configuration filelanggraph.json\n:\nlanggraph.json\ncreate_agent\nautomatically returns a compiled LangGraph graph that we can pass to the graphs\nkey in our configuration file.\nSo far, our project structure looks like this:\n5. Install dependencies\nIn the root of your new LangGraph app, install the dependencies:6. View your agent in Studio\nStart your LangGraph server:Safari blocks\nlocalhost\nconnections to Studio. To work around this, run the above command with --tunnel\nto access Studio via a secure tunnel.http://127.0.0.1:2024\n) and the Studio UI https://smith.langchain.com/studio/?baseUrl=http://127.0.0.1:2024\n:", "tokens": 239, "node_type": "child"}
{"id": 464, "chunk_id": "c16acf8a553ac24f5d77e7d3a1c6e493", "parent_id": 437, "url": "", "namespace": "langchain", "title": "oss-python-langgraph-test", "headers": ["oss-python-langgraph-test"], "section_index": 0, "chunk_index": 0, "text": "# oss-python-langgraph-test\n\n> Source: https://docs.langchain.com/oss/python/langgraph/test\n\nLangGraph v1.0Welcome to the new LangGraph documentation! If you encounter any issues or have feedback, please open an issue so we can improve. Archived v0 documentation can be found here.See the release notes and migration guide for a complete list of changes and instructions on how to upgrade your code.\ncreate_agent\ninstead.\nPrerequisites\nFirst, make sure you havepytest\ninstalled:\nGetting started\nBecause many LangGraph agents depend on state, a useful pattern is to create your graph before each test where you use it, then compile it within tests with a new checkpointer instance. The below example shows how this works with a simple, linear graph that progresses throughnode1\nand node2\n. Each node updates the single state key my_key\n:\nTesting individual nodes and edges\nCompiled LangGraph agents expose references to each individual node asgraph.nodes\n. You can take advantage of this to test individual nodes within your agent. Note that this will bypass any checkpointers passed when compiling the graph:\nPartial execution\nFor agents made up of larger graphs, you may wish to test partial execution paths within your agent rather than the entire flow end-to-end. In some cases, it may make semantic sense to restructure these sections as subgraphs, which you can invoke in isolation as normal. However, if you do not wish to make changes to your agent graph\u2019s overall structure, you can use LangGraph\u2019s persistence mechanisms to simulate a state where your agent is paused right before the beginning of the desired section, and will pause again at the end of the desired section. The steps are as follows:- Compile your agent with a checkpointer (the in-memory checkpointer\nInMemorySaver\nwill suffice for testing). - Call your agent\u2019s\nupdate_state\nmethod with anas_node\nparameter set to the name of the node before the one you want to start your test. - Invoke your agent with the same\nthread_id\nyou used to update the state and aninterrupt_after\nparameter set to the name of the node you want to stop at.", "tokens": 336, "node_type": "child"}
{"id": 465, "chunk_id": "c16acf8a553ac24f5d77e7d3a1c6e493", "parent_id": 438, "url": "", "namespace": "langchain", "title": "oss-python-langgraph-thinking-in-langgraph", "headers": ["oss-python-langgraph-thinking-in-langgraph"], "section_index": 0, "chunk_index": 0, "text": "# oss-python-langgraph-thinking-in-langgraph > Source: https://docs.langchain.com/oss/python/langgraph/thinking-in-langgraph Start with the process you want to automate Imagine that you need to build an AI agent that handles customer support emails. Your product team has given you these requirements: The agent should:- Read incoming customer emails - Classify them by urgency and topic - Search relevant documentation to answer questions - Draft appropriate responses - Escalate complex issues to human agents - Schedule follow-ups when needed - Simple product question: \u201cHow do I reset my password?\u201d - Bug report: \u201cThe export feature crashes when I select PDF format\u201d - Urgent billing issue: \u201cI was charged twice for my subscription!\u201d - Feature request: \u201cCan you add dark mode to the mobile app?\u201d - Complex technical issue: \u201cOur API integration fails intermittently with 504 errors\u201d Step 1: Map out your workflow as discrete steps Start by identifying the distinct steps in your process. Each step will become a node (a function that does one specific thing). Then sketch how these steps connect to each other. The arrows show possible paths, but the actual decision of which path to take happens inside each node. Now that you\u2019ve identified the components in your workflow, let\u2019s understand what each node needs to do:- Read Email: Extract and parse the email content - Classify Intent: Use an LLM to categorize urgency and topic, then route to appropriate action - Doc Search: Query your knowledge base for relevant information - Bug Track: Create or update issue in tracking system - Draft Reply: Generate an appropriate response - Human Review: Escalate to human agent for approval or handling - Send Reply: Dispatch the email response Step 2: Identify what each step needs to do For each node in your graph, determine what type of operation it represents and what context it needs to work properly.LLM Steps Data Steps Action Steps User Input Steps LLM Steps When a step needs to understand, analyze, generate text, or make reasoning decisions:Classify Intent Node Classify Intent Node - Static context (prompt): Classification categories, urgency definitions, response format - Dynamic context (from state): Email content, sender information - Desired outcome: Structured classification that determines routing Draft Reply Node Draft Reply Node - Static context (prompt): Tone guidelines, company policies, response templates - Dynamic context (from state): Classification results, search results, customer history - Desired outcome: Professional email response ready for review Data Steps When a step needs to retrieve information from external sources:Doc Search Node Doc Search Node - Parameters: Query built from intent and topic - Retry strategy: Yes, with exponential backoff for transient failures - Caching: Could cache common queries to reduce API calls Customer History Lookup Customer History Lookup - Parameters: Customer email or ID from state - Retry strategy: Yes, but with fallback to basic info if unavailable - Caching: Yes, with time-to-live to balance freshness and performance Action Steps When a step needs to perform an external action:Send Reply Node Send Reply Node - When to execute: After approval (human or automated) - Retry strategy: Yes, with exponential backoff for network issues - Should not cache: Each send is a unique action Bug Track Node Bug Track Node - When to execute: Always when intent is \u201cbug\u201d - Retry strategy: Yes, critical to not lose bug reports - Returns: Ticket ID to include in response User Input Steps When a step needs human intervention:Human Review Node Human Review Node - Context for decision: Original email, draft response, urgency, classification - Expected input format: Approval boolean plus optional edited response - When triggered: High urgency, complex issues, or quality concerns Step 3: Design your state State is the shared memory accessible to all nodes in your agent. Think of it as the notebook your agent uses to keep track of everything it learns and decides as it works through the process.What belongs in state? Ask yourself these questions about each piece of data:Include in State Don't Store - The original email and sender info (can\u2019t reconstruct these) - Classification results (needed by multiple downstream nodes) - Search results and customer data (expensive to re-fetch) - The draft response (needs to persist through review) - Execution metadata (for debugging and recovery) Keep state raw, format prompts on-demand - Different nodes can format the same data differently for their needs - You can change prompt templates without modifying your state schema - Debugging is clearer - you see exactly what data each node received - Your agent can evolve without breaking existing state Step 4: Build your nodes Now we implement each step as a function. A node in LangGraph is just a Python function that takes the current state and returns updates to it.Handle errors appropriately Different errors need different handling strategies:| Error Type | Who Fixes It | Strategy | When to Use | |---|---|---|---| | Transient errors (network issues, rate limits) | System (automatic) | Retry policy | Temporary failures that usually resolve on retry | | LLM-recoverable errors (tool failures, parsing issues) | LLM | Store error in state and loop back | LLM can see the error and adjust its approach | | User-fixable errors (missing information, unclear instructions) | Human | Pause with interrupt() | Need user input to proceed | | Unexpected errors | Developer | Let them bubble up | Unknown issues that need debugging | - Transient errors - LLM-recoverable - User-fixable - Unexpected Implementing our email agent nodes We\u2019ll implement each node as a simple function. Remember: nodes take state, do work, and return updates.Read and classify nodes Read and classify nodes Search and tracking nodes Search and tracking nodes Response nodes Response nodes Step 5: Wire it together Now we connect our nodes into a working graph. Since our nodes handle their own routing decisions, we only need a few essential edges. To enable human-in-the-loop withinterrupt() , we need to compile with a checkpointer to save state between runs: Graph compilation code Graph compilation code Command objects. Each node declares where it can go", "tokens": 1000, "node_type": "child"}
{"id": 466, "chunk_id": "9b0fd3b850698317bef57fa6e71a38ba", "parent_id": 438, "url": "", "namespace": "langchain", "title": "oss-python-langgraph-thinking-in-langgraph", "headers": ["oss-python-langgraph-thinking-in-langgraph"], "section_index": 0, "chunk_index": 1, "text": "approach | | User-fixable errors (missing information, unclear instructions) | Human | Pause with interrupt() | Need user input to proceed | | Unexpected errors | Developer | Let them bubble up | Unknown issues that need debugging | - Transient errors - LLM-recoverable - User-fixable - Unexpected Implementing our email agent nodes We\u2019ll implement each node as a simple function. Remember: nodes take state, do work, and return updates.Read and classify nodes Read and classify nodes Search and tracking nodes Search and tracking nodes Response nodes Response nodes Step 5: Wire it together Now we connect our nodes into a working graph. Since our nodes handle their own routing decisions, we only need a few essential edges. To enable human-in-the-loop withinterrupt() , we need to compile with a checkpointer to save state between runs: Graph compilation code Graph compilation code Command objects. Each node declares where it can go using type hints like Command[Literal[\"node1\", \"node2\"]] , making the flow explicit and traceable. Try out your agent Let\u2019s run our agent with an urgent billing issue that needs human review:Testing the agent Testing the agent interrupt() , saves everything to the checkpointer, and waits. It can resume days later, picking up exactly where it left off. The thread_id ensures all state for this conversation is preserved together. Summary and next steps Key Insights Building this email agent has shown us the LangGraph way of thinking:Break into discrete steps State is shared memory Nodes are functions Errors are part of the flow Human input is first-class interrupt() function pauses execution indefinitely, saves all state, and resumes exactly where it left off when you provide input. When combined with other operations in a node, it must come first.Graph structure emerges naturally Advanced considerations Node granularity trade-offs Node granularity trade-offs - Isolation of external services: Doc Search and Bug Track are separate nodes because they call external APIs. If the search service is slow or fails, we want to isolate that from the LLM calls. We can add retry policies to these specific nodes without affecting others. - Intermediate visibility: Having Classify Intent as its own node lets us inspect what the LLM decided before taking action. This is valuable for debugging and monitoring\u2014you can see exactly when and why the agent routes to human review. - Different failure modes: LLM calls, database lookups, and email sending have different retry strategies. Separate nodes let you configure these independently. - Reusability and testing: Smaller nodes are easier to test in isolation and reuse in other workflows. \"async\" mode writes checkpoints in the background for good performance while maintaining durability. Use \"exit\" mode to checkpoint only at completion (faster for long-running graphs where mid-execution recovery isn\u2019t needed), or \"sync\" mode to guarantee checkpoints are written before proceeding to the next step (useful when you need to ensure state is persisted before continuing execution).", "tokens": 478, "node_type": "child"}
{"id": 467, "chunk_id": "c16acf8a553ac24f5d77e7d3a1c6e493", "parent_id": 439, "url": "", "namespace": "langchain", "title": "oss-python-langgraph-ui", "headers": ["oss-python-langgraph-ui"], "section_index": 0, "chunk_index": 0, "text": "# oss-python-langgraph-ui\n\n> Source: https://docs.langchain.com/oss/python/langgraph/ui\n\nLangGraph v1.0Welcome to the new LangGraph documentation! If you encounter any issues or have feedback, please open an issue so we can improve. Archived v0 documentation can be found here.See the release notes and migration guide for a complete list of changes and instructions on how to upgrade your code.\ncreate_agent\n. This UI is designed to provide rich, interactive experiences for your agents with minimal setup, whether you\u2019re running locally or in a deployed context (such as LangSmith).\nAgent Chat UI\nAgent Chat UI is a Next.js application that provides a conversational interface for interacting with any LangChain agent. It supports real-time chat, tool visualization, and advanced features like time-travel debugging and state forking. Agent Chat UI is open source and can be adapted to your application needs.Features\nTool visualization\nTool visualization\nStudio automatically renders tool calls and results in an intuitive interface.\nTime-travel debugging\nTime-travel debugging\nNavigate through conversation history and fork from any point\nState inspection\nState inspection\nView and modify agent state at any point during execution\nHuman-in-the-loop\nHuman-in-the-loop\nBuilt-in support for reviewing and responding to agent requests\nQuick start\nThe fastest way to get started is using the hosted version:- Visit Agent Chat UI\n- Connect your agent by entering your deployment URL or local server address\n- Start chatting - the UI will automatically detect and render tool calls and interrupts\nLocal development\nFor customization or local development, you can run Agent Chat UI locally:Connect to your agent\nAgent Chat UI can connect to both local and deployed agents. After starting Agent Chat UI, you\u2019ll need to configure it to connect to your agent:- Graph ID: Enter your graph name (find this under\ngraphs\nin yourlanggraph.json\nfile) - Deployment URL: Your LangGraph server\u2019s endpoint (e.g.,\nhttp://localhost:2024\nfor local development, or your deployed agent\u2019s URL) - LangSmith API key (optional): Add your LangSmith API key (not required if you\u2019re using a local LangGraph server)", "tokens": 324, "node_type": "child"}
{"id": 468, "chunk_id": "c16acf8a553ac24f5d77e7d3a1c6e493", "parent_id": 440, "url": "", "namespace": "langchain", "title": "oss-python-langgraph-use-functional-api", "headers": ["oss-python-langgraph-use-functional-api"], "section_index": 0, "chunk_index": 0, "text": "# oss-python-langgraph-use-functional-api\n\n> Source: https://docs.langchain.com/oss/python/langgraph/use-functional-api\n\nCreating a simple workflow\nWhen defining anentrypoint\n, input is restricted to the first argument of the function. To pass multiple inputs, you can use a dictionary.\nExtended example: simple workflow\nExtended example: simple workflow\nExtended example: Compose an essay with an LLM\nExtended example: Compose an essay with an LLM\n@task\nand @entrypoint\ndecorators\nsyntactically. Given that a checkpointer is provided, the workflow results will\nbe persisted in the checkpointer.Parallel execution\nTasks can be executed in parallel by invoking them concurrently and waiting for the results. This is useful for improving performance in IO bound tasks (e.g., calling APIs for LLMs).Extended example: parallel LLM calls\nExtended example: parallel LLM calls\n@task\n. Each call generates a paragraph on a different topic, and results are joined into a single text output.Calling graphs\nThe Functional API and the Graph API can be used together in the same application as they share the same underlying runtime.Extended example: calling a simple graph from the functional API\nExtended example: calling a simple graph from the functional API\nCall other entrypoints\nYou can call other entrypoints from within an entrypoint or a task.Extended example: calling another entrypoint\nExtended example: calling another entrypoint\nStreaming\nThe Functional API uses the same streaming mechanism as the Graph API. Please read the streaming guide section for more details. Example of using the streaming API to stream both updates and custom data.- Import\nget_stream_writer\nfromlanggraph.config\n. - Obtain a stream writer instance within the entrypoint.\n- Emit custom data before computation begins.\n- Emit another custom message after computing the result.\n- Use\n.stream()\nto process streamed output. - Specify which streaming modes to use.\nget_stream_writer\nwill not work. Instead please\nuse the StreamWriter\nclass directly. See Async with Python < 3.11 for more details.Retry policy\nCaching Tasks\nttl\nis specified in seconds. The cache will be invalidated after this time.\nResuming after an error\nslow_task\nas its result is already saved in the checkpoint.\nHuman-in-the-loop\nThe functional API supports human-in-the-loop workflows using theinterrupt\nfunction and the Command\nprimitive.\nBasic human-in-the-loop workflow\nWe will create three tasks:- Append\n\"bar\"\n. - Pause for human input. When resuming, append human input.\n- Append\n\"qux\"\n.\nstep_1\n\u2014 are persisted, so that they are not run again following the interrupt\n.\nLet\u2019s send in a query string:\ninterrupt\nafter step_1\n. The interrupt provides instructions to resume the run. To resume, we issue a Command containing the data expected by the human_feedback\ntask.\nReview tool calls\nTo review tool calls before execution, we add areview_tool_call\nfunction that calls interrupt\n. When this function is called, execution will be paused until we issue a command to resume it.\nGiven a tool call, our function will interrupt\nfor human review. At that point we can either:\n- Accept the tool call\n- Revise the tool call and continue\n- Generate a custom tool message (e.g., instructing the model to re-format its tool call)\nToolMessage\nsupplied by the human. The results of prior tasks \u2014 in this case the initial model call \u2014 are persisted, so that they are not run again following the interrupt\n.\nShort-term memory\nShort-term memory allows storing information across different invocations of the same thread id. See short-term memory for more details.Manage checkpoints\nYou can view and delete the information stored by the checkpointer.View thread state\nView the history of the thread\nDecouple return value from saved value\nUseentrypoint.final\nto decouple what is returned to the caller from what is persisted in the checkpoint. This is useful when:\n- You want to return a computed result (e.g., a summary or status), but save a different internal value for use on the next invocation.\n- You need to control what gets passed to the previous parameter on the next run.\nChatbot example\nAn example of a simple chatbot using the functional API and theInMemorySaver\ncheckpointer.\nThe bot is able to remember the previous conversation and continue from where it left off.\nLong-term memory\nlong-term memory allows storing information across different thread ids. This could be useful for learning information about a given user in one conversation and using it in another.Workflows\n- Workflows and agent guide for more examples of how to build workflows using the Functional API.\nIntegrate with other libraries\n- Add LangGraph\u2019s features to other frameworks using the functional API: Add LangGraph features like persistence, memory and streaming to other agent frameworks that do not provide them out of the box.", "tokens": 748, "node_type": "child"}
{"id": 469, "chunk_id": "c16acf8a553ac24f5d77e7d3a1c6e493", "parent_id": 441, "url": "", "namespace": "langchain", "title": "oss-python-langgraph-use-graph-api", "headers": ["oss-python-langgraph-use-graph-api"], "section_index": 0, "chunk_index": 0, "text": "# oss-python-langgraph-use-graph-api > Source: https://docs.langchain.com/oss/python/langgraph/use-graph-api Setup Installlanggraph : Define and update state Here we show how to define and update state in LangGraph. We will demonstrate:- How to use state to define a graph\u2019s schema - How to use reducers to control how state updates are processed. Define state State in LangGraph can be aTypedDict , Pydantic model, or dataclass. Below we will use TypedDict . See this section for detail on using Pydantic. By default, graphs will have the same input and output schema, and the state determines that schema. See this section for how to define distinct input and output schemas. Let\u2019s consider a simple example using messages. This represents a versatile formulation of state for many LLM applications. See our concepts page for more detail. Update state Let\u2019s build an example graph with a single node. Our node is just a Python function that reads our graph\u2019s state and makes updates to it. The first argument to this function will always be the state:- We kicked off invocation by updating a single key of the state. - We receive the entire state in the invocation result. Process state updates with reducers Each key in the state can have its own independent reducer function, which controls how updates from nodes are applied. If no reducer function is explicitly specified then it is assumed that all updates to the key should override it. ForTypedDict state schemas, we can define reducers by annotating the corresponding field of the state with a reducer function. In the earlier example, our node updated the \"messages\" key in the state by appending a message to it. Below, we add a reducer to this key, such that updates are automatically appended: MessagesState In practice, there are additional considerations for updating lists of messages:- We may wish to update an existing message in the state. - We may want to accept short-hands for message formats, such as OpenAI format. add_messages that handles these considerations: MessagesState for convenience, so that we can have: Define input and output schemas By default,StateGraph operates with a single schema, and all nodes are expected to communicate using that schema. However, it\u2019s also possible to define distinct input and output schemas for a graph. When distinct schemas are specified, an internal schema will still be used for communication between nodes. The input schema ensures that the provided input matches the expected structure, while the output schema filters the internal data to return only the relevant information according to the defined output schema. Below, we\u2019ll see how to define distinct input and output schema. Pass private state between nodes In some cases, you may want nodes to exchange information that is crucial for intermediate logic but doesn\u2019t need to be part of the main schema of the graph. This private data is not relevant to the overall input/output of the graph and should only be shared between certain nodes. Below, we\u2019ll create an example sequential graph consisting of three nodes (node_1, node_2 and node_3), where private data is passed between the first two steps (node_1 and node_2), while the third step (node_3) only has access to the public overall state.Use Pydantic models for graph state A StateGraph accepts astate_schema argument on initialization that specifies the \u201cshape\u201d of the state that the nodes in the graph can access and update. In our examples, we typically use a python-native TypedDict or dataclass for state_schema , but state_schema can be any type. Here, we\u2019ll see how a Pydantic BaseModel can be used for state_schema to add run-time validation on inputs. - Currently, the output of the graph will NOT be an instance of a pydantic model. - Run-time validation only occurs on inputs into nodes, not on the outputs. - The validation error trace from pydantic does not show which node the error arises in. - Pydantic\u2019s recursive validation can be slow. For performance-sensitive applications, you may want to consider using a dataclass instead. Serialization Behavior Serialization Behavior - Passing Pydantic objects as inputs - Receiving outputs from the graph - Working with nested Pydantic models Runtime Type Coercion Runtime Type Coercion Working with Message Models Working with Message Models AnyMessage (rather than BaseMessage ) for proper serialization/deserialization when using message objects over the wire.Add runtime configuration Sometimes you want to be able to configure your graph when calling it. For example, you might want to be able to specify what LLM or system prompt to use at runtime, without polluting the graph state with these parameters. To add runtime configuration:- Specify a schema for your configuration - Add the configuration to the function signature for nodes or conditional edges - Pass the configuration into the graph. Extended example: specifying LLM at runtime Extended example: specifying LLM at runtime Extended example: specifying model and system message at runtime Extended example: specifying model and system message at runtime Add retry policies There are many use cases where you may wish for your node to have a custom retry policy, for example if you are calling an API, querying a database, or calling an LLM, etc. LangGraph lets you add retry policies to nodes. To configure a retry policy, pass theretry_policy parameter to the add_node. The retry_policy parameter takes in a RetryPolicy named tuple object. Below we instantiate a RetryPolicy object with the default parameters and associate it with a node: retry_on parameter uses the default_retry_on function, which retries on any exception except for the following: ValueError TypeError ArithmeticError ImportError LookupError NameError SyntaxError RuntimeError ReferenceError StopIteration StopAsyncIteration OSError requests and httpx it only retries on 5xx status codes. Extended example: customizing retry policies Extended example: customizing retry policies Add node caching Node caching is useful in cases where you want to avoid repeating operations, like when doing something expensive (either in terms of time or cost). LangGraph lets you add individualized caching policies to nodes in a graph. To configure a cache policy, pass thecache_policy parameter to the add_node function. In the following example, a CachePolicy object is", "tokens": 1000, "node_type": "child"}
{"id": 470, "chunk_id": "9b0fd3b850698317bef57fa6e71a38ba", "parent_id": 441, "url": "", "namespace": "langchain", "title": "oss-python-langgraph-use-graph-api", "headers": ["oss-python-langgraph-use-graph-api"], "section_index": 0, "chunk_index": 1, "text": "you add retry policies to nodes. To configure a retry policy, pass theretry_policy parameter to the add_node. The retry_policy parameter takes in a RetryPolicy named tuple object. Below we instantiate a RetryPolicy object with the default parameters and associate it with a node: retry_on parameter uses the default_retry_on function, which retries on any exception except for the following: ValueError TypeError ArithmeticError ImportError LookupError NameError SyntaxError RuntimeError ReferenceError StopIteration StopAsyncIteration OSError requests and httpx it only retries on 5xx status codes. Extended example: customizing retry policies Extended example: customizing retry policies Add node caching Node caching is useful in cases where you want to avoid repeating operations, like when doing something expensive (either in terms of time or cost). LangGraph lets you add individualized caching policies to nodes in a graph. To configure a cache policy, pass thecache_policy parameter to the add_node function. In the following example, a CachePolicy object is instantiated with a time to live of 120 seconds and the default key_func generator. Then it is associated with a node: cache argument when compiling the graph. The example below uses InMemoryCache to set up a graph with in-memory cache, but SqliteCache is also available. Create a sequence of steps Here we demonstrate how to construct a simple sequence of steps. We will show:- How to build a sequential graph - Built-in short-hand for constructing similar graphs. .add_node and .add_edge methods of our graph: .add_sequence : Why split application steps into a sequence with LangGraph? Why split application steps into a sequence with LangGraph? - How state updates are checkpointed - How interruptions are resumed in human-in-the-loop workflows - How we can \u201crewind\u201d and branch-off executions using LangGraph\u2019s time travel features - Populate a value in a key of the state - Update the same value - Populate a different value .add_node :.add_edge takes the names of nodes, which for functions defaults tonode.__name__ .- We must specify the entry point of the graph. For this we add an edge with the START node. - The graph halts when there are no more nodes to execute. - We kicked off invocation by providing a value for a single state key. We must always provide a value for at least one key. - The value we passed in was overwritten by the first node. - The second node updated the value. - The third node populated a different value. langgraph>=0.2.46 includes a built-in short-hand add_sequence for adding node sequences. You can compile the same graph as follows:Create branches Parallel execution of nodes is essential to speed up overall graph operation. LangGraph offers native support for parallel execution of nodes, which can significantly enhance the performance of graph-based workflows. This parallelization is achieved through fan-out and fan-in mechanisms, utilizing both standard edges and conditional_edges. Below are some examples showing how to add create branching dataflows that work for you.Run graph nodes in parallel In this example, we fan out fromNode A to B and C and then fan in to D . With our state, we specify the reducer add operation. This will combine or accumulate values for the specific key in the State, rather than simply overwriting the existing value. For lists, this means concatenating the new list with the existing list. See the above section on state reducers for more detail on updating state with reducers. \"b\" and \"c\" are executed concurrently in the same superstep. Because they are in the same step, node \"d\" executes after both \"b\" and \"c\" are finished.Importantly, updates from a parallel superstep may not be ordered consistently. If you need a consistent, predetermined ordering of updates from a parallel superstep, you should write the outputs to a separate field in the state together with a value with which to order them.Exception handling? Exception handling? - You can write regular python code within your node to catch and handle exceptions. - You can set a retry_policy to direct the graph to retry nodes that raise certain types of exceptions. Only failing branches are retried, so you needn\u2019t worry about performing redundant work. Defer node execution Deferring node execution is useful when you want to delay the execution of a node until all other pending tasks are completed. This is particularly relevant when branches have different lengths, which is common in workflows like map-reduce flows. The above example showed how to fan-out and fan-in when each path was only one step. But what if one branch had more than one step? Let\u2019s add a node\"b_2\" in the \"b\" branch: \"b\" and \"c\" are executed concurrently in the same superstep. We set defer=True on node d so it will not execute until all pending tasks are finished. In this case, this means that \"d\" waits to execute until the entire \"b\" branch is finished. Conditional branching If your fan-out should vary at runtime based on the state, you can use add_conditional_edges to select one or more paths using the graph state. See example below, where nodea generates a state update that determines the following node. Map-Reduce and the Send API LangGraph supports map-reduce and other advanced branching patterns using the Send API. Here is an example of how to use it:Create and control loops When creating a graph with a loop, we require a mechanism for terminating execution. This is most commonly done by adding a conditional edge that routes to the END node once we reach some termination condition. You can also set the graph recursion limit when invoking or streaming the graph. The recursion limit sets the number of supersteps that the graph is allowed to execute before it raises an error. Read more about the concept of recursion limits here. Let\u2019s consider a simple graph with a loop to better understand how these mechanisms work.\"recursionLimit\" in the config. This will raise a GraphRecursionError , which you can catch and handle: \"a\" is a tool-calling model, and node \"b\" represents the tools. In our route conditional edge, we specify that we should end after the \"aggregate\"", "tokens": 1000, "node_type": "child"}
{"id": 471, "chunk_id": "5345104ec252a41359fa4a3d2dfbec23", "parent_id": 441, "url": "", "namespace": "langchain", "title": "oss-python-langgraph-use-graph-api", "headers": ["oss-python-langgraph-use-graph-api"], "section_index": 0, "chunk_index": 2, "text": "using the Send API. Here is an example of how to use it:Create and control loops When creating a graph with a loop, we require a mechanism for terminating execution. This is most commonly done by adding a conditional edge that routes to the END node once we reach some termination condition. You can also set the graph recursion limit when invoking or streaming the graph. The recursion limit sets the number of supersteps that the graph is allowed to execute before it raises an error. Read more about the concept of recursion limits here. Let\u2019s consider a simple graph with a loop to better understand how these mechanisms work.\"recursionLimit\" in the config. This will raise a GraphRecursionError , which you can catch and handle: \"a\" is a tool-calling model, and node \"b\" represents the tools. In our route conditional edge, we specify that we should end after the \"aggregate\" list in the state passes a threshold length. Invoking the graph, we see that we alternate between nodes \"a\" and \"b\" before terminating once we reach the termination condition. Impose a recursion limit In some applications, we may not have a guarantee that we will reach a given termination condition. In these cases, we can set the graph\u2019s recursion limit. This will raise aGraphRecursionError after a given number of supersteps. We can then catch and handle this exception: Extended example: return state on hitting recursion limit Extended example: return state on hitting recursion limit GraphRecursionError , we can introduce a new key to the state that keeps track of the number of steps remaining until reaching the recursion limit. We can then use this key to determine if we should end the run.LangGraph implements a special RemainingSteps annotation. Under the hood, it creates a ManagedValue channel \u2014 a state channel that will exist for the duration of our graph run and no longer.Extended example: loops with branches Extended example: loops with branches - Node A - Node B - Nodes C and D - Node A - \u2026 Async Using the async programming paradigm can produce significant performance improvements when running IO-bound code concurrently (e.g., making concurrent API requests to a chat model provider). To convert async implementation of the graph to an async implementation, you will need to: - Update nodes useasync def instead ofdef . - Update the code inside to use await appropriately. - Invoke the graph with .ainvoke or.astream as desired. async variants of all the sync methods it\u2019s typically fairly quick to upgrade a sync graph to an async graph. See example below. To demonstrate async invocations of underlying LLMs, we will include a chat model: Combine control flow and state updates with Command It can be useful to combine control flow (edges) and state updates (nodes). For example, you might want to BOTH perform state updates AND decide which node to go to next in the SAME node. LangGraph provides a way to do so by returning a Command object from node functions: StateGraph with the above nodes. Notice that the graph doesn\u2019t have conditional edges for routing! This is because control flow is defined with Command inside node_a . Command as a return type annotation, e.g. Command[Literal[\"node_b\", \"node_c\"]] . This is necessary for the graph rendering and tells LangGraph that node_a can navigate to node_b and node_c .Navigate to a node in a parent graph If you are using subgraphs, you might want to navigate from a node within a subgraph to a different subgraph (i.e. a different node in the parent graph). To do so, you can specifygraph=Command.PARENT in Command : nodeA in the above example into a single-node graph that we\u2019ll add as a subgraph to our parent graph. Command.PARENT When you send updates from a subgraph node to a parent graph node for a key that\u2019s shared by both parent and subgraph state schemas, you must define a reducer for the key you\u2019re updating in the parent graph state. See the example below.Use inside tools A common use case is updating graph state from inside a tool. For example, in a customer support application you might want to look up customer information based on their account number or ID in the beginning of the conversation. To update the graph state from the tool, you can returnCommand(update={\"my_custom_key\": \"foo\", \"messages\": [...]}) from the tool: messages (or any state key used for the message history) in Command.update when returning Command from a tool and the list of messages in messages MUST contain a ToolMessage . This is necessary for the resulting message history to be valid (LLM providers require AI messages with tool calls to be followed by the tool result messages).Command , we recommend using prebuilt ToolNode which automatically handles tools returning Command objects and propagates them to the graph state. If you\u2019re writing a custom node that calls tools, you would need to manually propagate Command objects returned by the tools as the update from the node. Visualize your graph Here we demonstrate how to visualize the graphs you create. You can visualize any arbitrary Graph, including StateGraph. Let\u2019s have some fun by drawing fractals :).Mermaid We can also convert a graph class into Mermaid syntax.PNG If preferred, we could render the Graph into a.png . Here we could use three options: - Using Mermaid.ink API (does not require additional packages) - Using Mermaid + Pyppeteer (requires pip install pyppeteer ) - Using graphviz (which requires pip install graphviz ) draw_mermaid_png() uses Mermaid.Ink\u2019s API to generate the diagram.", "tokens": 917, "node_type": "child"}
{"id": 472, "chunk_id": "c16acf8a553ac24f5d77e7d3a1c6e493", "parent_id": 442, "url": "", "namespace": "langchain", "title": "oss-python-langgraph-use-subgraphs", "headers": ["oss-python-langgraph-use-subgraphs"], "section_index": 0, "chunk_index": 0, "text": "# oss-python-langgraph-use-subgraphs\n\n> Source: https://docs.langchain.com/oss/python/langgraph/use-subgraphs\n\n- Building multi-agent systems\n- Re-using a set of nodes in multiple graphs\n- Distributing development: when you want different teams to work on different parts of the graph independently, you can define each part as a subgraph, and as long as the subgraph interface (the input and output schemas) is respected, the parent graph can be built without knowing any details of the subgraph\n- Invoke a graph from a node \u2014 subgraphs are called from inside a node in the parent graph\n- Add a graph as a node \u2014 a subgraph is added directly as a node in the parent and shares state keys with the parent\nSetup\nInvoke a graph from a node\nA simple way to implement a subgraph is to invoke a graph from inside the node of another graph. In this case subgraphs can have completely different schemas from the parent graph (no shared keys). For example, you might want to keep a private message history for each of the agents in a multi-agent system. If that\u2019s the case for your application, you need to define a node function that invokes the subgraph. This function needs to transform the input (parent) state to the subgraph state before invoking the subgraph, and transform the results back to the parent state before returning the state update from the node.Full example: different state schemas\nFull example: different state schemas\nFull example: different state schemas (two levels of subgraphs)\nFull example: different state schemas (two levels of subgraphs)\nAdd a graph as a node\nWhen the parent graph and subgraph can communicate over a shared state key (channel) in the schema, you can add a graph as a node in another graph. For example, in multi-agent systems, the agents often communicate over a shared messages key. If your subgraph shares state keys with the parent graph, you can follow these steps to add it to your graph:- Define the subgraph workflow (\nsubgraph_builder\nin the example below) and compile it - Pass compiled subgraph to the\n.add_node\nmethod when defining the parent graph workflow\nFull example: shared state schemas\nFull example: shared state schemas\nAdd persistence\nYou only need to provide the checkpointer when compiling the parent graph. LangGraph will automatically propagate the checkpointer to the child subgraphs.View subgraph state\nWhen you enable persistence, you can inspect the graph state (checkpoint) via the appropriate method. To view the subgraph state, you can use the subgraphs option. You can inspect the graph state viagraph.get_state(config)\n. To view the subgraph state, you can use graph.get_state(config, subgraphs=True)\n.\nView interrupted subgraph state\nView interrupted subgraph state\n- This will be available only when the subgraph is interrupted. Once you resume the graph, you won\u2019t be able to access the subgraph state.\nStream subgraph outputs\nTo include outputs from subgraphs in the streamed outputs, you can set the subgraphs option in the stream method of the parent graph. This will stream outputs from both the parent graph and any subgraphs.Stream from subgraphs\nStream from subgraphs", "tokens": 509, "node_type": "child"}
{"id": 473, "chunk_id": "c16acf8a553ac24f5d77e7d3a1c6e493", "parent_id": 443, "url": "", "namespace": "langchain", "title": "oss-python-langgraph-use-time-travel", "headers": ["oss-python-langgraph-use-time-travel"], "section_index": 0, "chunk_index": 0, "text": "# oss-python-langgraph-use-time-travel\n\n> Source: https://docs.langchain.com/oss/python/langgraph/use-time-travel\n\nLangGraph v1.0Welcome to the new LangGraph documentation! If you encounter any issues or have feedback, please open an issue so we can improve. Archived v0 documentation can be found here.See the release notes and migration guide for a complete list of changes and instructions on how to upgrade your code.\n- Understand reasoning: Analyze the steps that led to a successful result.\n- Debug mistakes: Identify where and why errors occurred.\n- Explore alternatives: Test different paths to uncover better solutions.\n- Run the graph with initial inputs using\ninvoke\norstream\nmethods. - Identify a checkpoint in an existing thread: Use the\nget_state_history()\nmethod to retrieve the execution history for a specificthread_id\nand locate the desiredcheckpoint_id\n. Alternatively, set an interrupt before the node(s) where you want execution to pause. You can then find the most recent checkpoint recorded up to that interrupt. - Update the graph state (optional): Use the\nupdate_state\nmethod to modify the graph\u2019s state at the checkpoint and resume execution from alternative state. - Resume execution from the checkpoint: Use the\ninvoke\norstream\nmethods with an input ofNone\nand a configuration containing the appropriatethread_id\nandcheckpoint_id\n.\nIn a workflow\nThis example builds a simple LangGraph workflow that generates a joke topic and writes a joke using an LLM. It demonstrates how to run the graph, retrieve past execution checkpoints, optionally modify the state, and resume execution from a chosen checkpoint to explore alternate outcomes.Setup\nFirst we need to install the packages required1. Run the graph\n2. Identify a checkpoint\n3. Update the state\nupdate_state\nwill create a new checkpoint. The new checkpoint will be associated with the same thread, but a new checkpoint ID.", "tokens": 283, "node_type": "child"}
{"id": 474, "chunk_id": "c16acf8a553ac24f5d77e7d3a1c6e493", "parent_id": 444, "url": "", "namespace": "langchain", "title": "oss-python-langgraph-workflows-agents", "headers": ["oss-python-langgraph-workflows-agents"], "section_index": 0, "chunk_index": 0, "text": "# oss-python-langgraph-workflows-agents\n\n> Source: https://docs.langchain.com/oss/python/langgraph/workflows-agents\n\nLangGraph v1.0Welcome to the new LangGraph documentation! If you encounter any issues or have feedback, please open an issue so we can improve. Archived v0 documentation can be found here.See the release notes and migration guide for a complete list of changes and instructions on how to upgrade your code.\n- Workflows have predetermined code paths and are designed to operate in a certain order.\n- Agents are dynamic and define their own processes and tool usage.\nSetup\nTo build a workflow or agent, you can use any chat model that supports structured outputs and tool calling. The following example uses Anthropic:- Install dependencies:\n- Initialize the LLM:\nLLMs and augmentations\nWorkflows and agentic systems are based on LLMs and the various augmentations you add to them. Tool calling, structured outputs, and short term memory are a few options for tailoring LLMs to your needs.Prompt chaining\nPrompt chaining is when each LLM call processes the output of the previous call. It\u2019s often used for performing well-defined tasks that can be broken down into smaller, verifiable steps. Some examples include:- Translating documents into different languages\n- Verifying generated content for consistency\nParallelization\nWith parallelization, LLMs work simultaneously on a task. This is either done by running multiple independent subtasks at the same time, or running the same task multiple times to check for different outputs. Parallelization is commonly used to:- Split up subtasks and run them in parallel, which increases speed\n- Run tasks multiple times to check for different outputs, which increases confidence\n- Running one subtask that processes a document for keywords, and a second subtask to check for formatting errors\n- Running a task multiple times that scores a document for accuracy based on different criteria, like the number of citations, the number of sources used, and the quality of the sources\nRouting\nRouting workflows process inputs and then directs them to context-specific tasks. This allows you to define specialized flows for complex tasks. For example, a workflow built to answer product related questions might process the type of question first, and then route the request to specific processes for pricing, refunds, returns, etc.Orchestrator-worker\nIn an orchestrator-worker configuration, the orchestrator:- Breaks down tasks into subtasks\n- Delegates subtasks to workers\n- Synthesizes worker outputs into a final result\nCreating workers in LangGraph\nOrchestrator-worker workflows are common and LangGraph has built-in support for them. TheSend\nAPI lets you dynamically create worker nodes and send them specific inputs. Each worker has its own state, and all worker outputs are written to a shared state key that is accessible to the orchestrator graph. This gives the orchestrator access to all worker output and allows it to synthesize them into a final output. The example below iterates over a list of sections and uses the Send\nAPI to send a section to each worker.\nEvaluator-optimizer\nIn evaluator-optimizer workflows, one LLM call creates a response and the other evaluates that response. If the evaluator or a human-in-the-loop determines the response needs refinement, feedback is provided and the response is recreated. This loop continues until an acceptable response is generated. Evaluator-optimizer workflows are commonly used when there\u2019s particular success criteria for a task, but iteration is required to meet that criteria. For example, there\u2019s not always a perfect match when translating text between two languages. It might take a few iterations to generate a translation with the same meaning across the two languages.Agents\nAgents are typically implemented as an LLM performing actions using tools. They operate in continuous feedback loops, and are used in situations where problems and solutions are unpredictable. Agents have more autonomy than workflows, and can make decisions about the tools they use and how to solve problems. You can still define the available toolset and guidelines for how agents behave.Using tools", "tokens": 636, "node_type": "child"}
{"id": 475, "chunk_id": "c16acf8a553ac24f5d77e7d3a1c6e493", "parent_id": 445, "url": "", "namespace": "langchain", "title": "oss-python-learn", "headers": ["oss-python-learn"], "section_index": 0, "chunk_index": 0, "text": "# oss-python-learn\n\n> Source: https://docs.langchain.com/oss/python/learn\n\nLangGraph v1.0Welcome to the new LangGraph documentation! If you encounter any issues or have feedback, please open an issue so we can improve. Archived v0 documentation can be found here.See the release notes and migration guide for a complete list of changes and instructions on how to upgrade your code.\nUse Cases\nBelow are tutorials for common use cases, organized by framework.LangChain\nLangChain agent implementations make it easy to get started for most use cases.Semantic Search\nBuild a semantic search engine over a PDF with LangChain components.\nRAG Agent\nCreate a Retrieval Augmented Generation (RAG) agent.\nSQL Agent\nBuild a SQL agent to interact with databases with human-in-the-loop review.\nSupervisor Agent\nBuild a personal assistant that delegates to sub-agents.\nLangGraph\nLangChain\u2019s agent implementations use LangGraph primitives. If deeper customization is required, agents can be implemented directly in LangGraph.Custom RAG Agent\nBuild a RAG agent using LangGraph primitives for fine-grained control.\nCustom SQL Agent\nImplement a SQL agent directly in LangGraph for maximum flexibility.\nConceptual Overviews\nThese guides explain the core concepts and APIs underlying LangChain and LangGraph.Memory\nUnderstand persistence of interactions within and across threads.\nContext engineering\nLearn methods for providing AI applications the right information and tools to accomplish a task.\nGraph API\nExplore LangGraph\u2019s declarative graph-building API.\nFunctional API\nBuild agents as a single function.\nAdditional Resources\nLangChain Academy\nCourses and exercises to level up your LangChain skills.\nCase Studies\nSee how teams are using LangChain and LangGraph in production.", "tokens": 247, "node_type": "child"}
{"id": 476, "chunk_id": "c16acf8a553ac24f5d77e7d3a1c6e493", "parent_id": 446, "url": "", "namespace": "langchain", "title": "oss-python-migrate-langchain-v1", "headers": ["oss-python-migrate-langchain-v1"], "section_index": 0, "chunk_index": 0, "text": "# oss-python-migrate-langchain-v1 > Source: https://docs.langchain.com/oss/python/migrate/langchain-v1 Simplified package Thelangchain package namespace has been significantly reduced in v1 to focus on essential building blocks for agents. The streamlined package makes it easier to discover and use the core functionality. Namespace | Module | What\u2019s available | Notes | |---|---|---| langchain.agents | create_agent , AgentState | Core agent creation functionality | langchain.messages | Message types, content blocks, trim_messages | Re-exported from langchain-core | langchain.tools | @tool , BaseTool , injection helpers | Re-exported from langchain-core | langchain.chat_models | init_chat_model , BaseChatModel | Unified model initialization | langchain.embeddings | init_embeddings , Embeddings | Embedding models | langchain-classic If you were using any of the following from the langchain package, you\u2019ll need to install langchain-classic and update your imports: - Legacy chains ( LLMChain ,ConversationChain , etc.) - The indexing API langchain-community re-exports- Other deprecated functionality Migrate to create_agent Prior to v1.0, we recommended using langgraph.prebuilt.create_react_agent to build agents. Now, we recommend you use langchain.agents.create_agent to build agents. The table below outlines what functionality has changed from create_react_agent to create_agent : | Section | TL;DR - What\u2019s changed | |---|---| | Import path | Package moved from langgraph.prebuilt to langchain.agents | | Prompts | Parameter renamed to system_prompt , dynamic prompts use middleware | | Pre-model hook | Replaced by middleware with before_model method | | Post-model hook | Replaced by middleware with after_model method | | Custom state | TypedDict only, can be defined via state_schema or middleware | | Model | Dynamic selection via middleware, pre-bound models not supported | | Tools | Tool error handling moved to middleware with wrap_tool_call | | Structured output | prompted output removed, use ToolStrategy /ProviderStrategy | | Streaming node name | Node name changed from \"agent\" to \"model\" | | Runtime context | Dependency injection via context argument instead of config[\"configurable\"] | | Namespace | Streamlined to focus on agent building blocks, legacy code moved to langchain-classic | Import path The import path for the agent prebuilt has changed fromlanggraph.prebuilt to langchain.agents . The name of the function has changed from create_react_agent to create_agent : Prompts Static prompt rename Theprompt parameter has been renamed to system_prompt : SystemMessage to string If using SystemMessage objects in the system prompt, extract the string content: Dynamic prompts Dynamic prompts are a core context engineering pattern\u2014 they adapt what you tell the model based on the current conversation state. To do this, use the@dynamic_prompt decorator: Pre-model hook Pre-model hooks are now implemented as middleware with thebefore_model method. This new pattern is more extensible\u2014you can define multiple middlewares to run before the model is called, reusing common patterns across different agents. Common use cases include: - Summarizing conversation history - Trimming messages - Input guardrails, like PII redaction Post-model hook Post-model hooks are now implemented as middleware with theafter_model method. This new pattern is more extensible\u2014you can define multiple middlewares to run after the model is called, reusing common patterns across different agents. Common use cases include: - Human in the loop - Output guardrails Custom state Custom state extends the default agent state with additional fields. You can define custom state in two ways:- Via state_schema oncreate_agent - Best for state used in tools - Via middleware - Best for state managed by specific middleware hooks and tools attached to said middleware Defining custom state via middleware is preferred over defining it via state_schema on create_agent because it allows you to keep state extensions conceptually scoped to the relevant middleware and tools.state_schema is still supported for backwards compatibility on create_agent .Defining state via state_schema Use the state_schema parameter when your custom state needs to be accessed by tools: Defining state via middleware Middleware can also define custom state by setting thestate_schema attribute. This helps to keep state extensions conceptually scoped to the relevant middleware and tools. State type restrictions create_agent only supports TypedDict for state schemas. Pydantic models and dataclasses are no longer supported. langchain.agents.AgentState instead of BaseModel or decorating with dataclass . If you need to perform validation, handle it in middleware hooks instead. Model Dynamic model selection allows you to choose different models based on runtime context (e.g., task complexity, cost constraints, or user preferences).create_react_agent released in v0.6 of langgraph-prebuilt supported dynamic model and tool selection via a callable passed to the model parameter. This functionality has been ported to the middleware interface in v1. Dynamic model selection Pre-bound models To better support structured output,create_agent no longer accepts pre-bound models with tools or configuration: Dynamic model functions can return pre-bound models if structured output is not used. Tools Thetools argument to create_agent accepts a list of: - LangChain BaseTool instances (functions decorated with@tool ) - Callable objects (functions) with proper type hints and a docstring dict that represents a built-in provider tools ToolNode instances. Handling tool errors You can now configure the handling of tool errors with middleware implementing thewrap_tool_call method. Structured output Node changes Structured output used to be generated in a separate node from the main agent. This is no longer the case. We generate structured output in the main loop, reducing cost and latency.Tool and provider strategies In v1, there are two new structured output strategies:ToolStrategy uses artificial tool calling to generate structured outputProviderStrategy uses provider-native structured output generation Prompted output removed Prompted output is no longer supported via theresponse_format argument. Compared to strategies like artificial tool calling and provider native structured output, prompted output has not proven to be particularly reliable. Streaming node name rename When streaming events from agents, the node name has changed from\"agent\" to \"model\" to better reflect the node\u2019s purpose. Runtime context When you invoke an agent, it\u2019s often the case that you want to pass two types of data:- Dynamic state that changes throughout the conversation (e.g., message history) - Static context that doesn\u2019t change during the conversation (e.g., user metadata) context parameter to invoke and stream . The old config[\"configurable\"] pattern still works for backward compatibility, but using the new context parameter is recommended for new applications or", "tokens": 1000, "node_type": "child"}
{"id": 477, "chunk_id": "9b0fd3b850698317bef57fa6e71a38ba", "parent_id": 446, "url": "", "namespace": "langchain", "title": "oss-python-migrate-langchain-v1", "headers": ["oss-python-migrate-langchain-v1"], "section_index": 0, "chunk_index": 1, "text": "provider strategies In v1, there are two new structured output strategies:ToolStrategy uses artificial tool calling to generate structured outputProviderStrategy uses provider-native structured output generation Prompted output removed Prompted output is no longer supported via theresponse_format argument. Compared to strategies like artificial tool calling and provider native structured output, prompted output has not proven to be particularly reliable. Streaming node name rename When streaming events from agents, the node name has changed from\"agent\" to \"model\" to better reflect the node\u2019s purpose. Runtime context When you invoke an agent, it\u2019s often the case that you want to pass two types of data:- Dynamic state that changes throughout the conversation (e.g., message history) - Static context that doesn\u2019t change during the conversation (e.g., user metadata) context parameter to invoke and stream . The old config[\"configurable\"] pattern still works for backward compatibility, but using the new context parameter is recommended for new applications or applications migrating to v1.Standard content In v1, messages gain provider-agnostic standard content blocks. Access them via @[message.content_blocks ][content_blocks] for a consistent, typed view across providers. The existing message.content field remains unchanged for strings or provider-native structures. What changed - New content_blocks property on messages for normalized content - Standardized block shapes, documented in Messages - Optional serialization of standard blocks into content viaLC_OUTPUT_VERSION=v1 oroutput_version=\"v1\" Read standardized content Create multimodal messages Example block shapes Serialize standard content Standard content blocks are not serialized into thecontent attribute by default. If you need to access standard content blocks in the content attribute (e.g., when sending messages to a client), you can opt-in to serializing them into content . Simplified package Thelangchain package namespace has been significantly reduced in v1 to focus on essential building blocks for agents. The streamlined package makes it easier to discover and use the core functionality. Namespace | Module | What\u2019s available | Notes | |---|---|---| langchain.agents | create_agent , AgentState | Core agent creation functionality | langchain.messages | Message types, content blocks, trim_messages | Re-exported from langchain-core | langchain.tools | @tool , BaseTool , injection helpers | Re-exported from langchain-core | langchain.chat_models | init_chat_model , BaseChatModel | Unified model initialization | langchain.embeddings | init_embeddings , Embeddings | Embedding models | langchain-classic If you were using any of the following from the langchain package, you\u2019ll need to install langchain-classic and update your imports: - Legacy chains ( LLMChain ,ConversationChain , etc.) - The indexing API langchain-community re-exports- Other deprecated functionality Breaking changes Dropped Python 3.9 support All LangChain packages now require Python 3.10 or higher. Python 3.9 reaches end of life in October 2025.Updated return type for chat models The return type signature for chat model invocation has been fixed fromBaseMessage to AIMessage . Custom chat models implementing bind_tools should update their return signature: Default message format for OpenAI Responses API When interacting with the Responses API,langchain-openai now defaults to storing response items in message content . To restore previous behavior, set the LC_OUTPUT_VERSION environment variable to v0 , or specify output_version=\"v0\" when instantiating ChatOpenAI . Default max_tokens in langchain-anthropic The max_tokens parameter in langchain-anthropic now defaults to higher values based on the model chosen, rather than the previous default of 1024 . If you relied on the old default, explicitly set max_tokens=1024 . Legacy code moved to langchain-classic Existing functionality outside the focus of standard interfaces and agents has been moved to the langchain-classic package. See the Simplified namespace section for details on what\u2019s available in the core langchain package and what moved to langchain-classic . Removal of deprecated APIs Methods, functions, and other objects that were already deprecated and slated for removal in 1.0 have been deleted. Check the deprecation notices from previous versions for replacement APIs..text() is now a property Use of the .text() method on message objects should drop the parentheses: .text() ) will continue to function but now emit a warning.", "tokens": 637, "node_type": "child"}
{"id": 478, "chunk_id": "c16acf8a553ac24f5d77e7d3a1c6e493", "parent_id": 448, "url": "", "namespace": "langchain", "title": "oss-python-release-policy", "headers": ["oss-python-release-policy"], "section_index": 0, "chunk_index": 0, "text": "# oss-python-release-policy\n\n> Source: https://docs.langchain.com/oss/python/release-policy\n\nLangChain v1.0Welcome to the new LangChain documentation! If you encounter any issues or have feedback, please open an issue so we can improve. Archived v0 documentation can be found here.See the release notes and migration guide for a complete list of changes and instructions on how to upgrade your code.\n- LangChain\n- LangGraph\nThe LangChain ecosystem is composed of different component packages (e.g.,\nlangchain-core\n, langchain\n, langchain-community\n, partner packages, etc.)Release cadence\nWe expect to space out minor releases (e.g., from0.2.x\nto 0.3.0\n) of langchain\nand langchain-core\nby at least 2-3 months, as such releases may contain breaking changes.Patch versions are released frequently, up to a few times per week, as they contain bug fixes and new features.API stability\nThe development of LLM applications is a rapidly evolving field, and we are constantly learning from our users and the community. As such, we expect that the APIs inlangchain\nand langchain-core\nwill continue to evolve to better serve the needs of our users.- Breaking changes to the public API will result in a minor version bump (the second digit)\n- Any bug fixes or new features will result in a patch version bump (the third digit)\nStability of other packages\nThe stability of other packages in the LangChain ecosystem may vary:-\nlangchain-community\nis a community maintained package that contains 3rd party integrations. While we do our best to review and test changes inlangchain-community\n,langchain-community\nis expected to experience more breaking changes thanlangchain\nandlangchain-core\nas it contains many community contributions. - Partner packages may follow different stability and versioning policies, and users should refer to the documentation of those packages for more information; however, in general these packages are expected to be stable.\nDeprecation policy\nWe will generally avoid deprecating features until a better alternative is available.When a feature is deprecated, it will continue to work in the current and next minor version oflangchain\nand langchain-core\n. After that, the feature will be removed.Since we\u2019re expecting to space out minor releases by at least 2-3 months, this means that a feature can be removed within 2-6 months of being deprecated.In some situations, we may allow the feature to remain in the code base for longer periods of time, if it\u2019s not causing issues in the packages, to reduce the burden on users.", "tokens": 388, "node_type": "child"}
{"id": 479, "chunk_id": "c16acf8a553ac24f5d77e7d3a1c6e493", "parent_id": 449, "url": "", "namespace": "langchain", "title": "oss-python-releases-langchain-v1", "headers": ["oss-python-releases-langchain-v1"], "section_index": 0, "chunk_index": 0, "text": "# oss-python-releases-langchain-v1\n\n> Source: https://docs.langchain.com/oss/python/releases/langchain-v1\n\ncreate_agent\nThe new standard for building agents in LangChain, replacing\nlanggraph.prebuilt.create_react_agent\n.Standard content blocks\nA new\ncontent_blocks\nproperty that provides unified access to modern LLM features across providers.Simplified namespace\nThe\nlangchain\nnamespace has been streamlined to focus on essential building blocks for agents, with legacy functionality moved to langchain-classic\n.create_agent\ncreate_agent\nis the standard way to build agents in LangChain 1.0. It provides a simpler interface than langgraph.prebuilt.create_react_agent\nwhile offering greater customization potential by using middleware.\ncreate_agent\nis built on the basic agent loop \u2014 calling a model, letting it choose tools to execute, and then finishing when it calls no more tools:\nFor more information, see Agents.\nMiddleware\nMiddleware is the defining feature ofcreate_agent\n. It offers a highly customizable entry-point, raising the ceiling for what you can build.\nGreat agents require context engineering: getting the right information to the model at the right time. Middleware helps you control dynamic prompts, conversation summarization, selective tool access, state management, and guardrails through a composable abstraction.\nPrebuilt middleware\nLangChain provides a few prebuilt middlewares for common patterns, including:PIIMiddleware\n: Redact sensitive information before sending to the modelSummarizationMiddleware\n: Condense conversation history when it gets too longHumanInTheLoopMiddleware\n: Require approval for sensitive tool calls\nCustom middleware\nYou can also build custom middleware to fit your needs. Middleware exposes hooks at each step in an agent\u2019s execution:\nBuild custom middleware by implementing any of these hooks on a subclass of the\nAgentMiddleware\nclass:\n| Hook | When it runs | Use cases |\n|---|---|---|\nbefore_agent | Before calling the agent | Load memory, validate input |\nbefore_model | Before each LLM call | Update prompts, trim messages |\nwrap_model_call | Around each LLM call | Intercept and modify requests/responses |\nwrap_tool_call | Around each tool call | Intercept and modify tool execution |\nafter_model | After each LLM response | Validate output, apply guardrails |\nafter_agent | After agent completes | Save results, cleanup |\nBuilt on LangGraph\nBecausecreate_agent\nis built on LangGraph, you automatically get built in support for long running and reliable agents via:\nPersistence\nConversations automatically persist across sessions with built-in checkpointing\nStreaming\nStream tokens, tool calls, and reasoning traces in real-time\nHuman-in-the-loop\nPause agent execution for human approval before sensitive actions\nTime travel\nRewind conversations to any point and explore alternate paths and prompts\nStructured output\ncreate_agent\nhas improved structured output generation:\n- Main loop integration: Structured output is now generated in the main loop instead of requiring an additional LLM call\n- Structured output strategy: Models can choose between calling tools or using provider-side structured output generation\n- Cost reduction: Eliminates extra expense from additional LLM calls\nhandle_errors\nparameter to ToolStrategy\n:\n- Parsing errors: Model generates data that doesn\u2019t match desired structure\n- Multiple tool calls: Model generates 2+ tool calls for structured output schemas\nStandard content blocks\nContent block support is currently only available for the following integrations:Broader support for content blocks will be rolled out gradually across more providers.\ncontent_blocks\nproperty introduces a standard representation for message content that works across providers:\nBenefits\n- Provider agnostic: Access reasoning traces, citations, built-in tools (web search, code interpreters, etc.), and other features using the same API regardless of provider\n- Type safe: Full type hints for all content block types\n- Backward compatible: Standard content can be loaded lazily, so there are no associated breaking changes\nSimplified package\nLangChain v1 streamlines thelangchain\npackage namespace to focus on essential building blocks for agents. The refined namespace exposes the most useful and relevant functionality:\nNamespace\n| Module | What\u2019s available | Notes |\n|---|---|---|\nlangchain.agents | create_agent , AgentState | Core agent creation functionality |\nlangchain.messages | Message types, content blocks, trim_messages | Re-exported from @[langchain-core ] |\nlangchain.tools | @tool , BaseTool , injection helpers | Re-exported from @[langchain-core ] |\nlangchain.chat_models | init_chat_model , BaseChatModel | Unified model initialization |\nlangchain.embeddings | Embeddings , init_embeddings | Embedding models |\nlangchain-core\nfor convenience, which gives you a focused API surface for building agents.\nlangchain-classic\nLegacy functionality has moved to langchain-classic\nto keep the core packages lean and focused.\nWhat\u2019s in langchain-classic\n- Legacy chains and chain implementations\n- The indexing API\nlangchain-community\nexports- Other deprecated functionality\nlangchain-classic\n:\nMigration guide\nSee our migration guide for help updating your code to LangChain v1.Reporting issues\nPlease report any issues discovered with 1.0 on GitHub using the'v1'\nlabel.\nAdditional resources\nLangChain 1.0\nRead the announcement\nMiddleware Guide\nDeep dive into middleware\nAgents Documentation\nFull agent documentation\nMessage Content\nNew content blocks API\nMigration guide\nHow to migrate to LangChain v1\nGitHub\nReport issues or contribute\nSee also\n- Versioning - Understanding version numbers\n- Release policy - Detailed release policies", "tokens": 784, "node_type": "child"}
{"id": 480, "chunk_id": "c16acf8a553ac24f5d77e7d3a1c6e493", "parent_id": 450, "url": "", "namespace": "langchain", "title": "oss-python-releases-langgraph-v1", "headers": ["oss-python-releases-langgraph-v1"], "section_index": 0, "chunk_index": 0, "text": "# oss-python-releases-langgraph-v1\n\n> Source: https://docs.langchain.com/oss/python/releases/langgraph-v1\n\ncreate_agent\nis built on LangGraph) so you can start high-level and drop down to granular control when needed.\nStable core APIs\nGraph primitives (state, nodes, edges) and the execution/runtime model are unchanged, making upgrades straightforward.\nReliability, by default\nDurable execution with checkpointing, persistence, streaming, and human-in-the-loop continues to be first-class.\nSeamless with LangChain v1\nLangChain\u2019s\ncreate_agent\nruns on LangGraph. Use LangChain for a fast start; drop to LangGraph for custom orchestration.Deprecation of create_react_agent\nThe LangGraph create_react_agent\nprebuilt has been deprecated in favor of LangChain\u2019s create_agent\n. It provides a simpler interface, and offers greater customization potential through the introduction of middleware.\n- For information on the new\ncreate_agent\nAPI, see the LangChain v1 release notes. - For information on migrating from\ncreate_react_agent\ntocreate_agent\n, see the LangChain v1 migration guide.\nReporting issues\nPlease report any issues discovered with 1.0 on GitHub using the'v1'\nlabel.\nAdditional resources\nLangGraph 1.0\nRead the announcement\nOverview\nWhat LangGraph is and when to use it\nGraph API\nBuild graphs with state, nodes, and edges\nLangChain Agents\nHigh-level agents built on LangGraph\nMigration guide\nHow to migrate to LangGraph v1\nGitHub\nReport issues or contribute\nSee also\n- Versioning - Understanding version numbers\n- Release policy - Detailed release policies", "tokens": 209, "node_type": "child"}
{"id": 481, "chunk_id": "c16acf8a553ac24f5d77e7d3a1c6e493", "parent_id": 451, "url": "", "namespace": "langchain", "title": "oss-python-versioning", "headers": ["oss-python-versioning"], "section_index": 0, "chunk_index": 0, "text": "# oss-python-versioning\n\n> Source: https://docs.langchain.com/oss/python/versioning\n\nLangChain v1.0Welcome to the new LangChain documentation! If you encounter any issues or have feedback, please open an issue so we can improve. Archived v0 documentation can be found here.See the release notes and migration guide for a complete list of changes and instructions on how to upgrade your code.\nMAJOR.MINOR.PATCH\n- Major: Breaking API updates that require code changes.\n- Minor: New features and improvements that maintain backward compatibility.\n- Patch: Bug fixes and minor improvements.\nVersion numbering\nLangChain and LangGraph follow Semantic Versioning principles:1.0.0\n: First stable release with production-ready APIs1.1.0\n: New features added in a backward-compatible manner1.0.1\n: Backward-compatible bug fixes\nAPI stability\nWe communicate the stability of our APIs as follows:Stable APIs\nAll APIs without special prefixes are considered stable and ready for production use. We maintain backward compatibility for stable features and only introduce breaking changes in major releases.Beta APIs\nAPIs marked asbeta\nare feature-complete but may undergo minor changes based on user feedback. They are safe for production use but may require small adjustments in future releases.\nAlpha APIs\nAPIs marked asalpha\nare experimental and subject to significant changes. Use these with caution in production environments.\nDeprecated APIs\nAPIs marked asdeprecated\nwill be removed in future major releases. When possible, we specify the intended version of removal. To handle deprecations:\n- Switch to the recommended alternative API\n- Follow the migration guide (released alongside major releases)\n- Use automated migration tools when available\nInternal APIs\nCertain APIs are explicitly marked as \u201cinternal\u201d in a couple of ways:- Some documentation refers to internals and mentions them as such. If the documentation says that something is internal, it may change.\n- Functions, methods, and other objects prefixed by a leading underscore (\n_\n). This is the standard Python convention of indicating that something is private; if any method starts with a single_\n, it\u2019s an internal API.- Exception: Certain methods are prefixed with\n_\n, but do not contain an implementation. These methods are meant to be overridden by sub-classes that provide the implementation. Such methods are generally part of the Public API of LangChain.\n- Exception: Certain methods are prefixed with\nRelease cycles\nMajor releases\nMajor releases\nMajor releases (e.g.,\n1.0.0\n\u2192 2.0.0\n) may include:- Breaking API changes\n- Removal of deprecated features\n- Significant architectural improvements\n- Detailed migration guides\n- Automated migration tools when possible\n- Extended support period for the previous major version\nMinor releases\nMinor releases\nMinor releases (e.g.,\n1.0.0\n\u2192 1.1.0\n) include:- New features and capabilities\n- Performance improvements\n- New optional parameters\n- Backward-compatible enhancements\nPatch releases\nPatch releases\nPatch releases (e.g.,\n1.0.0\n\u2192 1.0.1\n) include:- Bug fixes\n- Security updates\n- Documentation improvements\n- Performance optimizations without API changes\nVersion support policy\n- Latest major version: Full support with active development\n- Previous major version: Security updates and critical bug fixes for 12 months after the next major release\n- Older versions: Community support only\nCheck your version\nTo check your installed version:Upgrade\nPre-release versions\nWe occasionally release alpha and beta versions for early testing:- Alpha (e.g.,\n1.0.0a1\n): Early preview, significant changes expected - Beta (e.g.,\n1.0.0b1\n): Feature-complete, minor changes possible - Release Candidate (e.g.,\n1.0.0rc1\n): Final testing before stable release\nSee also\n- Release policy - Detailed release and deprecation policies\n- Releases - Version-specific release notes and migration guides", "tokens": 566, "node_type": "child"}
