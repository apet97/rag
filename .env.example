# Clockify RAG Environment Configuration
# Copy this file to .env and fill in your values

# ===================================
# API Server Configuration
# ===================================
API_PORT=8000
API_HOST=0.0.0.0
DEBUG=false

# CORS Configuration (comma-separated list of allowed origins)
# Default: http://localhost:8080,http://127.0.0.1:8080
# IMPORTANT: Do not use wildcards (*) - always specify exact origins
# Example for production: https://example.com,https://app.example.com
CORS_ALLOWED_ORIGINS=http://localhost:8080,http://127.0.0.1:8080

# ===================================
# LLM Configuration (NO API KEY NEEDED!)
# ===================================
# Your internal Ollama server
LLM_BASE_URL=http://10.127.0.192:11434

# Available model on your server (note the colon!)
LLM_MODEL=gpt-oss:20b

# Embedding model (HuggingFace format - no colons!)
EMBEDDING_MODEL=nomic-ai/nomic-embed-text-v1.5

# LLM timeout and retry settings
LLM_TIMEOUT_SECONDS=30
LLM_RETRIES=3
LLM_BACKOFF=0.75

# Streaming support (requires Ollama with streaming enabled)
STREAMING_ENABLED=false

# ===================================
# Harmony Chat Format (for gpt-oss:20b optimal performance)
# ===================================
# IMPORTANT: gpt-oss:20b was post-trained to expect Harmony chat format.
# Without Harmony, responses degrade significantly. Always keep enabled for oss models.
#
# Use Harmony encoding: true=auto-detect for gpt-oss*, false=disable, auto=default behavior
# - true: Force enable Harmony (recommended for gpt-oss:20b models)
# - false: Disable Harmony (for standard OpenAI models)
# - auto: Auto-detect based on model name (default, detects gpt-oss* variants)
LLM_USE_HARMONY=auto

# API type for Harmony: "ollama" (Ollama with vLLM) or "openai" (OpenAI-compatible)
LLM_API_TYPE=ollama

# LLM temperature (default 0.0 for deterministic RAG responses)
# Recommended: 0.0-0.2 for RAG (minimize hallucination)
LLM_TEMPERATURE=0.0

# Circuit Breaker Configuration (fault tolerance for LLM calls)
LLM_CIRCUIT_BREAKER_THRESHOLD=5      # Consecutive failures before opening circuit
LLM_CIRCUIT_BREAKER_TIMEOUT=60       # Recovery timeout in seconds
LLM_CIRCUIT_BREAKER_SUCCESS=2        # Consecutive successes needed to close circuit

# Alternative local setup:
# LLM_BASE_URL=http://localhost:11434

# ===================================
# Search Configuration
# ===================================
# Default number of results to return
RETRIEVAL_K=5

# Lexical vs Dense weighting for hybrid retrieval
# SEARCH_LEXICAL_WEIGHT = weight for BM25 (0..1). Vector weight = 1 - this value
# Default 0.35 -> 35% lexical (BM25), 65% dense
SEARCH_LEXICAL_WEIGHT=0.50

# Maximum cache size (number of queries)
CACHE_SIZE=1000

# Minimum query length (characters)
MIN_QUERY_LENGTH=2

# ===================================
# Data Paths
# ===================================
# Path to FAISS index
FAISS_INDEX_PATH=index/faiss/clockify-improved/index.bin

# Path to FAISS metadata
FAISS_METADATA_PATH=index/faiss/clockify-improved/meta.json

# Path to glossary (for query expansion)
GLOSSARY_PATH=clockify-help/pages/help__getting-started__clockify-glossary.md

# ===================================
# Performance Tuning
# ===================================
# Number of worker processes
MAX_WORKERS=4

# Batch size for embedding
BATCH_SIZE=32

# ===================================
# Logging
# ===================================
LOG_LEVEL=INFO
LOG_FILE=logs/api.log

# ===================================
# v2 Corpus Ingestion & Policy
# ===================================
# Strict domain policy for ingestion/runtime
ALLOWLIST_PATH=codex/ALLOWLIST.txt
DENYLIST_PATH=codex/DENYLIST.txt

# Enriched corpus from Phase 2
ENRICHED_LINKS_JSON=codex/CRAWLED_LINKS_enriched.json

# Namespace controls (index under index/faiss/<namespace>)
NAMESPACE=clockify
NAMESPACES=clockify

# Embedding configuration (single source of truth)
# EMBEDDING_MODEL already set above; ensure EMBEDDING_DIM matches your model
EMBEDDING_DIM=768

# ===================================
# Chunking Ablation
# ===================================
# Strategies: url_level (one chunk per URL), h2_h3_blocks (H2/H3 sections + token packing)
CHUNK_STRATEGY=url_level
CHUNK_SIZE=1200
CHUNK_OVERLAP=200

# ===================================
# Development (Uncomment for development)
# ===================================
# DEBUG=true
# API_HOST=127.0.0.1
# LOG_LEVEL=DEBUG
# CACHE_SIZE=100
